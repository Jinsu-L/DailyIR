# Daily Papers Report - 2026-03-01

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Beyond the Click: A Framework for Inferring Cognitive Traces in Search

- **LLM Score**: 9
- **Keyword Score**: 9
- **Authors**: Saber Zerhoudi, Michael Granitzer
- **URL**: <http://arxiv.org/abs/2602.24265v1>
- **Submitted**: 2026-02-27 18:32:59
- **Topic Keywords**: rag, user action, click, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of user behavior modeling and click models. The framework presented for inferring cognitive traces from behavior logs aligns with your focus on deep semantic understanding and real-time relevance optimization. The application of Information Foraging Theory (IFT) and human expert judgment also resonates with your background in e-commerce and NLP.

#### Abstract
> User simulators are essential for evaluating search systems, but they primarily copy user actions without understanding the underlying thought process. This gap exists since large-scale interaction logs record what users do, but not what they might be thinking or feeling, such as confusion or satisfaction. To solve this problem, we present a framework to infer cognitive traces from behavior logs. Our method uses a multi-agent system grounded in Information Foraging Theory (IFT) and human expert judgment. These traces improve model performance on tasks like forecasting session outcomes and user struggle recovery. We release a collection of annotations for several public datasets, including AOL and Stack Overflow, and an open-source tool that allows researchers to apply our method to their own data. This work provides the tools and data needed to build more human-like user simulators and to assess retrieval systems on user-oriented dimensions of performance.

---

### 2. Synthetic Data Powers Product Retrieval for Long-tail Knowledge-Intensive Queries in E-commerce Search

- **LLM Score**: 8
- **Keyword Score**: 17
- **Authors**: Gui Ling, Weiyuan Li, Yue Jiang, Wenjun Peng, Xingxian Liu, Dongshuai Li, Fuyu Lv, Dan Ou, Haihong Tang
- **URL**: <http://arxiv.org/abs/2602.23620v1>
- **Submitted**: 2026-02-27 02:53:17
- **Topic Keywords**: query, queries, ranking, rag, retrieval, commerce, e-commerce, rank, search
- **Reason**: The paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on long-tail knowledge-intensive queries and the use of synthetic data to improve retrieval performance aligns with your interests in deep semantic understanding and real-time relevance optimization. The e-commerce domain is also a relevant area of application.

#### Abstract
> Product retrieval is the backbone of e-commerce search: for each user query, it identifies a high-recall candidate set from billions of items, laying the foundation for high-quality ranking and user experience. Despite extensive optimization for mainstream queries, existing systems still struggle with long-tail queries, especially knowledge-intensive ones. These queries exhibit diverse linguistic patterns, often lack explicit purchase intent, and require domain-specific knowledge reasoning for accurate interpretation. They also suffer from a shortage of reliable behavioral logs, which makes such queries a persistent challenge for retrieval optimization. To address these issues, we propose an efficient data synthesis framework tailored to retrieval involving long-tail, knowledge-intensive queries. The key idea is to implicitly distill the capabilities of a powerful offline query-rewriting model into an efficient online retrieval system. Leveraging the strong language understanding of LLMs, we train a multi-candidate query rewriting model with multiple reward signals and capture its rewriting capability in well-curated query-product pairs through a powerful offline retrieval pipeline. This design mitigates distributional shift in rewritten queries, which might otherwise limit incremental recall or introduce irrelevant products. Experiments demonstrate that without any additional tricks, simply incorporating this synthetic data into retrieval model training leads to significant improvements. Online Side-By-Side (SBS) human evaluation results indicate a notable enhancement in user search experience.

---

### 3. Geodesic Semantic Search: Learning Local Riemannian Metrics for Citation Graph Retrieval

- **LLM Score**: 8
- **Keyword Score**: 16
- **Authors**: Brandon Yee, Lucas Wang, Kundana Kommini, Krishna Sharma
- **URL**: <http://arxiv.org/abs/2602.23665v1>
- **Submitted**: 2026-02-27 04:17:41
- **Topic Keywords**: semantic search, ranking, rerank, relevance, retrieval, rank, search
- **Reason**: This paper presents a novel approach to semantic search in citation graphs, leveraging Riemannian metrics and geodesic distances. While primarily focused on citation retrieval, the use of geometry-aware search and ranking models aligns with your interests in Information Retrieval and Search technologies. The paper's emphasis on semantic understanding and real-time relevance optimization also resonates with your research focus.

#### Abstract
> We present Geodesic Semantic Search (GSS), a retrieval system that learns node-specific Riemannian metrics on citation graphs to enable geometry-aware semantic search. Unlike standard embedding-based retrieval that relies on fixed Euclidean distances, \gss{} learns a low-rank metric tensor $\mL_i \in \R^{d \times r}$ at each node, inducing a local positive semi-definite metric $\mG_i = \mL_i \mL_i^\top + \eps \mI$. This parameterization guarantees valid metrics while keeping the model tractable. Retrieval proceeds via multi-source Dijkstra on the learned geodesic distances, followed by Maximal Marginal Relevance reranking and path coherence filtering. On citation prediction benchmarks with 169K papers, \gss{} achieves 23\% relative improvement in Recall@20 over SPECTER+FAISS baselines while providing interpretable citation paths. Our hierarchical coarse-to-fine search with k-means pooling reduces computational cost by 4$\times$ compared to flat geodesic search while maintaining 97\% retrieval quality. We provide theoretical analysis of when geodesic distances outperform direct similarity, characterize the approximation quality of low-rank metrics, and validate predictions empirically. Code and trained models are available at https://github.com/YCRG-Labs/geodesic-search.

---

### 4. Unified Learning-to-Rank for Multi-Channel Retrieval in Large-Scale E-Commerce Search

- **LLM Score**: 8
- **Keyword Score**: 16
- **Authors**: Aditya Gaydhani, Guangyue Xu, Dhanush Kamath, Ankit Singh, Alex Li
- **URL**: <http://arxiv.org/abs/2602.23530v1>
- **Submitted**: 2026-02-26 22:26:59
- **Topic Keywords**: query, ranking, user behavior, click, retrieval, commerce, e-commerce, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on multi-channel retrieval in e-commerce search aligns with your background in the e-commerce domain, and the use of learning-to-rank and user behavioral signals is consistent with your interests in IR and NLP.

#### Abstract
> Large-scale e-commerce search must surface a broad set of items from a vast catalog, ranging from bestselling products to new, trending, or seasonal items. Modern systems therefore rely on multiple specialized retrieval channels to surface products, each designed to satisfy a specific objective. A key challenge is how to effectively merge documents from these heterogeneous channels into a single ranked list under strict latency constraints while optimizing for business KPIs such as user conversion. Rank-based fusion methods such as Reciprocal Rank Fusion (RRF) and Weighted Interleaving rely on fixed global channel weights and treat channels independently, failing to account for query-specific channel utility and cross-channel interactions. We observe that multi-channel fusion can be reformulated as a query-dependent learning-to-rank problem over heterogeneous candidate sources. In this paper, we propose a unified ranking model that learns to merge and rank documents from multiple retrieval channels. We formulate the problem as a channel-aware learning-to-rank task that jointly optimizes clicks, add-to-carts, and purchases while incorporating channel-specific objectives. We further incorporate recent user behavioral signals to capture short-term intent shifts that are critical for improving conversion in multi-channel ranking. Our online A/B experiments show that the proposed approach outperforms rank-based fusion methods, leading to a +2.85\% improvement in user conversion. The model satisfies production latency requirements, achieving a p95 latency of under 50\,ms, and is deployed on Target.com.

---

### 5. AgenticOCR: Parsing Only What You Need for Efficient Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Zhengren Wang, Dongsheng Ma, Huaping Zhong, Jiayu Li, Wentao Zhang, Bin Wang, Conghui He
- **URL**: <http://arxiv.org/abs/2602.24134v1>
- **Submitted**: 2026-02-27 16:09:38
- **Topic Keywords**: query, ranking, rerank, rag, retrieval, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of retrieval-augmented generation and multimodal domains. The focus on efficient retrieval and real-time relevance optimization aligns with your interests, and the use of OCR and visual token budget management is an interesting application of query understanding and ranking models.

#### Abstract
> The expansion of retrieval-augmented generation (RAG) into multimodal domains has intensified the challenge for processing complex visual documents, such as financial reports. While page-level chunking and retrieval is a natural starting point, it creates a critical bottleneck: delivering entire pages to the generator introduces excessive extraneous context. This not only overloads the generator's attention mechanism but also dilutes the most salient evidence. Moreover, compressing these information-rich pages into a limited visual token budget further increases the risk of hallucinations. To address this, we introduce AgenticOCR, a dynamic parsing paradigm that transforms optical character recognition (OCR) from a static, full-text process into a query-driven, on-demand extraction system. By autonomously analyzing document layout in a "thinking with images" manner, AgenticOCR identifies and selectively recognizes regions of interest. This approach performs on-demand decompression of visual tokens precisely where needed, effectively decoupling retrieval granularity from rigid page-level chunking. AgenticOCR has the potential to serve as the "third building block" of the visual document RAG stack, operating alongside and enhancing standard Embedding and Reranking modules. Experimental results demonstrate that AgenticOCR improves both the efficiency and accuracy of visual RAG systems, achieving expert-level performance in long document understanding. Code and models are available at https://github.com/OpenDataLab/AgenticOCR.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. HotelQuEST: Balancing Quality and Efficiency in Agentic Search

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Guy Hadad, Shadi Iskander, Oren Kalinsky, Sofia Tolmach, Ran Levy, Haggai Roitman
- **URL**: <http://arxiv.org/abs/2602.23949v1>
- **Submitted**: 2026-02-27 11:50:57
- **Comment**: To be published in EACL 2026
- **Topic Keywords**: retriever, query, queries, ctr, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of agentic search and large language models. The focus on balancing quality and efficiency, as well as the evaluation of user queries with underspecified preferences, aligns with your interests in query understanding and ranking models. Although the paper's domain is e-commerce, the concepts and techniques discussed are applicable to broader IR and NLP topics.

#### Abstract
> Agentic search has emerged as a promising paradigm for adaptive retrieval systems powered by large language models (LLMs). However, existing benchmarks primarily focus on quality, overlooking efficiency factors that are critical for real-world deployment. Moreover, real-world user queries often contain underspecified preferences, a challenge that remains largely underexplored in current agentic search evaluation. As a result, many agentic search systems remain impractical despite their impressive performance. In this work, we introduce HotelQuEST, a benchmark comprising 214 hotel search queries that range from simple factual requests to complex queries, enabling evaluation across the full spectrum of query difficulty. We further address the challenge of evaluating underspecified user preferences by collecting clarifications that make annotators' implicit preferences explicit for evaluation. We find that LLM-based agents achieve higher accuracy than traditional retrievers, but at substantially higher costs due to redundant tool calls and suboptimal routing that fails to match query complexity to model capability. Our analysis exposes inefficiencies in current agentic search systems and demonstrates substantial potential for cost-aware optimization.

### 7. UniFAR: A Unified Facet-Aware Retrieval Framework for Scientific Documents

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Zheng Dou, Zhao Zhang, Deqing Wang, Yikun Ban, Fuzhen Zhuang
- **URL**: <http://arxiv.org/abs/2602.23766v1>
- **Submitted**: 2026-02-27 07:44:02
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: This paper presents a unified framework for scientific document retrieval that addresses query understanding and ranking models, aligning with your interests in Information Retrieval and Search technologies. The focus on reconciling granularity differences and aligning document structure with question intent is particularly relevant to your research in query understanding and ranking models. However, the paper's specific focus on scientific documents and question-driven retrieval limits its generalizability to other domains.

#### Abstract
> Existing scientific document retrieval (SDR) methods primarily rely on document-centric representations learned from inter-document relationships for document-document (doc-doc) retrieval. However, the rise of LLMs and RAG has shifted SDR toward question-driven retrieval, where documents are retrieved in response to natural-language questions (q-doc). This change has led to systematic mismatches between document-centric models and question-driven retrieval, including (1) input granularity (long documents vs. short questions), (2) semantic focus (scientific discourse structure vs. specific question intent), and (3) training signals (citation-based similarity vs. question-oriented relevance). To this end, we propose UniFAR, a Unified Facet-Aware Retrieval framework to jointly support doc-doc and q-doc SDR within a single architecture. UniFAR reconciles granularity differences through adaptive multi-granularity aggregation, aligns document structure with question intent via learnable facet anchors, and unifies doc-doc and q-doc supervision through joint training. Experimental results show that UniFAR consistently outperforms prior methods across multiple retrieval tasks and base models, confirming its effectiveness and generality.

### 8. Truncated Step-Level Sampling with Process Rewards for Retrieval-Augmented Reasoning

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Chris Samarinas, Haw-Shiuan Chang, Hamed Zamani
- **URL**: <http://arxiv.org/abs/2602.23440v1>
- **Submitted**: 2026-02-26 19:05:40
- **Topic Keywords**: query, retrieval, search
- **Reason**: This paper is highly relevant to Information Retrieval, specifically in the area of query understanding and ranking models, as it proposes a new framework (SLATE) for training large language models to reason with search engines. The paper's focus on reinforcement learning and process-reward methods aligns with your interests in Learning to Rank and user behavior modeling. However, the specific domain of question-answering (QA) benchmarks is somewhat outside your primary focus on e-commerce, but the underlying IR concepts are still applicable.

#### Abstract
> Training large language models to reason with search engines via reinforcement learning is hindered by a fundamental credit assignment problem: existing methods such as Search-R1 provide only a sparse outcome reward after an entire multi-step trajectory, making it infeasible to attribute success or failure to individual reasoning and retrieval decisions. Process-reward methods like StepSearch alleviate this by introducing step-level supervision, but rely on heuristic rewards such as TF-IDF overlap with gold documents, and still sample k complete trajectories per example, retaining high gradient variance. We propose SLATE, a framework built on two complementary ideas: (1) truncated step-level sampling, which generates k trajectories that share a common prefix and differ only at the next step, and (2) dense LLM-as-judge rewards, which replace heuristic scoring with a capable LLM evaluator that assesses the quality of each reasoning step, search query, and answer, providing richer and more reliable supervision. We theoretically prove that under the same dense reward structure, truncated sampling reduces the variance of advantage estimates by up to a factor of T compared to full-trajectory sampling for T-step trajectories, yielding lower-variance, better-targeted policy gradients. Experiments on seven QA benchmarks confirm that SLATE consistently outperforms both sparse-reward and process-reward baselines, with the largest gains on harder multi-hop tasks and smaller models.

### 9. Towards Efficient and Generalizable Retrieval: Adaptive Semantic Quantization and Residual Knowledge Transfer

- **LLM Score**: 8
- **Keyword Score**: 3
- **Authors**: Huimu Wang, Xingzhi Yao, Yiming Qiu, Qinghong Zhang, Haotian Wang, Yufan Cui, Songlin Wang, Sulong Xu, Mingming Li
- **URL**: <http://arxiv.org/abs/2602.23978v1>
- **Submitted**: 2026-02-27 12:39:38
- **Topic Keywords**: retrieval, search
- **Reason**: This paper is highly relevant to Information Retrieval, specifically addressing challenges in query understanding and ranking models. The proposed framework, SA^2CRQ, focuses on efficient and generalizable retrieval, which aligns with your interests in deep semantic understanding and real-time relevance optimization. Although the paper's primary focus is on industrial search systems, its contributions to addressing ID collisions and data sparsity are applicable to various domains.

#### Abstract
> While semantic ID-based generative retrieval enables efficient end-to-end modeling in industrial applications, these methods face a persistent trade-off: head items are susceptible to ID collisions that negatively impact downstream tasks, whereas data-sparse tail items, including cold-start items, exhibit limited generalization. To address this issue, we propose the Anchored Curriculum with Sequential Adaptive Quantization (SA^2CRQ) framework. The framework introduces Sequential Adaptive Residual Quantization (SARQ) to dynamically allocate code lengths based on item path entropy, assigning longer, discriminative IDs to head items and shorter, generalizable IDs to tail items. To mitigate data sparsity, the Anchored Curriculum Residual Quantization (ACRQ) component utilizes a frozen semantic manifold learned from head items to regularize and accelerate the representation learning of tail items. Experimental results from a large-scale industrial search system and multiple public datasets indicate that SA^2CRQ yields consistent improvements over existing baselines, particularly in cold-start retrieval scenarios.

### 10. RAD-DPO: Robust Adaptive Denoising Direct Preference Optimization for Generative Retrieval in E-commerce

- **LLM Score**: 7
- **Keyword Score**: 14
- **Authors**: Zhiguo Chen, Guohao Sun, Yiming Qiu, Xingzhi Yao, Mingming Li, Huimu Wang, Yangqi Zhang, Songlin Wang, Sulong Xu
- **URL**: <http://arxiv.org/abs/2602.23964v1>
- **Submitted**: 2026-02-27 12:17:06
- **Topic Keywords**: queries, ranking, rag, retrieval, commerce, e-commerce, rank, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of e-commerce search. It explores Generative Retrieval and Direct Preference Optimization, which are relevant to your focus on query understanding and ranking models. However, the paper's emphasis on Generative Retrieval and its specific application to e-commerce search is not a central match to your broader interests in Information Retrieval and Natural Language Processing.

#### Abstract
> Generative Retrieval (GR) has emerged as a powerful paradigm in e-commerce search, retrieving items via autoregressive decoding of Semantic IDs (SIDs). However, aligning GR with complex user preferences remains challenging. While Direct Preference Optimization (DPO) offers an efficient alignment solution, its direct application to structured SIDs suffers from three limitations: (i) it penalizes shared hierarchical prefixes, causing gradient conflicts; (ii) it is vulnerable to noisy pseudo-negatives from implicit feedback; and (iii) in multi-label queries with multiple relevant items, it exacerbates a probability "squeezing effect" among valid candidates. To address these issues, we propose RAD-DPO, which introduces token-level gradient detachment to protect prefix structures, similarity-based dynamic reward weighting to mitigate label noise, and a multi-label global contrastive objective integrated with global SFT loss to explicitly expand positive coverage. Extensive offline experiments and online A/B testing on a large-scale e-commerce platform demonstrate significant improvements in ranking quality and training efficiency.

### 11. UXSim: Towards a Hybrid User Search Simulation

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Saber Zerhoudi, Michael Granitzer
- **URL**: <http://arxiv.org/abs/2602.24241v1>
- **Submitted**: 2026-02-27 18:14:34
- **Topic Keywords**: rag, user behavior, personalization, search
- **Reason**: The paper UXSim: Towards a Hybrid User Search Simulation is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in user behavior modeling. However, the focus on simulation and validation of cognitive processes is not a central match to your primary interests in query understanding, ranking models, and real-time relevance optimization.

#### Abstract
> Simulating nuanced user experiences within complex interactive search systems poses distinct challenge for traditional methodologies, which often rely on static user proxies or, more recently, on standalone large language model (LLM) agents that may lack deep, verifiable grounding. The true dynamism and personalization inherent in human-computer interaction demand a more integrated approach. This work introduces UXSim, a novel framework that integrates both approaches. It leverages grounded data from traditional simulators to inform and constrain the reasoning of an adaptive LLM agent. This synthesis enables more accurate and dynamic simulations of user behavior while also providing a pathway for the explainable validation of the underlying cognitive processes.

### 12. TRIZ-RAGNER: A Retrieval-Augmented Large Language Model for TRIZ-Aware Named Entity Recognition in Patent-Based Contradiction Mining

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Zitong Xu, Yuqing Wu, Yue Zhao
- **URL**: <http://arxiv.org/abs/2602.23656v1>
- **Submitted**: 2026-02-27 03:40:45
- **Topic Keywords**: dense retrieval, ranking, rerank, rag, retrieval, rank
- **Reason**: The paper is somewhat related to information retrieval and natural language processing, but its focus on patent-based contradiction mining and TRIZ-aware named entity recognition is not directly aligned with the user's core research themes. While it involves deep semantic understanding and knowledge grounding, the application domain is quite specific and not primarily related to search technologies or user behavior modeling.

#### Abstract
> TRIZ-based contradiction mining is a fundamental task in patent analysis and systematic innovation, as it enables the identification of improving and worsening technical parameters that drive inventive problem solving. However, existing approaches largely rely on rule-based systems or traditional machine learning models, which struggle with semantic ambiguity, domain dependency, and limited generalization when processing complex patent language. Recently, large language models (LLMs) have shown strong semantic understanding capabilities, yet their direct application to TRIZ parameter extraction remains challenging due to hallucination and insufficient grounding in structured TRIZ knowledge. To address these limitations, this paper proposes TRIZ-RAGNER, a retrieval-augmented large language model framework for TRIZ-aware named entity recognition in patent-based contradiction mining. TRIZ-RAGNER reformulates contradiction mining as a semantic-level NER task and integrates dense retrieval over a TRIZ knowledge base, cross-encoder reranking for context refinement, and structured LLM prompting to extract improving and worsening parameters from patent sentences. By injecting domain-specific TRIZ knowledge into the LLM reasoning process, the proposed framework effectively reduces semantic noise and improves extraction consistency. Experiments on the PaTRIZ dataset demonstrate that TRIZ-RAGNER consistently outperforms traditional sequence labeling models and LLM-based baselines. The proposed framework achieves a precision of 85.6%, a recall of 82.9%, and an F1-score of 84.2% in TRIZ contradiction pair identification. Compared with the strongest baseline using prompt-enhanced GPT, TRIZ-RAGNER yields an absolute F1-score improvement of 7.3 percentage points, confirming the effectiveness of retrieval-augmented TRIZ knowledge grounding for robust and accurate patent-based contradiction mining.

### 13. FHIRPath-QA: Executable Question Answering over FHIR Electronic Health Records

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Michael Frew, Nishit Bheda, Bryan Tripp
- **URL**: <http://arxiv.org/abs/2602.23479v1>
- **Submitted**: 2026-02-26 20:14:21
- **Comment**: Submitted to LREC 2026 CL4Health Workshop
- **Topic Keywords**: query, queries, ctr, retrieval, search
- **Reason**: The paper discusses a novel approach to clinical question answering using FHIRPath queries, which is related to information retrieval and query understanding. However, the focus on electronic health records and clinical applications is somewhat outside the user's primary area of interest in e-commerce and general search technologies. The paper's emphasis on natural language processing and text-to-query synthesis is relevant, but not a central match for the user's research themes.

#### Abstract
> Though patients are increasingly granted digital access to their electronic health records (EHRs), existing interfaces may not support precise, trustworthy answers to patient-specific questions. Large language models (LLM) show promise in clinical question answering (QA), but retrieval-based approaches are computationally inefficient, prone to hallucination, and difficult to deploy over real-life EHRs. In this work, we introduce FHIRPath-QA, the first open dataset and benchmark for patient-specific QA that includes open-standard FHIRPath queries over real-world clinical data. We propose a text-to-FHIRPath QA paradigm that shifts reasoning from free-text generation to FHIRPath query synthesis, significantly reducing LLM usage. Built on MIMIC-IV on FHIR Demo, the dataset pairs over 14k natural language questions in patient and clinician phrasing with validated FHIRPath queries and answers. Further, we demonstrate that state-of-the-art LLMs struggle to deal with ambiguity in patient language and perform poorly in FHIRPath query synthesis. However, they benefit strongly from supervised fine-tuning. Our results highlight that text-to-FHIRPath synthesis has the potential to serve as a practical foundation for safe, efficient, and interoperable consumer health applications, and our dataset and benchmark serve as a starting point for future research on the topic. The full dataset and generation code is available at: https://github.com/mooshifrew/fhirpath-qa.

### 14. RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Daniel Yang, Samuel Stante, Florian Redhardt, Lena Libon, Parnian Kassraie, Ido Hakimi, Barna P√°sztor, Andreas Krause
- **URL**: <http://arxiv.org/abs/2602.24040v1>
- **Submitted**: 2026-02-27 14:15:57
- **Topic Keywords**: ranking, pointwise, rank
- **Reason**: The paper RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, particularly in the context of large language models and uncertainty quantification. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on reward models and uncertainty-awareness in LLMs is somewhat tangential to the user's primary research themes.

#### Abstract
> Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.

### 15. CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zhengqing Yuan, Kaiwen Shi, Zheyuan Zhang, Lichao Sun, Nitesh V. Chawla, Yanfang Ye
- **URL**: <http://arxiv.org/abs/2602.23452v1>
- **Submitted**: 2026-02-26 19:17:39
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper presents a benchmark and detection framework for hallucinated citations in scientific writing, which is related to information retrieval and NLP. However, the focus on citation verification and its application in the LLM era does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.

### 16. Recommendation Algorithms: A Comparative Study in Movie Domain

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Rohit Chivukula, T. Jaya Lakshmi, Hemlata Sharma, C. H. S. N. P. Sairam Rallabandi
- **URL**: <http://arxiv.org/abs/2602.24125v1>
- **Submitted**: 2026-02-27 16:01:10
- **Topic Keywords**: recommend, commerce, e-commerce, search
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval and recommender systems, but it focuses on the movie domain and uses regression models, which is not the primary focus of the user's research. Although it involves feature extraction and recommendation algorithms, it lacks the deep semantic understanding and real-time relevance optimization aspects that are central to the user's interests.

#### Abstract
> Intelligent recommendation systems have clearly increased the revenue of well-known e-commerce firms. Users receive product recommendations from recommendation systems. Cinematic recommendations are made to users by a movie recommendation system. There have been numerous approaches to the problem of recommendation in the literature. It is viewed as a regression task in this research. A regression model was built using novel properties extracted from the dataset and used as features in the model. For experimentation, the Netflix challenge dataset has been used. Video streaming service Netflix is a popular choice for many. Customers' prior viewing habits are taken into account when Netflix makes movie recommendations to them. An exploratory data analysis on the Netflix dataset was conducted to gain insights into user rating behaviour and movie characteristics. Various kinds of features, including aggregating, Matrix Factorization (MF) based, and user and movie similarity based, have been extracted in the subsequent stages. In addition to a feature in the XGBoost regression algorithm, the K-Nearest Neighbors and MF algorithms from Python's Surprise library are used for recommendations. Based on Root Mean Square Error (RMSE), MF-based algorithms have provided the best recommendations.

### 17. CLFEC: A New Task for Unified Linguistic and Factual Error Correction in paragraph-level Chinese Professional Writing

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Jian Kai, Zidong Zhang, Jiwen Chen, Zhengxiang Wu, Songtao Sun, Fuyang Li, Yang Cao, Qiang Liu
- **URL**: <http://arxiv.org/abs/2602.23845v1>
- **Submitted**: 2026-02-27 09:36:05
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to the user's research interests in Information Retrieval and Natural Language Processing, particularly in the context of text correction and error handling. However, its focus on Chinese language and professional writing, as well as its emphasis on factual error correction, makes it less central to the user's core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Chinese text correction has traditionally focused on spelling and grammar, while factual error correction is usually treated separately. However, in paragraph-level Chinese professional writing, linguistic (word/grammar/punctuation) and factual errors frequently co-occur and interact, making unified correction both necessary and challenging. This paper introduces CLFEC (Chinese Linguistic & Factual Error Correction), a new task for joint linguistic and factual correction. We construct a mixed, multi-domain Chinese professional writing dataset spanning current affairs, finance, law, and medicine. We then conduct a systematic study of LLM-based correction paradigms, from prompting to retrieval-augmented generation (RAG) and agentic workflows. The analysis reveals practical challenges, including limited generalization of specialized correction models, the need for evidence grounding for factual repair, the difficulty of mixed-error paragraphs, and over-correction on clean inputs. Results further show that handling linguistic and factual Error within the same context outperform decoupled processes, and that agentic workflows can be effective with suitable backbone models. Overall, our dataset and empirical findings provide guidance for building reliable, fully automatic proofreading systems in industrial settings.

### 18. From Static Benchmarks to Dynamic Protocol: Agent-Centric Text Anomaly Detection for Evaluating LLM Reasoning

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Seungdong Yoa, Sanghyu Yoon, Suhee Yoon, Dongmin Kim, Ye Seul Sim, Junhyun Lee, Woohyung Lim
- **URL**: <http://arxiv.org/abs/2602.23729v1>
- **Submitted**: 2026-02-27 06:54:32
- **Comment**: Accepted to ICLR 2026
- **Topic Keywords**: pairwise, search
- **Reason**: This paper proposes a dynamic protocol for evaluating large language models, which involves agent-centric benchmarking. While it touches on the evaluation of language models, it doesn't directly relate to information retrieval, query understanding, or ranking models, which are core areas of interest. The paper's focus on text anomaly detection and cross-sentence logical inference is somewhat related to deep semantic understanding, but it's not a central match for your research interests.

#### Abstract
> The evaluation of large language models (LLMs) has predominantly relied on static datasets, which offer limited scalability and fail to capture the evolving reasoning capabilities of recent models. To overcome these limitations, we propose an agent-centric benchmarking paradigm that moves beyond static datasets by introducing a dynamic protocol in which autonomous agents iteratively generate, validate, and solve problems. Within this protocol, a teacher agent generates candidate problems, an orchestrator agent rigorously verifies their validity and guards against adversarial attacks, and a student agent attempts to solve the validated problems. An invalid problem is revised by the teacher agent until it passes validation. If the student correctly solves the problem, the orchestrator prompts the teacher to generate more challenging variants. Consequently, the benchmark scales in difficulty automatically as more capable agents are substituted into any role, enabling progressive evaluation of large language models without manually curated datasets. Adopting text anomaly detection as our primary evaluation format, which demands cross-sentence logical inference and resists pattern-matching shortcuts, we demonstrate that this protocol systematically exposes corner-case reasoning errors that conventional benchmarks fail to reveal. We further advocate evaluating systems along several complementary axes including cross-model pairwise performance and progress between the initial and orchestrator-finalized problems. By shifting the focus from fixed datasets to dynamic protocols, our approach offers a sustainable direction for evaluating ever-evolving language models and introduces a research agenda centered on the co-evolution of agent-centric benchmarks.

### 19. Recommending Search Filters To Improve Conversions At Airbnb

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Hao Li, Kedar Bellare, Siyu Yang, Sherry Chen, Liwei He, Stephanie Moyerman, Sanjeev Katariya
- **URL**: <http://arxiv.org/abs/2602.23717v1>
- **Submitted**: 2026-02-27 06:36:14
- **Topic Keywords**: rag, recommend, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of search filters and their impact on conversions. However, its focus on recommender systems and the e-commerce domain, specifically Airbnb, is not a central match to your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Airbnb, a two-sided online marketplace connecting guests and hosts, offers a diverse and unique inventory of accommodations, experiences, and services. Search filters play an important role in helping guests navigate this variety by refining search results to align with their needs. Yet, while search filters are designed to facilitate conversions in online marketplaces, their direct impact on driving conversions remains underexplored in the existing literature.
  This paper bridges this gap by presenting a novel application of machine learning techniques to recommend search filters aimed at improving booking conversions. We introduce a modeling framework that directly targets lower-funnel conversions (bookings) by recommending intermediate tools, i.e. search filters. Leveraging the framework, we designed and built the filter recommendation system at Airbnb from the ground up, addressing challenges like cold start and stringent serving requirements.
  The filter recommendation system we developed has been successfully deployed at Airbnb, powering multiple user interfaces and driving incremental booking conversion lifts, as validated through online A/B testing. An ablation study further validates the effectiveness of our approach and key design choices. By focusing on conversion-oriented filter recommendations, our work ensures that search filters serve their ultimate purpose at Airbnb - helping guests find and book their ideal accommodations.

### 20. Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Gregory Kang Ruey Lau, Hieu Dao, Nicole Kan Hui Lin, Bryan Kian Hsiang Low
- **URL**: <http://arxiv.org/abs/2602.24195v1>
- **Submitted**: 2026-02-27 17:18:42
- **Comment**: Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop
- **Topic Keywords**: queries
- **Reason**: This paper is somewhat related to your interests in Natural Language Processing (NLP) and related topics, particularly in the context of multimodal large language models. However, it does not directly address your core research themes in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's focus on uncertainty quantification and multimodal large language models is not a central match to your research interests.

#### Abstract
> Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.

### 21. ArgLLM-App: An Interactive System for Argumentative Reasoning with Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Adam Dejl, Deniz Gorur, Francesca Toni
- **URL**: <http://arxiv.org/abs/2602.24172v1>
- **Submitted**: 2026-02-27 16:52:27
- **Comment**: AAMAS 2026 Demonstration Track
- **Topic Keywords**: rag
- **Reason**: The paper discusses argumentative reasoning with Large Language Models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on decision-making and explainability is not directly aligned with the user's primary research themes. While the paper touches on human-computer interaction, it does not explicitly address user behavior modeling or click models.

#### Abstract
> Argumentative LLMs (ArgLLMs) are an existing approach leveraging Large Language Models (LLMs) and computational argumentation for decision-making, with the aim of making the resulting decisions faithfully explainable to and contestable by humans. Here we propose a web-based system implementing ArgLLM-empowered agents for binary tasks. ArgLLM-App supports visualisation of the produced explanations and interaction with human users, allowing them to identify and contest any mistakes in the system's reasoning. It is highly modular and enables drawing information from trusted external sources. ArgLLM-App is publicly available at https://argllm.app, with a video demonstration at https://youtu.be/vzwlGOr0sPM.

### 22. Terminology Rarity Predicts Catastrophic Failure in LLM Translation of Low-Resource Ancient Languages: Evidence from Ancient Greek

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: James L. Zainaldin, Cameron Pattison, Manuela Marai, Jacob Wu, Mark J. Schiefsky
- **URL**: <http://arxiv.org/abs/2602.24119v1>
- **Submitted**: 2026-02-27 15:57:15
- **Comment**: Article + supplementary information
- **Topic Keywords**: rag
- **Reason**: This paper explores the performance of large language models in translating Ancient Greek texts, focusing on the impact of terminology rarity on translation quality. While it touches on aspects of natural language processing and machine translation, it is not directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> This study presents the first systematic, reference-free human evaluation of large language model (LLM) machine translation (MT) for Ancient Greek (AG) technical prose. We evaluate translations by three commercial LLMs (Claude, Gemini, ChatGPT) of twenty paragraph-length passages from two works by the Greek physician Galen of Pergamum (ca. 129-216 CE): On Mixtures, which has two published English translations, and On the Composition of Drugs according to Kinds, which has never been fully translated into English. We assess translation quality using both standard automated evaluation metrics (BLEU, chrF++, METEOR, ROUGE-L, BERTScore, COMET, BLEURT) and expert human evaluation via a modified Multidimensional Quality Metrics (MQM) framework applied to all 60 translations by a team of domain specialists. On the previously translated expository text, LLMs achieved high translation quality (mean MQM score 95.2/100), with performance approaching expert level. On the untranslated pharmacological text, aggregate quality was lower (79.9/100) but with high variance driven by two passages presenting extreme terminological density; excluding these, scores converged to within 4 points of the translated text. Terminology rarity, operationalized via corpus frequency in the literary Diorisis Ancient Greek Corpus, emerged as a strong predictor of translation failure (r = -.97 for passage-level quality on the untranslated text). Automated metrics showed moderate correlation with human judgment overall on the text with a wide quality spread (Composition), but no metric discriminated among high-quality translations. We discuss implications for the use of LLMs in Classical scholarship and for the design of automated evaluation pipelines for low-resource ancient languages.

### 23. FuXi-Linear: Unleashing the Power of Linear Attention in Long-term Time-aware Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yufei Ye, Wei Guo, Hao Wang, Luankang Zhang, Heng Chang, Hong Zhu, Yuyang Ye, Yong Liu, Defu Lian, Enhong Chen
- **URL**: <http://arxiv.org/abs/2602.23671v1>
- **Submitted**: 2026-02-27 04:38:28
- **Topic Keywords**: recommend, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, but it primarily focuses on recommender systems and sequential recommendation, which is not your primary area of interest. Although it involves attention mechanisms and deep semantic understanding, the context is different from your usual e-commerce domain and is more focused on long-term time-aware sequential recommendation.

#### Abstract
> Modern recommendation systems primarily rely on attention mechanisms with quadratic complexity, which limits their ability to handle long user sequences and slows down inference. While linear attention is a promising alternative, existing research faces three critical challenges: (1) temporal signals are often overlooked or integrated via naive coupling that causes mutual interference between temporal and semantic signals while neglecting behavioral periodicity; (2) insufficient positional information provided by existing linear frameworks; and (3) a primary focus on short sequences and shallow architectures. To address these issues, we propose FuXi-Linear, a linear-complexity model designed for efficient long-sequence recommendation. Our approach introduces two key components: (1) a Temporal Retention Channel that independently computes periodic attention weights using temporal data, preventing crosstalk between temporal and semantic signals; (2) a Linear Positional Channel that integrates positional information through learnable kernels within linear complexity. Moreover, we demonstrate that FuXi-Linear exhibits a robust power-law scaling property at a thousand-length scale, a characteristic largely unexplored in prior linear recommendation studies. Extensive experiments on sequences of several thousand tokens demonstrate that FuXi-Linear outperforms state-of-the-art models in recommendation quality, while achieving up to 10$\times$ speedup in the prefill stage and up to 21$\times$ speedup in the decode stage compared to competitive baselines. Our code has been released in a public repository https://github.com/USTC-StarTeam/fuxi-linear.

### 24. IDP Accelerator: Agentic Document Intelligence from Extraction to Compliance Validation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Md Mofijul Islam, Md Sirajus Salekin, Joe King, Priyashree Roy, Vamsi Thilak Gudi, Spencer Romo, Akhil Nooney, Boyi Xie, Bob Strahan, Diego A. Socolinsky
- **URL**: <http://arxiv.org/abs/2602.23481v1>
- **Submitted**: 2026-02-26 20:20:38
- **Topic Keywords**: rag
- **Reason**: The paper explores Intelligent Document Processing (IDP) using Large Language Models (LLMs), which is somewhat related to Information Retrieval, particularly in the context of deep semantic understanding and real-time relevance optimization. However, the focus on document extraction, compliance validation, and agentic AI is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The connection to NLP and data mining is present, but the application domain is more focused on industrial NLP and compliance validation.

#### Abstract
> Understanding and extracting structured insights from unstructured documents remains a foundational challenge in industrial NLP. While Large Language Models (LLMs) enable zero-shot extraction, traditional pipelines often fail to handle multi-document packets, complex reasoning, and strict compliance requirements. We present IDP (Intelligent Document Processing) Accelerator, a framework enabling agentic AI for end-to-end document intelligence with four key components: (1) DocSplit, a novel benchmark dataset and multimodal classifier using BIO tagging to segment complex document packets; (2) configurable Extraction Module leveraging multimodal LLMs to transform unstructured content into structured data; (3) Agentic Analytics Module, compliant with the Model Context Protocol (MCP) providing data access through secure, sandboxed code execution; and (4) Rule Validation Module replacing deterministic engines with LLM-driven logic for complex compliance checks. The interactive demonstration enables users to upload document packets, visualize classification results, and explore extracted data through an intuitive web interface. We demonstrate effectiveness across industries, highlighting a production deployment at a leading healthcare provider achieving 98% classification accuracy, 80% reduced processing latency, and 77% lower operational costs over legacy baselines. IDP Accelerator is open-sourced with a live demonstration available to the community.

### 25. Cross-Representation Knowledge Transfer for Improved Sequential Recommendations

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Artur Gimranov, Viacheslav Yusupov, Elfat Sabitov, Tatyana Matveeva, Anton Lysenko, Ruslan Israfilov, Evgeny Frolov
- **URL**: <http://arxiv.org/abs/2602.23471v1>
- **Submitted**: 2026-02-26 20:03:23
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on sequential recommender systems, which is somewhat related to information retrieval, but it primarily deals with recommender systems and does not explicitly address query understanding, ranking models, or user behavior modeling. The use of graph neural networks and transformers is relevant to NLP, but the application is in a different domain.

#### Abstract
> Transformer architectures, capable of capturing sequential dependencies in the history of user interactions, have become the dominant approach in sequential recommender systems. Despite their success, such models consider sequence elements in isolation, implicitly accounting for the complex relationships between them. Graph neural networks, in contrast, explicitly model these relationships through higher order interactions but are often unable to adequately capture their evolution over time, limiting their use for predicting the next interaction. To fill this gap, we present a new framework that combines transformers and graph neural networks and aligns different representations for solving next-item prediction task. Our solution simultaneously encodes structural dependencies in the interaction graph and tracks their dynamic change. Experimental results on a number of open datasets demonstrate that the proposed framework consistently outperforms both pure sequential and graph approaches in terms of recommendation quality, as well as recent methods that combine both types of signals.

### 26. GPU-Native Approximate Nearest Neighbor Search with IVF-RaBitQ: Fast Index Build and Search

- **LLM Score**: 2
- **Keyword Score**: 13
- **Authors**: Jifan Shi, Jianyang Gao, James Xia, Tam√°s B√©la Feh√©r, Cheng Long
- **URL**: <http://arxiv.org/abs/2602.23999v1>
- **Submitted**: 2026-02-27 13:23:30
- **Topic Keywords**: ranking, rerank, rag, retrieval, recommend, rank, search
- **Reason**: This paper focuses on Approximate Nearest Neighbor Search (ANNS) on GPUs, which is a specific problem in the broader field of Information Retrieval. While it touches on retrieval workloads, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research.

#### Abstract
> Approximate nearest neighbor search (ANNS) on GPUs is gaining increasing popularity for modern retrieval and recommendation workloads that operate over massive high-dimensional vectors. Graph-based indexes deliver high recall and throughput but incur heavy build-time and storage costs. In contrast, cluster-based methods build and scale efficiently yet often need many probes for high recall, straining memory bandwidth and compute. Aiming to simultaneously achieve fast index build, high-throughput search, high recall, and low storage requirement for GPUs, we present IVF-RaBitQ (GPU), a GPU-native ANNS solution that integrates the cluster-based method IVF with RaBitQ quantization into an efficient GPU index build/search pipeline. Specifically, for index build, we develop a scalable GPU-native RaBitQ quantization method that enables fast and accurate low-bit encoding at scale. For search, we develop GPU-native distance computation schemes for RaBitQ codes and a fused search kernel to achieve high throughput with high recall. With IVF-RaBitQ implemented and integrated into the NVIDIA cuVS Library, experiments on cuVS Bench across multiple datasets show that IVF-RaBitQ offers a strong performance frontier in recall, throughput, index build time, and storage footprint. For Recall approximately equal to 0.95, IVF-RaBitQ achieves 2.2x higher QPS than the state-of-the-art graph-based method CAGRA, while also constructing indices 7.7x faster on average. Compared to the cluster-based method IVF-PQ, IVF-RaBitQ delivers on average over 2.7x higher throughput while avoiding accessing the raw vectors for reranking.

### 27. Resources for Automated Evaluation of Assistive RAG Systems that Help Readers with News Trustworthiness Assessment

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Dake Zhang, Mark D. Smucker, Charles L. A. Clarke
- **URL**: <http://arxiv.org/abs/2602.24277v1>
- **Submitted**: 2026-02-27 18:49:31
- **Topic Keywords**: rag, retrieval, rank, search, trec
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves a specific application of RAG systems for news trustworthiness assessment, it does not focus on query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Many readers today struggle to assess the trustworthiness of online news because reliable reporting coexists with misinformation. The TREC 2025 DRAGUN (Detection, Retrieval, and Augmented Generation for Understanding News) Track provided a venue for researchers to develop and evaluate assistive RAG systems that support readers' news trustworthiness assessment by producing reader-oriented, well-attributed reports. As the organizers of the DRAGUN track, we describe the resources that we have newly developed to allow for the reuse of the track's tasks. The track had two tasks: (Task 1) Question Generation, producing 10 ranked investigative questions; and (Task 2, the main task) Report Generation, producing a 250-word report grounded in the MS MARCO V2.1 Segmented Corpus. As part of the track's evaluation, we had TREC assessors create importance-weighted rubrics of questions with expected short answers for 30 different news articles. These rubrics represent the information that assessors believe is important for readers to assess an article's trustworthiness. The assessors then used their rubrics to manually judge the participating teams' submitted runs. To make these tasks and their rubrics reusable, we have created an automated process to judge runs not part of the original assessing. We show that our AutoJudge ranks existing runs well compared to the TREC human-assessed evaluation (Kendall's $œÑ= 0.678$ for Task 1 and $œÑ= 0.872$ for Task 2). These resources enable both the evaluation of RAG systems for assistive news trustworthiness assessment and, with the human evaluation as a benchmark, research on improving automated RAG evaluation.

### 28. EDDA-Coordinata: An Annotated Dataset of Historical Geographic Coordinates

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ludovic Moncla, Pierre Nugues, Thierry Joliveau, Katherine McDonough
- **URL**: <http://arxiv.org/abs/2602.23941v1>
- **Submitted**: 2026-02-27 11:43:17
- **Comment**: Accepted at LREC 2026
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves text analysis and model training, its focus on historical geographic coordinates and dataset creation is not aligned with your core research themes.

#### Abstract
> This paper introduces a dataset of enriched geographic coordinates retrieved from Diderot and d'Alembert's eighteenth-century Encyclopedie. Automatically recovering geographic coordinates from historical texts is a complex task, as they are expressed in a variety of ways and with varying levels of precision. To improve retrieval of coordinates from similar digitized early modern texts, we have created a gold standard dataset, trained models, published the resulting inferred and normalized coordinate data, and experimented applying these models to new texts. From 74,000 total articles in each of the digitized versions of the Encyclopedie from ARTFL and ENCCRE, we examined 15,278 geographical entries, manually identifying 4,798 containing coordinates, and 10,480 with descriptive but non-numerical references. Leveraging our gold standard annotations, we trained transformer-based models to retrieve and normalize coordinates. The pipeline presented here combines a classifier to identify coordinate-bearing entries and a second model for retrieval, tested across encoder-decoder and decoder architectures. Cross-validation yielded an 86% EM score. On an out-of-domain eighteenth-century Trevoux dictionary (also in French), our fine-tuned model had a 61% EM score, while for the nineteenth-century, 7th edition of the Encyclopaedia Britannica in English, the EM was 77%. These findings highlight the gold standard dataset's usefulness as training data, and our two-step method's cross-lingual, cross-domain generalizability.

### 29. Multi-Agent Causal Reasoning for Suicide Ideation Detection Through Online Conversations

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jun Li, Xiangmeng Wang, Haoyang Li, Yifei Yan, Shijie Zhang, Hong Va Leong, Ling Feng, Nancy Xiaonan Yu, Qing Li
- **URL**: <http://arxiv.org/abs/2602.23577v1>
- **Submitted**: 2026-02-27 01:06:18
- **Topic Keywords**: rag, ctr
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves analyzing online conversations, the focus is on suicide ideation detection and multi-agent causal reasoning, which is not a central match to the user's interests.

#### Abstract
> Suicide remains a pressing global public health concern. While social media platforms offer opportunities for early risk detection through online conversation trees, existing approaches face two major limitations: (1) They rely on predefined rules (e.g., quotes or relies) to log conversations that capture only a narrow spectrum of user interactions, and (2) They overlook hidden influences such as user conformity and suicide copycat behavior, which can significantly affect suicidal expression and propagation in online communities. To address these limitations, we propose a Multi-Agent Causal Reasoning (MACR) framework that collaboratively employs a Reasoning Agent to scale user interactions and a Bias-aware Decision-Making Agent to mitigate harmful biases arising from hidden influences. The Reasoning Agent integrates cognitive appraisal theory to generate counterfactual user reactions to posts, thereby scaling user interactions. It analyses these reactions through structured dimensions, i.e., cognitive, emotional, and behavioral patterns, with a dedicated sub-agent responsible for each dimension. The Bias-aware Decision-Making Agent mitigates hidden biases through a front-door adjustment strategy, leveraging the counterfactual user reactions produced by the Reasoning Agent. Through the collaboration of reasoning and bias-aware decision making, the proposed MACR framework not only alleviates hidden biases, but also enriches contextual information of user interactions with counterfactual knowledge. Extensive experiments on real-world conversational datasets demonstrate the effectiveness and robustness of MACR in identifying suicide risk.

### 30. A Novel Hierarchical Multi-Agent System for Payments Using LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Joon Kiat Chua, Donghao Huang, Zhaoxia Wang
- **URL**: <http://arxiv.org/abs/2602.24068v1>
- **Submitted**: 2026-02-27 14:58:13
- **Comment**: 12 pages, 1 figure, 3 tables. Accepted at PAKDD 2026
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on a novel hierarchical multi-agent system for payments using LLMs, which is not directly related to information retrieval, search technologies, or query understanding. While it involves LLMs, the application domain is payments, and the system's architecture is not centered around ranking models or user behavior modeling.

#### Abstract
> Large language model (LLM) agents, such as OpenAI's Operator and Claude's Computer Use, can automate workflows but unable to handle payment tasks. Existing agentic solutions have gained significant attention; however, even the latest approaches face challenges in implementing end-to-end agentic payment workflows. To address this gap, this research proposes the Hierarchical Multi-Agent System for Payments (HMASP), which provides an end-to-end agentic method for completing payment workflows. The proposed HMASP leverages either open-weight or proprietary LLMs and employs a modular architecture consisting of the Conversational Payment Agent (CPA - first agent level), Supervisor agents (second agent level), Routing agents (third agent level), and the Process summary agent (fourth agent level). The CPA serves as the central entry point, handling all external requests and coordinating subsequent tasks across hierarchical levels. HMASP incorporates architectural patterns that enable modular task execution across agents and levels for payment operations, including shared state variables, decoupled message states, and structured handoff protocols that facilitate coordination across agents and workflows. Experimental results demonstrate the feasibility of the proposed HMASP. To our knowledge, HMASP is the first LLM-based multi-agent system to implement end-to-end agentic payment workflows. This work lays a foundation for extending agentic capabilities into the payment domain.

### 31. Robust Aggregation for Federated Sequential Recommendation with Sparse and Poisoned Data

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Minh Hieu Nguyen
- **URL**: <http://arxiv.org/abs/2602.23982v1>
- **Submitted**: 2026-02-27 12:50:44
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on federated sequential recommendation, which is somewhat related to information retrieval, but it does not align with the user's core research themes of query understanding, ranking models, and user behavior modeling. The paper's emphasis on robust aggregation and defense against malicious updates is also not directly relevant to the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Federated sequential recommendation distributes model training across user devices so that behavioural data remains local, reducing privacy risks. Yet, this setting introduces two intertwined difficulties. On the one hand, individual clients typically contribute only short and highly sparse interaction sequences, limiting the reliability of learned user representations. On the other hand, the federated optimisation process is vulnerable to malicious or corrupted client updates, where poisoned gradients can significantly distort the global model. These challenges are particularly severe in sequential recommendation, where temporal dynamics further complicate signal aggregation. To address this problem, we propose a robust aggregation framework tailored for federated sequential recommendation under sparse and adversarial conditions. Instead of relying on standard averaging, our method introduces a defence-aware aggregation mechanism that identifies and down-weights unreliable client updates while preserving informative signals from sparse but benign participants. The framework incorporates representation-level constraints to stabilise user and item embeddings, preventing poisoned or anomalous contributions from dominating the global parameter space. In addition, we integrate sequence-aware regularisation to maintain temporal coherence in user modelling despite limited local observations.

### 32. UTPTrack: Towards Simple and Unified Token Pruning for Visual Tracking

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hao Wu, Xudong Wang, Jialiang Zhang, Junlong Tong, Xinghao Chen, Junyan Lin, Yunpu Ma, Xiaoyu Shen
- **URL**: <http://arxiv.org/abs/2602.23734v1>
- **Submitted**: 2026-02-27 06:58:09
- **Comment**: Accepted to CVPR 2026
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests as it focuses on visual object tracking and token pruning in the context of computer vision, which is outside your primary areas of interest in Information Retrieval and Natural Language Processing.

#### Abstract
> One-stream Transformer-based trackers achieve advanced performance in visual object tracking but suffer from significant computational overhead that hinders real-time deployment. While token pruning offers a path to efficiency, existing methods are fragmented. They typically prune the search region, dynamic template, and static template in isolation, overlooking critical inter-component dependencies, which yields suboptimal pruning and degraded accuracy. To address this, we introduce UTPTrack, a simple and Unified Token Pruning framework that, for the first time, jointly compresses all three components. UTPTrack employs an attention-guided, token type-aware strategy to holistically model redundancy, a design that seamlessly supports unified tracking across multimodal and language-guided tasks within a single model. Extensive evaluations on 10 benchmarks demonstrate that UTPTrack achieves a new state-of-the-art in the accuracy-efficiency trade-off for pruning-based trackers, pruning 65.4% of vision tokens in RGB-based tracking and 67.5% in unified tracking while preserving 99.7% and 100.5% of baseline performance, respectively. This strong performance across both RGB and multimodal scenarios underlines its potential as a robust foundation for future research in efficient visual tracking. Code will be released at https://github.com/EIT-NLP/UTPTrack.

### 33. LFQA-HP-1M: A Large-Scale Human Preference Dataset for Long-Form Question Answering

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Rafid Ishrak Jahan, Fahmid Shahriar Iqbal, Sagnik Ray Choudhury
- **URL**: <http://arxiv.org/abs/2602.23603v1>
- **Submitted**: 2026-02-27 02:14:15
- **Comment**: LREC 2026 Accepted. https://huggingface.co/datasets/nlpatunt/LFQA-HP-1M
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on long-form question answering and proposes a dataset for evaluating answer quality. While it involves Natural Language Processing (NLP) and evaluation metrics, it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Long-form question answering (LFQA) demands nuanced evaluation of multi-sentence explanatory responses, yet existing metrics often fail to reflect human judgment. We present LFQA-HP-1M, a large-scale dataset comprising 1.3M human pairwise preference annotations for LFQA. We propose nine rubrics for answer quality evaluation, and show that simple linear models based on these features perform comparably to state-of-the-art LLM evaluators. We further examine transitivity consistency, positional bias, and verbosity biases in LLM evaluators and demonstrate their vulnerability to adversarial perturbations. Overall, this work provides one of the largest public LFQA preference datasets and a rubric-driven framework for transparent and reliable evaluation.

### 34. Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yanwei Ren, Haotian Zhang, Likang Xiao, Xikai Zhang, Jiaxing Huang, Jiayan Qiu, Baosheng Yu, Quan Chen, Liu Liu
- **URL**: <http://arxiv.org/abs/2602.24110v1>
- **Submitted**: 2026-02-27 15:49:23
- **Topic Keywords**: rag
- **Reason**: This paper appears to be unrelated to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on Reinforcement Learning and Process Reward Models seems to be more aligned with areas like Recommender Systems or Control Theory, which are not your primary focus.

#### Abstract
> Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.

### 35. ARGUS: Seeing the Influence of Narrative Features on Persuasion in Argumentative Texts

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sara Nabhani, Federico Pianzola, Khalid Al-Khatib, Malvina Nissim
- **URL**: <http://arxiv.org/abs/2602.24109v1>
- **Submitted**: 2026-02-27 15:47:57
- **Comment**: 22 pages, 8 figures, submitted to ACM Transactions on Intelligent Systems and Technology
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on the analysis of narrative features in argumentative texts, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the use of large language models, the primary focus is on persuasion and narrative analysis, making it somewhat tangential to the user's interests.

#### Abstract
> Can narratives make arguments more persuasive? And to this end, which narrative features matter most? Although stories are often seen as powerful tools for persuasion, their specific role in online, unstructured argumentation remains underexplored. To address this gap, we present ARGUS, a framework for studying the impact of narration on persuasion in argumentative discourse. ARGUS introduces a new ChangeMyView corpus annotated for story presence and six key narrative features, integrating insights from two established theoretical frameworks that capture both textual narrative features and their effects on recipients. Leveraging both encoder-based classifiers and zero-shot large language models (LLMs), ARGUS identifies stories and narrative features and applies them at scale to examine how different narrative dimensions influence persuasion success in online argumentation.

### 36. Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral
- **URL**: <http://arxiv.org/abs/2602.24044v1>
- **Submitted**: 2026-02-27 14:22:51
- **Comment**: journal extension of the workshop paper titled as "A data-driven ml approach for maximizing performance in llm-adapter serving"
- **Topic Keywords**: rag
- **Reason**: This paper focuses on optimizing GPU efficiency for Large Language Model (LLM) adapter serving, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves machine learning and optimization, the context is specific to LLM serving and does not align with your primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.

### 37. LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Alexander Samarin, Sergei Krutikov, Anton Shevtsov, Sergei Skvortsov, Filipp Fisin, Alexander Golubev
- **URL**: <http://arxiv.org/abs/2602.23881v1>
- **Submitted**: 2026-02-27 10:20:11
- **Topic Keywords**: rag
- **Reason**: This paper focuses on optimizing acceptance rates in speculative decoding for large language models, which is not directly related to information retrieval, query understanding, or ranking models. While it involves deep learning and optimization techniques, the context is specific to language models and does not align with the user's primary research interests in information retrieval and search technologies.

#### Abstract
> Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives.

### 38. NAU-QMUL: Utilizing BERT and CLIP for Multi-modal AI-Generated Image Detection

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xiaoyu Guo, Arkaitz Zubiaga
- **URL**: <http://arxiv.org/abs/2602.23863v1>
- **Submitted**: 2026-02-27 10:03:54
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on AI-generated image detection using multi-modal models, which is outside your primary areas of interest in Information Retrieval and Search technologies, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> With the aim of detecting AI-generated images and identifying the specific models responsible for their generation, we propose a multi-modal multi-task model. The model leverages pre-trained BERT and CLIP Vision encoders for text and image feature extraction, respectively, and employs cross-modal feature fusion with a tailored multi-task loss function. Additionally, a pseudo-labeling-based data augmentation strategy was utilized to expand the training dataset with high-confidence samples. The model achieved fifth place in both Tasks A and B of the `CT2: AI-Generated Image Detection' competition, with F1 scores of 83.16\% and 48.88\%, respectively. These findings highlight the effectiveness of the proposed architecture and its potential for advancing AI-generated content detection in real-world scenarios. The source code for our method is published on https://github.com/xxxxxxxxy/AIGeneratedImageDetection.

### 39. Learning to Reflect and Correct: Towards Better Decoding Trajectories for Large-Scale Generative Recommendation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haibo Xing, Hao Deng, Lingyu Mu, Jinxin Hu, Yu Zhang, Xiaoyi Zeng, Jing Zhang
- **URL**: <http://arxiv.org/abs/2602.23639v1>
- **Submitted**: 2026-02-27 03:22:58
- **Topic Keywords**: recommend, search
- **Reason**: This paper focuses on Generative Recommendation, a topic somewhat related to Information Retrieval, but with a strong emphasis on recommendation systems and deep learning techniques. While it touches on aspects of real-time relevance optimization, the primary focus is on recommendation quality and not directly on query understanding or ranking models, making it less relevant to the user's core research themes.

#### Abstract
> Generative Recommendation (GR) has become a promising paradigm for large-scale recommendation systems. However, existing GR models typically perform single-pass decoding without explicit refinement, causing early deviations to accumulate and ultimately degrade recommendation quality. To tackle this problem, we propose GRC, which is, to our knowledge, the first structured reflection-correction framework for GR that extends standard decoding into a Generation-Reflection-Correction (GRC) process. Concretely, GRC introduces a supervised reflection-correction template that decomposes the decoding process into initial draft generation, multi-granular reflection, and reflection-guided correction, thereby enabling structured reflection and correction in the semantic token space. To further explore the enlarged refinement space introduced by the GRC process, we optimize the entire GRC trajectory with GRPO-based reinforcement learning, under a carefully designed reward function with token-level and trajectory-level signals. For efficient online serving, we propose an Entropy-Guided Reflection Scheduling (EGRS) strategy that dynamically allocates more correction budget to high-uncertainty decoding trajectories during beam search. Extensive experiments on real-world datasets show that GRC consistently outperforms six state-of-the-art baselines by up to 15.74%, and online A/B tests demonstrate its substantial practical value in large-scale industrial recommendation, delivering a 1.79% lift in advertising revenue with only modest latency overhead.

### 40. LLM-Driven Multi-Turn Task-Oriented Dialogue Synthesis for Realistic Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yu Zhu, Kai Yang
- **URL**: <http://arxiv.org/abs/2602.23610v1>
- **Submitted**: 2026-02-27 02:23:37
- **Topic Keywords**: rag
- **Reason**: This paper focuses on task-oriented dialogue synthesis and large language models, which is somewhat related to your interests in Natural Language Processing (NLP) and related topics. However, the primary focus on dialogue synthesis and reasoning capabilities of LLMs does not directly align with your core research themes in Information Retrieval (IR), query understanding, ranking models, and user behavior modeling.

#### Abstract
> The reasoning capability of large language models (LLMs), defined as their ability to analyze, infer, and make decisions based on input information, is essential for building intelligent task-oriented dialogue systems. However, existing benchmarks do not sufficiently reflect the complexity of real-world scenarios, which limits their effectiveness in evaluating and enhancing LLM reasoning in practical contexts. Many current reasoning datasets are overly simplistic and abstract, often disconnected from realistic task flows, domain constraints, and operational rules, making it difficult to effectively evaluate LLMs' logical reasoning ability. In addition, data contamination from pretraining corpora undermines the reliability of evaluation results, and traditional crowdsourcing methods for dataset construction are labor-intensive and difficult to scale. To address these challenges, we propose a LLM-driven framework for synthesizing multi-turn, task-oriented dialogues grounded in realistic reasoning scenarios, leveraging trilevel optimization to enhance dialogue quality. Our method generates dialogues grounded in authentic task scenarios, enriched with real-world information, and exhibiting strong contextual coherence. Corresponding reasoning tasks are carefully designed around these dialogues and iteratively refined to continuously improve the tasks' quality and challenge. The resulting dataset serves as a valuable benchmark for assessing and advancing the realistic logical reasoning capabilities of LLMs. Experimental results show that our synthetic data-based reasoning tasks introduce non-trivial reasoning challenges and provide meaningful support for improving the reasoning capabilities of LLMs.

### 41. MemEmo: Evaluating Emotion in Memory Systems of Agents

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Peng Liu, Zhen Tao, Jihao Zhao, Ding Chen, Yansong Zhang, Cuiping Li, Zhiyu Li, Hong Chen
- **URL**: <http://arxiv.org/abs/2602.23944v1>
- **Submitted**: 2026-02-27 11:46:08
- **Topic Keywords**: search
- **Reason**: This paper focuses on memory systems for Large Language Models, which is a related but distinct area from Information Retrieval and Search technologies. While it touches on the concept of context loss, it does not directly address query understanding, ranking models, or user behavior modeling, making it less relevant to your core research interests.

#### Abstract
> Memory systems address the challenge of context loss in Large Language Model during prolonged interactions. However, compared to human cognition, the efficacy of these systems in processing emotion-related information remains inconclusive. To address this gap, we propose an emotion-enhanced memory evaluation benchmark to assess the performance of mainstream and state-of-the-art memory systems in handling affective information. We developed the \textbf{H}uman-\textbf{L}ike \textbf{M}emory \textbf{E}motion (\textbf{HLME}) dataset, which evaluates memory systems across three dimensions: emotional information extraction, emotional memory updating, and emotional memory question answering. Experimental results indicate that none of the evaluated systems achieve robust performance across all three tasks. Our findings provide an objective perspective on the current deficiencies of memory systems in processing emotional memories and suggest a new trajectory for future research and system optimization.

### 42. Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Zhengbo Wang, Jian Liang, Ran He, Zilei Wang, Tieniu Tan
- **URL**: <http://arxiv.org/abs/2602.24283v1>
- **Submitted**: 2026-02-27 18:57:06
- **Comment**: Camera-ready version. Accepted as Oral at ICLR 2026
- **Topic Keywords**: rag, rank
- **Reason**: This paper is about optimizing deep learning models using a novel low-rank optimizer, which is unrelated to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, and Natural Language Processing.

#### Abstract
> Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre.

### 43. Jailbreak Foundry: From Papers to Runnable Attacks for Reproducible Benchmarking

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Zhicheng Fang, Jingjie Zheng, Chenxu Fu, Wei Xu
- **URL**: <http://arxiv.org/abs/2602.24009v1>
- **Submitted**: 2026-02-27 13:32:53
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on a security-related topic, specifically the development of a system for reproducibly benchmarking attacks on large language models. It does not align with the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing, and is not related to query understanding, ranking models, or user behavior modeling.

#### Abstract
> Jailbreak techniques for large language models (LLMs) evolve faster than benchmarks, making robustness estimates stale and difficult to compare across papers due to drift in datasets, harnesses, and judging protocols. We introduce JAILBREAK FOUNDRY (JBF), a system that addresses this gap via a multi-agent workflow to translate jailbreak papers into executable modules for immediate evaluation within a unified harness. JBF features three core components: (i) JBF-LIB for shared contracts and reusable utilities; (ii) JBF-FORGE for the multi-agent paper-to-module translation; and (iii) JBF-EVAL for standardizing evaluations. Across 30 reproduced attacks, JBF achieves high fidelity with a mean (reproduced-reported) attack success rate (ASR) deviation of +0.26 percentage points. By leveraging shared infrastructure, JBF reduces attack-specific implementation code by nearly half relative to original repositories and achieves an 82.5% mean reused-code ratio. This system enables a standardized AdvBench evaluation of all 30 attacks across 10 victim models using a consistent GPT-4o judge. By automating both attack integration and standardized evaluation, JBF offers a scalable solution for creating living benchmarks that keep pace with the rapidly shifting security landscape.

### 44. GLUScope: A Tool for Analyzing GLU Neurons in Transformer Language Models

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Sebastian Gerstner, Hinrich Sch√ºtze
- **URL**: <http://arxiv.org/abs/2602.23826v1>
- **Submitted**: 2026-02-27 09:07:45
- **Comment**: 6 pages for main body, 9 pages in total. 4 figures
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on analyzing neurons in Transformer language models for interpretability, which is outside your primary focus on Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> We present GLUScope, an open-source tool for analyzing neurons in Transformer-based language models, intended for interpretability researchers. We focus on more recent models than previous tools do; specifically we consider gated activation functions such as SwiGLU. This introduces a new challenge: understanding positive activations is not enough. Instead, both the gate and the in activation of a neuron can be positive or negative, leading to four different possible sign combinations that in some cases have quite different functionalities. Accordingly, for any neuron, our tool shows text examples for each of the four sign combinations, and indicates how often each combination occurs. We describe examples of how our tool can lead to novel insights. A demo is available at https: //sjgerstner.github.io/gluscope.

---

