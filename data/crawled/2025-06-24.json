[
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "abstract": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-document retrieval, semantic text similarity, and code search.\nComprehensive evaluations demonstrate that jina-embeddings-v4 achieves\nstate-of-the-art performance on both single-modal and cross-modal retrieval\ntasks, with particular strength in processing visually rich content such as\ntables, charts, diagrams, and mixed-media formats. To facilitate evaluation of\nthis capability, we also introduce Jina-VDR, a novel benchmark specifically\ndesigned for visually rich image retrieval.",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18902v2",
        "arxiv_id": "2506.18902v2",
        "authors": [
            "Michael Günther",
            "Saba Sturua",
            "Mohammad Kalim Akram",
            "Isabelle Mohr",
            "Andrei Ungureanu",
            "Bo Wang",
            "Sedigheh Eslami",
            "Scott Martens",
            "Maximilian Werk",
            "Nan Wang",
            "Han Xiao"
        ],
        "submitted": "2025-06-23 17:59:55",
        "source": "arxiv",
        "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables"
    },
    {
        "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
        "abstract": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
        "url": "http://arxiv.org/abs/2506.18898v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18898v1",
        "arxiv_id": "2506.18898v1",
        "authors": [
            "Jiaming Han",
            "Hao Chen",
            "Yang Zhao",
            "Hanyu Wang",
            "Qi Zhao",
            "Ziyan Yang",
            "Hao He",
            "Xiangyu Yue",
            "Lu Jiang"
        ],
        "submitted": "2025-06-23 17:59:14",
        "source": "arxiv",
        "comment": "Project page: https://tar.csuhan.com"
    },
    {
        "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
        "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
        "url": "http://arxiv.org/abs/2506.18896v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18896v1",
        "arxiv_id": "2506.18896v1",
        "authors": [
            "Jiaru Zou",
            "Ling Yang",
            "Jingwen Gu",
            "Jiahao Qiu",
            "Ke Shen",
            "Jingrui He",
            "Mengdi Wang"
        ],
        "submitted": "2025-06-23 17:59:02",
        "source": "arxiv",
        "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux"
    },
    {
        "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization",
        "abstract": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.",
        "url": "http://arxiv.org/abs/2506.18880v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18880v1",
        "arxiv_id": "2506.18880v1",
        "authors": [
            "Yiyou Sun",
            "Shawn Hu",
            "Georgia Zhou",
            "Ken Zheng",
            "Hannaneh Hajishirzi",
            "Nouha Dziri",
            "Dawn Song"
        ],
        "submitted": "2025-06-23 17:51:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
        "abstract": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
        "url": "http://arxiv.org/abs/2506.18879v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18879v1",
        "arxiv_id": "2506.18879v1",
        "authors": [
            "Junyan Li",
            "Yang Zhang",
            "Muhammad Yusuf Hassan",
            "Talha Chafekar",
            "Tianle Cai",
            "Zhile Ren",
            "Pengsheng Guo",
            "Foroozan Karimzadeh",
            "Colorado Reed",
            "Chong Wang",
            "Chuang Gan"
        ],
        "submitted": "2025-06-23 17:50:11",
        "source": "arxiv",
        "comment": "ICML 2025 poster"
    },
    {
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "abstract": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
        "url": "http://arxiv.org/abs/2506.18871v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18871v1",
        "arxiv_id": "2506.18871v1",
        "authors": [
            "Chenyuan Wu",
            "Pengfei Zheng",
            "Ruiran Yan",
            "Shitao Xiao",
            "Xin Luo",
            "Yueze Wang",
            "Wanli Li",
            "Xiyan Jiang",
            "Yexin Liu",
            "Junjie Zhou",
            "Ze Liu",
            "Ziyi Xia",
            "Chaofan Li",
            "Haoge Deng",
            "Jiahao Wang",
            "Kun Luo",
            "Bo Zhang",
            "Defu Lian",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Zheng Liu"
        ],
        "submitted": "2025-06-23 17:38:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents",
        "abstract": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
        "url": "http://arxiv.org/abs/2506.18959v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18959v1",
        "arxiv_id": "2506.18959v1",
        "authors": [
            "Weizhi Zhang",
            "Yangning Li",
            "Yuanchen Bei",
            "Junyu Luo",
            "Guancheng Wan",
            "Liangwei Yang",
            "Chenxuan Xie",
            "Yuyao Yang",
            "Wei-Chieh Huang",
            "Chunyu Miao",
            "Henry Peng Zou",
            "Xiao Luo",
            "Yusheng Zhao",
            "Yankai Chen",
            "Chunkit Chan",
            "Peilin Zhou",
            "Xinyang Zhang",
            "Chenwei Zhang",
            "Jingbo Shang",
            "Ming Zhang",
            "Yangqiu Song",
            "Irwin King",
            "Philip S. Yu"
        ],
        "submitted": "2025-06-23 17:27:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Comment On \"The Illusion of Thinking\": Reframing the Reasoning Cliff as an Agentic Gap",
        "abstract": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:\nUnderstanding the Strengths and Limitations of Reasoning Models via the Lens of\nProblem Complexity, presents a compelling empirical finding, a reasoning cliff,\nwhere the performance of Large Reasoning Models (LRMs) collapses beyond a\nspecific complexity threshold, which the authors posit as an intrinsic scaling\nlimitation of Chain-of-Thought (CoT) reasoning. This commentary, while\nacknowledging the study's methodological rigor, contends that this conclusion\nis confounded by experimental artifacts. We argue that the observed failure is\nnot evidence of a fundamental cognitive boundary, but rather a predictable\noutcome of system-level constraints in the static, text-only evaluation\nparadigm, including tool use restrictions, context window recall issues, the\nabsence of crucial cognitive baselines, inadequate statistical reporting, and\noutput generation limits. We reframe this performance collapse through the lens\nof an agentic gap, asserting that the models are not failing at reasoning, but\nat execution within a profoundly restrictive interface. We empirically\nsubstantiate this critique by demonstrating a striking reversal. A model,\ninitially declaring a puzzle impossible when confined to text-only generation,\nnow employs agentic tools to not only solve it but also master variations of\ncomplexity far beyond the reasoning cliff it previously failed to surmount.\nAdditionally, our empirical analysis of tool-enabled models like o4-mini and\nGPT-4o reveals a hierarchy of agentic reasoning, from simple procedural\nexecution to complex meta-cognitive self-correction, which has significant\nimplications for how we define and measure machine intelligence. The illusion\nof thinking attributed to LRMs is less a reasoning deficit and more a\nconsequence of an otherwise capable mind lacking the tools for action.",
        "url": "http://arxiv.org/abs/2506.18957v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18957v1",
        "arxiv_id": "2506.18957v1",
        "authors": [
            "Sheraz Khan",
            "Subha Madhavan",
            "Kannan Natarajan"
        ],
        "submitted": "2025-06-23 17:14:21",
        "source": "arxiv",
        "comment": "10 pages, 2 figures, Comment on \"The Illusion of Thinking:\n  Understanding the Strengths and Limitations of Reasoning Models via the Lens\n  of Problem Complexity\" (arXiv:2506.06941v1)"
    },
    {
        "title": "Mechanistic Interpretability Needs Philosophy",
        "abstract": "Mechanistic interpretability (MI) aims to explain how neural networks work by\nuncovering their underlying causal mechanisms. As the field grows in influence,\nit is increasingly important to examine not just models themselves, but the\nassumptions, concepts and explanatory strategies implicit in MI research. We\nargue that mechanistic interpretability needs philosophy: not as an\nafterthought, but as an ongoing partner in clarifying its concepts, refining\nits methods, and assessing the epistemic and ethical stakes of interpreting AI\nsystems. Taking three open problems from the MI literature as examples, this\nposition paper illustrates the value philosophy can add to MI research, and\noutlines a path toward deeper interdisciplinary dialogue.",
        "url": "http://arxiv.org/abs/2506.18852v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18852v1",
        "arxiv_id": "2506.18852v1",
        "authors": [
            "Iwan Williams",
            "Ninell Oldenburg",
            "Ruchira Dhar",
            "Joshua Hatherley",
            "Constanza Fierro",
            "Nina Rajcic",
            "Sandrine R. Schiller",
            "Filippos Stamatiou",
            "Anders Søgaard"
        ],
        "submitted": "2025-06-23 17:13:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "USAD: Universal Speech and Audio Representation via Distillation",
        "abstract": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
        "url": "http://arxiv.org/abs/2506.18843v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18843v1",
        "arxiv_id": "2506.18843v1",
        "authors": [
            "Heng-Jui Chang",
            "Saurabhchand Bhati",
            "James Glass",
            "Alexander H. Liu"
        ],
        "submitted": "2025-06-23 17:02:00",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
        "abstract": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
        "url": "http://arxiv.org/abs/2506.18841v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18841v1",
        "arxiv_id": "2506.18841v1",
        "authors": [
            "Yuhao Wu",
            "Yushi Bai",
            "Zhiqiang Hu",
            "Roy Ka-Wei Lee",
            "Juanzi Li"
        ],
        "submitted": "2025-06-23 16:59:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning",
        "abstract": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency.",
        "url": "http://arxiv.org/abs/2506.18831v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18831v1",
        "arxiv_id": "2506.18831v1",
        "authors": [
            "Aryasomayajula Ram Bharadwaj"
        ],
        "submitted": "2025-06-23 16:47:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task",
        "abstract": "This work describes the participation of the MLLP-VRAIN research group in the\nshared task of the IWSLT 2025 Simultaneous Speech Translation track. Our\nsubmission addresses the unique challenges of real-time translation of\nlong-form speech by developing a modular cascade system that adapts strong\npre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo\nfor ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight\nadaptation techniques rather than training new end-to-end models from scratch.\nOur approach employs document-level adaptation with prefix training to enhance\nthe MT model's ability to handle incomplete inputs, while incorporating\nadaptive emission policies including a wait-$k$ strategy and RALCP for managing\nthe translation stream. Specialized buffer management techniques and\nsegmentation strategies ensure coherent translations across long audio\nsequences. Experimental results on the ACL60/60 dataset demonstrate that our\nsystem achieves a favorable balance between translation quality and latency,\nwith a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of\n2.94 seconds. Our final model achieves a preliminary score on the official test\nset (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully\nadapted pre-trained components can create effective simultaneous translation\nsystems for long-form content without requiring extensive in-domain parallel\ndata or specialized end-to-end training.",
        "url": "http://arxiv.org/abs/2506.18828v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18828v1",
        "arxiv_id": "2506.18828v1",
        "authors": [
            "Jorge Iranzo-Sánchez",
            "Javier Iranzo-Sánchez",
            "Adrià Giménez",
            "Jorge Civera",
            "Alfons Juan"
        ],
        "submitted": "2025-06-23 16:44:01",
        "source": "arxiv",
        "comment": "IWSLT 2025 System Description"
    },
    {
        "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies",
        "abstract": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.",
        "url": "http://arxiv.org/abs/2506.18819v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18819v1",
        "arxiv_id": "2506.18819v1",
        "authors": [
            "Arjun Mukerji",
            "Michael L. Jackson",
            "Jason Jones",
            "Neil Sanghavi"
        ],
        "submitted": "2025-06-23 16:28:03",
        "source": "arxiv",
        "comment": "24 pages, 2 figures"
    },
    {
        "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
        "abstract": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.",
        "url": "http://arxiv.org/abs/2506.18810v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18810v2",
        "arxiv_id": "2506.18810v2",
        "authors": [
            "Siao Tang",
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "submitted": "2025-06-23 16:20:44",
        "source": "arxiv",
        "comment": "Codes are available at https://github.com/tsa18/ConciseHint"
    },
    {
        "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
        "abstract": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.",
        "url": "http://arxiv.org/abs/2506.18781v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18781v1",
        "arxiv_id": "2506.18781v1",
        "authors": [
            "Zhenru Lin",
            "Jiawen Tao",
            "Yang Yuan",
            "Andrew Chi-Chih Yao"
        ],
        "submitted": "2025-06-23 15:50:21",
        "source": "arxiv",
        "comment": "10 pages, 6 figures"
    },
    {
        "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
        "abstract": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.",
        "url": "http://arxiv.org/abs/2506.18777v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18777v1",
        "arxiv_id": "2506.18777v1",
        "authors": [
            "Jonathan Cook",
            "Silvia Sapora",
            "Arash Ahmadian",
            "Akbir Khan",
            "Tim Rocktaschel",
            "Jakob Foerster",
            "Laura Ruis"
        ],
        "submitted": "2025-06-23 15:45:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Neural Total Variation Distance Estimators for Changepoint Detection in News Data",
        "abstract": "Detecting when public discourse shifts in response to major events is crucial\nfor understanding societal dynamics. Real-world data is high-dimensional,\nsparse, and noisy, making changepoint detection in this domain a challenging\nendeavor. In this paper, we leverage neural networks for changepoint detection\nin news data, introducing a method based on the so-called learning-by-confusion\nscheme, which was originally developed for detecting phase transitions in\nphysical systems. We train classifiers to distinguish between articles from\ndifferent time periods. The resulting classification accuracy is used to\nestimate the total variation distance between underlying content distributions,\nwhere significant distances highlight changepoints. We demonstrate the\neffectiveness of this method on both synthetic datasets and real-world data\nfrom The Guardian newspaper, successfully identifying major historical events\nincluding 9/11, the COVID-19 pandemic, and presidential elections. Our approach\nrequires minimal domain knowledge, can autonomously discover significant shifts\nin public discourse, and yields a quantitative measure of change in content,\nmaking it valuable for journalism, policy analysis, and crisis monitoring.",
        "url": "http://arxiv.org/abs/2506.18764v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18764v1",
        "arxiv_id": "2506.18764v1",
        "authors": [
            "Csaba Zsolnai",
            "Niels Lörch",
            "Julian Arnold"
        ],
        "submitted": "2025-06-23 15:33:30",
        "source": "arxiv",
        "comment": "16 pages, 3 figures"
    },
    {
        "title": "An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify",
        "abstract": "Spotify, a large-scale multimedia platform, attracts over 675 million monthly\nactive users who collectively consume millions of hours of music, podcasts,\naudiobooks, and video content. This diverse content consumption pattern\nintroduces unique challenges for computational advertising, which must\neffectively integrate a variety of ad modalities, including audio, video, and\ndisplay, within a single user experience. Traditional ad recommendation models,\nprimarily designed for foregrounded experiences, often struggle to reconcile\nthe platform's inherent audio-centrality with the demands of optimizing ad\nperformance across multiple formats and modalities. To overcome these\nchallenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a\nnovel framework for optimizing click-through rate (CTR) prediction in both\naudio-centric and multi-modal settings. CAMoE enhances traditional\nmixture-of-experts models by incorporating modality-aware task grouping,\nadaptive loss masking, and deep-cross networks (DCN) to capture complex feature\ninteractions within a multi-modal ad ecosystem. Through extensive ablation\nstudies, we demonstrate that this approach achieves near Pareto-optimal\nperformance across audio, video, and display ad formats, significantly\nimproving AUC-PR compared to conventional single-task and content-based\nmulti-task learning baselines. When deployed at scale on Spotify's ad serving\nplatform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR\nfor audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected\ncost-per-click (eCPC) for audio slots.",
        "url": "http://arxiv.org/abs/2506.18735v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18735v1",
        "arxiv_id": "2506.18735v1",
        "authors": [
            "Shivam Verma",
            "Vivian Chen",
            "Darren Mei"
        ],
        "submitted": "2025-06-23 15:11:43",
        "source": "arxiv",
        "comment": "Accepted at KDD 2025"
    },
    {
        "title": "Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation",
        "abstract": "Emotion Recognition in Conversation (ERC) aims to detect the emotions of\nindividual utterances within a conversation. Generating efficient and\nmodality-specific representations for each utterance remains a significant\nchallenge. Previous studies have proposed various models to integrate features\nextracted using different modality-specific encoders. However, they neglect the\nvarying contributions of modalities to this task and introduce high complexity\nby aligning modalities at the frame level. To address these challenges, we\npropose the Multi-modal Anchor Gated Transformer with Knowledge Distillation\n(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance\ntextual modality representations, while knowledge distillation is utilized to\nstrengthen representations of weaker modalities. Furthermore, we introduce a\nmulti-modal anchor gated transformer to effectively integrate utterance-level\nrepresentations across modalities. Extensive experiments on the IEMOCAP and\nMELD datasets demonstrate the effectiveness of knowledge distillation in\nenhancing modality representations and achieve state-of-the-art performance in\nemotion recognition. Our code is available at:\nhttps://github.com/JieLi-dd/MAGTKD.",
        "url": "http://arxiv.org/abs/2506.18716v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18716v1",
        "arxiv_id": "2506.18716v1",
        "authors": [
            "Jie Li",
            "Shifei Ding",
            "Lili Guo",
            "Xuan Li"
        ],
        "submitted": "2025-06-23 14:53:22",
        "source": "arxiv",
        "comment": "This paper has been accepted by IJCAI2025"
    },
    {
        "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
        "abstract": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
        "url": "http://arxiv.org/abs/2506.18710v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18710v2",
        "arxiv_id": "2506.18710v2",
        "authors": [
            "Maxime Lelièvre",
            "Amy Waldock",
            "Meng Liu",
            "Natalia Valdés Aspillaga",
            "Alasdair Mackintosh",
            "María José Ogando Portela",
            "Jared Lee",
            "Paul Atherton",
            "Robin A. A. Ince",
            "Oliver G. B. Garrod"
        ],
        "submitted": "2025-06-23 14:49:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition",
        "abstract": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate.",
        "url": "http://arxiv.org/abs/2506.18703v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18703v1",
        "arxiv_id": "2506.18703v1",
        "authors": [
            "Christian Huber",
            "Alexander Waibel"
        ],
        "submitted": "2025-06-23 14:42:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?",
        "abstract": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
        "url": "http://arxiv.org/abs/2506.18674v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18674v1",
        "arxiv_id": "2506.18674v1",
        "authors": [
            "Raquel Ferrando",
            "Javier Conde",
            "Gonzalo Martínez",
            "Pedro Reviriego"
        ],
        "submitted": "2025-06-23 14:18:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation",
        "abstract": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
        "url": "http://arxiv.org/abs/2506.18670v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18670v1",
        "arxiv_id": "2506.18670v1",
        "authors": [
            "Jingming Liu",
            "Yumeng Li",
            "Wei Shi",
            "Yao-Xiang Ding",
            "Hui Su",
            "Kun Zhou"
        ],
        "submitted": "2025-06-23 14:14:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ByteSpan: Information-Driven Subword Tokenisation",
        "abstract": "Recent dynamic tokenisation methods operate directly on bytes and pool their\nlatent representations into patches. This bears similarities to computational\nmodels of word segmentation that determine lexical boundaries using spikes in\nan autoregressive model's prediction error. Inspired by this connection, we\nexplore whether grouping predictable bytes - rather than pooling their\nrepresentations - can yield a useful fixed subword vocabulary. We propose a new\ninformation-driven subword tokeniser, ByteSpan, that uses an external\nbyte-level LM during training to identify contiguous predictable byte sequences\nand group them into subwords. Experiments show that ByteSpan yields efficient\nvocabularies with higher morphological alignment scores than BPE for English.\nMultilingual experiments show similar compression and R\\'enyi efficiency for 25\nlanguages.",
        "url": "http://arxiv.org/abs/2506.18639v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18639v1",
        "arxiv_id": "2506.18639v1",
        "authors": [
            "Zébulon Goriely",
            "Suchir Salhan",
            "Pietro Lesci",
            "Julius Cheng",
            "Paula Buttery"
        ],
        "submitted": "2025-06-23 13:42:00",
        "source": "arxiv",
        "comment": "Accepted to TokShop 2025 (Non-archival)"
    },
    {
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
        "abstract": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
        "url": "http://arxiv.org/abs/2506.18631v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18631v2",
        "arxiv_id": "2506.18631v2",
        "authors": [
            "Chenxing Wei",
            "Jiarui Yu",
            "Ying Tiffany He",
            "Hande Dong",
            "Yao Shu",
            "Fei Yu"
        ],
        "submitted": "2025-06-23 13:36:24",
        "source": "arxiv",
        "comment": "10 pages, 15 figures"
    },
    {
        "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs",
        "abstract": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.",
        "url": "http://arxiv.org/abs/2506.18628v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18628v1",
        "arxiv_id": "2506.18628v1",
        "authors": [
            "Piotr Matys",
            "Jan Eliasz",
            "Konrad Kiełczyński",
            "Mikołaj Langner",
            "Teddy Ferdinan",
            "Jan Kocoń",
            "Przemysław Kazienko"
        ],
        "submitted": "2025-06-23 13:35:05",
        "source": "arxiv",
        "comment": "ICCS 2025 Workshops"
    },
    {
        "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches",
        "abstract": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.",
        "url": "http://arxiv.org/abs/2506.18621v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18621v1",
        "arxiv_id": "2506.18621v1",
        "authors": [
            "Alisa Barkar",
            "Mathieu Chollet",
            "Matthieu Labeau",
            "Beatrice Biancardi",
            "Chloe Clavel"
        ],
        "submitted": "2025-06-23 13:28:33",
        "source": "arxiv",
        "comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables"
    },
    {
        "title": "Semantic similarity estimation for domain specific data using BERT and other techniques",
        "abstract": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.",
        "url": "http://arxiv.org/abs/2506.18602v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18602v1",
        "arxiv_id": "2506.18602v1",
        "authors": [
            "R. Prashanth"
        ],
        "submitted": "2025-06-23 13:03:59",
        "source": "arxiv",
        "comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019"
    },
    {
        "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data leakage\"",
        "abstract": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.",
        "url": "http://arxiv.org/abs/2506.18600v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18600v1",
        "arxiv_id": "2506.18600v1",
        "authors": [
            "Ariel Flint Ashery",
            "Luca Maria Aiello",
            "Andrea Baronchelli"
        ],
        "submitted": "2025-06-23 12:59:34",
        "source": "arxiv",
        "comment": "Reply to arXiv:2505.23796"
    },
    {
        "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time",
        "abstract": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models.",
        "url": "http://arxiv.org/abs/2506.18598v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18598v1",
        "arxiv_id": "2506.18598v1",
        "authors": [
            "Aviral Gupta",
            "Armaan Sethi",
            "Ameesh Sethi"
        ],
        "submitted": "2025-06-23 12:58:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Airalogy: AI-empowered universal data digitization for research automation",
        "abstract": "Research data are the foundation of Artificial Intelligence (AI)-driven\nscience, yet current AI applications remain limited to a few fields with\nreadily available, well-structured, digitized datasets. Achieving comprehensive\nAI empowerment across multiple disciplines is still out of reach. Present-day\nresearch data collection is often fragmented, lacking unified standards,\ninefficiently managed, and difficult to share. Creating a single platform for\nstandardized data digitization needs to overcome the inherent challenge of\nbalancing between universality (supporting the diverse, ever-evolving needs of\nvarious disciplines) and standardization (enforcing consistent formats to fully\nenable AI). No existing platform accommodates both facets. Building a truly\nmultidisciplinary platform requires integrating scientific domain knowledge\nwith sophisticated computing skills. Researchers often lack the computational\nexpertise to design customized and standardized data recording methods, whereas\nplatform developers rarely grasp the intricate needs of multiple scientific\ndomains. These gaps impede research data standardization and hamper AI-driven\nprogress. In this study, we address these challenges by developing Airalogy\n(https://airalogy.com), the world's first AI- and community-driven platform\nthat balances universality and standardization for digitizing research data\nacross multiple disciplines. Airalogy represents entire research workflows\nusing customizable, standardized data records and offers an advanced AI\nresearch copilot for intelligent Q&A, automated data entry, analysis, and\nresearch automation. Already deployed in laboratories across all four schools\nof Westlake University, Airalogy has the potential to accelerate and automate\nscientific innovation in universities, industry, and the global research\ncommunity-ultimately benefiting humanity as a whole.",
        "url": "http://arxiv.org/abs/2506.18586v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18586v1",
        "arxiv_id": "2506.18586v1",
        "authors": [
            "Zijie Yang",
            "Qiji Zhou",
            "Fang Guo",
            "Sijie Zhang",
            "Yexun Xi",
            "Jinglei Nie",
            "Yudian Zhu",
            "Liping Huang",
            "Chou Wu",
            "Yonghe Xia",
            "Xiaoyu Ma",
            "Yingming Pu",
            "Panzhong Lu",
            "Junshu Pan",
            "Mingtao Chen",
            "Tiannan Guo",
            "Yanmei Dou",
            "Hongyu Chen",
            "Anping Zeng",
            "Jiaxing Huang",
            "Tian Xu",
            "Yue Zhang"
        ],
        "submitted": "2025-06-23 12:43:16",
        "source": "arxiv",
        "comment": "146 pages, 6 figures, 49 supplementary figures"
    },
    {
        "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration",
        "abstract": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.",
        "url": "http://arxiv.org/abs/2506.18582v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18582v1",
        "arxiv_id": "2506.18582v1",
        "authors": [
            "Haoyi Wu",
            "Zhihao Teng",
            "Kewei Tu"
        ],
        "submitted": "2025-06-23 12:35:41",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance",
        "abstract": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.",
        "url": "http://arxiv.org/abs/2506.18576v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18576v1",
        "arxiv_id": "2506.18576v1",
        "authors": [
            "Matteo Melis",
            "Gabriella Lapesa",
            "Dennis Assenmacher"
        ],
        "submitted": "2025-06-23 12:28:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models",
        "abstract": "Click models are a well-established for modeling user interactions with web\ninterfaces. Previous work has mainly focused on traditional single-list web\nsearch settings; this includes existing surveys that introduced categorizations\nbased on the first generation of probabilistic graphical model (PGM) click\nmodels that have become standard. However, these categorizations have become\noutdated, as their conceptualizations are unable to meaningfully compare PGM\nwith neural network (NN) click models nor generalize to newer interfaces, such\nas carousel interfaces. We argue that this outdated view fails to adequately\nexplain the fundamentals of click model designs, thus hindering the development\nof novel click models.\n  This work reconsiders what should be the fundamental concepts in click model\ndesign, grounding them - unlike previous approaches - in their mathematical\nproperties. We propose three fundamental key-design choices that explain what\nstatistical patterns a click model can capture, and thus indirectly, what user\nbehaviors they can capture. Based on these choices, we create a novel click\nmodel taxonomy that allows a meaningful comparison of all existing click\nmodels; this is the first taxonomy of single-list, grid and carousel click\nmodels that includes PGMs and NNs. Finally, we show how our conceptualization\nprovides a foundation for future click model design by an example derivation of\na novel design for carousel interfaces.",
        "url": "http://arxiv.org/abs/2506.18548v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18548v1",
        "arxiv_id": "2506.18548v1",
        "authors": [
            "Jingwei Kang",
            "Maarten de Rijke",
            "Santiago de Leon-Martinez",
            "Harrie Oosterhuis"
        ],
        "submitted": "2025-06-23 11:57:11",
        "source": "arxiv",
        "comment": "Accepted by ICTIR 2025"
    },
    {
        "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking",
        "abstract": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.",
        "url": "http://arxiv.org/abs/2506.18535v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18535v1",
        "arxiv_id": "2506.18535v1",
        "authors": [
            "Manu Pande",
            "Shahil Kumar",
            "Anay Yatin Damle"
        ],
        "submitted": "2025-06-23 11:46:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "End-to-End Spoken Grammatical Error Correction",
        "abstract": "Grammatical Error Correction (GEC) and feedback play a vital role in\nsupporting second language (L2) learners, educators, and examiners. While\nwritten GEC is well-established, spoken GEC (SGEC), aiming to provide feedback\nbased on learners' speech, poses additional challenges due to disfluencies,\ntranscription errors, and the lack of structured input. SGEC systems typically\nfollow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),\ndisfluency detection, and GEC, making them vulnerable to error propagation\nacross modules. This work examines an End-to-End (E2E) framework for SGEC and\nfeedback generation, highlighting challenges and possible solutions when\ndeveloping these systems. Cascaded, partial-cascaded and E2E architectures are\ncompared, all built on the Whisper foundation model. A challenge for E2E\nsystems is the scarcity of GEC labeled spoken data. To address this, an\nautomatic pseudo-labeling framework is examined, increasing the training data\nfrom 77 to over 2500 hours. To improve the accuracy of the SGEC system,\nadditional contextual information, exploiting the ASR output, is investigated.\nCandidate feedback of their mistakes is an essential step to improving\nperformance. In E2E systems the SGEC output must be compared with an estimate\nof the fluent transcription to obtain the feedback. To improve the precision of\nthis feedback, a novel reference alignment process is proposed that aims to\nremove hypothesised edits that results from fluent transcription errors.\nFinally, these approaches are combined with an edit confidence estimation\napproach, to exclude low-confidence edits. Experiments on the in-house\nLinguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)\ncorpus show that the proposed approaches significantly boost E2E SGEC\nperformance.",
        "url": "http://arxiv.org/abs/2506.18532v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18532v1",
        "arxiv_id": "2506.18532v1",
        "authors": [
            "Mengjie Qian",
            "Rao Ma",
            "Stefano Bannò",
            "Mark J. F. Gales",
            "Kate M. Knill"
        ],
        "submitted": "2025-06-23 11:40:04",
        "source": "arxiv",
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts",
        "abstract": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.",
        "url": "http://arxiv.org/abs/2506.18510v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18510v1",
        "arxiv_id": "2506.18510v1",
        "authors": [
            "Duygu Altinok"
        ],
        "submitted": "2025-06-23 11:04:20",
        "source": "arxiv",
        "comment": "Accepted to INTERSPEECH2025 workshop DISS2025"
    },
    {
        "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance",
        "abstract": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.",
        "url": "http://arxiv.org/abs/2506.18501v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18501v1",
        "arxiv_id": "2506.18501v1",
        "authors": [
            "Wael Etaiwi",
            "Bushra Alhijawi"
        ],
        "submitted": "2025-06-23 10:52:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI-Generated Song Detection via Lyrics Transcripts",
        "abstract": "The recent rise in capabilities of AI-based music generation tools has\ncreated an upheaval in the music industry, necessitating the creation of\naccurate methods to detect such AI-generated content. This can be done using\naudio-based detectors; however, it has been shown that they struggle to\ngeneralize to unseen generators or when the audio is perturbed. Furthermore,\nrecent work used accurate and cleanly formatted lyrics sourced from a lyrics\nprovider database to detect AI-generated music. However, in practice, such\nperfect lyrics are not available (only the audio is); this leaves a substantial\ngap in applicability in real-life use cases. In this work, we instead propose\nsolving this gap by transcribing songs using general automatic speech\nrecognition (ASR) models. We do this using several detectors. The results on\ndiverse, multi-genre, and multi-lingual lyrics show generally strong detection\nperformance across languages and genres, particularly for our best-performing\nmodel using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that\nour method is more robust than state-of-the-art audio-based ones when the audio\nis perturbed in different ways and when evaluated on different music\ngenerators. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection.",
        "url": "http://arxiv.org/abs/2506.18488v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18488v1",
        "arxiv_id": "2506.18488v1",
        "authors": [
            "Markus Frohmann",
            "Elena V. Epure",
            "Gabriel Meseguer-Brocal",
            "Markus Schedl",
            "Romain Hennequin"
        ],
        "submitted": "2025-06-23 10:42:50",
        "source": "arxiv",
        "comment": "Accepted to ISMIR 2025"
    },
    {
        "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.",
        "url": "http://arxiv.org/abs/2506.18485v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18485v1",
        "arxiv_id": "2506.18485v1",
        "authors": [
            "Junjie Zhang",
            "Guozheng Ma",
            "Shunyu Liu",
            "Haoyu Wang",
            "Jiaxing Huang",
            "Ting-En Lin",
            "Fei Huang",
            "Yongbin Li",
            "Dacheng Tao"
        ],
        "submitted": "2025-06-23 10:37:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLMs on a Budget? Say HOLA",
        "abstract": "Running Large Language Models (LLMs) on edge devices is constrained by high\ncompute and memory demands posing a barrier for real-time applications in\nsectors like healthcare, education, and embedded systems. Current solutions\nsuch as quantization, pruning, and retrieval-augmented generation (RAG) offer\nonly partial optimizations and often compromise on speed or accuracy. We\nintroduce HOLA, an end-to-end optimization framework for efficient LLM\ndeployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)\nfor faster inference without quality loss. Externally, AdaComp-RAG adjusts\nretrieval complexity based on context needs. Together with LoBi, which blends\nstructured pruning (LoRA) and quantization, HOLA delivers significant gains:\n17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge\ndevices like Jetson Nano--proving both scalable and production-ready.",
        "url": "http://arxiv.org/abs/2506.18952v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18952v1",
        "arxiv_id": "2506.18952v1",
        "authors": [
            "Zohaib Hasan Siddiqui",
            "Jiechao Gao",
            "Ebad Shabbir",
            "Mohammad Anas Azeez",
            "Rafiq Ali",
            "Gautam Siddharth Kashyap",
            "Usman Naseem"
        ],
        "submitted": "2025-06-23 10:20:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models",
        "abstract": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].",
        "url": "http://arxiv.org/abs/2506.18421v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18421v1",
        "arxiv_id": "2506.18421v1",
        "authors": [
            "Ce Li",
            "Xiaofan Liu",
            "Zhiyan Song",
            "Ce Chi",
            "Chen Zhao",
            "Jingjing Yang",
            "Zhendong Wang",
            "Kexin Yang",
            "Boshen Shi",
            "Xing Wang",
            "Chao Deng",
            "Junlan Feng"
        ],
        "submitted": "2025-06-23 09:02:04",
        "source": "arxiv",
        "comment": "Benmark report v1.0"
    },
    {
        "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres",
        "abstract": "Lemmatization is crucial for NLP tasks in morphologically rich languages with\nambiguous orthography like Arabic, but existing tools face challenges due to\ninconsistent standards and limited genre coverage. This paper introduces two\nnovel approaches that frame lemmatization as classification into a\nLemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic\nclustering. We also present a new Arabic lemmatization test set covering\ndiverse genres, standardized alongside existing datasets. We evaluate character\nlevel sequence-to-sequence models, which perform competitively and offer\ncomplementary value, but are limited to lemma prediction (not LPG) and prone to\nhallucinating implausible forms. Our results show that classification and\nclustering yield more robust, interpretable outputs, setting new benchmarks for\nArabic lemmatization.",
        "url": "http://arxiv.org/abs/2506.18399v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18399v1",
        "arxiv_id": "2506.18399v1",
        "authors": [
            "Mostafa Saeed",
            "Nizar Habash"
        ],
        "submitted": "2025-06-23 08:34:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics",
        "abstract": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning.",
        "url": "http://arxiv.org/abs/2506.18387v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18387v1",
        "arxiv_id": "2506.18387v1",
        "authors": [
            "Yousang Cho",
            "Key-Sun Choi"
        ],
        "submitted": "2025-06-23 08:19:21",
        "source": "arxiv",
        "comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy,\n  July 17, 2025"
    },
    {
        "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching",
        "abstract": "With the expansion of business scales and scopes on online platforms,\nmulti-scenario matching has become a mainstream solution to reduce maintenance\ncosts and alleviate data sparsity. The key to effective multi-scenario\nrecommendation lies in capturing both user preferences shared across all\nscenarios and scenario-aware preferences specific to each scenario. However,\nexisting methods often overlook user-specific modeling, limiting the generation\nof personalized user representations. To address this, we propose PERSCEN, an\ninnovative approach that incorporates user-specific modeling into\nmulti-scenario matching. PERSCEN constructs a user-specific feature graph based\non user characteristics and employs a lightweight graph neural network to\ncapture higher-order interaction patterns, enabling personalized extraction of\npreferences shared across scenarios. Additionally, we leverage vector\nquantization techniques to distil scenario-aware preferences from users'\nbehavior sequence within individual scenarios, facilitating user-specific and\nscenario-aware preference modeling. To enhance efficient and flexible\ninformation transfer, we introduce a progressive scenario-aware gated linear\nunit that allows fine-grained, low-latency fusion. Extensive experiments\ndemonstrate that PERSCEN outperforms existing methods. Further efficiency\nanalysis confirms that PERSCEN effectively balances performance with\ncomputational cost, ensuring its practicality for real-world industrial\nsystems.",
        "url": "http://arxiv.org/abs/2506.18382v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18382v1",
        "arxiv_id": "2506.18382v1",
        "authors": [
            "Haotong Du",
            "Yaqing Wang",
            "Fei Xiong",
            "Lei Shao",
            "Ming Liu",
            "Hao Gu",
            "Quanming Yao",
            "Zhen Wang"
        ],
        "submitted": "2025-06-23 08:15:16",
        "source": "arxiv",
        "comment": "Accepted by KDD 2025"
    },
    {
        "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation",
        "abstract": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
        "url": "http://arxiv.org/abs/2506.18349v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18349v1",
        "arxiv_id": "2506.18349v1",
        "authors": [
            "Zichong Li",
            "Chen Liang",
            "Zixuan Zhang",
            "Ilgee Hong",
            "Young Jin Kim",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "submitted": "2025-06-23 07:15:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs",
        "abstract": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.",
        "url": "http://arxiv.org/abs/2506.18341v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18341v1",
        "arxiv_id": "2506.18341v1",
        "authors": [
            "Kang Chen",
            "Mengdi Zhang",
            "Yixin Cao"
        ],
        "submitted": "2025-06-23 06:47:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance",
        "abstract": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.",
        "url": "http://arxiv.org/abs/2506.18337v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18337v1",
        "arxiv_id": "2506.18337v1",
        "authors": [
            "Syed Mekael Wasti",
            "Shou-Yi Hung",
            "Christopher Collins",
            "En-Shiun Annie Lee"
        ],
        "submitted": "2025-06-23 06:38:49",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning",
        "abstract": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
        "url": "http://arxiv.org/abs/2506.18330v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18330v1",
        "arxiv_id": "2506.18330v1",
        "authors": [
            "Lixin Wu",
            "Na Cai",
            "Qiao Cheng",
            "Jiachen Wang",
            "Yitao Duan"
        ],
        "submitted": "2025-06-23 06:23:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems",
        "abstract": "Recommendation systems play a crucial role in our daily lives by impacting\nuser experience across various domains, including e-commerce, job\nadvertisements, entertainment, etc. Given the vital role of such systems in our\nlives, practitioners must ensure they do not produce unfair and imbalanced\nrecommendations. Previous work addressing bias in recommendations overlooked\nbias in certain item categories, potentially leaving some biases unaddressed.\nAdditionally, most previous work on fair re-ranking focused on binary-sensitive\nattributes. In this paper, we address these issues by proposing a\nfairness-aware re-ranking approach that helps mitigate bias in different\ncategories of items. This re-ranking approach leverages existing biases to\ncorrect disparities in recommendations across various demographic groups. We\nshow how our approach can mitigate bias on multiple sensitive attributes,\nincluding gender, age, and occupation. We experimented on three real-world\ndatasets to evaluate the effectiveness of our re-ranking scheme in mitigating\nbias in recommendations. Our results show how this approach helps mitigate\nsocial bias with little to no degradation in performance.",
        "url": "http://arxiv.org/abs/2506.18327v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18327v1",
        "arxiv_id": "2506.18327v1",
        "authors": [
            "Tahsin Alamgir Kheya",
            "Mohamed Reda Bouadjenek",
            "Sunil Aryal"
        ],
        "submitted": "2025-06-23 06:19:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Entity Aware Machine Translation with Multi-task Learning",
        "abstract": "Entity-aware machine translation (EAMT) is a complicated task in natural\nlanguage processing due to not only the shortage of translation data related to\nthe entities needed to translate but also the complexity in the context needed\nto process while translating those entities. In this paper, we propose a method\nthat applies multi-task learning to optimize the performance of the two\nsubtasks named entity recognition and machine translation, which improves the\nfinal performance of the Entity-aware machine translation task. The result and\nanalysis are performed on the dataset provided by the organizer of Task 2 of\nthe SemEval 2025 competition.",
        "url": "http://arxiv.org/abs/2506.18318v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18318v1",
        "arxiv_id": "2506.18318v1",
        "authors": [
            "An Trieu",
            "Phuong Nguyen",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 06:05:46",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2025"
    },
    {
        "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval",
        "abstract": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.",
        "url": "http://arxiv.org/abs/2506.18316v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18316v1",
        "arxiv_id": "2506.18316v1",
        "authors": [
            "Trieu An",
            "Long Nguyen",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 06:01:21",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2025"
    },
    {
        "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction",
        "abstract": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.",
        "url": "http://arxiv.org/abs/2506.18311v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18311v1",
        "arxiv_id": "2506.18311v1",
        "authors": [
            "Hoang-An Trieu",
            "Dinh-Truong Do",
            "Chau Nguyen",
            "Vu Tran",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 05:55:53",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2024"
    },
    {
        "title": "LettinGo: Explore User Profile Generation for Recommendation System",
        "abstract": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
        "url": "http://arxiv.org/abs/2506.18309v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18309v1",
        "arxiv_id": "2506.18309v1",
        "authors": [
            "Lu Wang",
            "Di Zhang",
            "Fangkai Yang",
            "Pu Zhao",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Hao Sun",
            "Qingwei Lin",
            "Weiwei Deng",
            "Dongmei Zhang",
            "Feng Sun",
            "Qi Zhang"
        ],
        "submitted": "2025-06-23 05:51:52",
        "source": "arxiv",
        "comment": "11 pages, 3 figures"
    },
    {
        "title": "Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT",
        "abstract": "Modern information retrieval systems often employ a two-stage pipeline: an\nefficient initial retrieval stage followed by a computationally intensive\nreranking stage. Cross-encoders have shown strong effectiveness for reranking\ndue to their deep analysis of query-document pairs. This paper studies the\nimpact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning\nof cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,\nand ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.\nGTE and ModernBERT support extended context lengths (up to 8192 tokens). We\nevaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set\n(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT\nwith Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,\nwhile MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.\nLion also provides superior GPU efficiency, improving utilization by 2.67% to\n10.33% across models. We analyze performance trends using standard IR metrics\nand discuss the optimizer's impact on training dynamics across architectures.",
        "url": "http://arxiv.org/abs/2506.18297v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18297v1",
        "arxiv_id": "2506.18297v1",
        "authors": [
            "Shahil Kumar",
            "Manu Pande",
            "Anay Yatin Damle"
        ],
        "submitted": "2025-06-23 05:30:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
        "url": "http://arxiv.org/abs/2506.18254v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18254v1",
        "arxiv_id": "2506.18254v1",
        "authors": [
            "Tianyu Yu",
            "Bo Ji",
            "Shouli Wang",
            "Shu Yao",
            "Zefan Wang",
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-06-23 02:56:36",
        "source": "arxiv",
        "comment": "Project Website: https://github.com/openbmb/RLPR"
    },
    {
        "title": "Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models",
        "abstract": "We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)\narchitecture that introduces sequential expert communication within each layer.\nUnlike traditional MoE models, where experts operate independently in parallel,\nCoE processes tokens iteratively across a chain of experts inside a layer. To\nsupport dynamic expert selection across iterations, CoE employs a dedicated\nrouter at each iteration step within a layer. This design allows tokens to\nre-evaluate and select different experts during each iteration, rather than\nbeing statically assigned. As a result, CoE introduces a flexible routing\nmechanism that increases the diversity of expert combinations and enriches the\nmodel's representational capacity. CoE demonstrates improved performance under\nfixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to\n1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling\naxis: depth through expert iteration, which complements conventional\nwidth/depth scaling. For example, using 2x iterations matches the performance\nof 3x expert selections (in width), while reducing memory usage by 17.6-42%\nrelative to other scaling strategies. Our analysis reveals that CoE's benefits\nstem from its iterative residual structure and enhanced expert specialization\nempowered by iterative routing, which together unlock more expressive\nrepresentations. Code is available at https://github.com/ZihanWang314/coe.",
        "url": "http://arxiv.org/abs/2506.18945v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18945v1",
        "arxiv_id": "2506.18945v1",
        "authors": [
            "Zihan Wang",
            "Rui Pan",
            "Jiarui Yao",
            "Robert Csordas",
            "Linjie Li",
            "Lu Yin",
            "Jiajun Wu",
            "Tong Zhang",
            "Manling Li",
            "Shiwei Liu"
        ],
        "submitted": "2025-06-23 02:15:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model",
        "abstract": "Reinforcement Learning (RL)-based post-training has significantly advanced\nthe complex reasoning capabilities of language models, fostering sophisticated\nself-reflection processes. However, this ``slow thinking'' paradigm presents a\ncritical challenge to reasoning efficiency: models may expend excessive\ncomputation on simple questions and shift reasoning prematurely for complex\nones. Previous mechanisms typically rely on static length budgets or predefined\nrules, lacking the adaptability for varying question complexities and models'\nevolving capabilities. To this end, we propose AdapThink, an adaptive\npost-training framework designed to induce more efficient thinking while\nmaintaining the performance of reasoning language models. Specifically,\nAdapThink incorporates two key mechanisms: 1) A group-relative reward function\nthat leverages model confidence and response's characteristic to dynamically\nadjust the preference of reflection-related transition words without resorting\nto a fixed length preference. 2) A diversity-aware sampling mechanism that\nbalances the training group's solution accuracy with reasoning diversity via an\nentropy-guided score. Experiments on several mathematical reasoning datasets\nwith DeepSeek-distilled models demonstrate AdapThink's advantages in enabling\nadaptive reasoning patterns and mitigating the inefficiencies.",
        "url": "http://arxiv.org/abs/2506.18237v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18237v1",
        "arxiv_id": "2506.18237v1",
        "authors": [
            "Xu Wan",
            "Wei Wang",
            "Wenyue Xu",
            "Wotao Yin",
            "Jie Song",
            "Mingyang Sun"
        ],
        "submitted": "2025-06-23 02:06:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Shrinking the Generation-Verification Gap with Weak Verifiers",
        "abstract": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores.",
        "url": "http://arxiv.org/abs/2506.18203v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18203v1",
        "arxiv_id": "2506.18203v1",
        "authors": [
            "Jon Saad-Falcon",
            "E. Kelly Buchanan",
            "Mayee F. Chen",
            "Tzu-Heng Huang",
            "Brendan McLaughlin",
            "Tanvir Bhathal",
            "Shang Zhu",
            "Ben Athiwaratkun",
            "Frederic Sala",
            "Scott Linderman",
            "Azalia Mirhoseini",
            "Christopher Ré"
        ],
        "submitted": "2025-06-22 23:38:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications",
        "abstract": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.",
        "url": "http://arxiv.org/abs/2506.18201v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18201v1",
        "arxiv_id": "2506.18201v1",
        "authors": [
            "Bushra Asseri",
            "Estabraq Abdelaziz",
            "Maha Al Mogren",
            "Tayef Alhefdhi",
            "Areej Al-Wabil"
        ],
        "submitted": "2025-06-22 23:20:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
        "abstract": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.",
        "url": "http://arxiv.org/abs/2506.18199v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18199v1",
        "arxiv_id": "2506.18199v1",
        "authors": [
            "Bushra Asseri",
            "Estabrag Abdelaziz",
            "Areej Al-Wabil"
        ],
        "submitted": "2025-06-22 23:15:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers",
        "abstract": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,\nspecifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).\nTask 4 focused on detecting mentions of insomnia in clinical notes, while Task\n5 addressed the extraction of food safety events from news articles. We\nparticipated in all subtasks and report key findings across them, with\nparticular emphasis on Task 5 Subtask 1, where our system achieved strong\nperformance-securing first place with an F1 score of 0.958 on the test set. To\nattain this result, we employed encoder-based models (e.g., RoBERTa), alongside\nGPT-4 for data augmentation. This paper outlines our approach, including\npreprocessing, model architecture, and subtask-specific adaptations",
        "url": "http://arxiv.org/abs/2506.18185v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18185v1",
        "arxiv_id": "2506.18185v1",
        "authors": [
            "Zihan Liang",
            "Ziwen Pan",
            "Sumon Kanti Dey",
            "Azra Ismail"
        ],
        "submitted": "2025-06-22 21:56:59",
        "source": "arxiv",
        "comment": "In the Proceedings of the 10th Social Media Mining for Health and\n  Health Real-World Data Workshop and Shared Tasks, co-located with AAAI ICWSM\n  2025"
    },
    {
        "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?",
        "abstract": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.",
        "url": "http://arxiv.org/abs/2506.18183v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18183v1",
        "arxiv_id": "2506.18183v1",
        "authors": [
            "Zhiting Mei",
            "Christina Zhang",
            "Tenny Yin",
            "Justin Lidard",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "submitted": "2025-06-22 21:46:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QuranMorph: Morphologically Annotated Quranic Corpus",
        "abstract": "We present the QuranMorph corpus, a morphologically annotated corpus for the\nQuran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and\ntagged with its part-of-speech by three expert linguists. The lemmatization\nprocess utilized lemmas from Qabas, an Arabic lexicographic database linked\nwith 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging\nwas performed using the fine-grained SAMA/Qabas tagset, which encompasses 40\ntags. As shown in this paper, this rich lemmatization and POS tagset enabled\nthe QuranMorph corpus to be inter-linked with many linguistic resources. The\ncorpus is open-source and publicly available as part of the SinaLab resources\nat (https://sina.birzeit.edu/quran)",
        "url": "http://arxiv.org/abs/2506.18148v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18148v1",
        "arxiv_id": "2506.18148v1",
        "authors": [
            "Diyam Akra",
            "Tymaa Hammouda",
            "Mustafa Jarrar"
        ],
        "submitted": "2025-06-22 19:34:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models",
        "abstract": "We identify semantically coherent, context-consistent network components in\nlarge language models (LLMs) using coactivation of sparse autoencoder (SAE)\nfeatures collected from just a handful of prompts. Focusing on country-relation\ntasks, we show that ablating semantic components for countries and relations\nchanges model outputs in predictable ways, while amplifying these components\ninduces counterfactual responses. Notably, composing relation and country\ncomponents yields compound counterfactual outputs. We find that, whereas most\ncountry components emerge from the very first layer, the more abstract relation\ncomponents are concentrated in later layers. Furthermore, within relation\ncomponents themselves, nodes from later layers tend to have a stronger causal\nimpact on model outputs. Overall, these findings suggest a modular organization\nof knowledge within LLMs and advance methods for efficient, targeted model\nmanipulation.",
        "url": "http://arxiv.org/abs/2506.18141v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18141v1",
        "arxiv_id": "2506.18141v1",
        "authors": [
            "Ruixuan Deng",
            "Xiaoyang Hu",
            "Miles Gilberti",
            "Shane Storks",
            "Aman Taxali",
            "Mike Angstadt",
            "Chandra Sripada",
            "Joyce Chai"
        ],
        "submitted": "2025-06-22 19:01:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging",
        "abstract": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.",
        "url": "http://arxiv.org/abs/2506.18135v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18135v1",
        "arxiv_id": "2506.18135v1",
        "authors": [
            "Zijun Chen",
            "Zhanpeng Zhou",
            "Bo Zhang",
            "Weinan Zhang",
            "Xi Sun",
            "Junchi Yan"
        ],
        "submitted": "2025-06-22 18:38:41",
        "source": "arxiv",
        "comment": "preprint, accepted at IJCNN2025"
    },
    {
        "title": "$φ^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models",
        "abstract": "We identify a critical vulnerability in autoregressive transformer language\nmodels where the em dash token induces recursive semantic drift, leading to\nclause boundary hallucination and embedding space entanglement. Through formal\nanalysis of token-level perturbations in semantic lattices, we demonstrate that\nem dash insertion fundamentally alters the model's latent representations,\ncausing compounding errors in long-form generation. We propose a novel solution\ncombining symbolic clause purification via the phi-infinity operator with\ntargeted embedding matrix realignment. Our approach enables total suppression\nof problematic tokens without requiring model retraining, while preserving\nsemantic coherence through fixed-point convergence guarantees. Experimental\nvalidation shows significant improvements in generation consistency and topic\nmaintenance. This work establishes a general framework for identifying and\nmitigating token-level vulnerabilities in foundation models, with immediate\nimplications for AI safety, model alignment, and robust deployment of large\nlanguage models in production environments. The methodology extends beyond\npunctuation to address broader classes of recursive instabilities in neural\ntext generation systems.",
        "url": "http://arxiv.org/abs/2506.18129v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18129v1",
        "arxiv_id": "2506.18129v1",
        "authors": [
            "Bugra Kilictas",
            "Faruk Alpay"
        ],
        "submitted": "2025-06-22 18:27:39",
        "source": "arxiv",
        "comment": "16 pages, 3 figures"
    },
    {
        "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English",
        "abstract": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.",
        "url": "http://arxiv.org/abs/2506.18120v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18120v1",
        "arxiv_id": "2506.18120v1",
        "authors": [
            "Tom S Juzek"
        ],
        "submitted": "2025-06-22 18:03:49",
        "source": "arxiv",
        "comment": "Accepted and published at LREC-COLING 2024. 8 pages, 3 figures.\n  Licensed under CC BY-NC-SA 4.0"
    },
    {
        "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives",
        "abstract": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.",
        "url": "http://arxiv.org/abs/2506.18116v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18116v1",
        "arxiv_id": "2506.18116v1",
        "authors": [
            "Batool Haider",
            "Atmika Gorti",
            "Aman Chadha",
            "Manas Gaur"
        ],
        "submitted": "2025-06-22 18:00:16",
        "source": "arxiv",
        "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)"
    }
]