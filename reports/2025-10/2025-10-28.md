# Daily Papers Report - 2025-10-28

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. LimRank: Less is More for Reasoning-Intensive Information Reranking

- **LLM Score**: 9
- **Keyword Score**: 10
- **Authors**: Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan
- **URL**: <http://arxiv.org/abs/2510.23544v1>
- **Submitted**: 2025-10-27 17:19:37
- **Comment**: EMNLP 2025 Main (Short)
- **Topic Keywords**: ranking, rerank, retrieval, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on adapting Large Language Models (LLMs) for information reranking tasks and the use of synthetic data to improve model performance aligns well with your interests. The evaluation of the model on challenging benchmarks and its application to scientific literature search also resonates with your research themes.

#### Abstract
> Existing approaches typically rely on large-scale fine-tuning to adapt LLMs
for information reranking tasks, which is computationally expensive. In this
work, we demonstrate that modern LLMs can be effectively adapted using only
minimal, high-quality supervision. To enable this, we design
LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating
diverse, challenging, and realistic reranking examples. Using this synthetic
data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two
challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and
FollowIR for instruction-following retrieval. Our experiments demonstrate that
LIMRANK achieves competitive performance, while being trained on less than 5%
of the data typically used in prior work. Further ablation studies demonstrate
the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization
capabilities of LIMRANK across downstream tasks, including scientific
literature search and retrieval-augmented generation for knowledge-intensive
problem solving.

---

### 2. Improving Product Search Relevance with EAR-MP: A Solution for the CIKM 2025 AnalytiCup

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: JaeEun Lim, Soomin Kim, Jaeyong Seo, Iori Ono, Qimu Ran, Jae-woong Lee
- **URL**: <http://arxiv.org/abs/2510.23018v1>
- **Submitted**: 2025-10-27 05:32:13
- **Topic Keywords**: query, queries, relevance, commerce, e-commerce, search, cikm
- **Reason**: This paper is highly relevant to the field of Information Retrieval, particularly in the context of e-commerce search. The authors' focus on query understanding, relevance optimization, and robustness in multilingual environments aligns with the user's interests. The use of deep learning models and task-specific upgrades also resonates with the user's background in ranking models and NLP.

#### Abstract
> Multilingual e-commerce search is challenging due to linguistic diversity and
the noise inherent in user-generated queries. This paper documents the solution
employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two
core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our
approach first normalizes the multilingual dataset by translating all text into
English, then mitigates noise through extensive data cleaning and
normalization. For model training, we build on DeBERTa-v3-large and improve
performance with label smoothing, self-distillation, and dropout. In addition,
we introduce task-specific upgrades, including hierarchical token injection for
QC and a hybrid scoring mechanism for QI. Under constrained compute, our method
achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744
on QI. These findings underscore the importance of systematic data
preprocessing and tailored training strategies for building robust,
resource-efficient multilingual relevance systems.

---

### 3. Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen
- **URL**: <http://arxiv.org/abs/2510.23224v1>
- **Submitted**: 2025-10-27 11:22:28
- **Topic Keywords**: queries, retrieval, search
- **Reason**: This paper presents a novel retrieval framework, PathSearch, which combines vision-language contrastive learning for accurate and flexible retrieval of whole slide images in pathology. While primarily focused on digital pathology, the use of attentive vision-language alignment and fine-grained representations aligns with the user's interest in query understanding and ranking models, making it relevant to the broader field of Information Retrieval.

#### Abstract
> The rapid digitization of histopathology slides has opened up new
possibilities for computational tools in clinical and research workflows. Among
these, content-based slide retrieval stands out, enabling pathologists to
identify morphologically and semantically similar cases, thereby supporting
precise diagnoses, enhancing consistency across observers, and assisting
example-based education. However, effective retrieval of whole slide images
(WSIs) remains challenging due to their gigapixel scale and the difficulty of
capturing subtle semantic differences amid abundant irrelevant content. To
overcome these challenges, we present PathSearch, a retrieval framework that
unifies fine-grained attentive mosaic representations with global-wise slide
embeddings aligned through vision-language contrastive learning. Trained on a
corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained
morphological cues and high-level semantic patterns to enable accurate and
flexible retrieval. The framework supports two key functionalities: (1)
mosaic-based image-to-image retrieval, ensuring accurate and efficient slide
research; and (2) multi-modal retrieval, where text queries can directly
retrieve relevant slides. PathSearch was rigorously evaluated on four public
pathology datasets and three in-house cohorts, covering tasks including
anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,
and grading across diverse organs such as breast, lung, kidney, liver, and
stomach. External results show that PathSearch outperforms traditional
image-to-image retrieval frameworks. A multi-center reader study further
demonstrates that PathSearch improves diagnostic accuracy, boosts confidence,
and enhances inter-observer agreement among pathologists in real clinical
scenarios. These results establish PathSearch as a scalable and generalizable
retrieval solution for digital pathology.

---

### 4. IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Jieyong Kim, Maryam Amirizaniani, Soojin Yoon, Dongha Lee
- **URL**: <http://arxiv.org/abs/2510.23536v1>
- **Submitted**: 2025-10-27 17:12:49
- **Topic Keywords**: retrieval, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of personalized question answering and intent identification. However, the focus on core intent identification and personalized question answering is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Intent identification serves as the foundation for generating appropriate
responses in personalized question answering (PQA). However, existing
benchmarks evaluate only response quality or retrieval performance without
directly measuring intent identification capabilities. This gap is critical
because without understanding which intents users prioritize, systems cannot
generate responses satisfying individual information needs. To address this, we
introduce the concept of core intents: intents users prioritize when selecting
answers to satisfy their information needs. To evaluate these core intents, we
propose IPQA, a benchmark for core Intent identification in Personalized
Question Answering. Since users do not explicitly state their prioritized
intents, we derive core intents from observable behavior patterns in answer
selection, grounded in satisficing theory where users choose answers meeting
their acceptance thresholds. We construct a dataset with various domains
through systematic filtering, LLM-based annotation, and rigorous quality
control combining automated verification with human validation. Experimental
evaluations across state-of-the-art language models reveal that current systems
struggle with core intent identification in personalized contexts. Models fail
to identify core intents from user histories, with performance degrading as
question complexity increases. The code and dataset will be made publicly
available to facilitate future research in this direction.

---

### 5. Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam
- **URL**: <http://arxiv.org/abs/2510.22956v1>
- **Submitted**: 2025-10-27 03:23:25
- **Comment**: Paper accepted at EMNLP 2025
- **Topic Keywords**: queries, ranking, rag, retrieval, rank, emnlp
- **Reason**: This paper focuses on improving the performance of large language models in question-answering tasks over long contexts, using a tagging-augmented generation strategy. While it touches on retrieval and generation, its primary focus is on NLP and language model performance, which is somewhat related to your interests in IR and NLP. However, it doesn't directly address your core research themes of query understanding, ranking models, or user behavior modeling.

#### Abstract
> Recent investigations into effective context lengths of modern flagship large
language models (LLMs) have revealed major limitations in effective question
answering (QA) and reasoning over long and complex contexts for even the
largest and most impressive cadre of models. While approaches like
retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to
mitigate this issue, they are sensitive to chunking, embedding and retrieval
strategies and models, and furthermore, rely on extensive pre-processing,
knowledge acquisition and indexing steps. In this paper, we propose
Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy
that boosts LLM performance in long-context scenarios, without degrading and
altering the integrity and composition of retrieved documents. We validate our
hypothesis by augmenting two challenging and directly relevant
question-answering benchmarks -- NoLima and NovelQA -- and show that tagging
the context or even just adding tag definitions into QA prompts leads to
consistent performance gains over the baseline -- up to 17% for 32K token
contexts, and 2.9% in complex reasoning question-answering for multi-hop
queries requiring knowledge across a wide span of text. Additional details are
available at https://sites.google.com/view/tag-emnlp.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, Yejin Choi
- **URL**: <http://arxiv.org/abs/2510.22954v1>
- **Submitted**: 2025-10-27 03:16:21
- **Comment**: NeurIPS 2025 D&B Paper (Oral); Camera-Ready Version
- **Topic Keywords**: queries, pairwise, ctr, search
- **Reason**: This paper explores the homogenization of language model outputs, which is somewhat related to query understanding and ranking models in information retrieval. However, the focus on language model diversity and human preferences in response to open-ended queries is not directly aligned with the user's core research themes.

#### Abstract
> Language models (LMs) often struggle to generate diverse, human-like creative
content, raising concerns about the long-term homogenization of human thought
through repeated exposure to similar outputs. Yet scalable methods for
evaluating LM output diversity remain limited, especially beyond narrow tasks
such as random number or name generation, or beyond repeated sampling from a
single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse,
real-world, open-ended user queries that admit a wide range of plausible
answers with no single ground truth. We introduce the first comprehensive
taxonomy for characterizing the full spectrum of open-ended prompts posed to
LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that
further breaks down to 17 subcategories. Using Infinity-Chat, we present a
large-scale study of mode collapse in LMs, revealing a pronounced Artificial
Hivemind effect in open-ended generation of LMs, characterized by (1)
intra-model repetition, where a single model consistently generates similar
responses, and more so (2) inter-model homogeneity, where different models
produce strikingly similar outputs. Infinity-Chat also includes 31,250 human
annotations, across absolute ratings and pairwise preferences, with 25
independent human annotations per example. This enables studying collective and
individual-specific human preferences in response to open-ended queries. Our
findings show that LMs, reward models, and LM judges are less well calibrated
to human ratings on model generations that elicit differing idiosyncratic
annotator preferences, despite maintaining comparable overall quality. Overall,
INFINITY-CHAT presents the first large-scale resource for systematically
studying real-world open-ended queries to LMs, revealing critical insights to
guide future research for mitigating long-term AI safety risks posed by the
Artificial Hivemind.

### 7. Quality-Aware Translation Tagging in Multilingual RAG system

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Hoyeon Moon, Byeolhee Kim, Nikhil Verma
- **URL**: <http://arxiv.org/abs/2510.23070v1>
- **Submitted**: 2025-10-27 07:11:01
- **Comment**: EMNLP 2025 MRL Workshop
- **Topic Keywords**: query, rag, retrieval, korea
- **Reason**: The paper discusses a novel approach to translation quality evaluation in multilingual retrieval systems, which is somewhat related to information retrieval and natural language processing. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The focus on translation quality evaluation and its application in low-resource settings is an interesting aspect, but it does not align with the user's primary research themes.

#### Abstract
> Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English
documents and translates them into the query language for low-resource
settings. However, poor translation quality degrades response generation
performance. Existing approaches either assume sufficient translation quality
or utilize the rewriting method, which introduces factual distortion and
hallucinations. To mitigate these problems, we propose Quality-Aware
Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation
quality along three dimensions-semantic equivalence, grammatical accuracy, and
naturalness&fluency-and attach these scores as metadata without altering the
original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines
in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs
ranging from 2.4B to 14B parameters, covering two low-resource languages
(Korean and Finnish) and one high-resource language (Chinese). QTT-RAG
outperforms the baselines by preserving factual integrity while enabling
generator models to make informed decisions based on translation reliability.
This approach allows for effective usage of cross-lingual documents in
low-resource settings with limited native language documents, offering a
practical and robust solution across multilingual domains.

### 8. Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi
- **URL**: <http://arxiv.org/abs/2510.23271v1>
- **Submitted**: 2025-10-27 12:29:27
- **Comment**: 21 pages, 2 figures, 3 tables. Includes appendices on ethical
  guidelines and training framework. Submitted September 04, 2025
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper discusses a specialized Arabic language model, Mubeen, that focuses on deep understanding of Arabic linguistics and cultural heritage. While it touches on user intent understanding and relevance optimization, the primary focus is on Arabic language preservation, which is somewhat related to information retrieval but not directly aligned with the user's core research themes.

#### Abstract
> Mubeen is a proprietary Arabic language model developed by MASARAT SA,
optimized for deep understanding of Arabic linguistics, Islamic studies, and
cultural heritage. Trained on an extensive collection of authentic Arabic
sources significantly expanded by digitizing historical manuscripts via a
proprietary Arabic OCR engine, the model incorporates seminal scholarly works
in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside
thousands of academic theses and peer-reviewed research papers. Conditioned
through a deep linguistic engineering framework, Mubeen masters not just the
meaning but the eloquence of Arabic, enabling precise understanding across
classical texts, contemporary writing, and regional dialects with focus on
comprehending user intent and delivering accurate, contextually relevant
responses. Unlike other Arabic models relying on translated English data that
often fail in intent detection or retrieval-augmented generation (RAG), Mubeen
uses native Arabic sources to ensure cultural authenticity and accuracy. Its
core innovation is the Practical Closure Architecture, designed to solve the
"Utility Gap Crisis" where factually correct answers fail to resolve users'
core needs, forcing them into frustrating cycles of re-prompting. By
prioritizing clarity and decisive guidance, Mubeen transforms from an
information repository into a decisive guide, aligning with Saudi Vision 2030.
The model's architecture combines deep heritage specialization with
multi-disciplinary expert modules, enabling robust performance across both
cultural preservation and general knowledge domains.

### 9. Leveraging Hierarchical Organization for Medical Multi-document Summarization

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang
- **URL**: <http://arxiv.org/abs/2510.23104v1>
- **Submitted**: 2025-10-27 08:18:02
- **Topic Keywords**: relevance, rag
- **Reason**: This paper is somewhat related to your interests in Information Retrieval, specifically in the context of multi-document summarization. However, it focuses on medical text summarization and does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of your research. While it involves natural language processing and deep semantic understanding, the application domain is quite different from your typical focus in e-commerce and other areas.

#### Abstract
> Medical multi-document summarization (MDS) is a complex task that requires
effectively managing cross-document relationships. This paper investigates
whether incorporating hierarchical structures in the inputs of MDS can improve
a model's ability to organize and contextualize information across documents
compared to traditional flat summarization methods. We investigate two ways of
incorporating hierarchical organization across three large language models
(LLMs), and conduct comprehensive evaluations of the resulting summaries using
automated metrics, model-based metrics, and domain expert evaluation of
preference, understandability, clarity, complexity, relevance, coverage,
factuality, and coherence. Our results show that human experts prefer
model-generated summaries over human-written summaries. Hierarchical approaches
generally preserve factuality, coverage, and coherence of information, while
also increasing human preference for summaries. Additionally, we examine
whether simulated judgments from GPT-4 align with human judgments, finding
higher agreement along more objective evaluation facets. Our findings
demonstrate that hierarchical structures can improve the clarity of medical
summaries generated by models while maintaining content coverage, providing a
practical way to improve human preference for generated summaries.

### 10. Think before Recommendation: Autonomous Reasoning-enhanced Recommender

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Xiaoyu Kong, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Jiancan Wu, Xiang Wang
- **URL**: <http://arxiv.org/abs/2510.23077v1>
- **Submitted**: 2025-10-27 07:26:32
- **Comment**: NeurIPS 2025 poster
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper explores the use of large language models for recommender systems, leveraging reasoning capabilities to enhance rating prediction tasks. While it touches on aspects related to information retrieval, such as understanding user interests and item features, its primary focus is on recommender systems, which is somewhat related to your interests in information retrieval, but not a central match.

#### Abstract
> The core task of recommender systems is to learn user preferences from
historical user-item interactions. With the rapid development of large language
models (LLMs), recent research has explored leveraging the reasoning
capabilities of LLMs to enhance rating prediction tasks. However, existing
distillation-based methods suffer from limitations such as the teacher model's
insufficient recommendation capability, costly and static supervision, and
superficial transfer of reasoning ability. To address these issues, this paper
proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm
that abandons the traditional multi-model and multi-stage distillation
approach. Instead, RecZero trains a single LLM through pure RL to autonomously
develop reasoning capabilities for rating prediction. RecZero consists of two
key components: (1) "Think-before-Recommendation" prompt construction, which
employs a structured reasoning template to guide the model in step-wise
analysis of user interests, item features, and user-item compatibility; and (2)
rule-based reward modeling, which adopts group relative policy optimization
(GRPO) to compute rewards for reasoning trajectories and optimize the LLM.
Additionally, the paper explores a hybrid paradigm, RecOne, which combines
supervised fine-tuning with RL, initializing the model with cold-start
reasoning samples and further optimizing it with RL. Experimental results
demonstrate that RecZero and RecOne significantly outperform existing baseline
methods on multiple benchmark datasets, validating the superiority of the RL
paradigm in achieving autonomous reasoning-enhanced recommender systems.

### 11. BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Pengjun Xie, Jingren Zhou, Yong Jiang
- **URL**: <http://arxiv.org/abs/2510.23458v2>
- **Submitted**: 2025-10-27 15:58:51
- **Comment**: 25 pages
- **Topic Keywords**: rag, search
- **Reason**: This paper explores the use of confidence scores in LLM-based search agents, which is related to query understanding and ranking models. However, the focus on verbalized confidence scores and test-time scaling is somewhat tangential to the user's core research themes of information retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Confidence in LLMs is a useful indicator of model uncertainty and answer
reliability. Existing work mainly focused on single-turn scenarios, while
research on confidence in complex multi-turn interactions is limited. In this
paper, we investigate whether LLM-based search agents have the ability to
communicate their own confidence through verbalized confidence scores after
long sequences of actions, a significantly more challenging task compared to
outputting confidence in a single interaction. Experimenting on open-source
agentic models, we first find that models exhibit much higher task accuracy at
high confidence while having near-zero accuracy when confidence is low. Based
on this observation, we propose Test-Time Scaling (TTS) methods that use
confidence scores to determine answer quality, encourage the model to try again
until reaching a satisfactory confidence level. Results show that our proposed
methods significantly reduce token consumption while demonstrating competitive
performance compared to baseline fixed budget TTS methods.

### 12. Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik
- **URL**: <http://arxiv.org/abs/2510.23334v1>
- **Submitted**: 2025-10-27 13:48:59
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on Large Language Models (LLMs) and introduces a novel search strategy called AdaSearch. While it touches on aspects of search and optimization, it is primarily concerned with LLM alignment and sequential decoding, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the paper's focus on LLMs and NLP tasks, such as harmlessness generation and controlled sentiment generation, is not a central match for the user's research themes.

#### Abstract
> LLM alignment remains a critical challenge. Inference-time methods provide a
flexible alternative to fine-tuning, but their uniform computational effort
often yields suboptimal alignment. We hypothesize that for many alignment
tasks, the initial tokens of a response are disproportionately more critical.
To leverage this principle, we introduce AdaSearch, a novel blockwise search
strategy. It adaptively allocates a fixed computational budget using a sampling
schedule, focusing search effort on these critical tokens. We apply AdaSearch
to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our
comprehensive evaluation across eight LLMs demonstrates that AdaSearch
outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates
improve by over 10% for harmlessness generation, controlled sentiment
generation, and for mathematical reasoning tasks relative to Best-of-N.

### 13. Knocking-Heads Attention

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li
- **URL**: <http://arxiv.org/abs/2510.23052v1>
- **Submitted**: 2025-10-27 06:28:58
- **Topic Keywords**: query
- **Reason**: This paper proposes a novel attention mechanism called Knocking-Heads Attention, which enables cross-head feature-level interactions. While it's related to Natural Language Processing (NLP) and attention mechanisms, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on improving attention mechanisms in large language models is somewhat relevant to IR, but it's not a central match.

#### Abstract
> Multi-head attention (MHA) has become the cornerstone of modern large
language models, enhancing representational capacity through parallel attention
heads. However, increasing the number of heads inherently weakens individual
head capacity, and existing attention mechanisms - whether standard MHA or its
variants like grouped-query attention (GQA) and grouped-tied attention (GTA) -
simply concatenate outputs from isolated heads without strong interaction. To
address this limitation, we propose knocking-heads attention (KHA), which
enables attention heads to "knock" on each other - facilitating cross-head
feature-level interactions before the scaled dot-product attention. This is
achieved by applying a shared, diagonally-initialized projection matrix across
all heads. The diagonal initialization preserves head-specific specialization
at the start of training while allowing the model to progressively learn
integrated cross-head representations. KHA adds only minimal parameters and
FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention
variants. We validate KHA by training a 6.1B parameter MoE model (1.01B
activated) on 1T high-quality tokens. Compared to baseline attention
mechanisms, KHA brings superior and more stable training dynamics, achieving
better performance across downstream tasks.

### 14. ReCode: Unify Plan and Action for Universal Granularity Control

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu
- **URL**: <http://arxiv.org/abs/2510.23564v2>
- **Submitted**: 2025-10-27 17:35:15
- **Topic Keywords**: rag
- **Reason**: This paper proposes a novel paradigm for unifying planning and action in Large Language Model-based agents, which is somewhat related to information retrieval and search technologies. However, the focus is on decision-making processes and granularity control, which is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Real-world tasks require decisions at varying granularities, and humans excel
at this by leveraging a unified cognitive representation where planning is
fundamentally understood as a high-level form of action. However, current Large
Language Model (LLM)-based agents lack this crucial capability to operate
fluidly across decision granularities. This limitation stems from existing
paradigms that enforce a rigid separation between high-level planning and
low-level action, which impairs dynamic adaptability and limits generalization.
We propose ReCode (Recursive Code Generation), a novel paradigm that addresses
this limitation by unifying planning and action within a single code
representation. In this representation, ReCode treats high-level plans as
abstract placeholder functions, which the agent then recursively decomposes
into finer-grained sub-functions until reaching primitive actions. This
recursive approach dissolves the rigid boundary between plan and action,
enabling the agent to dynamically control its decision granularity.
Furthermore, the recursive structure inherently generates rich,
multi-granularity training data, enabling models to learn hierarchical
decision-making processes. Extensive experiments show ReCode significantly
surpasses advanced baselines in inference performance and demonstrates
exceptional data efficiency in training, validating our core insight that
unifying planning and action through recursive code generation is a powerful
and effective approach to achieving universal granularity control. The code is
available at https://github.com/FoundationAgents/ReCode.

### 15. Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea
- **URL**: <http://arxiv.org/abs/2510.23464v1>
- **Submitted**: 2025-10-27 16:03:20
- **Topic Keywords**: rag
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and data mining, but it focuses on a specific application in the financial domain, which is not the user's primary area of interest. The paper's use of large language models and stance detection is relevant to the user's interests in deep semantic understanding, but the context is not directly related to information retrieval or search technologies.

#### Abstract
> Financial narratives from U.S. Securities and Exchange Commission (SEC)
filing reports and quarterly earnings call transcripts (ECTs) are very
important for investors, auditors, and regulators. However, their length,
financial jargon, and nuanced language make fine-grained analysis difficult.
Prior sentiment analysis in the financial domain required a large, expensive
labeled dataset, making the sentence-level stance towards specific financial
targets challenging. In this work, we introduce a sentence-level corpus for
stance detection focused on three core financial metrics: debt, earnings per
share (EPS), and sales. The sentences were extracted from Form 10-K annual
reports and ECTs, and labeled for stance (positive, negative, neutral) using
the advanced ChatGPT-o3-pro model under rigorous human validation. Using this
corpus, we conduct a systematic evaluation of modern large language models
(LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting
strategies. Our results show that few-shot with CoT prompting performs best
compared to supervised baselines, and LLMs' performance varies across the SEC
and ECT datasets. Our findings highlight the practical viability of leveraging
LLMs for target-specific stance in the financial domain without requiring
extensive labeled data.

### 16. LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Teng Lin
- **URL**: <http://arxiv.org/abs/2510.23341v1>
- **Submitted**: 2025-10-27 13:55:13
- **Topic Keywords**: rag
- **Reason**: This paper introduces a novel framework for knowledge graph generation from textual data, leveraging small-scale language models. While it touches on natural language processing and structured NLP tasks, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> The scarcity of high-quality knowledge graphs (KGs) remains a critical
bottleneck for downstream AI applications, as existing extraction methods rely
heavily on error-prone pattern-matching techniques or resource-intensive large
language models (LLMs). While recent tools leverage LLMs to generate KGs, their
computational demands limit accessibility for low-resource environments. Our
paper introduces LightKGG, a novel framework that enables efficient KG
extraction from textual data using small-scale language models (SLMs) through
two key technical innovations: (1) Context-integrated Graph extraction
integrates contextual information with nodes and edges into a unified graph
structure, reducing the reliance on complex semantic processing while
maintaining more key information; (2) Topology-enhanced relationship inference
leverages the inherent topology of the extracted graph to efficiently infer
relationships, enabling relationship discovery without relying on complex
language understanding capabilities of LLMs. By enabling accurate KG
construction with minimal hardware requirements, this work bridges the gap
between automated knowledge extraction and practical deployment scenarios while
introducing scientifically rigorous methods for optimizing SLM efficiency in
structured NLP tasks.

### 17. Rethinking GSPO: The Perplexity-Entropy Equivalence

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chi Liu
- **URL**: <http://arxiv.org/abs/2510.23142v1>
- **Submitted**: 2025-10-27 09:19:10
- **Comment**: 10 pages, 2 figures
- **Topic Keywords**: rag
- **Reason**: The paper discusses a connection between GSPO's importance ratios and information-theoretic quantities, specifically perplexity and entropy. While it touches on concepts related to ranking models and optimization, the focus is on the theoretical aspects of GSPO, which is not a primary area of interest in your research. The connection to information retrieval is indirect, making it somewhat relevant but not a central match.

#### Abstract
> We provide a new perspective on GSPO's length-normalized importance ratios by
establishing their connection to information-theoretic quantities. We show that
GSPO's sequence-level weight $s(\theta) =
(\pi_\theta/\pi_{\theta_{\text{old}}})^{1/|y|}$ can be equivalently expressed
as the inverse perplexity ratio
$\text{PPL}_{\theta_{\text{old}}}/\text{PPL}_\theta$ and as the exponential
cross-entropy change $\exp(\Delta H)$. While the perplexity-entropy
relationship follows from standard definitions, this observation provides a
useful lens for understanding GSPO: the algorithm weights policy gradient
updates by perplexity ratios, offering an information-theoretic interpretation
of the importance weights. This perspective helps explain GSPO's empirical
properties, including log-domain variance reduction through geometric averaging
and stability in training mixture-of-experts models. We validate the
mathematical equivalences and variance predictions through controlled
experiments on mathematical reasoning tasks.

### 18. Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li
- **URL**: <http://arxiv.org/abs/2510.23123v1>
- **Submitted**: 2025-10-27 08:57:24
- **Comment**: Accepted by NeurIPS 2025
- **Topic Keywords**: rank, neurips
- **Reason**: This paper proposes a method for efficient low-rank adaptation in large language models, which is related to information retrieval and natural language processing. However, the focus on parameter-efficient fine-tuning and low-rank matrices is more aligned with deep learning and model optimization, rather than the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). LoRA essentially describes the
projection of an input space into a low-dimensional output space, with the
dimensionality determined by the LoRA rank. In standard LoRA, all input tokens
share the same weights and undergo an identical input-output projection. This
limits LoRA's ability to capture token-specific information due to the inherent
semantic differences among tokens. To address this limitation, we propose
Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts
LoRA weights according to the input token, thereby learning token-wise
input-output projections in an end-to-end manner. Formally, the weights of
TopLoRA can be expressed as $B\Sigma_X A$, where $A$ and $B$ are low-rank
matrices (as in standard LoRA), and $\Sigma_X$ is a diagonal matrix generated
from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA
weights but achieves more granular adaptation by learning token-wise LoRA
weights (i.e., token-wise input-output projections). Extensive experiments
across multiple models and datasets demonstrate that TopLoRA consistently
outperforms LoRA and its variants. The code is available at
https://github.com/Leopold1423/toplora-neurips25.

### 19. MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shihao Cai, Chongming Gao, Haoyan Liu, Wentao Shi, Jianshan Sun, Ruiming Tang, Fuli Feng
- **URL**: <http://arxiv.org/abs/2510.22888v1>
- **Submitted**: 2025-10-27 00:41:07
- **Topic Keywords**: recommend, search
- **Reason**: The paper explores reasoning-based recommendation tasks, leveraging large language models and incorporating item space understanding. While it touches on aspects of query understanding and user behavior modeling, its primary focus is on recommendation systems, which is somewhat related to the user's interests in Information Retrieval. However, the paper's emphasis on recommendation systems and language models limits its alignment with the user's core research themes.

#### Abstract
> The powerful reasoning and generative capabilities of large language models
(LLMs) have inspired researchers to apply them to reasoning-based
recommendation tasks, which require in-depth reasoning about user interests and
the generation of recommended items. However, previous reasoning-based
recommendation methods have typically performed inference within the language
space alone, without incorporating the actual item space. This has led to
over-interpreting user interests and deviating from real items. Towards this
research gap, we propose performing multiple rounds of grounding during
inference to help the LLM better understand the actual item space, which could
ensure that its reasoning remains aligned with real items. Furthermore, we
introduce a user agent that provides feedback during each grounding step,
enabling the LLM to better recognize and adapt to user interests. Comprehensive
experiments conducted on three Amazon review datasets demonstrate the
effectiveness of incorporating multiple groundings and feedback. These findings
underscore the critical importance of reasoning within the actual item space,
rather than being confined to the language space, for recommendation tasks.

### 20. Interpreting and Mitigating Unwanted Uncertainty in LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Tiasa Singha Roy, Ayush Rajesh Jhaveri, Ilias Triantafyllopoulos
- **URL**: <http://arxiv.org/abs/2510.22866v1>
- **Submitted**: 2025-10-26 23:16:59
- **Topic Keywords**: retrieval
- **Reason**: This paper explores the phenomenon of unwanted uncertainty in Large Language Models (LLMs), which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and their interpretability is not directly aligned with the user's primary research interests in IR and Search technologies. The paper's findings on mitigating uncertainty-driven failure modes may have some indirect implications for IR, but it is not a central match.

#### Abstract
> Despite their impressive capabilities, Large Language Models (LLMs) exhibit
unwanted uncertainty, a phenomenon where a model changes a previously correct
answer into an incorrect one when re-prompted. This behavior undermines trust
and poses serious risks in high-stakes domains. In this work, we investigate
the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack
retrieval framework and integrate a Flip-style re-evaluation prompt to simulate
realistic answer-flipping scenarios. We find that retrieval heads are not
primarily responsible for avoiding uncertainty. Instead, we identify a small
set of non-retrieval attention heads that disproportionately attend to
misleading tokens in uncertain contexts. Masking these heads yields significant
improvements, reducing flip behavior by up to 15% without introducing
incoherence or overcorrection. However, when tested for downstream tasks, we
observe trade-offs with flip behavior. Our findings contribute to the growing
field of mechanistic interpretability and present a simple yet effective
technique for mitigating uncertainty-driven failure modes in LLMs.

### 21. Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Prerna Ravi, Dong Won Lee, Beatriz Flamia, Jasmine David, Brandon Hanks, Cynthia Breazeal, Emma Anderson, Grace Lin
- **URL**: <http://arxiv.org/abs/2510.22844v1>
- **Submitted**: 2025-10-26 21:25:23
- **Comment**: In Submission: Journal of Educational Data Mining (jEDM) 2026
- **Topic Keywords**: rag
- **Reason**: This paper explores the application of large language models in conversational analysis, which shares some overlap with information retrieval and natural language processing. However, the focus on synchronous spoken dialogue and conversational thread identification is not directly related to the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Understanding how ideas develop and flow in small-group conversations is
critical for analyzing collaborative learning. A key structural feature of
these interactions is threading, the way discourse talk naturally organizes
into interwoven topical strands that evolve over time. While threading has been
widely studied in asynchronous text settings, detecting threads in synchronous
spoken dialogue remains challenging due to overlapping turns and implicit cues.
At the same time, large language models (LLMs) show promise for automating
discourse analysis but often struggle with long-context tasks that depend on
tracing these conversational links. In this paper, we investigate whether
explicit thread linkages can improve LLM-based coding of relational moves in
group talk. We contribute a systematic guidebook for identifying threads in
synchronous multi-party transcripts and benchmark different LLM prompting
strategies for automated threading. We then test how threading influences
performance on downstream coding of conversational analysis frameworks, that
capture core collaborative actions such as agreeing, building, and eliciting.
Our results show that providing clear conversational thread information
improves LLM coding performance and underscores the heavy reliance of
downstream analysis on well-structured dialogue. We also discuss practical
trade-offs in time and cost, emphasizing where human-AI hybrid approaches can
yield the best value. Together, this work advances methods for combining LLMs
and robust conversational thread structures to make sense of complex, real-time
group interactions.

### 22. Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu
- **URL**: <http://arxiv.org/abs/2510.23038v1>
- **Submitted**: 2025-10-27 06:03:37
- **Comment**: Work in Progress
- **Topic Keywords**: listwise, pointwise, pairwise
- **Reason**: This paper focuses on Large Language Models (LLMs) as judges for evaluating response quality, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some aspects of NLP, the primary focus is on training LLM judges through tool-integrated reinforcement learning, which is not a central match to your research themes.

#### Abstract
> Large Language Models (LLMs) are widely used as judges to evaluate response
quality, providing a scalable alternative to human evaluation. However, most
LLM judges operate solely on intrinsic text-based reasoning, limiting their
ability to verify complex constraints or perform accurate computation.
Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks,
we propose TIR-Judge, an end-to-end RL framework for training LLM judges that
integrates a code executor for precise evaluation. TIR-Judge is built on three
principles: (i) diverse training across verifiable and non-verifiable domains,
(ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii)
iterative RL that bootstraps directly from the initial model without
distillation. On seven public benchmarks, TIR-Judge surpasses strong
reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and
achieves listwise performance comparable to Claude-Opus-4 despite having only
8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled
judge trajectories, matches the performance of distilled variants,
demonstrating that tool-augmented judges can self-evolve through iterative
reinforcement learning.

### 23. LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: M√°t√© Gedeon, P√©ter Mihajlik
- **URL**: <http://arxiv.org/abs/2510.23320v1>
- **Submitted**: 2025-10-27 13:35:22
- **Comment**: Submitted to LREC 2026
- **Topic Keywords**: rag, rank, search
- **Reason**: This paper focuses on speech recognition and diarization, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it involves some NLP aspects, the context and application are not aligned with your core themes.

#### Abstract
> We introduce LibriConvo, a simulated multi-speaker conversational dataset
based on speaker-aware conversation simulation (SASC), designed to support
training and evaluation of speaker diarization and automatic speech recognition
(ASR) systems. Unlike prior resources that mostly rely on semantically
disconnected utterances and implausible temporal gaps, LibriConvo ensures
semantic coherence and realistic conversational timing. Our pipeline leverages
CallHome with external VAD for reliable boundaries, applies compression to
reduce unnaturally long silences, and organizes LibriTTS utterances by book to
maintain contextual consistency. Acoustic realism is enhanced via a novel room
impulse response selection procedure that ranks speaker-microphone
configurations by spatial plausibility, balancing realism and diversity. The
dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers,
split in a speaker-disjoint manner for robust evaluation. Baselines show that
the sortformer model outperforms the pyannote pipeline in diarization, while a
fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves
7.29\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides
a valuable resource for advancing multi-speaker speech processing research with
realistic conversational dynamics and controlled experimental conditions.

### 24. Fast-MIA: Efficient and Scalable Membership Inference for LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hiromu Takahashi, Shotaro Ishihara
- **URL**: <http://arxiv.org/abs/2510.23074v1>
- **Submitted**: 2025-10-27 07:18:32
- **Topic Keywords**: search, acl
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on Large Language Models and Membership Inference Attacks. While it involves NLP, the context is not aligned with the user's interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library
for efficiently evaluating membership inference attacks (MIA) against Large
Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due
to growing concerns over copyright, security, and data privacy, and has
attracted increasing research attention. However, the progress of this research
is significantly hindered by two main obstacles: (1) the high computational
cost of inference in LLMs, and (2) the lack of standardized and maintained
implementations of MIA methods, which makes large-scale empirical comparison
difficult. To address these challenges, our library provides fast batch
inference and includes implementations of representative MIA methods under a
unified evaluation framework. This library supports easy implementation of
reproducible benchmarks with simple configuration and extensibility. We release
Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and
transparent research on LLMs.

### 25. UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Huixuan Zhang, Xiaojun Wan
- **URL**: <http://arxiv.org/abs/2510.23023v1>
- **Submitted**: 2025-10-27 05:37:23
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests as it focuses on AI-generated image content detection and localization, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> With the rapid proliferation of image generative models, the authenticity of
digital images has become a significant concern. While existing studies have
proposed various methods for detecting AI-generated content, current benchmarks
are limited in their coverage of diverse generative models and image
categories, often overlooking end-to-end image editing and artistic images. To
address these limitations, we introduce UniAIDet, a unified and comprehensive
benchmark that includes both photographic and artistic images. UniAIDet covers
a wide range of generative models, including text-to-image, image-to-image,
image inpainting, image editing, and deepfake models. Using UniAIDet, we
conduct a comprehensive evaluation of various detection methods and answer
three key research questions regarding generalization capability and the
relation between detection and localization. Our benchmark and analysis provide
a robust foundation for future research.

### 26. GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu
- **URL**: <http://arxiv.org/abs/2510.22942v1>
- **Submitted**: 2025-10-27 02:56:08
- **Comment**: 14 pages, 8 figures, 4 tables, submitted to ICDE 2026
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on a specific application (POI recommendation) and uses a novel framework (GTR-Mamba) that combines hyperbolic geometry and Euclidean tangent space. However, it does not directly relate to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing.

#### Abstract
> Next Point-of-Interest (POI) recommendation is a critical task in modern
Location-Based Social Networks (LBSNs), aiming to model the complex
decision-making process of human mobility to provide personalized
recommendations for a user's next check-in location. Existing POI
recommendation models, predominantly based on Graph Neural Networks and
sequential models, have been extensively studied. However, these models face a
fundamental limitation: they struggle to simultaneously capture the inherent
hierarchical structure of spatial choices and the dynamics and irregular shifts
of user-specific temporal contexts. To overcome this limitation, we propose
GTR-Mamba, a novel framework for cross-manifold conditioning and routing.
GTR-Mamba leverages the distinct advantages of different mathematical spaces
for different tasks: it models the static, tree-like preference hierarchies in
hyperbolic geometry, while routing the dynamic sequence updates to a novel
Mamba layer in the computationally stable and efficient Euclidean tangent
space. This process is coordinated by a cross-manifold channel that fuses
spatio-temporal information to explicitly steer the State Space Model (SSM),
enabling flexible adaptation to contextual changes. Extensive experiments on
three real-world datasets demonstrate that GTR-Mamba consistently outperforms
state-of-the-art baseline models in next POI recommendation.

### 27. JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan
- **URL**: <http://arxiv.org/abs/2510.23538v1>
- **Submitted**: 2025-10-27 17:13:49
- **Comment**: Work in progress
- **Topic Keywords**: rag
- **Reason**: This paper focuses on code intelligence and multimodal code data, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.

### 28. Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Anwesha Das, John Duff, J√∂rg Hoffmann, Vera Demberg
- **URL**: <http://arxiv.org/abs/2510.23340v1>
- **Submitted**: 2025-10-27 13:54:54
- **Comment**: 11 pages, 3 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on adaptive signalling and human-AI collaboration in dynamic environments, using principles from cognitive science and Bayesian reference resolution. While it touches on communication and user awareness, it does not directly relate to information retrieval, search technologies, or query understanding, which are core areas of interest.

#### Abstract
> Adaptive agent design offers a way to improve human-AI collaboration on
time-sensitive tasks in rapidly changing environments. In such cases, to ensure
the human maintains an accurate understanding of critical task elements, an
assistive agent must not only identify the highest priority information but
also estimate how and when this information can be communicated most
effectively, given that human attention represents a zero-sum cognitive
resource where focus on one message diminishes awareness of other or upcoming
information. We introduce a theoretical framework for adaptive signalling which
meets these challenges by using principles of rational communication,
formalised as Bayesian reference resolution using the Rational Speech Act (RSA)
modelling framework, to plan a sequence of messages which optimise timely
alignment between user belief and a dynamic environment. The agent adapts
message specificity and timing to the particulars of a user and scenario based
on projections of how prior-guided interpretation of messages will influence
attention to the interface and subsequent belief update, across several
timesteps out to a fixed horizon. In a comparison to baseline methods, we show
that this effectiveness depends crucially on combining multi-step planning with
a realistic model of user awareness. As the first application of RSA for
communication in a dynamic environment, and for human-AI interaction in
general, we establish theoretical foundations for pragmatic communication in
human-agent teams, highlighting how insights from cognitive science can be
capitalised to inform the design of assistive agents.

### 29. A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Thai-Binh Nguyen, Katerina Zmolikova, Pingchuan Ma, Ngoc Quan Pham, Christian Fuegen, Alexander Waibel
- **URL**: <http://arxiv.org/abs/2510.23276v1>
- **Submitted**: 2025-10-27 12:36:43
- **Comment**: Submitted to ICASSP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on a multi-modal recognition task in a cocktail-party setting, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves audio and visual cues, the primary goal is speech recognition and conversation clustering, rather than query understanding, ranking models, or real-time relevance optimization.

#### Abstract
> We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in
the ninth CHiME Challenge, which addresses the cocktail-party problem of
overlapping conversations in a single-room setting using audio, visual, and
contextual cues. MCoRec captures natural multi-party conversations where the
recordings focus on unscripted, casual group chats, leading to extreme speech
overlap of up to 100% and highly fragmented conversational turns. The task
requires systems to answer the question "Who speaks when, what, and with whom?"
by jointly transcribing each speaker's speech and clustering them into their
respective conversations from audio-visual recordings. Audio-only baselines
exceed 100% word error rate, whereas incorporating visual cues yields
substantial 50% improvements, highlighting the importance of multi-modality. In
this manuscript, we present the motivation behind the task, outline the data
collection process, and report the baseline systems developed for the MCoRec.

### 30. Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu
- **URL**: <http://arxiv.org/abs/2510.23163v1>
- **Submitted**: 2025-10-27 09:41:29
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on screenwriting and Large Language Models in a creative writing context.

#### Abstract
> The screenplay serves as the foundation for television production, defining
narrative structure, character development, and dialogue. While Large Language
Models (LLMs) show great potential in creative writing, direct end-to-end
generation approaches often fail to produce well-crafted screenplays. We argue
this failure stems from forcing a single model to simultaneously master two
disparate capabilities: creative narrative construction and rigid format
adherence. The resulting outputs may mimic superficial style but lack the deep
structural integrity and storytelling substance required for professional use.
To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage
Refinement (DSR), a decomposed framework that decouples creative narrative
generation from format conversion. The first stage transforms a brief outline
into rich, novel-style prose. The second stage refines this narrative into a
professionally formatted screenplay. This separation enables the model to
specialize in one distinct capability at each stage. A key challenge in
implementing DSR is the scarcity of paired outline-to-novel training data. We
address this through hybrid data synthesis: reverse synthesis deconstructs
existing screenplays into structured inputs, while forward synthesis leverages
these inputs to generate high-quality narrative texts as training targets.
Blind evaluations by professional screenwriters show that DSR achieves a 75%
win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of
human-level performance. Our work demonstrates that decomposed generation
architecture with tailored data synthesis effectively specializes LLMs in
complex creative domains.

### 31. ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei
- **URL**: <http://arxiv.org/abs/2510.23160v1>
- **Submitted**: 2025-10-27 09:39:22
- **Topic Keywords**: rag
- **Reason**: This paper focuses on enhancing low-quality data for fine-tuning large language models, which is not directly related to information retrieval or search technologies. While it involves neural-symbolic text processing, the primary goal is data augmentation rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs)
to domain-specific instructions by training on a carefully curated subset of
high-quality instruction-response pairs, typically drawn from a larger dataset
that often contains many low-quality or noisy samples. However, existing
quality-first paradigms often overlook valuable signals in discarded
low-quality data and rely on imperfect quality filters. We introduce ENTP
(Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a
framework that revitalizes low-quality corpora through symbolic purification
and neural reconstruction. The symbolic module identifies and prunes noisy
samples based on statistical priors, while the neural component synthesizes
enriched instruction-response pairs by leveraging latent representations and
model knowledge. This neural-symbolic synergy enhances data informativeness and
diversity. Experiments show that ENTP-augmented datasets, constructed
exclusively from low-quality data, outperform 13 established data-selection
baselines across five instruction-following benchmarks, and even surpass
fine-tuning on the full original dataset (approximately 300K examples). Our
results highlight the untapped potential of low-quality data and underscore the
importance of intelligent purification and synthesis for efficient instruction
alignment.

### 32. Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yichao Jin, Yushuo Wang, Qishuai Zhong, Kent Chiu Jin-Chun, Kenneth Zhu Ke, Donald MacDonald
- **URL**: <http://arxiv.org/abs/2510.23066v1>
- **Submitted**: 2025-10-27 06:56:08
- **Topic Keywords**: rag
- **Reason**: This paper focuses on financial document processing and extraction, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves OCR and compact Vision-Language Models, the context and application are quite different from your areas of focus.

#### Abstract
> Financial documents are essential sources of information for regulators,
auditors, and financial institutions, particularly for assessing the wealth and
compliance of Small and Medium-sized Businesses. However, SMB documents are
often difficult to parse. They are rarely born digital and instead are
distributed as scanned images that are none machine readable. The scans
themselves are low in resolution, affected by skew or rotation, and often
contain noisy backgrounds. These documents also tend to be heterogeneous,
mixing narratives, tables, figures, and multilingual content within the same
report. Such characteristics pose major challenges for automated information
extraction, especially when relying on end to end large Vision Language Models,
which are computationally expensive, sensitive to noise, and slow when applied
to files with hundreds of pages.
  We propose a multistage pipeline that leverages traditional image processing
models and OCR extraction, together with compact VLMs for structured field
extraction of large-scale financial documents. Our approach begins with image
pre-processing, including segmentation, orientation detection, and size
normalization. Multilingual OCR is then applied to recover page-level text.
Upon analyzing the text information, pages are retrieved for coherent sections.
Finally, compact VLMs are operated within these narrowed-down scopes to extract
structured financial indicators.
  Our approach is evaluated using an internal corpus of multi-lingual, scanned
financial documents. The results demonstrate that compact VLMs, together with a
multistage pipeline, achieves 8.8 times higher field level accuracy relative to
directly feeding the whole document into large VLMs, only at 0.7 percent of the
GPU cost and 92.6 percent less end-to-end service latency.

### 33. LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim
- **URL**: <http://arxiv.org/abs/2510.23011v1>
- **Submitted**: 2025-10-27 05:11:07
- **Comment**: 14 pages
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on language learning and education, leveraging large language models, but does not align with the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Language educators strive to create a rich experience for learners, while
they may be restricted in the extend of feedback and practice they can provide.
We present the design and development of LangLingual, a conversational agent
built using the LangChain framework and powered by Large Language Models. The
system is specifically designed to provide real-time, grammar-focused feedback,
generate context-aware language exercises and track learner proficiency over
time. The paper discusses the architecture, implementation and evaluation of
LangLingual in detail. The results indicate strong usability, positive learning
outcomes and encouraging learner engagement.

### 34. Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shenran Wang, Timothy Tin-Long Tse, Jian Zhu
- **URL**: <http://arxiv.org/abs/2510.23006v1>
- **Submitted**: 2025-10-27 04:49:01
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on in-context learning and large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the specific topic of in-context learning and its mechanisms is not a central match for your research themes.

#### Abstract
> We perform in-depth evaluations of in-context learning (ICL) on
state-of-the-art transformer, state-space, and hybrid large language models
over two categories of knowledge-based ICL tasks. Using a combination of
behavioral probing and intervention-based methods, we have discovered that,
while LLMs of different architectures can behave similarly in task performance,
their internals could remain different. We discover that function vectors (FVs)
responsible for ICL are primarily located in the self-attention and Mamba
layers, and speculate that Mamba2 uses a different mechanism from FVs to
perform ICL. FVs are more important for ICL involving parametric knowledge
retrieval, but not for contextual knowledge understanding. Our work contributes
to a more nuanced understanding across architectures and task types.
Methodologically, our approach also highlights the importance of combining both
behavioural and mechanistic analyses to investigate LLM capabilities.

### 35. Batch Speculative Decoding Done Right

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ranran Haoran Zhang, Soumik Dey, Ashirbad Mishra, Hansi Wu, Binbin Li, Rui Zhang
- **URL**: <http://arxiv.org/abs/2510.22876v1>
- **Submitted**: 2025-10-26 23:59:23
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the efficiency of Large Language Model (LLM) inference through speculative decoding, which is not directly related to information retrieval, query understanding, or ranking models. While it involves some NLP aspects, the primary goal is to optimize LLM inference, making it less relevant to the user's core research themes.

#### Abstract
> Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3$\times$ throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.

### 36. Civic Ground Truth in News Recommenders: A Method for Public Value Scoring

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: James Meese, Kyle Herbertson
- **URL**: <http://arxiv.org/abs/2510.22865v1>
- **Submitted**: 2025-10-26 23:16:51
- **Comment**: Presented at NORMalize 2025: The Third Workshop on the Normative
  Design and Evaluation of Recommender Systems, co-located with the ACM
  Conference on Recommender Systems 2025 (RecSys 2025), Prague
- **Topic Keywords**: recommend, search
- **Reason**: This paper focuses on news recommendation systems and civic values, which is somewhat related to information retrieval, but it lacks direct connection to query understanding, ranking models, or user behavior modeling. The paper's emphasis on civic values and public service values also doesn't align with the user's background in e-commerce or interest in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Research in news recommendation systems (NRS) continues to explore the best
ways to integrate normative goals such as editorial objectives and public
service values into existing systems. Prior efforts have incorporated expert
input or audience feedback to quantify these values, laying the groundwork for
more civic-minded recommender systems. This paper contributes to that
trajectory, introducing a method for embedding civic values into NRS through
large-scale, structured audience evaluations. The proposed civic ground truth
approach aims to generate value-based labels through a nationally
representative survey that are generalisable across a wider news corpus, using
automated metadata enrichment.

### 37. A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chiara Bonfanti, Alessandro Druetto, Cataldo Basile, Tharindu Ranasinghe, Marcos Zampieri
- **URL**: <http://arxiv.org/abs/2510.23443v1>
- **Submitted**: 2025-10-27 15:46:02
- **Comment**: 7 pages
- **Topic Keywords**: search
- **Reason**: This paper appears to be focused on legal-cybersecurity knowledge integration, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves complex information spaces and knowledge integration, the context is specific to the intersection of law and cybersecurity, making it less relevant to the user's interests.

#### Abstract
> The growing intersection of cybersecurity and law creates a complex
information space where traditional legal research tools struggle to deal with
nuanced connections between cases, statutes, and technical vulnerabilities.
This knowledge divide hinders collaboration between legal experts and
cybersecurity professionals. To address this important gap, this work provides
a first step towards intelligent systems capable of navigating the increasingly
intricate cyber-legal domain. We demonstrate promising initial results on
multilingual tasks.

### 38. Arabic Little STT: Arabic Children Speech Recognition Dataset

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Mouhand Alkadri, Dania Desouki, Khloud Al Jallad
- **URL**: <http://arxiv.org/abs/2510.23319v1>
- **Submitted**: 2025-10-27 13:30:54
- **Topic Keywords**: search
- **Reason**: This paper focuses on Arabic children speech recognition, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve speech recognition, it's a specific application and not a general topic in IR or NLP.

#### Abstract
> The performance of Artificial Intelligence (AI) systems fundamentally depends
on high-quality training data. However, low-resource languages like Arabic
suffer from severe data scarcity. Moreover, the absence of child-specific
speech corpora is an essential gap that poses significant challenges. To
address this gap, we present our created dataset, Arabic Little STT, a dataset
of Levantine Arabic child speech recorded in classrooms, containing 355
utterances from 288 children (ages 6 - 13). We further conduct a systematic
assessment of Whisper, a state-of-the-art automatic speech recognition (ASR)
model, on this dataset and compare its performance with adult Arabic
benchmarks. Our evaluation across eight Whisper variants reveals that even the
best-performing model (Large_v3) struggles significantly, achieving a 0.66 word
error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on
adult datasets. These results align with other research on English speech.
Results highlight the critical need for dedicated child speech benchmarks and
inclusive training data in ASR development. Emphasizing that such data must be
governed by strict ethical and privacy frameworks to protect sensitive child
information. We hope that this study provides an initial step for future work
on equitable speech technologies for Arabic-speaking children. We hope that our
publicly available dataset enrich the children's demographic representation in
ASR datasets.

### 39. Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat
- **URL**: <http://arxiv.org/abs/2510.23252v1>
- **Submitted**: 2025-10-27 12:14:52
- **Comment**: This manuscript contains 11 pages, 5 tables and 16 figures This was
  accepted at International Joint Conference on Natural Language Processing &
  Asia-Pacific Chapter of the Association for Computational Linguistics
  (IJCNLP-AACL) 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on speech recognition modeling and automatic speech recognition for low-resource languages, which is outside your primary area of interest in Information Retrieval and Search technologies.

#### Abstract
> Conventional research on speech recognition modeling relies on the canonical
form for most low-resource languages while automatic speech recognition (ASR)
for regional dialects is treated as a fine-tuning task. To investigate the
effects of dialectal variations on ASR we develop a 78-hour annotated Bengali
Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and
data-driven perspectives shows that speech foundation models struggle heavily
in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe
that all deep learning methods struggle to model speech data under dialectal
variations but dialect specific model training alleviates the issue. Our
dataset also serves as a out of-distribution (OOD) resource for ASR modeling
under constrained resources in ASR algorithms. The dataset and code developed
for this project are publicly available

### 40. SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Shuai Huang, Wenxuan Zhao, Jun Gao
- **URL**: <http://arxiv.org/abs/2510.23182v1>
- **Submitted**: 2025-10-27 10:21:46
- **Comment**: 17 pages, 9 figures
- **Topic Keywords**: search
- **Reason**: This paper focuses on evaluating the social intelligence of large language models in human-to-human conversations, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the primary focus is on social intelligence and human-to-human conversations, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> As large language models (LLMs) develop anthropomorphic abilities, they are
increasingly being deployed as autonomous agents to interact with humans.
However, evaluating their performance in realistic and complex social
interactions remains a significant challenge. Most previous research built
datasets through simulated agent-to-agent interactions, which fails to capture
the authentic linguistic styles and relational dynamics found in real human
conversations. To address this gap, we introduce SI-Bench, a novel benchmark
designed to evaluate aspects of social intelligence in LLMs. Grounded in broad
social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues
collected from a social networking application. We further selected a subset of
312 dialogues for manual annotation across 8 major models. The experiments show
that SOTA models have surpassed the human expert in process reasoning under
complex social situations, yet they still fall behind humans in reply quality.
Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the
performance of LLMs in social dialogue tasks. All datasets are openly available
at https://github.com/SI-Bench/SI-Bench.git.

### 41. A Survey on LLM Mid-training

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai
- **URL**: <http://arxiv.org/abs/2510.23081v1>
- **Submitted**: 2025-10-27 07:32:19
- **Topic Keywords**: search
- **Reason**: This paper is about mid-training of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on model architecture optimization, it does not specifically address ranking models or user behavior modeling, making it only loosely relevant to your research interests.

#### Abstract
> Recent advances in foundation models have highlighted the significant
benefits of multi-stage training, with a particular emphasis on the emergence
of mid-training as a vital stage that bridges pre-training and post-training.
Mid-training is distinguished by its use of intermediate data and computational
resources, systematically enhancing specified capabilities such as mathematics,
coding, reasoning, and long-context extension, while maintaining foundational
competencies. This survey provides a formal definition of mid-training for
large language models (LLMs) and investigates optimization frameworks that
encompass data curation, training strategies, and model architecture
optimization. We analyze mainstream model implementations in the context of
objective-driven interventions, illustrating how mid-training serves as a
distinct and critical stage in the progressive development of LLM capabilities.
By clarifying the unique contributions of mid-training, this survey offers a
comprehensive taxonomy and actionable insights, supporting future research and
innovation in the advancement of LLMs.

### 42. Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Di Zhang, Xun Wu, Shaohan Huang, Yaru Hao, Li Dong, Zewen Chi, Zhifang Sui, Furu Wei
- **URL**: <http://arxiv.org/abs/2510.23027v1>
- **Submitted**: 2025-10-27 05:47:48
- **Topic Keywords**: search
- **Reason**: This paper focuses on reinforcement learning for Mixture-of-Experts architectures, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is more about model training and optimization rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Recent advances in reinforcement learning (RL) have substantially improved
the training of large-scale language models, leading to significant gains in
generation quality and reasoning ability. However, most existing research
focuses on dense models, while RL training for Mixture-of-Experts (MoE)
architectures remains underexplored. To address the instability commonly
observed in MoE training, we propose a novel router-aware approach to optimize
importance sampling (IS) weights in off-policy RL. Specifically, we design a
rescaling strategy guided by router logits, which effectively reduces gradient
variance and mitigates training divergence. Experimental results demonstrate
that our method significantly improves both the convergence stability and the
final performance of MoE models, highlighting the potential of RL algorithmic
innovations tailored to MoE architectures and providing a promising direction
for efficient training of large-scale expert models.

### 43. Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Haowei Hua, Hong Jiao, Xinyi Wang
- **URL**: <http://arxiv.org/abs/2510.22830v2>
- **Submitted**: 2025-10-26 20:59:22
- **Comment**: 19 pages, 5 Tables 7 Figures, Presentation at Artificial Intelligence
  in Measurement and Education Conference (AIME-Con)
- **Topic Keywords**: search
- **Reason**: This paper focuses on automated essay scoring using generative language models, which is somewhat related to information retrieval, but primarily falls under natural language processing. While it mentions 'Learning to Rank' in the abstract, it's not a central theme of the paper. The topic is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest.

#### Abstract
> BERT and its variants are extensively explored for automated scoring.
However, a limit of 512 tokens for these encoder-based models showed the
deficiency in automated scoring of long essays. Thus, this research explores
generative language models for automated scoring of long essays via
summarization and prompting. The results revealed great improvement of scoring
accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab
Automated Essay Scoring 2.0 dataset.

### 44. Measuring Teaching with LLMs

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Michael Hardy
- **URL**: <http://arxiv.org/abs/2510.22968v1>
- **Submitted**: 2025-10-27 03:42:04
- **Topic Keywords**: rag
- **Reason**: LLM scoring failed.

#### Abstract
> Objective and scalable measurement of teaching quality is a persistent
challenge in education. While Large Language Models (LLMs) offer potential,
general-purpose models have struggled to reliably apply complex, authentic
classroom observation instruments. This paper uses custom LLMs built on
sentence-level embeddings, an architecture better suited for the long-form,
interpretive nature of classroom transcripts than conventional subword
tokenization. We systematically evaluate five different sentence embeddings
under a data-efficient training regime designed to prevent overfitting. Our
results demonstrate that these specialized models can achieve human-level and
even super-human performance with expert human ratings above 0.65 and
surpassing the average human-human rater correlation. Further, through analysis
of annotation context windows, we find that more advanced models-those better
aligned with human judgments-attribute a larger share of score variation to
lesson-level features rather than isolated utterances, challenging the
sufficiency of single-turn annotation paradigms. Finally, to assess external
validity, we find that aggregate model scores align with teacher value-added
measures, indicating they are capturing features relevant to student learning.
However, this trend does not hold at the individual item level, suggesting that
while the models learn useful signals, they have not yet achieved full
generalization. This work establishes a viable and powerful new methodology for
AI-driven instructional measurement, offering a path toward providing scalable,
reliable, and valid feedback for educator development.

### 45. How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios
- **URL**: <http://arxiv.org/abs/2510.23358v1>
- **Submitted**: 2025-10-27 14:08:27
- **Comment**: 8 pages + Limitations + References
- **Topic Keywords**: search
- **Reason**: LLM scoring failed.

#### Abstract
> Artificial intelligence is reshaping labor markets, yet we lack tools to
systematically forecast its effects on employment. This paper introduces a
benchmark for evaluating how well large language models (LLMs) can anticipate
changes in job demand, especially in occupations affected by AI. Existing
research has shown that LLMs can extract sentiment, summarize economic reports,
and emulate forecaster behavior, but little work has assessed their use for
forward-looking labor prediction. Our benchmark combines two complementary
datasets: a high-frequency index of sector-level job postings in the United
States, and a global dataset of projected occupational changes due to AI
adoption. We format these data into forecasting tasks with clear temporal
splits, minimizing the risk of information leakage. We then evaluate LLMs using
multiple prompting strategies, comparing task-scaffolded, persona-driven, and
hybrid approaches across model families. We assess both quantitative accuracy
and qualitative consistency over time. Results show that structured task
prompts consistently improve forecast stability, while persona prompts offer
advantages on short-term trends. However, performance varies significantly
across sectors and horizons, highlighting the need for domain-aware prompting
and rigorous evaluation protocols. By releasing our benchmark, we aim to
support future research on labor forecasting, prompt design, and LLM-based
economic reasoning. This work contributes to a growing body of research on how
LLMs interact with real-world economic data, and provides a reproducible
testbed for studying the limits and opportunities of AI as a forecasting tool
in the context of labor markets.

---

