# Daily Papers Report - 2026-01-04

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies

- **LLM Score**: 9
- **Keyword Score**: 12
- **Authors**: Jetlir Duraj, Ishita Khan, Kilian Merkelbach, Mehran Elyasi
- **URL**: <http://arxiv.org/abs/2601.00510v1>
- **Submitted**: 2026-01-01 23:36:13
- **Comment**: 9 pages, accepted at SIGIR eCom 2025
- **Topic Keywords**: query, queries, relevance, commerce, e-commerce, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly query understanding and ranking models. The Chain-of-Thought approach for semantic query categorization in e-commerce taxonomies aligns with your focus on deep semantic understanding and real-time relevance optimization. The use of LLM semantic scoring also relates to your interests in Natural Language Processing.

#### Abstract
> Search in e-Commerce is powered at the core by a structured representation of the inventory, often formulated as a category taxonomy. An important capability in e-Commerce with hierarchical taxonomies is to select a set of relevant leaf categories that are semantically aligned with a given user query. In this scope, we address a fundamental problem of search query categorization in real-world e-Commerce taxonomies. A correct categorization of a query not only provides a way to zoom into the correct inventory space, but opens the door to multiple intent understanding capabilities for a query. A practical and accurate solution to this problem has many applications in e-commerce, including constraining retrieved items and improving the relevance of the search results. For this task, we explore a novel Chain-of-Thought (CoT) paradigm that combines simple tree-search with LLM semantic scoring. Assessing its classification performance on human-judged query-category pairs, relevance tests, and LLM-based reference methods, we find that the CoT approach performs better than a benchmark that uses embedding-based query category predictions. We show how the CoT approach can detect problems within a hierarchical taxonomy. Finally, we also propose LLM-based approaches for query-categorization of the same spirit, but which scale better at the range of millions of queries.

---

### 2. Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Yuelyu Ji, Zhuochun Li, Rui Meng, Daqing He
- **URL**: <http://arxiv.org/abs/2601.00536v1>
- **Submitted**: 2026-01-02 02:38:01
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The focus on multi-hop question answering and retrieval-reasoning processes aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the paper's specific focus on question answering and the four-axis design framework, while related, is not a central match to your primary research themes.

#### Abstract
> Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.

---

### 3. Improving Scientific Document Retrieval with Academic Concept Index

- **LLM Score**: 7
- **Keyword Score**: 16
- **Authors**: Jeyun Lee, Junhyoung Lee, Wonbin Kweon, Bowen Jin, Yu Zhang, Susik Yoon, Dongha Lee, Hwanjo Yu, Jiawei Han, Seongku Kang
- **URL**: <http://arxiv.org/abs/2601.00567v1>
- **Submitted**: 2026-01-02 04:47:49
- **Topic Keywords**: retriever, query, queries, relevance, rag, retrieval
- **Reason**: This paper is relevant to your interests in Information Retrieval, particularly in the area of query understanding and ranking models. The use of academic concept index and concept-focused auxiliary contexts is an innovative approach to improving scientific document retrieval, which aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's focus on scientific documents and academic concepts is somewhat specific and may not be directly applicable to your e-commerce background.

#### Abstract
> Adapting general-domain retrievers to scientific domains is challenging due to the scarcity of large-scale domain-specific relevance annotations and the substantial mismatch in vocabulary and information needs. Recent approaches address these issues through two independent directions that leverage large language models (LLMs): (1) generating synthetic queries for fine-tuning, and (2) generating auxiliary contexts to support relevance matching. However, both directions overlook the diverse academic concepts embedded within scientific documents, often producing redundant or conceptually narrow queries and contexts. To address this limitation, we introduce an academic concept index, which extracts key concepts from papers and organizes them guided by an academic taxonomy. This structured index serves as a foundation for improving both directions. First, we enhance the synthetic query generation with concept coverage-based generation (CCQGen), which adaptively conditions LLMs on uncovered concepts to generate complementary queries with broader concept coverage. Second, we strengthen the context augmentation with concept-focused auxiliary contexts (CCExpand), which leverages a set of document snippets that serve as concise responses to the concept-aware CCQGen queries. Extensive experiments show that incorporating the academic concept index into both query generation and context augmentation leads to higher-quality queries, better conceptual alignment, and improved retrieval performance.

---

### 4. Rule-Based Approaches to Atomic Sentence Extraction

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Lineesha Kamana, Akshita Ananda Subramanian, Mehuli Ghosh, Suman Saha
- **URL**: <http://arxiv.org/abs/2601.00506v1>
- **Submitted**: 2026-01-01 23:19:51
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: This paper on atomic sentence extraction and rule-based approaches is somewhat related to your research interests in Information Retrieval, particularly in areas that require deep semantic understanding. However, the focus on rule-based extraction and linguistic structures is not a central match for your primary focus on ranking models and user behavior modeling.

#### Abstract
> Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the "split-and-rephrase" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.

---

### 5. TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Mohamed Trabelsi, Huseyin Uzunalioglu
- **URL**: <http://arxiv.org/abs/2601.00691v1>
- **Submitted**: 2026-01-02 13:55:07
- **Topic Keywords**: ranking, ctr, retrieval, rank, search
- **Reason**: While the paper explores a domain-specific application of information retrieval (telecom ticket troubleshooting), it doesn't directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling. However, the use of domain-specific ranking and generative models for automating key steps of the troubleshooting workflow shares some similarities with the user's interests in information retrieval and NLP.

#### Abstract
> Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zhenhong Zhou, Shilinlu Yan, Chuanpu Liu, Qiankun Li, Kun Wang, Zhigang Zeng
- **URL**: <http://arxiv.org/abs/2601.00588v1>
- **Submitted**: 2026-01-02 06:21:41
- **Comment**: 18 pages
- **Topic Keywords**: queries
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, particularly in the context of query understanding and safety evaluation of language models. However, the focus on Chinese-specific adversarial patterns and safety in lightweight LLMs does not directly align with the user's primary research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.

### 7. Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jonathan Simkin, Lovedeep Gondara, Zeeshan Rizvi, Gregory Doyle, Jeff Dowden, Dan Bond, Desmond Martin, Raymond Ng
- **URL**: <http://arxiv.org/abs/2601.00787v1>
- **Submitted**: 2026-01-02 18:46:19
- **Topic Keywords**: rag
- **Reason**: This paper focuses on adapting NLP models for cancer surveillance in Canada, which is somewhat related to information retrieval and NLP. However, the primary focus on cancer registries and jurisdictional adaptation does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.

### 8. Exploring the Performance of Large Language Models on Subjective Span Identification Tasks

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Alphaeus Dmonte, Roland Oruche, Tharindu Ranasinghe, Marcos Zampieri, Prasad Calyam
- **URL**: <http://arxiv.org/abs/2601.00736v1>
- **Submitted**: 2026-01-02 16:30:14
- **Topic Keywords**: rag
- **Reason**: This paper explores the performance of Large Language Models on subjective span identification tasks, which is related to query understanding and ranking models in Information Retrieval. However, the focus on NLP and model explainability, while relevant to broader IR themes, is not a central match to the user's primary research interests. The paper's exploration of LLM strategies may have some indirect implications for IR, but it is not a direct contribution to the field.

#### Abstract
> Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.

### 9. Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Sumanth Balaji, Piyush Mishra, Aashraya Sachdeva, Suraj Agrawal
- **URL**: <http://arxiv.org/abs/2601.00596v1>
- **Submitted**: 2026-01-02 07:21:23
- **Comment**: 17 pages, 3 figures, preprint
- **Topic Keywords**: rag
- **Reason**: The paper explores the application of large language models in customer support, focusing on policy adherence and user behavior. While it touches on aspects of query understanding and user behavior modeling, the primary focus is on recommender systems and AI-driven customer support, which is somewhat related to information retrieval but not a central match for your research interests.

#### Abstract
> Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.

### 10. Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Muhammad Shahmeer Khan
- **URL**: <http://arxiv.org/abs/2601.00444v1>
- **Submitted**: 2026-01-01 19:05:25
- **Comment**: 11 pages, 6 figures. Code and reproducibility resources available on GitHub
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it focuses on lightweight Transformer models for enterprise NLP deployment, which is a specific application area. The paper's emphasis on efficiency metrics and model comparison does not directly align with the user's primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.

### 11. Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Valentin No√´l
- **URL**: <http://arxiv.org/abs/2601.00791v1>
- **Submitted**: 2026-01-02 18:49:37
- **Comment**: 58 pages, 19 figures, Under Review
- **Topic Keywords**: ctr
- **Reason**: This paper appears to be unrelated to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves spectral analysis and attention patterns, the focus is on mathematical reasoning and AI safety, which does not align with your primary areas of interest.

#### Abstract
> We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

### 12. Fast-weight Product Key Memory

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tianyu Zhao, Llion Jones
- **URL**: <http://arxiv.org/abs/2601.00671v1>
- **Submitted**: 2026-01-02 12:37:53
- **Topic Keywords**: rag
- **Reason**: This paper focuses on sequence modeling and episodic memory architectures in language models, which is somewhat related to your interests in Natural Language Processing and deep semantic understanding. However, it does not directly address information retrieval, ranking models, or user behavior modeling, making it less relevant to your core research themes.

#### Abstract
> Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, "fast-weight" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.

### 13. Noise-Aware Named Entity Recognition for Historical VET Documents

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Alexander M. Esser, Jens D√∂rpinghaus
- **URL**: <http://arxiv.org/abs/2601.00488v1>
- **Submitted**: 2026-01-01 21:43:35
- **Comment**: This is an extended, non-peer-reviewed version of the paper presented at VISAPP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Named Entity Recognition (NER) in a specific domain (Vocational Education and Training) and does not align with the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.

### 14. Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dimitris Vartziotis
- **URL**: <http://arxiv.org/abs/2601.00448v1>
- **Submitted**: 2026-01-01 19:15:17
- **Topic Keywords**: rag
- **Reason**: This paper explores the mathematical structure of language, which is somewhat related to the user's interest in deep semantic understanding in Information Retrieval. However, the focus on linguistic theories and language games is not directly aligned with the user's primary research themes in IR, NLP, and related topics.

#### Abstract
> Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.

### 15. Memory Bank Compression for Continual Adaptation of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Thomas Katraouras, Dimitrios Rafailidis
- **URL**: <http://arxiv.org/abs/2601.00756v1>
- **Submitted**: 2026-01-02 17:22:34
- **Comment**: Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC '26)
- **Topic Keywords**: rank
- **Reason**: This paper focuses on continual learning of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the primary goal is to adapt language models to new information, rather than improving search relevance or user behavior modeling.

#### Abstract
> Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

---

