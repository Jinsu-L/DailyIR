[
    {
        "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
        "abstract": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
        "url": "http://arxiv.org/abs/2508.09987v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09987v1",
        "arxiv_id": "2508.09987v1",
        "authors": [
            "Junyan Ye",
            "Dongzhi Jiang",
            "Zihao Wang",
            "Leqi Zhu",
            "Zhenghao Hu",
            "Zilong Huang",
            "Jun He",
            "Zhiyuan Yan",
            "Jinghua Yu",
            "Hongsheng Li",
            "Conghui He",
            "Weijia Li"
        ],
        "submitted": "2025-08-13 17:59:28",
        "source": "arxiv",
        "comment": "19 pages, 8 figures"
    },
    {
        "title": "On the Consistency and Performance of the Iterative Bayesian Update",
        "abstract": "For many social, scientific, and commercial purposes, it is often important\nto estimate the distribution of the users' data regarding a sensitive\nattribute, e.g., their ages, locations, etc. To allow this estimation while\nprotecting the users' privacy, every user applies a local privacy protection\nmechanism that releases a noisy (sanitized) version of their original datum to\nthe data collector; then the original distribution is estimated using one of\nthe known methods, such as the matrix inversion (INV), RAPPOR's estimator, and\nthe iterative Bayesian update (IBU). Unlike the other estimators, the\nconsistency of IBU, i.e., the convergence of its estimate to the real\ndistribution as the amount of noisy data grows, has been either ignored or\nincorrectly proved in the literature. In this article, we use the fact that IBU\nis a maximum likelihood estimator to prove that IBU is consistent. We also\nshow, through experiments on real datasets, that IBU significantly outperforms\nthe other methods when the users' data are sanitized by geometric, Laplace, and\nexponential mechanisms, whereas it is comparable to the other methods in the\ncase of the k-RR and RAPPOR mechanisms. Finally, we consider the case when the\nalphabet of the sensitive data is infinite, and we show a technique that allows\nIBU to operate in this case too.",
        "url": "http://arxiv.org/abs/2508.09980v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09980v1",
        "arxiv_id": "2508.09980v1",
        "authors": [
            "Ehab ElSalamouny",
            "Catuscia Palamidessi"
        ],
        "submitted": "2025-08-13 17:52:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks",
        "abstract": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.",
        "url": "http://arxiv.org/abs/2508.09958v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09958v1",
        "arxiv_id": "2508.09958v1",
        "authors": [
            "Baran Atalar",
            "Eddie Zhang",
            "Carlee Joe-Wong"
        ],
        "submitted": "2025-08-13 17:19:41",
        "source": "arxiv",
        "comment": "Submitted to AAAI 2026"
    },
    {
        "title": "Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)",
        "abstract": "Speech-to-text (STT) systems have a wide range of applications. They are\navailable in many languages, albeit at different quality levels. Although\nKurdish is considered a less-resourced language from a processing perspective,\nSST is available for some of the Kurdish dialects, for instance, Sorani\n(Central Kurdish). However, that is not applied to other Kurdish dialects,\nBadini and Hawrami, for example. This research is an attempt to address this\ngap. Bandin, approximately, has two million speakers, and STT systems can help\ntheir community use mobile and computer-based technologies while giving their\ndialect more global visibility. We aim to create a language model based on\nBadini's speech and evaluate its performance. To cover a conversational aspect,\nhave a proper confidence level of grammatical accuracy, and ready\ntranscriptions, we chose Badini kids' stories, eight books including 78\nstories, as the textual input. Six narrators narrated the books, which resulted\nin approximately 17 hours of recording. We cleaned, segmented, and tokenized\nthe input. The preprocessing produced nearly 15 hours of speech, including\n19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and\nWhisper-small to develop the language models. The experiments indicate that the\ntranscriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a\nsignificantly more accurate and readable output than the Whisper-small model,\nwith 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,\nrespectively.",
        "url": "http://arxiv.org/abs/2508.09957v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09957v1",
        "arxiv_id": "2508.09957v1",
        "authors": [
            "Renas Adnan",
            "Hossein Hassani"
        ],
        "submitted": "2025-08-13 17:19:22",
        "source": "arxiv",
        "comment": "21 pages, 20 figures, 7 tables"
    },
    {
        "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering",
        "abstract": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.",
        "url": "http://arxiv.org/abs/2508.09956v2",
        "pdf_url": "http://arxiv.org/pdf/2508.09956v2",
        "arxiv_id": "2508.09956v2",
        "authors": [
            "Fares Antaki",
            "David Mikhail",
            "Daniel Milad",
            "Danny A Mammo",
            "Sumit Sharma",
            "Sunil K Srivastava",
            "Bing Yu Chen",
            "Samir Touma",
            "Mertcan Sevgi",
            "Jonathan El-Khoury",
            "Pearse A Keane",
            "Qingyu Chen",
            "Yih Chung Tham",
            "Renaud Duval"
        ],
        "submitted": "2025-08-13 17:17:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts",
        "abstract": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations.",
        "url": "http://arxiv.org/abs/2508.09954v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09954v1",
        "arxiv_id": "2508.09954v1",
        "authors": [
            "Johannes Schäfer",
            "Roman Klinger"
        ],
        "submitted": "2025-08-13 17:15:52",
        "source": "arxiv",
        "comment": "May 2025 version"
    },
    {
        "title": "Specialised or Generic? Tokenization Choices for Radiology Language Models",
        "abstract": "The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings.",
        "url": "http://arxiv.org/abs/2508.09952v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09952v1",
        "arxiv_id": "2508.09952v1",
        "authors": [
            "Hermione Warr",
            "Wentian Xu",
            "Harry Anthony",
            "Yasin Ibrahim",
            "Daniel McGowan",
            "Konstantinos Kamnitsas"
        ],
        "submitted": "2025-08-13 17:13:56",
        "source": "arxiv",
        "comment": "Accepted to ELAMI@MICCAI2025"
    },
    {
        "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models",
        "abstract": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
        "url": "http://arxiv.org/abs/2508.09945v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09945v1",
        "arxiv_id": "2508.09945v1",
        "authors": [
            "Lingjie Jiang",
            "Shaohan Huang",
            "Xun Wu",
            "Yixia Li",
            "Dongdong Zhang",
            "Furu Wei"
        ],
        "submitted": "2025-08-13 17:00:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs",
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.",
        "url": "http://arxiv.org/abs/2508.09937v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09937v1",
        "arxiv_id": "2508.09937v1",
        "authors": [
            "Muneeza Azmat",
            "Momin Abbas",
            "Maysa Malfiza Garcia de Macedo",
            "Marcelo Carpinette Grave",
            "Luan Soares de Souza",
            "Tiago Machado",
            "Rogerio A de Paula",
            "Raya Horesh",
            "Yixin Chen",
            "Heloisa Caroline de Souza Pereira Candello",
            "Rebecka Nordenlow",
            "Aminat Adebiyi"
        ],
        "submitted": "2025-08-13 16:42:01",
        "source": "arxiv",
        "comment": "In submission"
    },
    {
        "title": "Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach",
        "abstract": "Business communication digitisation has reorganised the process of persuasive\ndiscourse, which\n  allows not only greater transparency but also advanced deception. This\ninquiry synthesises classical\n  rhetoric and communication psychology with linguistic theory and empirical\nstudies in the financial\n  reporting, sustainability discourse, and digital marketing to explain how\ndeceptive language can be\n  systematically detected using persuasive lexicon. In controlled settings,\ndetection accuracies of greater\n  than 99% were achieved by using computational textual analysis as well as\npersonalised transformer\n  models. However, reproducing this performance in multilingual settings is\nalso problematic and,\n  to a large extent, this is because it is not easy to find sufficient data,\nand because few multilingual\n  text-processing infrastructures are in place. This evidence shows that there\nhas been an increasing\n  gap between the theoretical representations of communication and those\nempirically approximated,\n  and therefore, there is a need to have strong automatic text-identification\nsystems where AI-based\n  discourse is becoming more realistic in communicating with humans.",
        "url": "http://arxiv.org/abs/2508.09935v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09935v1",
        "arxiv_id": "2508.09935v1",
        "authors": [
            "Sayem Hossen",
            "Monalisa Moon Joti",
            "Md. Golam Rashed"
        ],
        "submitted": "2025-08-13 16:38:31",
        "source": "arxiv",
        "comment": "21"
    },
    {
        "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets",
        "abstract": "Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/.",
        "url": "http://arxiv.org/abs/2508.09886v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09886v1",
        "arxiv_id": "2508.09886v1",
        "authors": [
            "Lingyu Chen",
            "Yawen Zeng",
            "Yue Wang",
            "Peng Wan",
            "Guo-chen Ning",
            "Hongen Liao",
            "Daoqiang Zhang",
            "Fang Chen"
        ],
        "submitted": "2025-08-13 15:43:20",
        "source": "arxiv",
        "comment": "ICCV 2025"
    },
    {
        "title": "A Survey of Cognitive Distortion Detection and Classification in NLP",
        "abstract": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area.",
        "url": "http://arxiv.org/abs/2508.09878v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09878v1",
        "arxiv_id": "2508.09878v1",
        "authors": [
            "Archie Sage",
            "Jeroen Keppens",
            "Helen Yannakoudakis"
        ],
        "submitted": "2025-08-13 15:21:17",
        "source": "arxiv",
        "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow"
    },
    {
        "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models",
        "abstract": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.",
        "url": "http://arxiv.org/abs/2508.09874v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09874v1",
        "arxiv_id": "2508.09874v1",
        "authors": [
            "Jiaqi Cao",
            "Jiarui Wang",
            "Rubin Wei",
            "Qipeng Guo",
            "Kai Chen",
            "Bowen Zhou",
            "Zhouhan Lin"
        ],
        "submitted": "2025-08-13 15:16:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription",
        "abstract": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.",
        "url": "http://arxiv.org/abs/2508.09865v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09865v1",
        "arxiv_id": "2508.09865v1",
        "authors": [
            "Abdul Rehman Antall",
            "Naveed Akhtar"
        ],
        "submitted": "2025-08-13 15:01:59",
        "source": "arxiv",
        "comment": "8 pages, 3 figures, 1 table, including references and appendix"
    },
    {
        "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts",
        "abstract": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.",
        "url": "http://arxiv.org/abs/2508.09848v2",
        "pdf_url": "http://arxiv.org/pdf/2508.09848v2",
        "arxiv_id": "2508.09848v2",
        "authors": [
            "Mo Yu",
            "Tsz Ting Chung",
            "Chulun Zhou",
            "Tong Li",
            "Rui Lu",
            "Jiangnan Li",
            "Liyan Xu",
            "Haoshu Lu",
            "Ning Zhang",
            "Jing Li",
            "Jie Zhou"
        ],
        "submitted": "2025-08-13 14:28:25",
        "source": "arxiv",
        "comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude"
    },
    {
        "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models",
        "abstract": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
        "url": "http://arxiv.org/abs/2508.09834v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09834v1",
        "arxiv_id": "2508.09834v1",
        "authors": [
            "Weigao Sun",
            "Jiaxi Hu",
            "Yucheng Zhou",
            "Jusen Du",
            "Disen Lan",
            "Kexin Wang",
            "Tong Zhu",
            "Xiaoye Qu",
            "Yu Zhang",
            "Xiaoyu Mo",
            "Daizong Liu",
            "Yuxuan Liang",
            "Wenliang Chen",
            "Guoqi Li",
            "Yu Cheng"
        ],
        "submitted": "2025-08-13 14:13:46",
        "source": "arxiv",
        "comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch"
    },
    {
        "title": "A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems",
        "abstract": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems.",
        "url": "http://arxiv.org/abs/2508.09809v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09809v1",
        "arxiv_id": "2508.09809v1",
        "authors": [
            "Aishik Mandal",
            "Prottay Kumar Adhikary",
            "Hiba Arnaout",
            "Iryna Gurevych",
            "Tanmoy Chakraborty"
        ],
        "submitted": "2025-08-13 13:42:35",
        "source": "arxiv",
        "comment": "14 pages, 3 figures"
    },
    {
        "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning",
        "abstract": "Charts are essential to data analysis, transforming raw data into clear\nvisual representations that support human decision-making. Although current\nvision-language models (VLMs) have made significant progress, they continue to\nstruggle with chart comprehension due to training on datasets that lack\ndiversity and real-world authenticity, or on automatically extracted underlying\ndata tables of charts, which can contain numerous estimation errors.\nFurthermore, existing models only rely on supervised fine-tuning using these\nlow-quality datasets, severely limiting their effectiveness. To address these\nissues, we first propose BigCharts, a dataset creation pipeline that generates\nvisually diverse chart images by conditioning the rendering process on\nreal-world charts sourced from multiple online platforms. Unlike purely\nsynthetic datasets, BigCharts incorporates real-world data, ensuring\nauthenticity and visual diversity, while still retaining accurate underlying\ndata due to our proposed replotting process. Additionally, we introduce a\ncomprehensive training framework that integrates supervised fine-tuning with\nGroup Relative Policy Optimization (GRPO)-based reinforcement learning. By\nintroducing novel reward signals specifically designed for chart reasoning, our\napproach enhances model robustness and generalization across diverse chart\nstyles and domains, resulting in a state-of-the-art chart reasoning model,\nBigCharts-R1. Extensive experiments demonstrate that our models surpass\nexisting methods on multiple chart question-answering benchmarks compared to\neven larger open-source and closed-source models.",
        "url": "http://arxiv.org/abs/2508.09804v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09804v1",
        "arxiv_id": "2508.09804v1",
        "authors": [
            "Ahmed Masry",
            "Abhay Puri",
            "Masoud Hashemi",
            "Juan A. Rodriguez",
            "Megh Thakkar",
            "Khyati Mahajan",
            "Vikas Yadav",
            "Sathwik Tejaswi Madhusudhan",
            "Alexandre Piché",
            "Dzmitry Bahdanau",
            "Christopher Pal",
            "David Vazquez",
            "Enamul Hoque",
            "Perouz Taslakian",
            "Sai Rajeswar",
            "Spandana Gella"
        ],
        "submitted": "2025-08-13 13:39:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations",
        "abstract": "Existing video recommender systems rely primarily on user-defined metadata or\non low-level visual and acoustic signals extracted by specialised encoders.\nThese low-level features describe what appears on the screen but miss deeper\nsemantics such as intent, humour, and world knowledge that make clips resonate\nwith viewers. For example, is a 30-second clip simply a singer on a rooftop, or\nan ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such\ndistinctions are critical to personalised recommendations yet remain invisible\nto traditional encoding pipelines. In this paper, we introduce a simple,\nrecommendation system-agnostic zero-finetuning framework that injects\nhigh-level semantics into the recommendation pipeline by prompting an\noff-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip\ninto a rich natural-language description (e.g. \"a superhero parody with\nslapstick fights and orchestral stabs\"), bridging the gap between raw content\nand user intent. We use MLLM output with a state-of-the-art text encoder and\nfeed it into standard collaborative, content-based, and generative\nrecommenders. On the MicroLens-100K dataset, which emulates user interactions\nwith TikTok-style videos, our framework consistently surpasses conventional\nvideo, audio, and metadata features in five representative models. Our findings\nhighlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to\nbuild more intent-aware video recommenders.",
        "url": "http://arxiv.org/abs/2508.09789v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09789v1",
        "arxiv_id": "2508.09789v1",
        "authors": [
            "Marco De Nadai",
            "Andreas Damianou",
            "Mounia Lalmas"
        ],
        "submitted": "2025-08-13 13:19:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges",
        "abstract": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.",
        "url": "http://arxiv.org/abs/2508.09786v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09786v1",
        "arxiv_id": "2508.09786v1",
        "authors": [
            "Mahdi Dhaini",
            "Tobias Müller",
            "Roksoliana Rabets",
            "Gjergji Kasneci"
        ],
        "submitted": "2025-08-13 13:12:18",
        "source": "arxiv",
        "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)"
    },
    {
        "title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study",
        "abstract": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
        "url": "http://arxiv.org/abs/2508.09776v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09776v1",
        "arxiv_id": "2508.09776v1",
        "authors": [
            "Mahdi Dhaini",
            "Juraj Vladika",
            "Ege Erdogan",
            "Zineb Attaoui",
            "Gjergji Kasneci"
        ],
        "submitted": "2025-08-13 12:59:08",
        "source": "arxiv",
        "comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)"
    },
    {
        "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech",
        "abstract": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.",
        "url": "http://arxiv.org/abs/2508.09767v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09767v1",
        "arxiv_id": "2508.09767v1",
        "authors": [
            "Shuhei Kato"
        ],
        "submitted": "2025-08-13 12:52:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation",
        "abstract": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.",
        "url": "http://arxiv.org/abs/2508.09755v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09755v1",
        "arxiv_id": "2508.09755v1",
        "authors": [
            "Seokgi Lee"
        ],
        "submitted": "2025-08-13 12:35:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion",
        "abstract": "Retrieval-augmented generation (RAG) for repository-level code completion\ncommonly relies on superficial text similarity, leading to results plagued by\nsemantic misguidance, redundancy, and homogeneity, while also failing to\nresolve external symbol ambiguity. To address these challenges, we introduce\nSaracoder, a Hierarchical Feature-Optimized retrieval framework. Its core\nHierarchical Feature Optimization module systematically refines candidates by\ndistilling deep semantic relationships, pruning exact duplicates, assessing\nstructural similarity with a novel graph-based metric that weighs edits by\ntheir topological importance, and reranking results to maximize both relevance\nand diversity. Furthermore, an External-Aware Identifier Disambiguator module\naccurately resolves cross-file symbol ambiguity via dependency analysis.\nExtensive experiments on the challenging CrossCodeEval and RepoEval-Updated\nbenchmarks demonstrate that Saracoder significantly outperforms existing\nbaselines across multiple programming languages and models. Our work proves\nthat systematically refining retrieval results across multiple dimensions\nprovides a new paradigm for building more accurate and robust repository-level\ncode completion systems.",
        "url": "http://arxiv.org/abs/2508.10068v1",
        "pdf_url": "http://arxiv.org/pdf/2508.10068v1",
        "arxiv_id": "2508.10068v1",
        "authors": [
            "Xiaohan Chen",
            "Zhongying Pan",
            "Quan Feng",
            "Yu Tian",
            "Shuqun Yang",
            "Mengru Wang",
            "Lina Gong",
            "Yuxia Geng",
            "Piji Li",
            "Xiang Chen"
        ],
        "submitted": "2025-08-13 11:56:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning",
        "abstract": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.",
        "url": "http://arxiv.org/abs/2508.09726v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09726v1",
        "arxiv_id": "2508.09726v1",
        "authors": [
            "Vaishnavi Shrivastava",
            "Ahmed Awadallah",
            "Vidhisha Balachandran",
            "Shivam Garg",
            "Harkirat Behl",
            "Dimitris Papailiopoulos"
        ],
        "submitted": "2025-08-13 11:43:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models",
        "abstract": "Information visualizations are powerful tools that help users quickly\nidentify patterns, trends, and outliers, facilitating informed decision-making.\nHowever, when visualizations incorporate deceptive design elements-such as\ntruncated or inverted axes, unjustified 3D effects, or violations of best\npractices-they can mislead viewers and distort understanding, spreading\nmisinformation. While some deceptive tactics are obvious, others subtly\nmanipulate perception while maintaining a facade of legitimacy. As\nVision-Language Models (VLMs) are increasingly used to interpret\nvisualizations, especially by non-expert users, it is critical to understand\nhow susceptible these models are to deceptive visual designs. In this study, we\nconduct an in-depth evaluation of VLMs' ability to interpret misleading\nvisualizations. By analyzing over 16,000 responses from ten different models\nacross eight distinct types of misleading chart designs, we demonstrate that\nmost VLMs are deceived by them. This leads to altered interpretations of\ncharts, despite the underlying data remaining the same. Our findings highlight\nthe need for robust safeguards in VLMs against visual misinformation.",
        "url": "http://arxiv.org/abs/2508.09716v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09716v1",
        "arxiv_id": "2508.09716v1",
        "authors": [
            "Ridwan Mahbub",
            "Mohammed Saidul Islam",
            "Md Tahmid Rahman Laskar",
            "Mizanur Rahman",
            "Mir Tafseer Nayeem",
            "Enamul Hoque"
        ],
        "submitted": "2025-08-13 11:11:18",
        "source": "arxiv",
        "comment": "Accepted to IEEE VIS 2025"
    },
    {
        "title": "Evaluating the Role of Large Language Models in Legal Practice in India",
        "abstract": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.",
        "url": "http://arxiv.org/abs/2508.09713v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09713v1",
        "arxiv_id": "2508.09713v1",
        "authors": [
            "Rahul Hemrajani"
        ],
        "submitted": "2025-08-13 11:04:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation",
        "abstract": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.",
        "url": "http://arxiv.org/abs/2508.09666v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09666v1",
        "arxiv_id": "2508.09666v1",
        "authors": [
            "Ziyang Ma",
            "Qingyue Yuan",
            "Linhai Zhang",
            "Deyu Zhou"
        ],
        "submitted": "2025-08-13 09:56:08",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Multimodal Fusion And Sparse Attention-based Alignment Model for Long Sequential Recommendation",
        "abstract": "Recent advances in multimodal recommendation enable richer item\nunderstanding, while modeling users' multi-scale interests across temporal\nhorizons has attracted growing attention. However, effectively exploiting\nmultimodal item sequences and mining multi-grained user interests to\nsubstantially bridge the gap between content comprehension and recommendation\nremain challenging. To address these issues, we propose MUFASA, a MUltimodal\nFusion And Sparse Attention-based Alignment model for long sequential\nrecommendation. Our model comprises two core components. First, the Multimodal\nFusion Layer (MFL) leverages item titles as a cross-genre semantic anchor and\nis trained with a joint objective of four tailored losses that promote: (i)\ncross-genre semantic alignment, (ii) alignment to the collaborative space for\nrecommendation, (iii) preserving the similarity structure defined by titles and\npreventing modality representation collapse, and (iv) distributional\nregularization of the fusion space. This yields high-quality fused item\nrepresentations for further preference alignment. Second, the Sparse\nAttention-guided Alignment Layer (SAL) scales to long user-behavior sequences\nvia a multi-granularity sparse attention mechanism, which incorporates windowed\nattention, block-level attention, and selective attention, to capture user\ninterests hierarchically and across temporal horizons. SAL explicitly models\nboth the evolution of coherent interest blocks and fine-grained intra-block\nvariations, producing robust user and item representations. Extensive\nexperiments on real-world benchmarks show that MUFASA consistently surpasses\nstate-of-the-art baselines. Moreover, online A/B tests demonstrate significant\ngains in production, confirming MUFASA's effectiveness in leveraging multimodal\ncues and accurately capturing diverse user preferences.",
        "url": "http://arxiv.org/abs/2508.09664v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09664v1",
        "arxiv_id": "2508.09664v1",
        "authors": [
            "Yongrui Fu",
            "Jian Liu",
            "Tao Li",
            "Zonggang Wu",
            "Shouke Qin",
            "Hanmeng Liu"
        ],
        "submitted": "2025-08-13 09:50:44",
        "source": "arxiv",
        "comment": "10 pages"
    },
    {
        "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization",
        "abstract": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs.",
        "url": "http://arxiv.org/abs/2508.09662v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09662v1",
        "arxiv_id": "2508.09662v1",
        "authors": [
            "Yaoning Wang",
            "Jiahao Ying",
            "Yixin Cao",
            "Yubo Ma",
            "Yugang Jiang"
        ],
        "submitted": "2025-08-13 09:48:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss",
        "abstract": "Increasing diversity in language models is a challenging yet essential\nobjective. A common approach is to raise the decoding temperature. In this\nwork, we investigate this approach through a simplistic yet common case to\nprovide insights into why decreasing temperature can improve quality\n(Precision), while increasing it often fails to boost coverage (Recall). Our\nanalysis reveals that for a model to be effectively tunable through temperature\nadjustments, it must be trained toward coverage. To address this, we propose\nrethinking loss functions in language models by leveraging the Precision-Recall\nframework. Our results demonstrate that this approach achieves a substantially\nbetter trade-off between Precision and Recall than merely combining negative\nlog-likelihood training with temperature scaling. These findings offer a\npathway toward more versatile and robust language modeling techniques.",
        "url": "http://arxiv.org/abs/2508.09654v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09654v1",
        "arxiv_id": "2508.09654v1",
        "authors": [
            "Alexandre Verine",
            "Florian Le Bronnec",
            "Kunhao Zheng",
            "Alexandre Allauzen",
            "Yann Chevaleyre",
            "Benjamin Negrevergne"
        ],
        "submitted": "2025-08-13 09:37:53",
        "source": "arxiv",
        "comment": "Forty-Second International Conference on Machine Learning, ICML2025"
    },
    {
        "title": "On Negative-aware Preference Optimization for Recommendation",
        "abstract": "Recommendation systems leverage user interaction data to suggest relevant\nitems while filtering out irrelevant (negative) ones. The rise of large\nlanguage models (LLMs) has garnered increasing attention for their potential in\nrecommendation tasks. However, existing methods for optimizing LLM-based\nrecommenders face challenges in effectively utilizing negative samples. Simply\nintegrating large numbers of negative samples can improve ranking accuracy and\nmitigate popularity bias but often leads to increased computational overhead\nand memory costs. Additionally, current approaches fail to account for the\nvarying informativeness of negative samples, leading to suboptimal optimization\nperformance. To address these issues, we propose NAPO\n(\\textbf{N}egative-\\textbf{A}ware \\textbf{P}reference \\textbf{O}ptimization),\nan enhanced framework for preference optimization in LLM-based recommendation.\nNAPO introduces two key innovations: (1) in-batch negative sharing, which\nexpands the pool of negative samples without additional memory overhead, and\n(2) dynamic reward margin adjustment, which adapts model updates based on the\nconfidence of negative samples. Extensive experiments on three public datasets\ndemonstrate that NAPO outperforms existing methods in both recommendation\naccuracy and popularity bias reduction.",
        "url": "http://arxiv.org/abs/2508.09653v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09653v1",
        "arxiv_id": "2508.09653v1",
        "authors": [
            "Chenlu Ding",
            "Daoxuan Liu",
            "Jiancan Wu",
            "Xingyu Hu",
            "Junkang Wu",
            "Haitao Wang",
            "Yongkang Wang",
            "Xingxing Wang",
            "Xiang Wang"
        ],
        "submitted": "2025-08-13 09:37:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories",
        "abstract": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach.",
        "url": "http://arxiv.org/abs/2508.09651v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09651v1",
        "arxiv_id": "2508.09651v1",
        "authors": [
            "Daniel Raffini",
            "Agnese Macori",
            "Marco Angelini",
            "Tiziana Catarci"
        ],
        "submitted": "2025-08-13 09:34:37",
        "source": "arxiv",
        "comment": "8-pages"
    },
    {
        "title": "Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data",
        "abstract": "In this paper, we present a novel model architecture for optimizing\npersonalized product search ranking using a multi-task learning (MTL)\nframework. Our approach uniquely integrates tabular and non-tabular data,\nleveraging a pre-trained TinyBERT model for semantic embeddings and a novel\nsampling technique to capture diverse customer behaviors. We evaluate our model\nagainst several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,\nand MMoE, focusing on their ability to handle mixed data types and optimize\npersonalized ranking. Additionally, we propose a scalable relevance labeling\nmechanism based on click-through rates, click positions, and semantic\nsimilarity, offering an alternative to traditional human-annotated labels.\nExperimental results show that combining non-tabular data with advanced\nembedding techniques in multi-task learning paradigm significantly enhances\nmodel performance. Ablation studies further underscore the benefits of\nincorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT\nquery-product embedding interactions. These results demonstrate the\neffectiveness of our approach in achieving improved personalized product search\nranking.",
        "url": "http://arxiv.org/abs/2508.09636v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09636v1",
        "arxiv_id": "2508.09636v1",
        "authors": [
            "Lalitesh Morishetti",
            "Abhay Kumar",
            "Jonathan Scott",
            "Kaushiki Nag",
            "Gunjan Sharma",
            "Shanu Vashishtha",
            "Rahul Sridhar",
            "Rohit Chatter",
            "Kannan Achan"
        ],
        "submitted": "2025-08-13 09:15:08",
        "source": "arxiv",
        "comment": "17 pages, 2 figures, The Pacific Rim International Conference on\n  Artificial Intelligence (PRICAI-2025) Conference"
    },
    {
        "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian",
        "abstract": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025.",
        "url": "http://arxiv.org/abs/2508.09622v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09622v1",
        "arxiv_id": "2508.09622v1",
        "authors": [
            "Tatiana Batura",
            "Elena Bruches",
            "Milana Shvenk",
            "Valentin Malykh"
        ],
        "submitted": "2025-08-13 08:53:17",
        "source": "arxiv",
        "comment": "AINL 2025 Conference"
    },
    {
        "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments",
        "abstract": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research.",
        "url": "http://arxiv.org/abs/2508.09614v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09614v1",
        "arxiv_id": "2508.09614v1",
        "authors": [
            "Daniel Raffini",
            "Agnese Macori",
            "Lorenzo Porcaro",
            "Tiziana Catarci",
            "Marco Angelini"
        ],
        "submitted": "2025-08-13 08:45:04",
        "source": "arxiv",
        "comment": "9-pages"
    },
    {
        "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage",
        "abstract": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.",
        "url": "http://arxiv.org/abs/2508.09603v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09603v1",
        "arxiv_id": "2508.09603v1",
        "authors": [
            "Skyler Hallinan",
            "Jaehun Jung",
            "Melanie Sclar",
            "Ximing Lu",
            "Abhilasha Ravichander",
            "Sahana Ramnath",
            "Yejin Choi",
            "Sai Praneeth Karimireddy",
            "Niloofar Mireshghallah",
            "Xiang Ren"
        ],
        "submitted": "2025-08-13 08:35:16",
        "source": "arxiv",
        "comment": "CoLM 2025"
    },
    {
        "title": "TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking",
        "abstract": "Reasoning-intensive ranking models built on Large Language Models (LLMs) have\nmade notable progress, but existing approaches often rely on large-scale LLMs\nand explicit Chain-of-Thought (CoT) reasoning, resulting in high computational\ncost and latency that limit real-world use. To address this, we propose\n\\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale\nLLMs. To improve ranking performance, TFRank effectively integrates CoT data,\nfine-grained score supervision, and multi-task training. Furthermore, it\nachieves an efficient ``\\textbf{T}hink-\\textbf{F}ree\" reasoning capability by\nemploying a ``think-mode switch'' and pointwise format constraints.\nSpecifically, this allows the model to leverage explicit reasoning during\ntraining while delivering precise relevance scores for complex queries at\ninference without generating any reasoning chains. Experiments show that TFRank\n(e.g., 1.7B) achieves performance comparable to models with four times more\nparameters on the BRIGHT benchmark, and demonstrates strong competitiveness on\nthe BEIR benchmark. Further analysis shows that TFRank achieves an effective\nbalance between performance and efficiency, providing a practical solution for\nintegrating advanced reasoning into real-world systems. Our code and data are\nreleased in the repository: https://github.com/JOHNNY-fans/TFRank.",
        "url": "http://arxiv.org/abs/2508.09539v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09539v1",
        "arxiv_id": "2508.09539v1",
        "authors": [
            "Yongqi Fan",
            "Xiaoyang Chen",
            "Dezhi Ye",
            "Jie Liu",
            "Haijin Liang",
            "Jin Ma",
            "Ben He",
            "Yingfei Sun",
            "Tong Ruan"
        ],
        "submitted": "2025-08-13 06:47:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI Blob! LLM-Driven Recontextualization of Italian Television Archives",
        "abstract": "This paper introduces AI Blob!, an experimental system designed to explore\nthe potential of semantic cataloging and Large Language Models (LLMs) for the\nretrieval and recontextualization of archival television footage. Drawing\nmethodological inspiration from Italian television programs such as Blob (RAI\nTre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic\nembeddings, and retrieval-augmented generation (RAG) to organize and\nreinterpret archival content. The system processes a curated dataset of 1,547\nItalian television videos by transcribing audio, segmenting it into\nsentence-level units, and embedding these segments into a vector database for\nsemantic querying. Upon user input of a thematic prompt, the LLM generates a\nrange of linguistically and conceptually related queries, guiding the retrieval\nand recombination of audiovisual fragments. These fragments are algorithmically\nselected and structured into narrative sequences producing montages that\nemulate editorial practices of ironic juxtaposition and thematic coherence. By\nforegrounding dynamic, content-aware retrieval over static metadata schemas, AI\nBlob! demonstrates how semantic technologies can facilitate new approaches to\narchival engagement, enabling novel forms of automated narrative construction\nand cultural analysis. The project contributes to ongoing debates in media\nhistoriography and AI-driven archival research, offering both a conceptual\nframework and a publicly available dataset to support further interdisciplinary\nexperimentation.",
        "url": "http://arxiv.org/abs/2508.09535v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09535v1",
        "arxiv_id": "2508.09535v1",
        "authors": [
            "Roberto Balestri"
        ],
        "submitted": "2025-08-13 06:38:32",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Improving Dense Passage Retrieval with Multiple Positive Passages",
        "abstract": "By leveraging a dual encoder architecture, Dense Passage Retrieval (DPR) has\noutperformed traditional sparse retrieval algorithms such as BM25 in terms of\npassage retrieval accuracy. Recently proposed methods have further enhanced\nDPR's performance. However, these models typically pair each question with only\none positive passage during training, and the effect of associating multiple\npositive passages has not been examined. In this paper, we explore the\nperformance of DPR when additional positive passages are incorporated during\ntraining. Experimental results show that equipping each question with multiple\npositive passages consistently improves retrieval accuracy, even when using a\nsignificantly smaller batch size, which enables training on a single GPU.",
        "url": "http://arxiv.org/abs/2508.09534v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09534v1",
        "arxiv_id": "2508.09534v1",
        "authors": [
            "Shuai Chang"
        ],
        "submitted": "2025-08-13 06:36:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation",
        "abstract": "Emotional support conversations are crucial for promoting emotional\nwell-being, yet current models often lack deep empathetic reasoning grounded in\npsychological principles. To address this, we propose controllable empathetic\nreasoning, which combines natural language reasoning with structured\npsychological steps. We construct a fine-grained dataset annotated with\nreasoning correctness and response preferences to enable this capability. To\nfurther enhance training, we employ reinforcement learning with a unified\nprocess-outcome reward model that delivers precise feedback. To mitigate\nresponse repetitiveness from entropy collapse, we introduce personality-based\ndialogue rewriting and a redundancy-aware reward reweighting strategy. Our\napproach significantly improves model's emotional support ability, advancing\nthe development of empathetic, human-like support systems.",
        "url": "http://arxiv.org/abs/2508.09521v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09521v1",
        "arxiv_id": "2508.09521v1",
        "authors": [
            "Yunxiao Wang",
            "Meng Liu",
            "Wenqi Liu",
            "Kaiyu Jiang",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Guorui Zhou",
            "Liqiang Nie"
        ],
        "submitted": "2025-08-13 06:09:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval",
        "abstract": "This paper presents a zero-shot system for fact-checked claim retrieval. We\nemployed several state-of-the-art large language models to obtain text\nembeddings. The models were then combined to obtain the best possible result.\nOur approach achieved 7th place in monolingual and 9th in cross-lingual\nsubtasks. We used only English translations as an input to the text embedding\nmodels since multilingual models did not achieve satisfactory results. We\nidentified the most relevant claims for each post by leveraging the embeddings\nand measuring cosine similarity. Overall, the best results were obtained by the\nNVIDIA NV-Embed-v2 model. For some languages, we benefited from model\ncombinations (NV-Embed & GPT or Mistral).",
        "url": "http://arxiv.org/abs/2508.09517v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09517v1",
        "arxiv_id": "2508.09517v1",
        "authors": [
            "Ladislav Lenc",
            "Daniel Cífka",
            "Jiří Martínek",
            "Jakub Šmíd",
            "Pavel Král"
        ],
        "submitted": "2025-08-13 05:55:59",
        "source": "arxiv",
        "comment": "Published in Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025). Official version:\n  https://aclanthology.org/2025.semeval-1.31/"
    },
    {
        "title": "Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges",
        "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that focuses on understanding opinions at the aspect level, including\nsentiment towards specific aspect terms, categories, and opinions. While ABSA\nresearch has seen significant progress, much of the focus has been on\nmonolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from\nresource-rich languages (such as English) to low-resource languages, remains an\nunder-explored area, with no systematic review of the field. This paper aims to\nfill that gap by providing a comprehensive survey of cross-lingual ABSA. We\nsummarize key ABSA tasks, including aspect term extraction, aspect sentiment\nclassification, and compound tasks involving multiple sentiment elements.\nAdditionally, we review the datasets, modelling paradigms, and cross-lingual\ntransfer methods used to solve these tasks. We also examine how existing work\nin monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to\nthe development of cross-lingual ABSA. Finally, we highlight the main\nchallenges and suggest directions for future research to advance cross-lingual\nABSA systems.",
        "url": "http://arxiv.org/abs/2508.09516v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09516v1",
        "arxiv_id": "2508.09516v1",
        "authors": [
            "Jakub Šmíd",
            "Pavel Král"
        ],
        "submitted": "2025-08-13 05:55:53",
        "source": "arxiv",
        "comment": "Submitted version prior to peer review. Updated version accepted in\n  Information Fusion. Official version:\n  https://www.sciencedirect.com/science/article/pii/S1566253525001460"
    },
    {
        "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation",
        "abstract": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed\nsentiment analysis in a target language by transferring knowledge from a source\nlanguage with available annotated data. Most existing methods depend heavily on\noften unreliable translation tools to bridge the language gap. In this paper,\nwe propose a new approach that leverages a large language model (LLM) to\ngenerate high-quality pseudo-labelled data in the target language without the\nneed for translation tools. First, the framework trains an ABSA model to obtain\npredictions for unlabelled target language data. Next, LLM is prompted to\ngenerate natural sentences that better represent these noisy predictions than\nthe original text. The ABSA model is then further fine-tuned on the resulting\npseudo-labelled dataset. We demonstrate the effectiveness of this method across\nsix languages and five backbone models, surpassing previous state-of-the-art\ntranslation-based approaches. The proposed framework also supports generative\nmodels, and we show that fine-tuned LLMs outperform smaller multilingual\nmodels.",
        "url": "http://arxiv.org/abs/2508.09515v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09515v1",
        "arxiv_id": "2508.09515v1",
        "authors": [
            "Jakub Šmíd",
            "Pavel Přibáň",
            "Pavel Král"
        ],
        "submitted": "2025-08-13 05:55:48",
        "source": "arxiv",
        "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).\n  Official version: https://aclanthology.org/2025.acl-long.41/"
    },
    {
        "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their\nreranking modules, which typically score passages independently and select a\nfixed Top-K size. This approach struggles with complex multi-hop queries that\nrequire synthesizing evidence across multiple documents, creating a trade-off\nwhere small K values omit crucial information and large K values introduce\nnoise. To address this, we introduce the Dynamic Passage Selector (DPS), a\nnovel reranking framework that treats passage selection as a supervised\nlearning problem. Unlike traditional point-wise or list-wise methods, DPS is\nfine-tuned to capture inter-passage dependencies and dynamically select the\nmost relevant set of passages for generation. As a seamless plug-and-play\nmodule, DPS requires no modifications to the standard RAG pipeline.\nComprehensive evaluations on five benchmarks show that DPS consistently\noutperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the\nchallenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over\nstrong baselines like Qwen3-reranker and RankingGPT, respectively. Our results\ndemonstrate that by enabling adaptive evidence selection, DPS substantially\nenhances reasoning capabilities in complex RAG scenarios.",
        "url": "http://arxiv.org/abs/2508.09497v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09497v1",
        "arxiv_id": "2508.09497v1",
        "authors": [
            "Siyuan Meng",
            "Junming Liu",
            "Yirong Chen",
            "Song Mao",
            "Pinlong Cai",
            "Guohang Yan",
            "Botian Shi",
            "Ding Wang"
        ],
        "submitted": "2025-08-13 05:05:34",
        "source": "arxiv",
        "comment": "9 pages, 4 tables"
    },
    {
        "title": "Learning Facts at Scale with Active Reading",
        "abstract": "LLMs are known to store vast amounts of knowledge in their parametric memory.\nHowever, learning and recalling facts from this memory is known to be\nunreliable, depending largely on the prevalence of particular facts in the\ntraining data and other factors which are poorly understood. Practitioners are\nlacking tools which will allow them to ensure that the models learn a given\nbody of knowledge reliably and consistently. To this end, we propose Active\nReading: a framework where we train models to study a given set of material\nwith self-generated learning strategies. First, we demonstrate models trained\nwith Active Reading on expert domains absorb significantly more knowledge than\nvanilla finetuning and other data augmentations. We train expert 8B models that\nachieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over\nvanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla\nfinetuning) by applying Active Reading to the source documents for each\nbenchmark. Finally, we show that Active Reading can be utilized at pre-training\nscale to build more factual models. As a demonstration of this, we release Meta\nWikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,\nwhich outcompetes models with hundreds of billions of parameters on factual QA.",
        "url": "http://arxiv.org/abs/2508.09494v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09494v1",
        "arxiv_id": "2508.09494v1",
        "authors": [
            "Jessy Lin",
            "Vincent-Pierre Berges",
            "Xilun Chen",
            "Wen-Tau Yih",
            "Gargi Ghosh",
            "Barlas Oğuz"
        ],
        "submitted": "2025-08-13 04:54:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs",
        "abstract": "Ensuring robust safety alignment while preserving utility is critical for the\nreliable deployment of Large Language Models (LLMs). However, current\ntechniques fundamentally suffer from intertwined deficiencies: insufficient\nrobustness against malicious attacks, frequent refusal of benign queries,\ndegradation in generated text quality and general task performance--the former\ntwo reflecting deficits in robust safety and the latter constituting utility\nimpairment. We trace these limitations to the coarse-grained layer-wise\ninterventions in existing methods. To resolve this, we propose NeuronTune, a\nfine-grained framework that dynamically modulates sparse neurons to achieve\nsimultaneous safety-utility optimization. Our approach first identifies\nsafety-critical and utility-preserving neurons across all layers via\nattribution, then employs meta-learning to adaptively amplify safety-neuron\nactivations and suppress utility-neuron activations. Crucially, NeuronTune\nenables tunable adjustment of intervention scope via neuron-count thresholds,\nsupporting flexible adaptation to security-critical or utility-priority\nscenarios. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing state-of-the-art technologies, achieving\nsuperior model safety while maintaining excellent utility.",
        "url": "http://arxiv.org/abs/2508.09473v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09473v1",
        "arxiv_id": "2508.09473v1",
        "authors": [
            "Birong Pan",
            "Mayi Xu",
            "Qiankun Pi",
            "Jianhao Chen",
            "Yuanyuan Zhu",
            "Ming Zhong",
            "Tieyun Qian"
        ],
        "submitted": "2025-08-13 04:05:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling",
        "abstract": "Existing benchmarks for large language models (LLMs) predominantely focus on\nassessing their capabilities through verifiable tasks. Such objective and\nstatic benchmarks offer limited utility for practical LLM selection, making it\ndifficult for users to find suitable models for their individual needs. To\nbridge this gap, we present the first User-Centric Subjective Leaderboard\n(USL), which provides a preference-driven, dynamic ranking of LLMs across\ndiverse real-world scenarios. Our work is built upon a thorough investigation\nof real human preference data, involving more than 10K subjective queries. Our\ninvestigation reveals significant diversity and contradictions in human\npreferences, which limit the effectiveness of state-of-the-art reward models.\nTo address this, we introduce Customizable Reward Models (CRMs). With only 4B\nparameters, our CRM surpasses the performance of leading models such as GPT-4.1\nand Gemini-2.5-pro, showing exceptional generalization capabilities across new\ntopics and criteria. The USL, powered by CRMs, exhibits strong negative\ncorrelations to contradictory preferences.",
        "url": "http://arxiv.org/abs/2508.09463v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09463v1",
        "arxiv_id": "2508.09463v1",
        "authors": [
            "Qi Jia",
            "Xiujie Song",
            "Zicheng Zhang",
            "Yijin Guo",
            "Kaiwei Zhang",
            "Zijian Chen",
            "Guangtao Zhai"
        ],
        "submitted": "2025-08-13 03:39:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation",
        "abstract": "Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) significantly\nenhances the reasoning capabilities of LargeLanguage Models by leveraging\nstructured knowledge. However, existing KG-RAG frameworks typically operate as\nopen-loop systems, suffering from cognitive blindness, an inability to\nrecognize their exploration deficiencies. This leads to relevance drift and\nincomplete evidence, which existing self-refinement methods, designed for\nunstructured text-based RAG, cannot effectively resolve due to the\npath-dependent nature of graph exploration. To address this challenge, we\npropose Metacognitive Knowledge Graph Retrieval Augmented Generation\n(MetaKGRAG), a novel framework inspired by the human metacognition process,\nwhich introduces a Perceive-Evaluate-Adjust cycle to enable path-aware,\nclosed-loop refinement. This cycle empowers the system to self-assess\nexploration quality, identify deficiencies in coverage or relevance, and\nperform trajectory-connected corrections from precise pivot points. Extensive\nexperiments across five datasets in the medical, legal, and commonsense\nreasoning domains demonstrate that MetaKGRAG consistently outperforms strong\nKG-RAG and self-refinement baselines. Our results validate the superiority of\nour approach and highlight the critical need for path-aware refinement in\nstructured knowledge retrieval.",
        "url": "http://arxiv.org/abs/2508.09460v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09460v1",
        "arxiv_id": "2508.09460v1",
        "authors": [
            "Xujie Yuan",
            "Shimin Di",
            "Jielong Tang",
            "Libin Zheng",
            "Jian Yin"
        ],
        "submitted": "2025-08-13 03:35:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
        "abstract": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.",
        "url": "http://arxiv.org/abs/2508.09456v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09456v1",
        "arxiv_id": "2508.09456v1",
        "authors": [
            "Junxian Li",
            "Beining Xu",
            "Di Zhang"
        ],
        "submitted": "2025-08-13 03:22:19",
        "source": "arxiv",
        "comment": "13 pages, 13 Figures"
    },
    {
        "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text",
        "abstract": "Charts are very common for exploring data and communicating insights, but\nextracting key takeaways from charts and articulating them in natural language\ncan be challenging. The chart-to-text task aims to automate this process by\ngenerating textual summaries of charts. While with the rapid advancement of\nlarge Vision-Language Models (VLMs), we have witnessed great progress in this\ndomain, little to no attention has been given to potential biases in their\noutputs. This paper investigates how VLMs can amplify geo-economic biases when\ngenerating chart summaries, potentially causing societal harm. Specifically, we\nconduct a large-scale evaluation of geo-economic biases in VLM-generated chart\nsummaries across 6,000 chart-country pairs from six widely used proprietary and\nopen-source models to understand how a country's economic status influences the\nsentiment of generated summaries. Our analysis reveals that existing VLMs tend\nto produce more positive descriptions for high-income countries compared to\nmiddle- or low-income countries, even when country attribution is the only\nvariable changed. We also find that models such as GPT-4o-mini,\nGemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further\nexplore inference-time prompt-based debiasing techniques using positive\ndistractors but find them only partially effective, underscoring the complexity\nof the issue and the need for more robust debiasing strategies. Our code and\ndataset are publicly available here.",
        "url": "http://arxiv.org/abs/2508.09450v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09450v1",
        "arxiv_id": "2508.09450v1",
        "authors": [
            "Ridwan Mahbub",
            "Mohammed Saidul Islam",
            "Mir Tafseer Nayeem",
            "Md Tahmid Rahman Laskar",
            "Mizanur Rahman",
            "Shafiq Joty",
            "Enamul Hoque"
        ],
        "submitted": "2025-08-13 03:09:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference",
        "abstract": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
        "url": "http://arxiv.org/abs/2508.09442v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09442v1",
        "arxiv_id": "2508.09442v1",
        "authors": [
            "Zhifan Luo",
            "Shuo Shao",
            "Su Zhang",
            "Lijing Zhou",
            "Yuke Hu",
            "Chenxu Zhao",
            "Zhihao Liu",
            "Zhan Qin"
        ],
        "submitted": "2025-08-13 02:48:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech",
        "abstract": "Code-switching and language identification in child-directed scenarios\npresent significant challenges, particularly in bilingual environments. This\npaper addresses this challenge by using Zipformer to handle the nuances of\nspeech, which contains two imbalanced languages, Mandarin and English, in an\nutterance. This work demonstrates that the internal layers of the Zipformer\neffectively encode the language characteristics, which can be leveraged in\nlanguage identification. We present the selection methodology of the inner\nlayers to extract the embeddings and make a comparison with different\nback-ends. Our analysis shows that Zipformer is robust across these backends.\nOur approach effectively handles imbalanced data, achieving a Balanced Accuracy\n(BAC) of 81.89%, a 15.47% improvement over the language identification\nbaseline. These findings highlight the potential of the transformer encoder\narchitecture model in real scenarios.",
        "url": "http://arxiv.org/abs/2508.09430v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09430v1",
        "arxiv_id": "2508.09430v1",
        "authors": [
            "Lavanya Shankar",
            "Leibny Paola Garcia Perera"
        ],
        "submitted": "2025-08-13 02:10:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models",
        "abstract": "Expanding the abbreviated column names of tables, such as \"esal\" to \"employee\nsalary\", is critical for numerous downstream data tasks. This problem arises in\nenterprises, domain sciences, government agencies, and more. In this paper we\nmake three contributions that significantly advances the state of the art.\nFirst, we show that synthetic public data used by prior work has major\nlimitations, and we introduce 4 new datasets in enterprise/science domains,\nwith real-world abbreviations. Second, we show that accuracy measures used by\nprior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences.",
        "url": "http://arxiv.org/abs/2508.09403v2",
        "pdf_url": "http://arxiv.org/pdf/2508.09403v2",
        "arxiv_id": "2508.09403v2",
        "authors": [
            "Ting Cai",
            "Stephen Sheen",
            "AnHai Doan"
        ],
        "submitted": "2025-08-13 00:39:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs",
        "abstract": "Prosody conveys rich emotional and semantic information of the speech signal\nas well as individual idiosyncrasies. We propose a stand-alone model that maps\ntext-to-prosodic features such as F0 and energy and can be used in downstream\ntasks such as TTS. The ProMode encoder takes as input acoustic features and\ntime-aligned textual content, both are partially masked, and obtains a\nfixed-length latent prosodic embedding. The decoder predicts acoustics in the\nmasked region using both the encoded prosody input and unmasked textual\ncontent. Trained on the GigaSpeech dataset, we compare our method with\nstate-of-the-art style encoders. For F0 and energy predictions, we show\nconsistent improvements for our model at different levels of granularity. We\nalso integrate these predicted prosodic features into a TTS system and conduct\nperceptual tests, which show higher prosody preference compared to the\nbaselines, demonstrating the model's potential in tasks where prosody modeling\nis important.",
        "url": "http://arxiv.org/abs/2508.09389v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09389v1",
        "arxiv_id": "2508.09389v1",
        "authors": [
            "Eray Eren",
            "Qingju Liu",
            "Hyeongwoo Kim",
            "Pablo Garrido",
            "Abeer Alwan"
        ],
        "submitted": "2025-08-12 23:12:18",
        "source": "arxiv",
        "comment": "Interspeech 2025; demo page at\n  https://promode8272.github.io/promode/index.html"
    },
    {
        "title": "APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification",
        "abstract": "Recent advancements in large language models (LLMs) have enabled a wide range\nof natural language processing (NLP) tasks to be performed through simple\nprompt-based interactions. Consequently, several approaches have been proposed\nto engineer prompts that most effectively enable LLMs to perform a given task\n(e.g., chain-of-thought prompting). In settings with a well-defined metric to\noptimize model performance, automatic prompt optimization (APO) methods have\nbeen developed to refine a seed prompt. Advancing this line of research, we\npropose APIO, a simple but effective prompt induction and optimization approach\nfor the tasks of Grammatical Error Correction (GEC) and Text Simplification,\nwithout relying on manually specified seed prompts. APIO achieves a new\nstate-of-the-art performance for purely LLM-based prompting methods on these\ntasks. We make our data, code, prompts, and outputs publicly available.",
        "url": "http://arxiv.org/abs/2508.09378v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09378v1",
        "arxiv_id": "2508.09378v1",
        "authors": [
            "Artem Chernodub",
            "Aman Saini",
            "Yejin Huh",
            "Vivek Kulkarni",
            "Vipul Raheja"
        ],
        "submitted": "2025-08-12 22:26:32",
        "source": "arxiv",
        "comment": "Accepted for publication at Recent Advances in Natural Language\n  Processing conference (RANLP 2025)"
    },
    {
        "title": "A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition",
        "abstract": "Continuous Sign Language Recognition (CSLR) faces multiple challenges,\nincluding significant inter-signer variability and poor generalization to novel\nsentence structures. Traditional solutions frequently fail to handle these\nissues efficiently. For overcoming these constraints, we propose a\ndual-architecture framework. For the Signer-Independent (SI) challenge, we\npropose a Signer-Invariant Conformer that combines convolutions with multi-head\nself-attention to learn robust, signer-agnostic representations from pose-based\nskeletal keypoints. For the Unseen-Sentences (US) task, we designed a\nMulti-Scale Fusion Transformer with a novel dual-path temporal encoder that\ncaptures both fine-grained posture dynamics, enabling the model's ability to\ncomprehend novel grammatical compositions. Experiments on the challenging\nIsharah-1000 dataset establish a new standard for both CSLR benchmarks. The\nproposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on\nthe SI challenge, a reduction of 13.53% from the state-of-the-art. On the US\ntask, the transformer model scores a WER of 47.78%, surpassing previous work.\nIn the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th\nin the SI task, demonstrating the performance of these models. The findings\nvalidate our key hypothesis: that developing task-specific networks designed\nfor the particular challenges of CSLR leads to considerable performance\nimprovements and establishes a new baseline for further research. The source\ncode is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.",
        "url": "http://arxiv.org/abs/2508.09372v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09372v1",
        "arxiv_id": "2508.09372v1",
        "authors": [
            "Md Rezwanul Haque",
            "Md. Milon Islam",
            "S M Taslim Uddin Raju",
            "Fakhri Karray"
        ],
        "submitted": "2025-08-12 21:59:53",
        "source": "arxiv",
        "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025"
    },
    {
        "title": "Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning",
        "abstract": "This study investigates whether large language models (LLMs) mirror human\nneurocognition during abstract reasoning. We compared the performance and\nneural representations of human participants with those of eight open-source\nLLMs on an abstract-pattern-completion task. We leveraged pattern type\ndifferences in task performance and in fixation-related potentials (FRPs) as\nrecorded by electroencephalography (EEG) during the task. Our findings indicate\nthat only the largest tested LLMs (~70 billion parameters) achieve\nhuman-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing\nsimilarities with the human pattern-specific difficulty profile. Critically,\nevery LLM tested forms representations that distinctly cluster the abstract\npattern categories within their intermediate layers, although the strength of\nthis clustering scales with their performance on the task. Moderate positive\ncorrelations were observed between the representational geometries of\ntask-optimal LLM layers and human frontal FRPs. These results consistently\ndiverged from comparisons with other EEG measures (response-locked ERPs and\nresting EEG), suggesting a potential shared representational space for abstract\npatterns. This indicates that LLMs might mirror human brain mechanisms in\nabstract reasoning, offering preliminary evidence of shared principles between\nbiological and artificial intelligence.",
        "url": "http://arxiv.org/abs/2508.10057v1",
        "pdf_url": "http://arxiv.org/pdf/2508.10057v1",
        "arxiv_id": "2508.10057v1",
        "authors": [
            "Christopher Pinier",
            "Sonia Acuña Vargas",
            "Mariia Steeghs-Turchina",
            "Dora Matzke",
            "Claire E. Stevenson",
            "Michael D. Nunez"
        ],
        "submitted": "2025-08-12 21:38:46",
        "source": "arxiv",
        "comment": "Presented at the 8th Annual Conference on Cognitive Computational\n  Neuroscience (August 12-15, 2025; Amsterdam, The Netherlands); 20 pages, 11\n  figures"
    },
    {
        "title": "Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling",
        "abstract": "Textless spoken language models (SLMs) are generative models of speech that\ndo not rely on text supervision. Most textless SLMs learn to predict the next\nsemantic token, a discrete representation of linguistic content, and rely on a\nseparate vocoder to add acoustic information to the generated speech. Such\nmodels have no access to acoustic context and no built-in control over acoustic\ndetails. In this work, we propose to jointly model linguistic and acoustic\ninformation by generating semantic tokens and a continuous real-valued\nrepresentation of the acoustic frame. We use a flow-matching objective to\npredict the continuous vector conditioned on the semantic tokens. We study the\ndesign space of this approach and find that predicting multiple future semantic\ntokens helps preserve linguistic information. Our approach achieves comparable\nperformance to existing models in terms of linguistic likelihood benchmarks,\nwhile providing better acoustic detail in prompted generation.",
        "url": "http://arxiv.org/abs/2508.09350v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09350v1",
        "arxiv_id": "2508.09350v1",
        "authors": [
            "Ju-Chieh Chou",
            "Jiawei Zhou",
            "Karen Livescu"
        ],
        "submitted": "2025-08-12 21:25:37",
        "source": "arxiv",
        "comment": "Accepted to ASRU 2025"
    },
    {
        "title": "The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains",
        "abstract": "Expert consensus plays a critical role in domains where evidence is complex,\nconflicting, or insufficient for direct prescription. Traditional methods, such\nas Delphi studies, consensus conferences, and systematic guideline synthesis,\noffer structure but face limitations including high panel burden, interpretive\noversimplification, and suppression of conditional nuance. These challenges are\nnow exacerbated by information overload, fragmentation of the evidence base,\nand increasing reliance on publicly available sources that lack expert\nfiltering. This study introduces and evaluates a Human-AI Hybrid Delphi\n(HAH-Delphi) framework designed to augment expert consensus development by\nintegrating a generative AI model (Gemini 2.5 Pro), small panels of senior\nhuman experts, and structured facilitation. The HAH-Delphi was tested in three\nphases: retrospective replication, prospective comparison, and applied\ndeployment in two applied domains (endurance training and resistance and mixed\ncardio/strength training). The AI replicated 95% of published expert consensus\nconclusions in Phase I and showed 95% directional agreement with senior human\nexperts in Phase II, though it lacked experiential and pragmatic nuance. In\nPhase III, compact panels of six senior experts achieved >90% consensus\ncoverage and reached thematic saturation before the final participant. The AI\nprovided consistent, literature-grounded scaffolding that supported divergence\nresolution and accelerated saturation. The HAH-Delphi framework offers a\nflexible, scalable approach for generating high-quality, context-sensitive\nconsensus. Its successful application across health, coaching, and performance\nscience confirms its methodological robustness and supports its use as a\nfoundation for generating conditional, personalised guidance and published\nconsensus frameworks at scale.",
        "url": "http://arxiv.org/abs/2508.09349v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09349v1",
        "arxiv_id": "2508.09349v1",
        "authors": [
            "Cathy Speed",
            "Ahmed A. Metwally"
        ],
        "submitted": "2025-08-12 21:24:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Decoding Neural Emotion Patterns through Natural Language Processing Embeddings",
        "abstract": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression.",
        "url": "http://arxiv.org/abs/2508.09337v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09337v1",
        "arxiv_id": "2508.09337v1",
        "authors": [
            "Gideon Vos",
            "Maryam Ebrahimpour",
            "Liza van Eijk",
            "Zoltan Sarnyai",
            "Mostafa Rahimi Azghadi"
        ],
        "submitted": "2025-08-12 20:51:56",
        "source": "arxiv",
        "comment": "26 pages, 9 figures"
    },
    {
        "title": "RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs",
        "abstract": "We propose RicciFlowRec, a geometric recommendation framework that performs\nroot cause attribution via Ricci curvature and flow on dynamic financial\ngraphs. By modelling evolving interactions among stocks, macroeconomic\nindicators, and news, we quantify local stress using discrete Ricci curvature\nand trace shock propagation via Ricci flow. Curvature gradients reveal causal\nsubstructures, informing a structural risk-aware ranking function. Preliminary\nresults on S\\&P~500 data with FinBERT-based sentiment show improved robustness\nand interpretability under synthetic perturbations. This ongoing work supports\ncurvature-based attribution and early-stage risk-aware ranking, with plans for\nportfolio optimization and return forecasting. To our knowledge, RicciFlowRec\nis the first recommender to apply geometric flow-based reasoning in financial\ndecision support.",
        "url": "http://arxiv.org/abs/2508.09334v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09334v1",
        "arxiv_id": "2508.09334v1",
        "authors": [
            "Zhongtian Sun",
            "Anoushka Harit"
        ],
        "submitted": "2025-08-12 20:45:02",
        "source": "arxiv",
        "comment": "Accepted at ACM RecSys 2025 (Late Breaking Results Track)"
    },
    {
        "title": "TEN: Table Explicitization, Neurosymbolically",
        "abstract": "We present a neurosymbolic approach, TEN, for extracting tabular data from\nsemistructured input text. This task is particularly challenging for text input\nthat does not use special delimiters consistently to separate columns and rows.\nPurely neural approaches perform poorly due to hallucinations and their\ninability to enforce hard constraints. TEN uses Structural Decomposition\nprompting - a specialized chain-of-thought prompting approach - on a large\nlanguage model (LLM) to generate an initial table, and thereafter uses a\nsymbolic checker to evaluate not only the well-formedness of that table, but\nalso detect cases of hallucinations or forgetting. The output of the symbolic\nchecker is processed by a critique-LLM to generate guidance for fixing the\ntable, which is presented to the original LLM in a self-debug loop. Our\nextensive experiments demonstrate that TEN significantly outperforms purely\nneural baselines across multiple datasets and metrics, achieving significantly\nhigher exact match accuracy and substantially reduced hallucination rates. A\n21-participant user study further confirms that TEN's tables are rated\nsignificantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are\nconsistently preferred for ease of verification and correction, with\nparticipants favoring our method in over 60% of the cases.",
        "url": "http://arxiv.org/abs/2508.09324v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09324v1",
        "arxiv_id": "2508.09324v1",
        "authors": [
            "Nikita Mehrotra",
            "Aayush Kumar",
            "Sumit Gulwani",
            "Arjun Radhakrishna",
            "Ashish Tiwari"
        ],
        "submitted": "2025-08-12 20:16:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition",
        "abstract": "Named Entity Recognition (NER) in the rare disease domain poses unique\nchallenges due to limited labeled data, semantic ambiguity between entity\ntypes, and long-tail distributions. In this study, we evaluate the capabilities\nof GPT-4o for rare disease NER under low-resource settings, using a range of\nprompt-based strategies including zero-shot prompting, few-shot in-context\nlearning, retrieval-augmented generation (RAG), and task-level fine-tuning. We\ndesign a structured prompting framework that encodes domain-specific knowledge\nand disambiguation rules for four entity types. We further introduce two\nsemantically guided few-shot example selection methods to improve in-context\nperformance while reducing labeling effort. Experiments on the RareDis Corpus\nshow that GPT-4o achieves competitive or superior performance compared to\nBioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art\n(SOTA) results. Cost-performance analysis reveals that few-shot prompting\ndelivers high returns at low token budgets, while RAG offers marginal\nadditional benefit. An error taxonomy highlights common failure modes such as\nboundary drift and type confusion, suggesting opportunities for post-processing\nand hybrid refinement. Our results demonstrate that prompt-optimized LLMs can\nserve as effective, scalable alternatives to traditional supervised models in\nbiomedical NER, particularly in rare disease applications where annotated data\nis scarce.",
        "url": "http://arxiv.org/abs/2508.09323v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09323v1",
        "arxiv_id": "2508.09323v1",
        "authors": [
            "Nan Miles Xi",
            "Yu Deng",
            "Lin Wang"
        ],
        "submitted": "2025-08-12 20:16:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning",
        "abstract": "Reasoning-augmented search agents such as Search-R1, trained via\nreinforcement learning with verifiable rewards (RLVR), demonstrate remarkable\ncapabilities in multi-step information retrieval from external knowledge\nsources. These agents address the limitations of their parametric memory by\ndynamically gathering relevant facts to address complex reasoning tasks.\nHowever, existing approaches suffer from a fundamental architectural\nlimitation: they process search queries strictly sequentially, even when\nhandling inherently parallelizable and logically independent comparisons. This\nsequential bottleneck significantly constrains computational efficiency,\nparticularly for queries that require multiple entity comparisons. To address\nthis critical limitation, we propose ParallelSearch, a novel reinforcement\nlearning framework that empowers large language models (LLMs) to recognize\nparallelizable query structures and execute multiple search operations\nconcurrently. Our approach introduces dedicated reward functions that\nincentivize the identification of independent query components while preserving\nanswer accuracy through jointly considering correctness, query decomposition\nquality, and parallel execution benefits. Comprehensive experiments demonstrate\nthat ParallelSearch outperforms state-of-the-art baselines by an average\nperformance gain of 2.9% across seven question-answering benchmarks. Notably,\non parallelizable questions, our method achieves a 12.7% performance\nimprovement while requiring only 69.6% of the LLM calls compared to sequential\napproaches.",
        "url": "http://arxiv.org/abs/2508.09303v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09303v1",
        "arxiv_id": "2508.09303v1",
        "authors": [
            "Shu Zhao",
            "Tan Yu",
            "Anbang Xu",
            "Japinder Singh",
            "Aaditya Shukla",
            "Rama Akkiraju"
        ],
        "submitted": "2025-08-12 19:38:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative",
        "abstract": "Advances in speech synthesis intensify security threats, motivating real-time\ndeepfake detection research. We investigate whether bidirectional Mamba can\nserve as a competitive alternative to Self-Attention in detecting synthetic\nspeech. Our solution, Fake-Mamba, integrates an XLSR front-end with\nbidirectional Mamba to capture both local and global artifacts. Our core\ninnovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and\nPN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can\neffectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof\n21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and\n5.85% EER, respectively, representing substantial relative gains over SOTA\nmodels XLSR-Conformer and XLSR-Mamba. The framework maintains real-time\ninference across utterance lengths, demonstrating strong generalization and\npractical viability. The code is available at\nhttps://github.com/xuanxixi/Fake-Mamba.",
        "url": "http://arxiv.org/abs/2508.09294v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09294v1",
        "arxiv_id": "2508.09294v1",
        "authors": [
            "Xi Xuan",
            "Zimo Zhu",
            "Wenxin Zhang",
            "Yi-Cheng Lin",
            "Tomi Kinnunen"
        ],
        "submitted": "2025-08-12 19:15:13",
        "source": "arxiv",
        "comment": "Accepted at IEEE ASRU 2025"
    },
    {
        "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs",
        "abstract": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research.",
        "url": "http://arxiv.org/abs/2508.09288v1",
        "pdf_url": "http://arxiv.org/pdf/2508.09288v1",
        "arxiv_id": "2508.09288v1",
        "authors": [
            "Aayush Gupta"
        ],
        "submitted": "2025-08-12 18:47:30",
        "source": "arxiv",
        "comment": "2 figures, 3 tables; code and certification harness:\n  https://github.com/ayushgupta4897/Contextual-Integrity-Verification ;\n  Elite-Attack dataset: https://huggingface.co/datasets/zyushg/elite-attack"
    }
]