[
    {
        "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting",
        "abstract": "Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.",
        "url": "http://arxiv.org/abs/2512.22101v1",
        "pdf_url": "https://arxiv.org/pdf/2512.22101v1",
        "arxiv_id": "2512.22101v1",
        "authors": [
            "Shuyu Gan",
            "Renxiang Wang",
            "James Mooney",
            "Dongyeop Kang"
        ],
        "submitted": "2025-12-26 18:02:12",
        "source": "arxiv",
        "comment": "3 pages, 3 figures; Accepted by 1st Workshop on GenAI, Agents and the Future of VIS as Mini-challenge paper and win the Honorable Mention award. Submit number is 7597 and the paper is archived on the workshop website: https://visxgenai.github.io/subs-2025/7597/7597-doc.pdf"
    },
    {
        "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
        "abstract": "Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.",
        "url": "http://arxiv.org/abs/2512.22100v1",
        "pdf_url": "https://arxiv.org/pdf/2512.22100v1",
        "arxiv_id": "2512.22100v1",
        "authors": [
            "Duygu Altinok"
        ],
        "submitted": "2025-12-26 18:02:09",
        "source": "arxiv",
        "comment": "under review by Springer"
    },
    {
        "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
        "abstract": "The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.\n  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Î˜(\\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.",
        "url": "http://arxiv.org/abs/2512.22088v1",
        "pdf_url": "https://arxiv.org/pdf/2512.22088v1",
        "arxiv_id": "2512.22088v1",
        "authors": [
            "Chiwun Yang"
        ],
        "submitted": "2025-12-26 17:20:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context as a Tool: Context Management for Long-Horizon SWE-Agents",
        "abstract": "Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.",
        "url": "http://arxiv.org/abs/2512.22087v1",
        "pdf_url": "https://arxiv.org/pdf/2512.22087v1",
        "arxiv_id": "2512.22087v1",
        "authors": [
            "Shukai Liu",
            "Jian Yang",
            "Bo Jiang",
            "Yizhi Li",
            "Jinyang Guo",
            "Xianglong Liu",
            "Bryan Dai"
        ],
        "submitted": "2025-12-26 17:15:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management",
        "abstract": "Natural Language Processing (NLP) systems are increasingly used in sensitive domains such as healthcare, finance, and government, where they handle large volumes of personal and regulated data. However, these systems introduce distinct risks related to security, privacy, and regulatory compliance that are not fully addressed by existing AI governance frameworks. This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a comprehensive six-phase model designed to ensure the secure operation of NLP systems from development to retirement. The framework, developed through a systematic PRISMA-based review of 45 peer-reviewed and regulatory sources, aligns with leading standards, including NIST AI RMF, ISO/IEC 42001:2023, the EU AI Act, and MITRE ATLAS. It integrates established methods for bias detection, privacy protection (differential privacy, federated learning), secure deployment, explainability, and secure model decommissioning. A healthcare case study illustrates how SC-NLP-LMF detects emerging terminology drift (e.g., COVID-related language) and guides compliant model updates. The framework offers organizations a practical, lifecycle-wide structure for developing, deploying, and maintaining secure and accountable NLP systems in high-risk environments.",
        "url": "http://arxiv.org/abs/2512.22060v1",
        "pdf_url": "https://arxiv.org/pdf/2512.22060v1",
        "arxiv_id": "2512.22060v1",
        "authors": [
            "Sunil Arora",
            "John Hastings"
        ],
        "submitted": "2025-12-26 15:28:20",
        "source": "arxiv",
        "comment": "9 pages, 2 tables, 1 figure"
    },
    {
        "title": "Self-attention vector output similarities reveal how machines pay attention",
        "abstract": "The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.",
        "url": "http://arxiv.org/abs/2512.21956v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21956v1",
        "arxiv_id": "2512.21956v1",
        "authors": [
            "Tal Halevi",
            "Yarden Tzach",
            "Ronit D. Gross",
            "Shalom Rosner",
            "Ido Kanter"
        ],
        "submitted": "2025-12-26 10:03:26",
        "source": "arxiv",
        "comment": "22 pages, 13 figures"
    },
    {
        "title": "Broken Words, Broken Performance: Effect of Tokenization on Performance of LLMs",
        "abstract": "Tokenization is the first step in training any Large Language Model (LLM), where the text is split into a sequence of tokens as per the model's fixed vocabulary. This tokenization in LLMs is different from the traditional tokenization in NLP where the text is split into a sequence of \"natural\" words. In LLMs, a natural word may also be broken into multiple tokens due to limited vocabulary size of the LLMs (e.g., Mistral's tokenizer splits \"martial\" into \"mart\" and \"ial\"). In this paper, we hypothesize that such breaking of natural words negatively impacts LLM performance on various NLP tasks. To quantify this effect, we propose a set of penalty functions that compute a tokenization penalty for a given text for a specific LLM, indicating how \"bad\" the tokenization is. We establish statistical significance of our hypothesis on multiple NLP tasks for a set of different LLMs.",
        "url": "http://arxiv.org/abs/2512.21933v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21933v1",
        "arxiv_id": "2512.21933v1",
        "authors": [
            "Sachin Pawar",
            "Manoj Apte",
            "Kshitij Jadhav",
            "Girish Keshav Palshikar",
            "Nitin Ramrakhiyani"
        ],
        "submitted": "2025-12-26 09:16:33",
        "source": "arxiv",
        "comment": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2025)"
    },
    {
        "title": "AutoPP: Towards Automated Product Poster Generation and Optimization",
        "abstract": "Product posters blend striking visuals with informative text to highlight the product and capture customer attention. However, crafting appealing posters and manually optimizing them based on online performance is laborious and resource-consuming. To address this, we introduce AutoPP, an automated pipeline for product poster generation and optimization that eliminates the need for human intervention. Specifically, the generator, relying solely on basic product information, first uses a unified design module to integrate the three key elements of a poster (background, text, and layout) into a cohesive output. Then, an element rendering module encodes these elements into condition tokens, efficiently and controllably generating the product poster. Based on the generated poster, the optimizer enhances its Click-Through Rate (CTR) by leveraging online feedback. It systematically replaces elements to gather fine-grained CTR comparisons and utilizes Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to isolated elements. Our work is supported by AutoPP1M, the largest dataset specifically designed for product poster generation and optimization, which contains one million high-quality posters and feedback collected from over one million users. Experiments demonstrate that AutoPP achieves state-of-the-art results in both offline and online settings. Our code and dataset are publicly available at: https://github.com/JD-GenX/AutoPP",
        "url": "http://arxiv.org/abs/2512.21921v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21921v1",
        "arxiv_id": "2512.21921v1",
        "authors": [
            "Jiahao Fan",
            "Yuxin Qin",
            "Wei Feng",
            "Yanyin Chen",
            "Yaoyu Li",
            "Ao Ma",
            "Yixiu Li",
            "Li Zhuang",
            "Haoyi Bian",
            "Zheng Zhang",
            "Jingjing Lv",
            "Junjie Shen",
            "Ching Law"
        ],
        "submitted": "2025-12-26 08:30:32",
        "source": "arxiv",
        "comment": "Accepted to AAAI 2026"
    },
    {
        "title": "SWE-RM: Execution-free Feedback For Software Engineering Agents",
        "abstract": "Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model's ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.",
        "url": "http://arxiv.org/abs/2512.21919v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21919v1",
        "arxiv_id": "2512.21919v1",
        "authors": [
            "KaShun Shum",
            "Binyuan Hui",
            "Jiawei Chen",
            "Lei Zhang",
            "X. W.",
            "Jiaxi Yang",
            "Yuzhen Huang",
            "Junyang Lin",
            "Junxian He"
        ],
        "submitted": "2025-12-26 08:26:18",
        "source": "arxiv",
        "comment": "21 pages"
    },
    {
        "title": "Accelerate Speculative Decoding with Sparse Computation in Verification",
        "abstract": "Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.",
        "url": "http://arxiv.org/abs/2512.21911v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21911v1",
        "arxiv_id": "2512.21911v1",
        "authors": [
            "Jikai Wang",
            "Jianchao Tan",
            "Yuxuan Hu",
            "Jiayu Qin",
            "Yerui Sun",
            "Yuchen Xie",
            "Xunliang Cai",
            "Juntao Li",
            "Min Zhang"
        ],
        "submitted": "2025-12-26 07:53:41",
        "source": "arxiv",
        "comment": "Pre-print"
    },
    {
        "title": "Explainable Statute Prediction via Attention-based Model and LLM Prompting",
        "abstract": "In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term \"statute\" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.",
        "url": "http://arxiv.org/abs/2512.21902v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21902v1",
        "arxiv_id": "2512.21902v1",
        "authors": [
            "Sachin Pawar",
            "Girish Keshav Palshikar",
            "Anindita Sinha Banerjee",
            "Nitin Ramrakhiyani",
            "Basit Ali"
        ],
        "submitted": "2025-12-26 07:29:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics",
        "abstract": "Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a \"Gold Standard\" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.",
        "url": "http://arxiv.org/abs/2512.21877v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21877v1",
        "arxiv_id": "2512.21877v1",
        "authors": [
            "Vaibhav Devraj",
            "Dhruv Kumar",
            "Jagat Sesh Challa"
        ],
        "submitted": "2025-12-26 05:59:19",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
        "abstract": "Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.",
        "url": "http://arxiv.org/abs/2512.21871v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21871v1",
        "arxiv_id": "2512.21871v1",
        "authors": [
            "Naen Xu",
            "Jinghuai Zhang",
            "Changjiang Li",
            "Hengyu An",
            "Chunyi Zhou",
            "Jun Wang",
            "Boyu Xu",
            "Yuyuan Li",
            "Tianyu Du",
            "Shouling Ji"
        ],
        "submitted": "2025-12-26 05:09:55",
        "source": "arxiv",
        "comment": "AAAI 2026 (Oral)"
    },
    {
        "title": "Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion",
        "abstract": "Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.",
        "url": "http://arxiv.org/abs/2512.21863v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21863v1",
        "arxiv_id": "2512.21863v1",
        "authors": [
            "Huatuan Sun",
            "Yunshan Ma",
            "Changguang Wu",
            "Yanxin Zhang",
            "Pengfei Wang",
            "Xiaoyu Du"
        ],
        "submitted": "2025-12-26 04:56:28",
        "source": "arxiv",
        "comment": "10 pages, 4 figures"
    },
    {
        "title": "TimeBill: Time-Budgeted Inference for Large Language Models",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.",
        "url": "http://arxiv.org/abs/2512.21859v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21859v1",
        "arxiv_id": "2512.21859v1",
        "authors": [
            "Qi Fan",
            "An Zou",
            "Yehan Ma"
        ],
        "submitted": "2025-12-26 04:49:35",
        "source": "arxiv",
        "comment": "Accepted to AAAI 2026"
    },
    {
        "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs",
        "abstract": "While Large Language Models (LLMs) have achieved remarkable success in cognitive and reasoning benchmarks, they exhibit a persistent deficit in anthropomorphic intelligence-the capacity to navigate complex social, emotional, and ethical nuances. This gap is particularly acute in the Chinese linguistic and cultural context, where a lack of specialized evaluation frameworks and high-quality socio-emotional data impedes progress. To address these limitations, we present HeartBench, a framework designed to evaluate the integrated emotional, cultural, and ethical dimensions of Chinese LLMs. Grounded in authentic psychological counseling scenarios and developed in collaboration with clinical experts, the benchmark is structured around a theory-driven taxonomy comprising five primary dimensions and 15 secondary capabilities. We implement a case-specific, rubric-based methodology that translates abstract human-like traits into granular, measurable criteria through a ``reasoning-before-scoring'' evaluation protocol. Our assessment of 13 state-of-the-art LLMs indicates a substantial performance ceiling: even leading models achieve only 60% of the expert-defined ideal score. Furthermore, analysis using a difficulty-stratified ``Hard Set'' reveals a significant performance decay in scenarios involving subtle emotional subtexts and complex ethical trade-offs. HeartBench establishes a standardized metric for anthropomorphic AI evaluation and provides a methodological blueprint for constructing high-quality, human-aligned training data.",
        "url": "http://arxiv.org/abs/2512.21849v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21849v1",
        "arxiv_id": "2512.21849v1",
        "authors": [
            "Jiaxin Liu",
            "Peiyi Tu",
            "Wenyu Chen",
            "Yihong Zhuang",
            "Xinxia Ling",
            "Anji Zhou",
            "Chenxi Wang",
            "Zhuo Han",
            "Zhengkai Yang",
            "Junbo Zhao",
            "Zenan Huang",
            "Yuanyuan Wang"
        ],
        "submitted": "2025-12-26 03:54:56",
        "source": "arxiv",
        "comment": "10 pages"
    },
    {
        "title": "AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts",
        "abstract": "High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that \"Easy\" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our \"Hard\" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.",
        "url": "http://arxiv.org/abs/2512.21842v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21842v1",
        "arxiv_id": "2512.21842v1",
        "authors": [
            "Baorong Huang",
            "Ali Asiri"
        ],
        "submitted": "2025-12-26 03:10:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco",
        "abstract": "This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.",
        "url": "http://arxiv.org/abs/2512.21837v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21837v1",
        "arxiv_id": "2512.21837v1",
        "authors": [
            "Siyu Li",
            "Chenwei Song",
            "Wan Zhou",
            "Xinyi Liu"
        ],
        "submitted": "2025-12-26 02:48:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Method Decoration (DeMe): A Framework for LLM-Driven Adaptive Method Generation in Dynamic IoT Environments",
        "abstract": "Intelligent IoT systems increasingly rely on large language models (LLMs) to generate task-execution methods for dynamic environments. However, existing approaches lack the ability to systematically produce new methods when facing previously unseen situations, and they often depend on fixed, device-specific logic that cannot adapt to changing environmental conditions.In this paper, we propose Method Decoration (DeMe), a general framework that modifies the method-generation path of an LLM using explicit decorations derived from hidden goals, accumulated learned methods, and environmental feedback. Unlike traditional rule augmentation, decorations in DeMe are not hardcoded; instead, they are extracted from universal behavioral principles, experience, and observed environmental differences. DeMe enables the agent to reshuffle the structure of its method path-through pre-decoration, post-decoration, intermediate-step modification, and step insertion-thereby producing context-aware, safety-aligned, and environment-adaptive methods. Experimental results show that method decoration allows IoT devices to derive ore appropriate methods when confronting unknown or faulty operating conditions.",
        "url": "http://arxiv.org/abs/2512.21817v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21817v1",
        "arxiv_id": "2512.21817v1",
        "authors": [
            "Hong Su"
        ],
        "submitted": "2025-12-26 01:08:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On The Conceptualization and Societal Impact of Cross-Cultural Bias",
        "abstract": "Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.\n  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.",
        "url": "http://arxiv.org/abs/2512.21809v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21809v1",
        "arxiv_id": "2512.21809v1",
        "authors": [
            "Vitthal Bhandari"
        ],
        "submitted": "2025-12-26 00:27:53",
        "source": "arxiv",
        "comment": "Term paper for LING 575 (Societal Impacts of Language Technologies)"
    },
    {
        "title": "KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering",
        "abstract": "In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.",
        "url": "http://arxiv.org/abs/2512.21799v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21799v1",
        "arxiv_id": "2512.21799v1",
        "authors": [
            "Hung-Nghiep Tran",
            "Atsuhiro Takasu"
        ],
        "submitted": "2025-12-25 22:29:54",
        "source": "arxiv",
        "comment": "Extracted and extended from the first author's PhD thesis titled \"Multi-Relational Embedding for Knowledge Graph Representation and Analysis\""
    },
    {
        "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning",
        "abstract": "Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.",
        "url": "http://arxiv.org/abs/2512.21789v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21789v1",
        "arxiv_id": "2512.21789v1",
        "authors": [
            "Ting-Hao K. Huang",
            "Ryan A. Rossi",
            "Sungchul Kim",
            "Tong Yu",
            "Ting-Yao E. Hsu",
            "Ho Yin",
            "Ng",
            "C. Lee Giles"
        ],
        "submitted": "2025-12-25 21:39:10",
        "source": "arxiv",
        "comment": "Accepted to the 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE 2026)"
    },
    {
        "title": "Ara-HOPE: Human-Centric Post-Editing Evaluation for Dialectal Arabic to Modern Standard Arabic Translation",
        "abstract": "Dialectal Arabic to Modern Standard Arabic (DA-MSA) translation is a challenging task in Machine Translation (MT) due to significant lexical, syntactic, and semantic divergences between Arabic dialects and MSA. Existing automatic evaluation metrics and general-purpose human evaluation frameworks struggle to capture dialect-specific MT errors, hindering progress in translation assessment. This paper introduces Ara-HOPE, a human-centric post-editing evaluation framework designed to systematically address these challenges. The framework includes a five-category error taxonomy and a decision-tree annotation protocol. Through comparative evaluation of three MT systems (Arabic-centric Jais, general-purpose GPT-3.5, and baseline NLLB-200), Ara-HOPE effectively highlights systematic performance differences between these systems. The results show that dialect-specific terminology and semantic preservation remain the most persistent challenges in DA-MSA translation. Ara-HOPE establishes a new framework for evaluating Dialectal Arabic MT quality and provides actionable guidance for improving dialect-aware MT systems.",
        "url": "http://arxiv.org/abs/2512.21787v1",
        "pdf_url": "https://arxiv.org/pdf/2512.21787v1",
        "arxiv_id": "2512.21787v1",
        "authors": [
            "Abdullah Alabdullah",
            "Lifeng Han",
            "Chenghua Lin"
        ],
        "submitted": "2025-12-25 21:29:59",
        "source": "arxiv",
        "comment": null
    }
]