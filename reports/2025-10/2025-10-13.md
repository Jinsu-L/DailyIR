# Daily Papers Report - 2025-10-13

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. PairSem: LLM-Guided Pairwise Semantic Matching for Scientific Document Retrieval

- **LLM Score**: 9
- **Keyword Score**: 20
- **Authors**: Wonbin Kweon, Runchu Tian, SeongKu Kang, Pengcheng Jiang, Zhiyong Lu, Jiawei Han, Hwanjo Yu
- **URL**: <http://arxiv.org/abs/2510.09897v1>
- **Submitted**: 2025-10-10 22:21:49
- **Topic Keywords**: information retrieval, retriever, dense retrieval, query, pairwise, rag, retrieval, search
- **Reason**: This paper aligns closely with your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The focus on semantic matching and entity-aspect pairs resonates with your interest in deep semantic understanding. The application to scientific document retrieval also aligns with your e-commerce background and broader interests in NLP and data mining.

#### Abstract
> Scientific document retrieval is a critical task for enabling knowledge
discovery and supporting research across diverse domains. However, existing
dense retrieval methods often struggle to capture fine-grained scientific
concepts in texts due to their reliance on holistic embeddings and limited
domain understanding. Recent approaches leverage large language models (LLMs)
to extract fine-grained semantic entities and enhance semantic matching, but
they typically treat entities as independent fragments, overlooking the
multi-faceted nature of scientific concepts. To address this limitation, we
propose Pairwise Semantic Matching (PairSem), a framework that represents
relevant semantics as entity-aspect pairs, capturing complex, multi-faceted
scientific concepts. PairSem is unsupervised, base retriever-agnostic, and
plug-and-play, enabling precise and context-aware matching without requiring
query-document labels or entity annotations. Extensive experiments on multiple
datasets and retrievers demonstrate that PairSem significantly improves
retrieval performance, highlighting the importance of modeling multi-aspect
semantics in scientific information retrieval.

---

### 2. QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking

- **LLM Score**: 9
- **Keyword Score**: 16
- **Authors**: Shubham Chatterjee, Jeff Dalton
- **URL**: <http://arxiv.org/abs/2510.11589v1>
- **Submitted**: 2025-10-13 16:31:06
- **Comment**: Published in: Proceedings of the 48th International ACM SIGIR
  Conference on Research and Development in Information Retrieval (SIGIR 2025)
- **Topic Keywords**: query, queries, ranking, rag, retrieval, rank, trec
- **Reason**: This paper aligns closely with your research interests in Information Retrieval, particularly in query understanding and ranking models. The proposed QDER model integrates knowledge graph semantics into a multi-vector model, which is a key area of interest in your research. The paper's focus on entity-aware retrieval and real-time relevance optimization also resonates with your background in e-commerce and NLP.

#### Abstract
> Neural IR has advanced through two distinct paths: entity-oriented approaches
leveraging knowledge graphs and multi-vector models capturing fine-grained
semantics. We introduce QDER, a neural re-ranking model that unifies these
approaches by integrating knowledge graph semantics into a multi-vector model.
QDER's key innovation lies in its modeling of query-document relationships:
rather than computing similarity scores on aggregated embeddings, we maintain
individual token and entity representations throughout the ranking process,
performing aggregation only at the final scoring stage - an approach we call
"late aggregation." We first transform these fine-grained representations
through learned attention patterns, then apply carefully chosen mathematical
operations for precise matches. Experiments across five standard benchmarks
show that QDER achieves significant performance gains, with improvements of 36%
in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar
improvements on other datasets. QDER particularly excels on difficult queries,
achieving an nDCG@20 of 0.70 where traditional approaches fail completely
(nDCG@20 = 0.0), setting a foundation for future work in entity-aware
retrieval.

---

### 3. REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking

- **LLM Score**: 9
- **Keyword Score**: 12
- **Authors**: Shubham Chatterjee
- **URL**: <http://arxiv.org/abs/2510.11592v1>
- **Submitted**: 2025-10-13 16:31:42
- **Comment**: To be published in: Proceedings of the 2025 Annual International ACM
  SIGIR Conference on Research and Development in Information Retrieval in the
  Asia Pacific Region (SIGIR-AP 2025)
- **Topic Keywords**: information retrieval, ranking, relevance, retrieval, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed REGENT model integrates entity semantics directly into neural attention, which aligns with your focus on deep semantic understanding and real-time relevance optimization. The paper's emphasis on entity-aware information retrieval also resonates with your background in the e-commerce domain.

#### Abstract
> Current neural re-rankers often struggle with complex information needs and
long, content-rich documents. The fundamental issue is not computational--it is
intelligent content selection: identifying what matters in lengthy,
multi-faceted texts. While humans naturally anchor their understanding around
key entities and concepts, neural models process text within rigid token
windows, treating all interactions as equally important and missing critical
semantic signals. We introduce REGENT, a neural re-ranking model that mimics
human-like understanding by using entities as a "semantic skeleton" to guide
attention. REGENT integrates relevance guidance directly into the attention
mechanism, combining fine-grained lexical matching with high-level semantic
reasoning. This relevance-guided attention enables the model to focus on
conceptually important content while maintaining sensitivity to precise term
matches. REGENT achieves new state-of-the-art performance in three challenging
datasets, providing up to 108% improvement over BM25 and consistently
outperforming strong baselines including ColBERT and RankVicuna. To our
knowledge, this is the first work to successfully integrate entity semantics
directly into neural attention, establishing a new paradigm for entity-aware
information retrieval.

---

### 4. Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning

- **LLM Score**: 9
- **Keyword Score**: 11
- **Authors**: Shu Zhao, Tan Yu, Anbang Xu
- **URL**: <http://arxiv.org/abs/2510.10009v1>
- **Submitted**: 2025-10-11 04:23:30
- **Topic Keywords**: query, queries, rag, retrieval, search
- **Reason**: This paper aligns well with your research interests in Information Retrieval, particularly query understanding and ranking models. The use of Reinforcement Learning for query expansion and the incorporation of a pre-trained model for document understanding are relevant to your focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

---

### 5. From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance

- **LLM Score**: 9
- **Keyword Score**: 9
- **Authors**: Runze Xia, Yupeng Ji, Yuxi Zhou, Haodong Liu, Teng Zhang, Piji Li
- **URL**: <http://arxiv.org/abs/2510.11056v1>
- **Submitted**: 2025-10-13 06:46:43
- **Topic Keywords**: query, relevance, commerce, e-commerce, search
- **Reason**: This paper is extremely relevant to your research interests in Information Retrieval, particularly in areas that require deep semantic understanding and real-time relevance optimization. The proposed two-stage distillation framework for search relevance prediction aligns with your focus on query understanding and ranking models. The application of the framework in an e-commerce search system also resonates with your background in the e-commerce domain.

#### Abstract
> Query-service relevance prediction in e-commerce search systems faces strict
latency requirements that prevent the direct application of Large Language
Models (LLMs). To bridge this gap, we propose a two-stage reasoning
distillation framework to transfer reasoning capabilities from a powerful
teacher LLM to a lightweight, deployment-friendly student model. In the first
stage, we address the limitations of general-purpose LLMs by constructing a
domain-adapted teacher model. This is achieved through a three-step process:
domain-adaptive pre-training to inject platform knowledge, supervised
fine-tuning to elicit reasoning skills, and preference optimization with a
multi-dimensional reward model to ensure the generation of reliable and
preference-aligned reasoning paths. This teacher can then automatically
annotate massive query-service pairs from search logs with both relevance
labels and reasoning chains. In the second stage, to address the challenges of
architectural heterogeneity in standard distillation, we introduce Contrastive
Reasoning Self-Distillation (CRSD). By modeling the behavior of the same
student model under "standard" and "reasoning-augmented" inputs as a
teacher-student relationship, CRSD enables the lightweight model to internalize
the teacher's complex decision-making mechanisms without needing the explicit
reasoning path at inference. Offline evaluations and online A/B testing in the
Meituan search advertising system demonstrate that our framework achieves
significant improvements across multiple metrics, validating its effectiveness
and practical value.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance

- **LLM Score**: 8
- **Keyword Score**: 20
- **Authors**: Tingqiao Xu, Shaowei Yao, Chenhe Dong, Yiming Jin, Zerui Huang, Dan Ou, Haihong Tang
- **URL**: <http://arxiv.org/abs/2510.11122v1>
- **Submitted**: 2025-10-13 08:08:59
- **Topic Keywords**: query, queries, ranking, relevance, rag, retrieval, commerce, e-commerce, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of e-commerce search relevance. The DyKnow-RAG framework's focus on dynamic knowledge utilization, noisy retrieval-augmented generation, and real-time relevance optimization aligns with your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Accurately modeling query-item relevance drives e-commerce ranking, yet
long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM
coverage. External context (reviews, attribute encyclopedias, UGC) can help but
is noisy, and single-pass latency and cost forbid any clean-then-summarize
step. The model must, per query, judge relevance and decide whether to use,
partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG
framework built on Group Relative Policy Optimization. It trains two rollout
groups (no external context vs a single retrieved chunk) and applies
posterior-driven inter-group advantage scaling that adaptively reweights their
contributions by the per-query correctness gap. This teaches when to trust
retrieval versus fall back to parametric knowledge, without process labels,
value networks, or extra inference passes, preserving single-pass, single-chunk
deployment under production latency. Training combines: (1) supervised
initialization with a structured rationale that explicitly records the
context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus
where context choice is most consequential; and (3) an optional lightweight DPO
warm start to stabilize with-context calibration. Under a unified
retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and
vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query
Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's
production relevance system, serving live traffic. To our knowledge, it is
among the first single-pass RAG solutions for e-commerce relevance, turning
noisy external signals into reliable gains without added online complexity.

### 7. ZeroGR: A Generalizable and Scalable Framework for Zero-Shot Generative Retrieval

- **LLM Score**: 8
- **Keyword Score**: 16
- **Authors**: Weiwei Sun, Keyi Kong, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, Maarten de Rijke, Zhaochun Ren, Yiming Yang
- **URL**: <http://arxiv.org/abs/2510.10419v1>
- **Submitted**: 2025-10-12 03:04:24
- **Topic Keywords**: information retrieval, dense retrieval, query, queries, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of generative retrieval and zero-shot IR scenarios. The proposed framework, ZeroGR, leverages natural language instructions and generative language models, aligning with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's primary focus is on generative retrieval, which is a related but distinct area from your core interests in query understanding and ranking models.

#### Abstract
> Generative retrieval (GR) reformulates information retrieval (IR) by framing
it as the generation of document identifiers (docids), thereby enabling an
end-to-end optimization and seamless integration with generative language
models (LMs). Despite notable progress under supervised training, GR still
struggles to generalize to zero-shot IR scenarios, which are prevalent in
real-world applications. To tackle this challenge, we propose \textsc{ZeroGR},
a zero-shot generative retrieval framework that leverages natural language
instructions to extend GR across a wide range of IR tasks. Specifically,
\textsc{ZeroGR} is composed of three key components: (i) an LM-based docid
generator that unifies heterogeneous documents (e.g., text, tables, code) into
semantically meaningful docids; (ii) an instruction-tuned query generator that
generates diverse types of queries from natural language task descriptions to
enhance corpus indexing; and (iii) a reverse annealing decoding strategy to
balance precision and recall during docid generation. We investigate the impact
of instruction fine-tuning scale and find that performance consistently
improves as the number of IR tasks encountered during training increases.
Empirical results on the BEIR and MAIR benchmarks demonstrate that
\textsc{ZeroGR} outperforms strong dense retrieval and generative baselines in
zero-shot settings, establishing a new state-of-the-art for instruction-driven
GR.

### 8. Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Alin Fan, Hanqing Li, Sihan Lu, Jingsong Yuan, Jiandong Zhang
- **URL**: <http://arxiv.org/abs/2510.11066v1>
- **Submitted**: 2025-10-13 07:06:26
- **Topic Keywords**: rag, click, ctr, click-through rate, cvr, recommend, commerce, e-commerce
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on click-through rate prediction and user interest modeling aligns with your background in e-commerce and interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Modern industrial recommendation systems improve recommendation performance
by integrating multimodal representations from pre-trained models into ID-based
Click-Through Rate (CTR) prediction frameworks. However, existing approaches
typically adopt modality-centric modeling strategies that process ID-based and
multimodal embeddings independently, failing to capture fine-grained
interactions between content semantics and behavioral signals. In this paper,
we propose Decoupled Multimodal Fusion (DMF), which introduces a
modality-enriched modeling strategy to enable fine-grained interactions between
ID-based collaborative representations and multimodal representations for user
interest modeling. Specifically, we construct target-aware features to bridge
the semantic gap across different embedding spaces and leverage them as side
information to enhance the effectiveness of user interest modeling.
Furthermore, we design an inference-optimized attention mechanism that
decouples the computation of target-aware features and ID-based embeddings
before the attention layer, thereby alleviating the computational bottleneck
introduced by incorporating target-aware features. To achieve comprehensive
multimodal integration, DMF combines user interest representations learned
under the modality-centric and modality-enriched modeling strategies. Offline
experiments on public and industrial datasets demonstrate the effectiveness of
DMF. Moreover, DMF has been deployed on the product recommendation system of
the international e-commerce platform Lazada, achieving relative improvements
of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.

### 9. Next Interest Flow: A Generative Pre-training Paradigm for Recommender Systems by Modeling All-domain Movelines

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Chen Gao, Zixin Zhao, Lv Shao, Tong Liu
- **URL**: <http://arxiv.org/abs/2510.11317v1>
- **Submitted**: 2025-10-13 12:13:17
- **Topic Keywords**: pairwise, click, ctr, click-through rate, recommend, commerce, e-commerce
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of recommender systems and query understanding. The proposed model, AMEN, addresses the limitations of existing generative paradigms and incorporates a novel bidirectional alignment strategy, which is a key aspect of your research focus on ranking models and user behavior modeling.

#### Abstract
> Click-Through Rate (CTR) prediction, a cornerstone of modern recommender
systems, has been dominated by discriminative models that react to past user
behavior rather than proactively modeling user intent. Existing generative
paradigms attempt to address this but suffer from critical limitations: Large
Language Model (LLM) based methods create a semantic mismatch by forcing
e-commerce signals into a linguistic space, while ID-based generation is
constrained by item memorization and cold-start issues. To overcome these
limitations, we propose a novel generative pre-training paradigm. Our model
learns to predict the Next Interest Flow, a dense vector sequence representing
a user's future intent, while simultaneously modeling its internal Interest
Diversity and Interest Evolution Velocity to ensure the representation is both
rich and coherent. However, this two-stage approach introduces a critical
objective mismatch between the generative and discriminative stages. We resolve
this via a bidirectional alignment strategy, which harmonizes the two stages
through cross-stage weight initialization and a dynamic Semantic Alignment
Module for fine-tuning. Additionally, we enhance the underlying discriminative
model with a Temporal Sequential Pairwise (TSP) mechanism to better capture
temporal causality. We present the All-domain Moveline Evolution Network
(AMEN), a unified framework implementing our entire pipeline. Extensive offline
experiments validate AMEN's superiority over strong baselines, and a
large-scale online A/B test demonstrates its significant real-world impact,
delivering substantial improvements in key business metrics.

### 10. LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Hengran Zhang, Keping Bi, Jiafeng Guo, Jiaming Zhang, Shuaiqiang Wang, Dawei Yin, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2510.11358v1>
- **Submitted**: 2025-10-13 12:57:45
- **Comment**: 13 pages, 9 figures
- **Topic Keywords**: queries, relevance, rag, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) and large language models (LLMs). The paper's focus on LLM-specific utility and its implications for RAG research aligns with your interests in query understanding and ranking models.

#### Abstract
> Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

### 11. Characterizing Web Search in The Age of Generative AI

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Elisabeth Kirsten, Jost Grosse Perdekamp, Mihir Upadhyay, Krishna P. Gummadi, Muhammad Bilal Zafar
- **URL**: <http://arxiv.org/abs/2510.11560v1>
- **Submitted**: 2025-10-13 16:04:03
- **Topic Keywords**: query, queries, web search, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of emerging technologies like Generative AI. The study explores the differences between traditional web search and generative search, which aligns with your focus on query understanding and ranking models. However, the paper's primary focus is on the impact of Generative AI on web search, rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> The advent of LLMs has given rise to a new type of web search: Generative
search, where LLMs retrieve web pages related to a query and generate a single,
coherent text as a response. This output modality stands in stark contrast to
traditional web search, where results are returned as a ranked list of
independent web pages. In this paper, we ask: Along what dimensions do
generative search outputs differ from traditional web search? We compare
Google, a traditional web search engine, with four generative search engines
from two providers (Google and OpenAI) across queries from four domains. Our
analysis reveals intriguing differences. Most generative search engines cover a
wider range of sources compared to web search. Generative search engines vary
in the degree to which they rely on internal knowledge contained within the
model parameters v.s. external knowledge retrieved from the web. Generative
search engines surface varying sets of concepts, creating new opportunities for
enhancing search diversity and serendipity. Our results also highlight the need
for revisiting evaluation criteria for web search in the age of Generative AI.

### 12. HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Shuwei Chen, Jiajun Cui, Zhengqi Xu, Fan Zhang, Jiangke Fan, Teng Zhang, Xingxing Wang
- **URL**: <http://arxiv.org/abs/2510.11100v1>
- **Submitted**: 2025-10-13 07:47:03
- **Comment**: 10 pages, 6 figures
- **Topic Keywords**: rag, click, ctr, click-through rate, recommend
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of click models and user behavior modeling. The proposed HoMer model addresses heterogeneities in CTR prediction, which is a key aspect of recommender systems. While the focus is on industrial deployments, the techniques and architectures discussed can be applied to other domains, including e-commerce.

#### Abstract
> Click-through rate (CTR) prediction, which models behavior sequence and
non-sequential features (e.g., user/item profiles or cross features) to infer
user interest, underpins industrial recommender systems. However, most methods
face three forms of heterogeneity that degrade predictive performance: (i)
Feature Heterogeneity persists when limited sequence side features provide less
granular interest representation compared to extensive non-sequential features,
thereby impairing sequence modeling performance; (ii) Context Heterogeneity
arises because a user's interest in an item will be influenced by other items,
yet point-wise prediction neglects cross-item interaction context from the
entire item set; (iii) Architecture Heterogeneity stems from the fragmented
integration of specialized network modules, which compounds the model's
effectiveness, efficiency and scalability in industrial deployments. To tackle
the above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for
modeling sequential and set-wise contexts. First, we align sequence side
features with non-sequential features for accurate sequence modeling and
fine-grained interest representation. Second, we shift the prediction paradigm
from point-wise to set-wise, facilitating cross-item interaction in a highly
parallel manner. Third, HoMer's unified encoder-decoder architecture achieves
dual optimization through structural simplification and shared computation,
ensuring computational efficiency while maintaining scalability with model
size. Without arduous modification to the prediction pipeline, HoMer
successfully scales up and outperforms our industrial baseline by 0.0099 in the
AUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%.
Additionally, HoMer saves 27% of GPU resources via preliminary engineering
optimization, further validating its superiority and practicality.

### 13. LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Luyao Zhuang, Shengyuan Chen, Yilin Xiao, Huachi Zhou, Yujing Zhang, Hao Chen, Qinggang Zhang, Xiao Huang
- **URL**: <http://arxiv.org/abs/2510.10114v1>
- **Submitted**: 2025-10-11 08:43:45
- **Topic Keywords**: queries, rag, retrieval augmented generation, retrieval
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the context of query understanding and ranking models. The focus on Retrieval-Augmented Generation and graph-based retrieval methods aligns with your research themes, although the specific application to large-scale corpora and language models is a notable extension.

#### Abstract
> Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

### 14. RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 8
- **Authors**: Zhichao Xu, Minheng Wang, Yawei Wang, Wenqian Ye, Yuntao Du, Yunpu Ma, Yijun Tian
- **URL**: <http://arxiv.org/abs/2510.10448v1>
- **Submitted**: 2025-10-12 05:00:05
- **Topic Keywords**: relevance, rag, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of retrieval-augmented generation and query understanding. The paper's focus on efficient context management and summarization aligns with your interests in ranking models and real-time relevance optimization. However, the paper's primary focus on NLP and QA applications means it is not a central match to your research themes.

#### Abstract
> Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

### 15. Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Mihir Gupte, Paolo Giusto, Ramesh S
- **URL**: <http://arxiv.org/abs/2510.10806v1>
- **Submitted**: 2025-10-12 20:52:43
- **Comment**: Waiting for Conference Response
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: This paper explores the application of Retrieval-Augmented Generation (RAG) in handling structured data, specifically tree-based structures, which aligns with your interests in Information Retrieval and query understanding. The focus on leveraging implicit knowledge and linearizing it for efficient retrieval is also relevant to your research themes. However, the specific domain of GitHub repositories and code files is somewhat narrow compared to your broader interests.

#### Abstract
> Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

### 16. What Generative Search Engines Like and How to Optimize Web Content Cooperatively

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Yujiang Wu, Shanshan Zhong, Yubin Kim, Chenyan Xiong
- **URL**: <http://arxiv.org/abs/2510.11438v1>
- **Submitted**: 2025-10-13 14:10:26
- **Topic Keywords**: queries, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Generative Search Engines and their optimization. The use of large language models and the introduction of AutoGEO, a framework for automatically learning generative engine preferences, aligns with your focus on query understanding and ranking models. However, the paper's primary focus on Generative Engines and GEO might not be a central match to your broader interests in user behavior modeling and click models.

#### Abstract
> By employing large language models (LLMs) to retrieve documents and generate
natural language responses, Generative Engines, such as Google AI overview and
ChatGPT, provide significantly enhanced user experiences and have rapidly
become the new form of search. Their rapid adoption also drives the needs of
Generative Engine Optimization (GEO), as content providers are eager to gain
more traction from them. In this paper, we introduce AutoGEO, a framework to
automatically learn generative engine preferences when using retrieved contents
for response generation, and rewrite web contents for more such traction.
AutoGEO first prompts frontier LLMs to explain generative engine preferences
and extract meaningful preference rules from these explanations. Then it uses
preference rules as context engineering for AutoGEO$_\text{API}$, a
prompt-based GEO system, and as rule-based rewards to train
AutoGEO$_\text{Mini}$, a cost-effective GEO model. Experiments on the standard
GEO-Bench and two newly constructed benchmarks using real user queries
demonstrate the effectiveness of AutoGEO in enhancing content traction while
preserving search utility. Analyses confirm the learned rules' robustness and
abilities to capture unique preferences in variant domains, and AutoGEO
systems' ability to embed them in content optimization. The code is released at
https://github.com/cxcscmu/AutoGEO.

### 17. CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms

- **LLM Score**: 7
- **Keyword Score**: 21
- **Authors**: Peiyuan Gong, Feiran Zhu, Yaqi Yin, Chenglei Dai, Chao Zhang, Kai Zheng, Wentian Bao, Jiaxin Mao, Yi Zhang
- **URL**: <http://arxiv.org/abs/2510.10095v1>
- **Submitted**: 2025-10-11 08:09:14
- **Topic Keywords**: query, queries, relevance, rag, click, ctr, click-through rate, retrieval, commerce, e-commerce
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in query understanding and ranking models. The focus on long-tail query rewriting and the use of knowledge cards to enhance query rewrites aligns with your interests, but the specific domain of short-video platforms and proprietary content is somewhat outside your primary focus in e-commerce.

#### Abstract
> Short-video platforms have rapidly become a new generation of information
retrieval systems, where users formulate queries to access desired videos.
However, user queries, especially long-tail ones, often suffer from spelling
errors, incomplete phrasing, and ambiguous intent, resulting in mismatches
between user expectations and retrieved results. While large language models
(LLMs) have shown success in long-tail query rewriting within e-commerce, they
struggle on short-video platforms, where proprietary content such as short
videos, live streams, micro dramas, and user social networks falls outside
their training distribution. To address this challenge, we introduce
\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific
knowledge to enhance long-tail query rewriting. For each query, our method
aggregates multi-source knowledge relevant to the query and summarizes it into
an informative and query-relevant knowledge card. This card then guides the LLM
to better capture user intent and produce more effective query rewrites. We
optimize CardRewriter using a two-stage training pipeline: supervised
fine-tuning followed by group relative policy optimization, with a tailored
reward system balancing query relevance and retrieval effectiveness. Offline
experiments show that CardRewriter substantially improves rewriting quality for
queries targeting proprietary content. Online A/B testing further confirms
significant gains in long-view rate (LVR) and click-through rate (CTR), along
with a notable reduction in initiative query reformulation rate (IQRR). Since
September 2025, CardRewriter has been deployed on Kuaishou, one of China's
largest short-video platforms, serving hundreds of millions of users daily.

### 18. VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Zhenghan Tai, Hanwei Wu, Qingchen Hu, Jijun Chi, Hailin He, Lei Ding, Tung Sum Thomas Kwok, Bohuai Xiao, Yuchen Hua, Suyuchen Wang, Peng Lu, Muzhi Li, Yihong Wu, Liheng Ma, Jerry Huang, Jiayi Zhang, Gonghao Zhang, Chaolong Jiang, Jingrui Tian, Sicheng Lyu, Zeyu Li, Boyu Han, Fengran Mo, Xinyue Yu, Yufei Cui, Ling Zhou, Xinyu Wang
- **URL**: <http://arxiv.org/abs/2510.10828v1>
- **Submitted**: 2025-10-12 22:45:24
- **Topic Keywords**: ranking, rag, retrieval, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models, as it presents a novel framework for multi-modal financial question answering. However, the focus on the financial sector and the specific application of Retrieval-Augmented Generation (RAG) may not be a central match to your primary research themes.

#### Abstract
> Retrieval-Augmented Generation (RAG) is becoming increasingly essential for
Question Answering (QA) in the financial sector, where accurate and
contextually grounded insights from complex public disclosures are crucial.
However, existing financial RAG systems face two significant challenges: (1)
they struggle to process heterogeneous data formats, such as text, tables, and
figures; and (2) they encounter difficulties in balancing general-domain
applicability with company-specific adaptation. To overcome these challenges,
we present VeritasFi, an innovative hybrid RAG framework that incorporates a
multi-modal preprocessing pipeline alongside a cutting-edge two-stage training
strategy for its re-ranking component. VeritasFi enhances financial QA through
three key innovations: (1) A multi-modal preprocessing pipeline that seamlessly
transforms heterogeneous data into a coherent, machine-readable format. (2) A
tripartite hybrid retrieval engine that operates in parallel, combining deep
multi-path retrieval over a semantically indexed document corpus, real-time
data acquisition through tool utilization, and an expert-curated memory bank
for high-frequency questions, ensuring comprehensive scope, accuracy, and
efficiency. (3) A two-stage training strategy for the document re-ranker, which
initially constructs a general, domain-specific model using anonymized data,
followed by rapid fine-tuning on company-specific data for targeted
applications. By integrating our proposed designs, VeritasFi presents a
groundbreaking framework that greatly enhances the adaptability and robustness
of financial RAG systems, providing a scalable solution for both general-domain
and company-specific QA tasks. Code accompanying this work is available at
https://github.com/simplew4y/VeritasFi.git.

### 19. Does LLM Focus on the Right Words? Diagnosing Language Bias in LLM-based Recommenders

- **LLM Score**: 7
- **Keyword Score**: 6
- **Authors**: Bohao Wang, Jiawei Chen, Feng Liu, Changwang Zhang, Jun Wang, Canghong Jin, Chun Chen, Can Wang
- **URL**: <http://arxiv.org/abs/2510.10978v1>
- **Submitted**: 2025-10-13 03:35:26
- **Topic Keywords**: relevance, rag, recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of recommender systems and language models. However, the focus on language bias and fairness in LLM-based recommenders, while relevant to your broader interests in NLP and data mining, does not directly align with your core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs), owing to their extensive open-domain knowledge
and semantic reasoning capabilities, have been increasingly integrated into
recommender systems (RS). However, a substantial gap remains between the
pre-training objectives of LLMs and the specific requirements of recommendation
tasks. To address this gap, supervised fine-tuning (SFT) is commonly performed
on specially curated recommendation datasets to further enhance their
predictive ability. Despite its success, SFT exhibits a critical limitation: it
induces Language Bias, whereby the model over-relies on auxiliary tokens-such
as task descriptions and prefix-generated tokens-while underutilizing core user
interaction tokens that encode user-specific preferences. This bias not only
undermines recommendation accuracy but also raises unfairness concerns.
  To address this issue, we propose Group Distributionally Robust
Optimization-based Tuning (GDRT), a novel fine-tuning paradigm that enforces
consistent model performance across token groups with varying degrees of
relevance to auxiliary tokens. By adaptively upweighting underperforming
groups, typically those weakly correlated with auxiliary tokens, GDRT shifts
the model's attention from superficial auxiliary cues to informative user
interaction tokens, thereby mitigating language bias. Extensive experiments
conducted on three public datasets demonstrate that GDRT effectively mitigates
language bias, yielding substantial improvements in recommendation accuracy
(with an average NDCG@10 gain of 24.29%) and significantly enhancing
recommendation fairness.

### 20. Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG

- **LLM Score**: 6
- **Keyword Score**: 12
- **Authors**: Zhichao Wang, Cheng Wan, Dong Nie
- **URL**: <http://arxiv.org/abs/2510.10787v1>
- **Submitted**: 2025-10-12 20:09:07
- **Topic Keywords**: query, rerank, rag, retrieval, rank, search
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of Large Language Models (LLMs) and inference-time scaling. However, the focus on LLMs and generation strategies, while related to search technologies, is not a central match for the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

### 21. Uncertainty Quantification for Retrieval-Augmented Reasoning

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Heydar Soudani, Hamed Zamani, Faegheh Hasibi
- **URL**: <http://arxiv.org/abs/2510.11483v1>
- **Submitted**: 2025-10-13 14:55:28
- **Topic Keywords**: retriever, queries, rag, retrieval
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of retrieval-augmented reasoning. However, the focus on uncertainty quantification and its application to retrieval-augmented reasoning systems is not a central match to the user's primary research themes. The connection to query understanding and ranking models is indirect, but the paper's relevance to real-time relevance optimization is a partial alignment.

#### Abstract
> Retrieval-augmented reasoning (RAR) is a recent evolution of
retrieval-augmented generation (RAG) that employs multiple reasoning steps for
retrieval and generation. While effective for some complex queries, RAR remains
vulnerable to errors and misleading outputs. Uncertainty quantification (UQ)
offers methods to estimate the confidence of systems' outputs. These methods,
however, often handle simple queries with no retrieval or single-step
retrieval, without properly handling RAR setup. Accurate estimation of UQ for
RAR requires accounting for all sources of uncertainty, including those arising
from retrieval and generation. In this paper, we account for all these sources
and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ
method for RAR. The core idea of R2C is to perturb the multi-step reasoning
process by applying various actions to reasoning steps. These perturbations
alter the retriever's input, which shifts its output and consequently modifies
the generator's input at the next step. Through this iterative feedback loop,
the retriever and generator continuously reshape one another's inputs, enabling
us to capture uncertainty arising from both components. Experiments on five
popular RAR systems across diverse QA datasets show that R2C improves AUROC by
over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic
evaluations using R2C as an external signal further confirm its effectiveness
for two downstream tasks: in Abstention, it achieves ~5% gains in both
F1Abstain and AccAbstain; in Model Selection, it improves the exact match by
~7% over single models and ~3% over selection methods.

### 22. One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Jens Van Nooten, Andriy Kosar, Guy De Pauw, Walter Daelemans
- **URL**: <http://arxiv.org/abs/2510.11160v1>
- **Submitted**: 2025-10-13 08:52:14
- **Topic Keywords**: query, relevance, rag, retrieval
- **Reason**: This paper explores distance-based multi-label text classification, which is somewhat related to your interests in Information Retrieval and Natural Language Processing. However, the focus on multi-label classification and text classification methods, while relevant to IR, does not directly align with your primary research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

### 23. Domain-Specific Data Generation Framework for RAG Adaptation

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Chris Xing Tian, Weihao Xie, Zhen Chen, Zhengyuan Yi, Hui Liu, Haoliang Li, Shiqi Wang, Siwei Ma
- **URL**: <http://arxiv.org/abs/2510.11217v1>
- **Submitted**: 2025-10-13 09:59:49
- **Topic Keywords**: retriever, rag, retrieval, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) and its adaptation to domain-specific settings. However, the focus on data generation and RAG adaptation is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

### 24. The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Saad Obaid ul Islam, Anne Lauscher, Goran Glava≈°
- **URL**: <http://arxiv.org/abs/2510.11218v1>
- **Submitted**: 2025-10-13 10:00:58
- **Topic Keywords**: query, queries
- **Reason**: This paper explores the reliability gap between simple and complex queries in Large Language Models (LLMs), which is somewhat related to my research interests in Information Retrieval, particularly query understanding and ranking models. However, the focus on LLMs and factual question-answering is not a central match to my primary research themes, but it does touch on aspects of deep semantic understanding.

#### Abstract
> Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

### 25. Steering Over-refusals Towards Safety in Retrieval Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Utsav Maskey, Mark Dras, Usman Naseem
- **URL**: <http://arxiv.org/abs/2510.10452v1>
- **Submitted**: 2025-10-12 05:09:45
- **Comment**: Preprint
- **Topic Keywords**: query, queries, rag, retrieval augmented generation, retrieval
- **Reason**: The paper explores the issue of over-refusals in retrieval-augmented generation, which is related to information retrieval, but its focus on safety alignment and large language models is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. While it touches on the intersection of IR and NLP, the context is more centered around model safety and generation rather than traditional IR or search technologies.

#### Abstract
> Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

### 26. AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Kai Zhang, Xinyuan Zhang, Ejaz Ahmed, Hongda Jiang, Caleb Kumar, Kai Sun, Zhaojiang Lin, Sanat Sharma, Shereen Oraby, Aaron Colak, Ahmed Aly, Anuj Kumar, Xiaozhong Liu, Xin Luna Dong
- **URL**: <http://arxiv.org/abs/2510.10397v1>
- **Submitted**: 2025-10-12 01:23:23
- **Topic Keywords**: query, ranking, relevance, retrieval, rank
- **Reason**: This paper, AssoMem, focuses on memory-augmented question answering and proposes a novel framework for associative memory graph construction and multi-dimensional retrieval signals. While it touches on ranking and retrieval, its primary focus is on question answering and conversational context, which is somewhat related to information retrieval but not directly aligned with your core research themes.

#### Abstract
> Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

### 27. Hierarchical LoRA MoE for Efficient CTR Model Scaling

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Zhichen Zeng, Mengyue Hang, Xiaolong Liu, Xiaoyi Liu, Xiao Lin, Ruizhong Qiu, Tianxin Wei, Zhining Liu, Siyang Yuan, Chaofei Yang, Yiqun Liu, Hang Yin, Jiyan Yang, Hanghang Tong
- **URL**: <http://arxiv.org/abs/2510.10432v1>
- **Submitted**: 2025-10-12 03:54:11
- **Comment**: 13 pages, 9 figures
- **Topic Keywords**: rag, click, ctr, click-through rate, recommend, rank
- **Reason**: The paper explores click-through rate (CTR) prediction using a novel hierarchical model, which is related to information retrieval and user behavior modeling. However, the focus is on efficient model scaling and optimization, rather than query understanding, ranking models, or deep semantic understanding, making it somewhat relevant but not a central match to your research interests.

#### Abstract
> Deep models have driven significant advances in click-through rate (CTR)
prediction. While vertical scaling via layer stacking improves model
expressiveness, the layer-by-layer sequential computation poses challenges to
efficient scaling. Conversely, horizontal scaling through Mixture of Experts
(MoE) achieves efficient scaling by activating a small subset of experts in
parallel, but flat MoE layers may struggle to capture the hierarchical
structure inherent in recommendation tasks. To push the Return-On-Investment
(ROI) boundary, we explore the complementary strengths of both directions and
propose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic
scaling in a parameter-efficient manner. Specifically, HiLoMoE employs
lightweight rank-1 experts for parameter-efficient horizontal scaling, and
stacks multiple MoE layers with hierarchical routing to enable combinatorially
diverse expert compositions. Unlike conventional stacking, HiLoMoE routes based
on prior layer scores rather than outputs, allowing all layers to execute in
parallel. A principled three-stage training framework ensures stable
optimization and expert diversity. Experiments on four public datasets show
that HiLoMoE achieving better performance-efficiency tradeoff, achieving an
average AUC improvement of 0.20\% in AUC and 18.5\% reduction in FLOPs compared
to the non-MoE baseline.

### 28. HUME: Measuring the Human-Model Performance Gap in Text Embedding Task

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Adnan El Assadi, Isaac Chung, Roman Solomatin, Niklas Muennighoff, Kenneth Enevoldsen
- **URL**: <http://arxiv.org/abs/2510.10062v1>
- **Submitted**: 2025-10-11 06:56:53
- **Comment**: Submitted to ICLR 2026
- **Topic Keywords**: ranking, rerank, rag, rank
- **Reason**: This paper focuses on evaluating text embedding models through human evaluation, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus on text embeddings and human evaluation does not directly align with the user's core research themes in IR and Search technologies. The connection to NLP is relevant, but the paper's scope is more narrow than the user's interests.

#### Abstract
> Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

### 29. Breaking the Likelihood Trap: Consistent Generative Recommendation with Graph-structured Model

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Qiya Yang, Xiaoxi Liang, Zeping Xiao, Yingjie Deng, Yalong Wang, Yongqi Liu, Han Li
- **URL**: <http://arxiv.org/abs/2510.10127v1>
- **Submitted**: 2025-10-11 09:21:01
- **Topic Keywords**: ranking, rerank, recommend, rank
- **Reason**: The paper focuses on recommender systems, specifically generative reranking, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and the 'likelihood trap' concept is not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Reranking, as the final stage of recommender systems, demands real-time
inference, accuracy, and diversity. It plays a crucial role in determining the
final exposure, directly influencing user experience. Recently, generative
reranking has gained increasing attention for its strong ability to model
complex dependencies among items. However, most existing methods suffer from
the "likelihood trap", where high-likelihood sequences are often perceived as
low-quality by humans. These models tend to repeatedly recommend a set of
high-frequency items, resulting in list homogeneity, thereby limiting user
engagement. In this work, we propose Consistent Graph-structured Generative
Recommendation (Congrats), a novel generative reranking framework. To break the
likelihood trap, we introduce a novel graph-structured decoder that can capture
diverse sequences along multiple paths. This design not only expands the
decoding space to promote diversity, but also improves prediction accuracy by
implicit item dependencies derived from vertex transitions. Furthermore, we
design a differentiable cascade system that incorporates an evaluator, enabling
the model to learn directly from user preferences as the training objective.
Extensive offline experiments validate the superior performance of Congrats
over state-of-the-art reranking methods. Moreover, Congrats has been evaluated
on a large-scale video-sharing app, Kuaishou, with over 300 million daily
active users, demonstrating that our approach significantly improves both
recommendation quality and diversity, validating our effectiveness in practical
industrial environments.

### 30. Beyond the Crowd: LLM-Augmented Community Notes for Governing Health Misinformation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Jiaying Wu, Zihang Fu, Haonan Wang, Fanxiao Li, Min-Yen Kan
- **URL**: <http://arxiv.org/abs/2510.11423v1>
- **Submitted**: 2025-10-13 13:57:23
- **Topic Keywords**: relevance, rag
- **Reason**: The paper explores a community-driven misinformation governance system, leveraging large language models (LLMs) to improve responsiveness and reliability. While it touches on aspects of information retrieval and relevance optimization, its primary focus is on fact-checking and governance, which is somewhat related to the user's interests in query understanding and ranking models, but not a central match.

#### Abstract
> Community Notes, the crowd-sourced misinformation governance system on X
(formerly Twitter), enables users to flag misleading posts, attach contextual
notes, and vote on their helpfulness. However, our analysis of 30.8K
health-related notes reveals significant latency, with a median delay of 17.6
hours before the first note receives a helpfulness status. To improve
responsiveness during real-world misinformation surges, we propose CrowdNotes+,
a unified framework that leverages large language models (LLMs) to augment
Community Notes for faster and more reliable health misinformation governance.
CrowdNotes+ integrates two complementary modes: (1) evidence-grounded note
augmentation and (2) utility-guided note automation, along with a hierarchical
three-step evaluation that progressively assesses relevance, correctness, and
helpfulness. We instantiate the framework through HealthNotes, a benchmark of
1.2K helpfulness-annotated health notes paired with a fine-tuned helpfulness
judge. Experiments on fifteen LLMs reveal an overlooked loophole in current
helpfulness evaluation, where stylistic fluency is mistaken for factual
accuracy, and demonstrate that our hierarchical evaluation and LLM-augmented
generation jointly enhance factual precision and evidence utility. These
results point toward a hybrid human-AI governance model that improves both the
rigor and timeliness of crowd-sourced fact-checking.

### 31. BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zhengbo Zhang, Zhiheng Lyu, Junhao Gong, Hongzhu Yi, Xinming Wang, Yuxuan Zhou, Jiabing Yang, Ping Nie, Yan Huang, Wenhu Chen
- **URL**: <http://arxiv.org/abs/2510.10666v1>
- **Submitted**: 2025-10-12 15:43:37
- **Comment**: 10 pages
- **Topic Keywords**: rag, click, search
- **Reason**: The paper discusses a novel approach to web interaction using browser actions, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on Large Language Models (LLMs) and their interaction with web environments is not a central match to the user's core research themes, despite some overlap with user behavior modeling.

#### Abstract
> Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

### 32. Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Donglin Zhou, Weike Pan, Zhong Ming
- **URL**: <http://arxiv.org/abs/2510.10556v1>
- **Submitted**: 2025-10-12 11:42:49
- **Topic Keywords**: rag, user behavior, recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the area of sequential recommendation and content-based sequential recommendation. However, it focuses more on recommender systems and sequential recommendation, which is not your primary area of interest. The paper's emphasis on deep semantic understanding and real-time relevance optimization is also not directly applicable to this work.

#### Abstract
> Sequential recommendation (SR) models often capture user preferences based on
the historically interacted item IDs, which usually obtain sub-optimal
performance when the interaction history is limited. Content-based sequential
recommendation has recently emerged as a promising direction that exploits
items' textual and visual features to enhance preference learning. However,
there are still three key challenges: (i) how to reduce the semantic gap
between different content modality representations; (ii) how to jointly model
user behavior preferences and content preferences; and (iii) how to design an
effective training strategy to align ID representations and content
representations. To address these challenges, we propose a novel model,
self-supervised representation learning with ID-Content modality alignment,
named SICSRec. Firstly, we propose a LLM-driven sample construction method and
develop a supervised fine-tuning approach to align item-level modality
representations. Secondly, we design a novel Transformer-based sequential
model, where an ID-modality sequence encoder captures user behavior
preferences, a content-modality sequence encoder learns user content
preferences, and a mix-modality sequence decoder grasps the intrinsic
relationship between these two types of preferences. Thirdly, we propose a
two-step training strategy with a content-aware contrastive learning task to
align modality representations and ID representations, which decouples the
training process of content modality dependency and item collaborative
dependency. Extensive experiments conducted on four public video streaming
datasets demonstrate our SICSRec outperforms the state-of-the-art ID-modality
sequential recommenders and content-modality sequential recommenders by 8.04%
on NDCG@5 and 6.62% on NDCD@10 on average, respectively.

### 33. Towards Long-Term User Welfare in Recommender Systems via Creator-Oriented Information Revelation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Xu Zhao, Xiaopeng Ye, Chen Xu, Weiran Shen, Jun Xu
- **URL**: <http://arxiv.org/abs/2510.10511v1>
- **Submitted**: 2025-10-12 09:18:15
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper is somewhat related to information retrieval, but its focus on recommender systems and user welfare optimization is not a central match to your research interests in query understanding, ranking models, and user behavior modeling. The paper does involve information revelation, which is a related concept, but it's applied in a different context.

#### Abstract
> Improving the long-term user welfare (e.g., sustained user engagement) has
become a central objective of recommender systems (RS). In real-world
platforms, the creation behaviors of content creators plays a crucial role in
shaping long-term welfare beyond short-term recommendation accuracy, making the
effective steering of creator behavior essential to foster a healthier RS
ecosystem. Existing works typically rely on re-ranking algorithms that
heuristically adjust item exposure to steer creators' behavior. However, when
embedded within recommendation pipelines, such a strategy often conflicts with
the short-term objective of improving recommendation accuracy, leading to
performance degradation and suboptimal long-term welfare. The well-established
economics studies offer us valuable insights for an alternative approach
without relying on recommendation algorithmic design: revealing information
from an information-rich party (sender) to a less-informed party (receiver) can
effectively change the receiver's beliefs and steer their behavior. Inspired by
this idea, we propose an information-revealing framework, named Long-term
Welfare Optimization via Information Revelation (LoRe). In this framework, we
utilize a classical information revelation method (i.e., Bayesian persuasion)
to map the stakeholders in RS, treating the platform as the sender and creators
as the receivers. To address the challenge posed by the unrealistic assumption
of traditional economic methods, we formulate the process of information
revelation as a Markov Decision Process (MDP) and propose a learning algorithm
trained and inferred in environments with boundedly rational creators.
Extensive experiments on two real-world RS datasets demonstrate that our method
can effectively outperform existing fair re-ranking methods and information
revealing strategies in improving long-term user welfare.

### 34. Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Shaobo Wang, Cong Wang, Wenjie Fu, Yue Min, Mingquan Feng, Isabel Guan, Xuming Hu, Conghui He, Cunxiang Wang, Kexin Yang, Xingzhang Ren, Fei Huang, Dayiheng Liu, Linfeng Zhang
- **URL**: <http://arxiv.org/abs/2510.10457v1>
- **Submitted**: 2025-10-12 05:38:10
- **Comment**: 18 pages, 5 figures
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it deals with model evaluation and benchmark compression. However, the focus on Large Language Models (LLMs) and their evaluation is not a central match for the user's research themes, which are more focused on query understanding, ranking models, and user behavior modeling.

#### Abstract
> As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

### 35. Don't Throw Away Your Pretrained Model

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Shangbin Feng, Wenhao Yu, Yike Wang, Hongming Zhang, Yulia Tsvetkov, Dong Yu
- **URL**: <http://arxiv.org/abs/2510.09913v1>
- **Submitted**: 2025-10-10 23:12:20
- **Topic Keywords**: queries, rag
- **Reason**: This paper explores model collaboration and switch generation in language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and their skills, such as creativity and calibration, is not directly aligned with the user's primary research interests in IR and Search technologies. While it touches on the idea of model reuse and repurposing, it is not a central match for the user's research themes.

#### Abstract
> Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

### 36. FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Daniel Berhane Araya, Duoduo Liao
- **URL**: <http://arxiv.org/abs/2510.11654v1>
- **Submitted**: 2025-10-13 17:31:49
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper FinVet is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, its focus on financial misinformation detection and fact-checking is not a central match to your primary research themes in e-commerce and deep semantic understanding. While it does involve Retrieval-Augmented Generation (RAG) pipelines, which is a relevant technique, the application domain and specific goals of the paper are not directly aligned with your research interests.

#### Abstract
> Financial markets face growing threats from misinformation that can trigger
billions in losses in minutes. Most existing approaches lack transparency in
their decision-making and provide limited attribution to credible sources. We
introduce FinVet, a novel multi-agent framework that integrates two
Retrieval-Augmented Generation (RAG) pipelines with external fact-checking
through a confidence-weighted voting mechanism. FinVet employs adaptive
three-tier processing that dynamically adjusts verification strategies based on
retrieval confidence, from direct metadata extraction to hybrid reasoning to
full model-based analysis. Unlike existing methods, FinVet provides
evidence-backed verdicts, source attribution, confidence scores, and explicit
uncertainty flags when evidence is insufficient. Experimental evaluation on the
FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a
10.4% improvement over the best individual pipeline (fact-check pipeline) and
37% improvement over standalone RAG approaches.

### 37. VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Haosheng Qian, Yixing Fan, Jiafeng Guo, Ruqing Zhang, Qi Chen, Dawei Yin, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2510.11394v1>
- **Submitted**: 2025-10-13 13:38:54
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG), a topic related to Information Retrieval, but the focus is on citation verification and answer attribution in NLP. While it touches on the use of external knowledge sources, which is relevant to IR, the primary contribution is in the NLP domain. The paper's relevance to the user's interests is somewhat limited due to the specific focus on citation verification and NLP applications.

#### Abstract
> Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for
enhancing the responses of large language models (LLMs) with external knowledge
sources. Despite the impressive performance in complex question-answering
tasks, RAG still struggles with hallucinations. Attributing RAG-generated
content through in-line citations has demonstrated potential in reducing
hallucinations and facilitating human verification. Existing citation
generation methods primarily rely on either fine-tuning the generator or
employing post-processing approaches for citation matching. However, the former
approach demands substantial annotated data and computational resources, while
the latter often encounters difficulties in managing multiple citations and
frequently produces suboptimal results. In this paper, we introduce a novel
framework, called VeriCite, designed to rigorously validate supporting evidence
and enhance answer attribution. Specifically, VeriCite breaks down into a
three-stage generation: 1) The initial answer generation first generates a
response based on all available contexts and has its claims verified through
the NLI model; 2) the supporting evidence selection assesses the utility of
each document and extracts useful supporting evidences; 3) the final answer
refinement integrates the initial response and collected evidences to produce
the final, refined answer.We conduct experiments across five open-source LLMs
and four datasets, demonstrating that VeriCite can significantly improve
citation quality while maintaining the correctness of the answers.

### 38. HatLLM: Hierarchical Attention Masking for Enhanced Collaborative Modeling in LLM-based Recommendation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Yu Cui, Feng Liu, Jiawei Chen, Canghong Jin, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Can Wang
- **URL**: <http://arxiv.org/abs/2510.10955v1>
- **Submitted**: 2025-10-13 03:05:03
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper explores a novel approach to collaborative modeling in LLM-based recommendation, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the focus on recommendation systems and sequential recommendation is not a central match to the user's primary research themes. The use of attention mechanisms and hierarchical modeling is relevant to the user's interests in ranking models and deep semantic understanding.

#### Abstract
> Recent years have witnessed a surge of research on leveraging large language
models (LLMs) for sequential recommendation. LLMs have demonstrated remarkable
potential in inferring users' nuanced preferences through fine-grained semantic
reasoning. However, they also exhibit a notable limitation in effectively
modeling collaborative signals, i.e., behavioral correlations inherent in
users' historical interactions. Our empirical analysis further reveals that the
attention mechanisms in LLMs tend to disproportionately focus on tokens within
the same item, thereby impeding the capture of cross-item correlations.
  To address this limitation, we propose a novel hierarchical attention masking
strategy for LLM-based recommendation, termed HatLLM. Specifically, in shallow
layers, HatLLM masks attention between tokens from different items,
facilitating intra-item semantic understanding; in contrast, in deep layers,
HatLLM masks attention within items, thereby compelling the model to capture
cross-item correlations. This progressive, layer-wise approach enables LLMs to
jointly model both token-level and item-level dependencies. Extensive
experiments on three real-world datasets demonstrate that HatLLM achieves
significant performance gains (9.13% on average) over existing LLM-based
methods.

### 39. Toward Human-Centered Readability Evaluation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Bahar ƒ∞lgen, Georges Hattab
- **URL**: <http://arxiv.org/abs/2510.10801v1>
- **Submitted**: 2025-10-12 20:38:32
- **Comment**: Accepted to the 4th Workshop on Bridging Human-Computer Interaction
  and NLP (HCI+NLP) at EMNLP 2025, Suzhou, China
- **Topic Keywords**: relevance, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and evaluation metrics, but it focuses on readability evaluation in health contexts, which is not a central match to your primary focus on information retrieval and search technologies.

#### Abstract
> Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

### 40. Evaluating Language Models' Evaluations of Games

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Katherine M. Collins, Cedegao E. Zhang, Graham Todd, Lance Ying, Mauricio Barba da Costa, Ryan Liu, Prafull Sharma, Adrian Weller, Ionatan Kuperwajs, Lionel Wong, Joshua B. Tenenbaum, Thomas L. Griffiths
- **URL**: <http://arxiv.org/abs/2510.10930v1>
- **Submitted**: 2025-10-13 02:45:37
- **Comment**: Pre-print
- **Topic Keywords**: query, queries, rag
- **Reason**: This paper appears to be primarily focused on evaluating language models' ability to assess games, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on aspects of query understanding and model evaluation, the context is more aligned with AI and game theory rather than your specific areas of interest.

#### Abstract
> Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

### 41. BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Euhid Aman, Esteban Carlin, Hsing-Kuo Pao, Giovanni Beltrame, Ghaluh Indah Permata Sari, Yie-Tarng Chen
- **URL**: <http://arxiv.org/abs/2510.10560v1>
- **Submitted**: 2025-10-12 11:59:41
- **Comment**: 6 pages, BabyLM Workshop, EMNLP 2025
- **Topic Keywords**: query, relevance, retrieval
- **Reason**: This paper focuses on multimodal fusion and edge device deployment, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, the primary focus is on hardware-oriented quantization and memory-augmented architectures, making it less relevant to your research areas.

#### Abstract
> Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

### 42. VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Prawaal Sharma, Poonam Goyal, Vidisha Sharma, Navneet Goyal
- **URL**: <http://arxiv.org/abs/2510.10490v1>
- **Submitted**: 2025-10-12 07:47:41
- **Comment**: 9 Pages, Plus Appendices, EACL 2024
- **Topic Keywords**: ltr, rag, recommend
- **Reason**: This paper focuses on Optical Character Recognition (OCR) for low-resource languages, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning and feature extraction, the context and application are distinct from the user's areas of interest.

#### Abstract
> UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

### 43. Bag of Tricks for Subverting Reasoning-based Safety Guardrails

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Shuo Chen, Zhen Han, Haokun Chen, Bailan He, Shengyun Si, Jingpei Wu, Philip Torr, Volker Tresp, Jindong Gu
- **URL**: <http://arxiv.org/abs/2510.11570v1>
- **Submitted**: 2025-10-13 16:16:44
- **Comment**: OpenAI Red-teaming Challenge Winner and Oral Presentation
- **Topic Keywords**: query, rag
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on Large Reasoning Models, the focus is on safety guardrails and manipulation techniques, which is not a central match to your areas of expertise.

#### Abstract
> Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs),
such as deliberative alignment, have shown strong defense against jailbreak
attacks. By leveraging LRMs' reasoning ability, these guardrails help the
models to assess the safety of user inputs before generating final responses.
The powerful reasoning ability can analyze the intention of the input query and
will refuse to assist once it detects the harmful intent hidden by the
jailbreak methods. Such guardrails have shown a significant boost in defense,
such as the near-perfect refusal rates on the open-source gpt-oss series.
Unfortunately, we find that these powerful reasoning-based guardrails can be
extremely vulnerable to subtle manipulation of the input prompts, and once
hijacked, can lead to even more harmful results. Specifically, we first uncover
a surprisingly fragile aspect of these guardrails: simply adding a few template
tokens to the input prompt can successfully bypass the seemingly powerful
guardrails and lead to explicit and harmful responses. To explore further, we
introduce a bag of jailbreak methods that subvert the reasoning-based
guardrails. Our attacks span white-, gray-, and black-box settings and range
from effortless template manipulations to fully automated optimization. Along
with the potential for scalable implementation, these methods also achieve
alarmingly high attack success rates (e.g., exceeding 90% across 5 different
benchmarks on gpt-oss series on both local host models and online API
services). Evaluations across various leading open-source LRMs confirm that
these vulnerabilities are systemic, underscoring the urgent need for stronger
alignment techniques for open-sourced LRMs to prevent malicious misuse. Code is
open-sourced at https://chenxshuo.github.io/bag-of-tricks.

### 44. DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Meiru Zhang, Philipp Borchert, Milan Gritta, Gerasimos Lampouras
- **URL**: <http://arxiv.org/abs/2510.10815v1>
- **Submitted**: 2025-10-12 21:42:04
- **Topic Keywords**: query, retrieval
- **Reason**: This paper focuses on automating the formalization of mathematical statements using Large Language Models, which is outside the scope of Information Retrieval and Search technologies. Although it involves retrieval-augmented autoformalization, the context and application are not relevant to the user's core research themes.

#### Abstract
> Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

### 45. Assessing Large Language Models for Structured Medical Order Extraction

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: A H M Rezaul Karim, Ozlem Uzuner
- **URL**: <http://arxiv.org/abs/2510.10475v1>
- **Submitted**: 2025-10-12 06:56:10
- **Topic Keywords**: rag, ctr, rank
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on medical order extraction and large language models in the clinical domain.

#### Abstract
> Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

### 46. Scaling Language-Centric Omnimodal Representation Learning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong
- **URL**: <http://arxiv.org/abs/2510.11693v1>
- **Submitted**: 2025-10-13 17:53:52
- **Comment**: NeurIPS 2025
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on multimodal representation learning using large language models, which is not directly related to information retrieval, query understanding, or ranking models. While it involves deep semantic understanding, the context is more aligned with NLP and multimodal learning rather than IR and search technologies.

#### Abstract
> Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

### 47. DocReward: A Document Reward Model for Structuring and Stylizing

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang, Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei
- **URL**: <http://arxiv.org/abs/2510.11391v1>
- **Submitted**: 2025-10-13 13:36:32
- **Topic Keywords**: ranking, rank
- **Reason**: This paper focuses on a document reward model for evaluating structure and style, which is not directly related to information retrieval, query understanding, or ranking models. While it involves NLP, the context is document generation and evaluation, rather than search or retrieval.

#### Abstract
> Recent advances in agentic workflows have enabled the automation of tasks
such as professional document generation. However, they primarily focus on
textual quality, neglecting visual structure and style, which are crucial for
readability and engagement. This gap arises mainly from the absence of suitable
reward models to guide agentic workflows toward producing documents with
stronger structural and stylistic quality. To address this, we propose
DocReward, a document reward model that evaluates documents based on their
structure and style. We construct a multi-domain dataset DocPair of 117K paired
documents, covering 32 domains and 267 document types, each including a high-
and low-professionalism document with identical content but different structure
and style. This enables the model to evaluate professionalism comprehensively,
and in a textual-quality-agnostic way. DocReward is trained using the
Bradley-Terry loss to score documents, penalizing predictions that contradict
the annotated ranking. To assess the performance of reward models, we create a
test dataset containing document bundles ranked by well-educated human
evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6
and 19.4 percentage points, respectively, demonstrating its superiority over
baselines. In an extrinsic evaluation of document generation, DocReward
achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%
win rate, demonstrating its utility in guiding generation agents toward
producing human-preferred documents.

### 48. Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hengyuan Zhang, Shiping Yang, Xiao Liang, Chenming Shang, Yuxuan Jiang, Chaofan Tao, Jing Xiong, Hayden Kwok-Hay So, Ruobing Xie, Angel X. Chang, Ngai Wong
- **URL**: <http://arxiv.org/abs/2510.10925v1>
- **Submitted**: 2025-10-13 02:36:36
- **Comment**: 19 pages, 10 figures
- **Topic Keywords**: query, search
- **Reason**: This paper focuses on data synthesis and model distillation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves a router-guided approach, the context is more aligned with model learning and optimization rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Training student models on synthetic data generated by strong teacher models
is a promising way to distilling the capabilities of teachers. However, recent
studies show that stronger models are not always optimal teachers, revealing a
mismatch between teacher outputs and student learnability. To address this
issue, we propose PerSyn (Personalized data Synthesis), a novel synthesis
strategy that operates under a new ``Route then Generate'' paradigm to create
data tailored to each student model, enabling it to learn more effectively.
Specifically, PerSyn first assigns each prompt to its optimal teacher via a
query-level router that jointly considers student learnability and teacher
response quality. Each teacher then synthesizes data only for its assigned
prompts, making the process more efficient than the conventional ``Generate
then Select'' paradigm, where all teachers must generate parallel responses for
the entire prompt set before constructing the final dataset. Extensive
experiments across different model families and scales demonstrate that PerSyn
consistently achieves superior or comparable performance to all baselines in
instruct tuning and math reasoning settings. Further analysis verifies the
effectiveness of PerSyn and offers extra insights to propel future research.

### 49. Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Zhuowei Chen, Bowei Zhang, Nankai Lin, Tian Hou, Lianxi Wang
- **URL**: <http://arxiv.org/abs/2510.10677v1>
- **Submitted**: 2025-10-12 16:05:56
- **Comment**: Accepted to MRL Workshop at EMNLP 2025
- **Topic Keywords**: queries, search
- **Reason**: This paper focuses on Large Language Models (LLMs) and proposes a safeguard to detect malicious requests, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context is more aligned with AI safety and security rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

### 50. ArtPerception: ASCII Art-based Jailbreak on LLMs with Recognition Pre-test

- **LLM Score**: 0
- **Keyword Score**: 5
- **Authors**: Guan-Yan Yang, Tzu-Yu Cheng, Ya-Wen Teng, Farn Wanga, Kuo-Hui Yeh
- **URL**: <http://arxiv.org/abs/2510.10281v1>
- **Submitted**: 2025-10-11 16:28:37
- **Comment**: 30 pages, 22 figures. This preprint has been accepted for publication
  in Elsevier JOURNAL OF NETWORK AND COMPUTER APPLICATIONS (JNCA)
- **Topic Keywords**: relevance, rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on Large Language Model security and ASCII art-based attacks, which is outside your primary areas of focus.

#### Abstract
> The integration of Large Language Models (LLMs) into computer applications
has introduced transformative capabilities but also significant security
challenges. Existing safety alignments, which primarily focus on semantic
interpretation, leave LLMs vulnerable to attacks that use non-standard data
representations. This paper introduces ArtPerception, a novel black-box
jailbreak framework that strategically leverages ASCII art to bypass the
security measures of state-of-the-art (SOTA) LLMs. Unlike prior methods that
rely on iterative, brute-force attacks, ArtPerception introduces a systematic,
two-phase methodology. Phase 1 conducts a one-time, model-specific pre-test to
empirically determine the optimal parameters for ASCII art recognition. Phase 2
leverages these insights to launch a highly efficient, one-shot malicious
jailbreak attack. We propose a Modified Levenshtein Distance (MLD) metric for a
more nuanced evaluation of an LLM's recognition capability. Through
comprehensive experiments on four SOTA open-source LLMs, we demonstrate
superior jailbreak performance. We further validate our framework's real-world
relevance by showing its successful transferability to leading commercial
models, including GPT-4o, Claude Sonnet 3.7, and DeepSeek-V3, and by conducting
a rigorous effectiveness analysis against potential defenses such as LLaMA
Guard and Azure's content filters. Our findings underscore that true LLM
security requires defending against a multi-modal space of interpretations,
even within text-only inputs, and highlight the effectiveness of strategic,
reconnaissance-based attacks. Content Warning: This paper includes potentially
harmful and offensive model outputs.

---

