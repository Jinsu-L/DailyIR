[
    {
        "title": "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs",
        "abstract": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
        "url": "http://arxiv.org/abs/2508.14896v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14896v1",
        "arxiv_id": "2508.14896v1",
        "authors": [
            "Haokun Lin",
            "Haobo Xu",
            "Yichen Wu",
            "Ziyu Guo",
            "Renrui Zhang",
            "Zhichao Lu",
            "Ying Wei",
            "Qingfu Zhang",
            "Zhenan Sun"
        ],
        "submitted": "2025-08-20 17:59:51",
        "source": "arxiv",
        "comment": "Technical Report, Work in Progress"
    },
    {
        "title": "Virtual Community: An Open World for Humans, Robots, and Society",
        "abstract": "The rapid progress in AI and Robotics may lead to a profound societal\ntransformation, as humans and robots begin to coexist within shared\ncommunities, introducing both opportunities and challenges. To explore this\nfuture, we present Virtual Community-an open-world platform for humans, robots,\nand society-built on a universal physics engine and grounded in real-world 3D\nscenes. With Virtual Community, we aim to study embodied social intelligence at\nscale: 1) How robots can intelligently cooperate or compete; 2) How humans\ndevelop social relations and build community; 3) More importantly, how\nintelligent robots and humans can co-exist in an open world. To support these,\nVirtual Community features: 1) An open-source multi-agent physics simulator\nthat supports robots, humans, and their interactions within a society; 2) A\nlarge-scale, real-world aligned community generation pipeline, including vast\noutdoor space, diverse indoor scenes, and a community of grounded agents with\nrich characters and appearances. Leveraging Virtual Community, we propose two\nnovel challenges. The Community Planning Challenge evaluates multi-agent\nreasoning and planning ability in open-world settings, such as cooperating to\nhelp agents with daily activities and efficiently connecting other agents. The\nCommunity Robot Challenge requires multiple heterogeneous robots to collaborate\nin solving complex open-world tasks. We evaluate various baselines on these\ntasks and demonstrate the challenges in both high-level open-world task\nplanning and low-level cooperation controls. We hope that Virtual Community\nwill unlock further study of human-robot coexistence within open-world\nenvironments.",
        "url": "http://arxiv.org/abs/2508.14893v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14893v1",
        "arxiv_id": "2508.14893v1",
        "authors": [
            "Qinhong Zhou",
            "Hongxin Zhang",
            "Xiangye Lin",
            "Zheyuan Zhang",
            "Yutian Chen",
            "Wenjun Liu",
            "Zunzhe Zhang",
            "Sunli Chen",
            "Lixing Fang",
            "Qiushi Lyu",
            "Xinyu Sun",
            "Jincheng Yang",
            "Zeyuan Wang",
            "Bao Chi Dang",
            "Zhehuan Chen",
            "Daksha Ladia",
            "Jiageng Liu",
            "Chuang Gan"
        ],
        "submitted": "2025-08-20 17:59:32",
        "source": "arxiv",
        "comment": "website https://virtual-community-ai.github.io/"
    },
    {
        "title": "MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework",
        "abstract": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts.We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions.Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains.",
        "url": "http://arxiv.org/abs/2508.14880v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14880v1",
        "arxiv_id": "2508.14880v1",
        "authors": [
            "Ailing Yu",
            "Lan Yao",
            "Jingnan Liu",
            "Zhe Chen",
            "Jiajun Yin",
            "Yuan Wang",
            "Xinhao Liao",
            "Zhiling Ye",
            "Ji Li",
            "Yun Yue",
            "Hansong Xiao",
            "Hualei Zhou",
            "Chunxiao Guo",
            "Peng Wei",
            "Jinjie Gu"
        ],
        "submitted": "2025-08-20 17:51:20",
        "source": "arxiv",
        "comment": "13 pages, 5 figures"
    },
    {
        "title": "The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models",
        "abstract": "Prompt engineering has rapidly emerged as a critical skill for effective\ninteraction with large language models (LLMs). However, the cognitive and\nneural underpinnings of this expertise remain largely unexplored. This paper\npresents findings from a cross-sectional pilot fMRI study investigating\ndifferences in brain functional connectivity and network activity between\nexperts and intermediate prompt engineers. Our results reveal distinct neural\nsignatures associated with higher prompt engineering literacy, including\nincreased functional connectivity in brain regions such as the left middle\ntemporal gyrus and the left frontal pole, as well as altered power-frequency\ndynamics in key cognitive networks. These findings offer initial insights into\nthe neurobiological basis of prompt engineering proficiency. We discuss the\nimplications of these neurocognitive markers in Natural Language Processing\n(NLP). Understanding the neural basis of human expertise in interacting with\nLLMs can inform the design of more intuitive human-AI interfaces, contribute to\ncognitive models of LLM interaction, and potentially guide the development of\nAI systems that better align with human cognitive workflows. This\ninterdisciplinary approach aims to bridge the gap between human cognition and\nmachine intelligence, fostering a deeper understanding of how humans learn and\nadapt to complex AI systems.",
        "url": "http://arxiv.org/abs/2508.14869v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14869v1",
        "arxiv_id": "2508.14869v1",
        "authors": [
            "Hend Al-Khalifa",
            "Raneem Almansour",
            "Layan Abdulrahman Alhuasini",
            "Alanood Alsaleh",
            "Mohamad-Hani Temsah",
            "Mohamad-Hani_Temsah",
            "Ashwag Rafea S Alruwaili"
        ],
        "submitted": "2025-08-20 17:31:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Long Chain-of-Thought Reasoning Across Languages",
        "abstract": "Scaling inference through long chains-of-thought (CoTs) has unlocked\nimpressive reasoning capabilities in large language models (LLMs), yet the\nreasoning process remains almost exclusively English-centric. We construct\ntranslated versions of two popular English reasoning datasets, fine-tune Qwen\n2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT\ngeneration across French, Japanese, Latvian, and Swahili. Our experiments\nreveal three key findings. First, the efficacy of using English as a pivot\nlanguage varies by language: it provides no benefit for French, improves\nperformance when used as the reasoning language for Japanese and Latvian, and\nproves insufficient for Swahili where both task comprehension and reasoning\nremain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but\ndoes not eliminate the cross-lingual performance gap. A lightweight fine-tune\nusing only 1k traces still improves performance by over 30\\% in Swahili. Third,\ndata quality versus scale trade-offs are language dependent: small, carefully\ncurated datasets suffice for English and French, whereas larger but noisier\ncorpora prove more effective for Swahili and Latvian. Together, these results\nclarify when and why long CoTs transfer across languages and provide translated\ndatasets to foster equitable multilingual reasoning research.",
        "url": "http://arxiv.org/abs/2508.14828v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14828v1",
        "arxiv_id": "2508.14828v1",
        "authors": [
            "Josh Barua",
            "Seun Eisape",
            "Kayo Yin",
            "Alane Suhr"
        ],
        "submitted": "2025-08-20 16:22:51",
        "source": "arxiv",
        "comment": "Accepted to SCALR @ COLM 2025"
    },
    {
        "title": "Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs",
        "abstract": "Electronic health records (EHRs) are long, noisy, and often redundant, posing\na major challenge for the clinicians who must navigate them. Large language\nmodels (LLMs) offer a promising solution for extracting and reasoning over this\nunstructured text, but the length of clinical notes often exceeds even\nstate-of-the-art models' extended context windows. Retrieval-augmented\ngeneration (RAG) offers an alternative by retrieving task-relevant passages\nfrom across the entire EHR, potentially reducing the amount of required input\ntokens. In this work, we propose three clinical tasks designed to be replicable\nacross health systems with minimal effort: 1) extracting imaging procedures, 2)\ngenerating timelines of antibiotic use, and 3) identifying key diagnoses. Using\nEHRs from actual hospitalized patients, we test three state-of-the-art LLMs\nwith varying amounts of provided context, using either targeted text retrieval\nor the most recent clinical notes. We find that RAG closely matches or exceeds\nthe performance of using recent notes, and approaches the performance of using\nthe models' full context while requiring drastically fewer input tokens. Our\nresults suggest that RAG remains a competitive and efficient approach even as\nnewer models become capable of handling increasingly longer amounts of text.",
        "url": "http://arxiv.org/abs/2508.14817v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14817v1",
        "arxiv_id": "2508.14817v1",
        "authors": [
            "Skatje Myers",
            "Dmitriy Dligach",
            "Timothy A. Miller",
            "Samantha Barr",
            "Yanjun Gao",
            "Matthew Churpek",
            "Anoop Mayampurath",
            "Majid Afshar"
        ],
        "submitted": "2025-08-20 16:09:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Privileged Self-Access Matters for Introspection in AI",
        "abstract": "Whether AI models can introspect is an increasingly important practical\nquestion. But there is no consensus on how introspection is to be defined.\nBeginning from a recently proposed ''lightweight'' definition, we argue instead\nfor a thicker one. According to our proposal, introspection in AI is any\nprocess which yields information about internal states through a process more\nreliable than one with equal or lower computational cost available to a third\nparty. Using experiments where LLMs reason about their internal temperature\nparameters, we show they can appear to have lightweight introspection while\nfailing to meaningfully introspect per our proposed definition.",
        "url": "http://arxiv.org/abs/2508.14802v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14802v1",
        "arxiv_id": "2508.14802v1",
        "authors": [
            "Siyuan Song",
            "Harvey Lederman",
            "Jennifer Hu",
            "Kyle Mahowald"
        ],
        "submitted": "2025-08-20 15:52:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns",
        "abstract": "We consider the task of learning from both positive and negative feedback in\na sequential recommendation scenario, as both types of feedback are often\npresent in user interactions. Meanwhile, conventional sequential learning\nmodels usually focus on considering and predicting positive interactions,\nignoring that reducing items with negative feedback in recommendations improves\nuser satisfaction with the service. Moreover, the negative feedback can\npotentially provide a useful signal for more accurate identification of true\nuser interests. In this work, we propose to train two transformer encoders on\nseparate positive and negative interaction sequences. We incorporate both types\nof feedback into the training objective of the sequential recommender using a\ncomposite loss function that includes positive and negative cross-entropy as\nwell as a cleverly crafted contrastive term, that helps better modeling\nopposing patterns. We demonstrate the effectiveness of this approach in terms\nof increasing true-positive metrics compared to state-of-the-art sequential\nrecommendation methods while reducing the number of wrongly promoted negative\nitems.",
        "url": "http://arxiv.org/abs/2508.14786v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14786v1",
        "arxiv_id": "2508.14786v1",
        "authors": [
            "Veronika Ivanova",
            "Evgeny Frolov",
            "Alexey Vasilev"
        ],
        "submitted": "2025-08-20 15:32:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting",
        "abstract": "Urban transportation systems encounter diverse challenges across multiple\ntasks, such as traffic forecasting, electric vehicle (EV) charging demand\nprediction, and taxi dispatch. Existing approaches suffer from two key\nlimitations: small-scale deep learning models are task-specific and\ndata-hungry, limiting their generalizability across diverse scenarios, while\nlarge language models (LLMs), despite offering flexibility through natural\nlanguage interfaces, struggle with structured spatiotemporal data and numerical\nreasoning in transportation domains. To address these limitations, we propose\nTransLLM, a unified foundation framework that integrates spatiotemporal\nmodeling with large language models through learnable prompt composition. Our\napproach features a lightweight spatiotemporal encoder that captures complex\ndependencies via dilated temporal convolutions and dual-adjacency graph\nattention networks, seamlessly interfacing with LLMs through structured\nembeddings. A novel instance-level prompt routing mechanism, trained via\nreinforcement learning, dynamically personalizes prompts based on input\ncharacteristics, moving beyond fixed task-specific templates. The framework\noperates by encoding spatiotemporal patterns into contextual representations,\ndynamically composing personalized prompts to guide LLM reasoning, and\nprojecting the resulting representations through specialized output layers to\ngenerate task-specific predictions. Experiments across seven datasets and three\ntasks demonstrate the exceptional effectiveness of TransLLM in both supervised\nand zero-shot settings. Compared to ten baseline models, it delivers\ncompetitive performance on both regression and planning problems, showing\nstrong generalization and cross-task adaptability. Our code is available at\nhttps://github.com/BiYunying/TransLLM.",
        "url": "http://arxiv.org/abs/2508.14782v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14782v1",
        "arxiv_id": "2508.14782v1",
        "authors": [
            "Jiaming Leng",
            "Yunying Bi",
            "Chuan Qin",
            "Bing Yin",
            "Yanyong Zhang",
            "Chao Wang"
        ],
        "submitted": "2025-08-20 15:27:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference",
        "abstract": "Large language models (LLMs) are increasingly applied in multilingual\ncontexts, yet their capacity for consistent, logically grounded alignment\nacross languages remains underexplored. We present a controlled evaluation\nframework for multilingual natural language inference (NLI) that generates\nsynthetic, logic-based premise-hypothesis pairs and translates them into a\ntypologically diverse set of languages. This design enables precise control\nover semantic relations and allows testing in both monolingual and\nmixed-language (code-switched) conditions. Surprisingly, code-switching does\nnot degrade, and can even improve, performance, suggesting that\ntranslation-induced lexical variation may serve as a regularization signal. We\nvalidate semantic preservation through embedding-based similarity analyses and\ncross-lingual alignment visualizations, confirming the fidelity of translated\npairs. Our findings expose both the potential and the brittleness of current\nLLM cross-lingual reasoning, and identify code-switching as a promising lever\nfor improving multilingual robustness. Code available at:\nhttps://github.com/KurbanIntelligenceLab/nli-stress-testing",
        "url": "http://arxiv.org/abs/2508.14735v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14735v1",
        "arxiv_id": "2508.14735v1",
        "authors": [
            "Samir Abdaljalil",
            "Erchin Serpedin",
            "Khalid Qaraqe",
            "Hasan Kurban"
        ],
        "submitted": "2025-08-20 14:30:34",
        "source": "arxiv",
        "comment": "Under review"
    },
    {
        "title": "Improving LLMs for Machine Translation Using Synthetic Preference Data",
        "abstract": "Large language models have emerged as effective machine translation systems.\nIn this paper, we explore how a general instruction-tuned large language model\ncan be improved for machine translation using relatively few easily produced\ndata resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct\nmodel using Direct Preference Optimization (DPO) training on a programmatically\ncurated and enhanced subset of a public dataset. As DPO requires pairs of\nquality-ranked instances, we generated its training dataset by translating\nEnglish Wikipedia articles using two LLMs, GaMS-9B-Instruct and\nEuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics\ncoupled with automatic evaluation metrics such as COMET. The evaluation shows\nthat our fine-tuned model outperforms both models involved in the dataset\ngeneration. In comparison to the baseline models, the fine-tuned model achieved\na COMET score gain of around 0.04 and 0.02, respectively, on translating\nWikipedia articles. It also more consistently avoids language and formatting\nerrors.",
        "url": "http://arxiv.org/abs/2508.14951v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14951v1",
        "arxiv_id": "2508.14951v1",
        "authors": [
            "Dario Vajda",
            "Domen Vreš",
            "Marko Robnik-Šikonja"
        ],
        "submitted": "2025-08-20 14:24:16",
        "source": "arxiv",
        "comment": "Paper with individual presentation at LUHME workshop at ECAI 2025"
    },
    {
        "title": "Transplant Then Regenerate: A New Paradigm for Text Data Augmentation",
        "abstract": "Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.",
        "url": "http://arxiv.org/abs/2508.14723v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14723v1",
        "arxiv_id": "2508.14723v1",
        "authors": [
            "Guangzhan Wang",
            "Hongyu Zhang",
            "Beijun Shen",
            "Xiaodong Gu"
        ],
        "submitted": "2025-08-20 14:05:18",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025"
    },
    {
        "title": "The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation",
        "abstract": "We established a rigorous benchmark for text-based recipe generation, a\nfundamental task in natural language generation. We present a comprehensive\ncomparative study contrasting a fine-tuned GPT-2 large (774M) model against the\nGPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine\ncorpus from RecipeDB. Our key contribution is a targeted tokenization strategy\nthat augments the vocabulary with 23 common fraction tokens and custom\nstructural markers. This approach addresses a critical limitation of generic\ntokenizers by preserving essential recipe structures and precise numerical\nquantities, thereby enhancing domain specificity. Performance is evaluated\nusing a comprehensive suite of seven automatic metrics spanning fluency\n(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and\ndiversity. Our experiments show that the large transformer-based approach\nyields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the\nbest recurrent baseline, while reducing perplexity by 69.8%. We conclude with a\ndiscussion of remaining challenges, particularly regarding factual accuracy,\nand outline how this foundational study paves the way for integrating\nreal-world constraints and multi-modal inputs in advanced recipe generation\nresearch.",
        "url": "http://arxiv.org/abs/2508.14718v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14718v1",
        "arxiv_id": "2508.14718v1",
        "authors": [
            "Shubham Pundhir",
            "Ganesh Bagler"
        ],
        "submitted": "2025-08-20 13:53:13",
        "source": "arxiv",
        "comment": "8 pages, 4 figures. Code is available at:\n  https://github.com/shubh-iiit/RecipeGPT2-Your-Own-AI-Chef"
    },
    {
        "title": "ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine",
        "abstract": "Despite the success of large language models (LLMs) in various domains, their\npotential in Traditional Chinese Medicine (TCM) remains largely underexplored\ndue to two critical barriers: (1) the scarcity of high-quality TCM data and (2)\nthe inherently multimodal nature of TCM diagnostics, which involve looking,\nlistening, smelling, and pulse-taking. These sensory-rich modalities are beyond\nthe scope of conventional LLMs. To address these challenges, we present\nShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data\nscarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text\nand 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and\nphysiological signals. ShizhenGPT is pretrained and instruction-tuned to\nachieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect\nrecent national TCM qualification exams and build a visual benchmark for\nMedicinal Recognition and Visual Diagnosis. Experiments demonstrate that\nShizhenGPT outperforms comparable-scale LLMs and competes with larger\nproprietary models. Moreover, it leads in TCM visual understanding among\nexisting multimodal LLMs and demonstrates unified perception across modalities\nlike sound, pulse, smell, and vision, paving the way toward holistic multimodal\nperception and diagnosis in TCM. Datasets, models, and code are publicly\navailable. We hope this work will inspire further exploration in this field.",
        "url": "http://arxiv.org/abs/2508.14706v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14706v1",
        "arxiv_id": "2508.14706v1",
        "authors": [
            "Junying Chen",
            "Zhenyang Cai",
            "Zhiheng Liu",
            "Yunjin Yang",
            "Rongsheng Wang",
            "Qingying Xiao",
            "Xiangyi Feng",
            "Zhan Su",
            "Jing Guo",
            "Xiang Wan",
            "Guangjun Yu",
            "Haizhou Li",
            "Benyou Wang"
        ],
        "submitted": "2025-08-20 13:30:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers",
        "abstract": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.",
        "url": "http://arxiv.org/abs/2508.14704v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14704v1",
        "arxiv_id": "2508.14704v1",
        "authors": [
            "Ziyang Luo",
            "Zhiqi Shen",
            "Wenzhuo Yang",
            "Zirui Zhao",
            "Prathyusha Jwalapuram",
            "Amrita Saha",
            "Doyen Sahoo",
            "Silvio Savarese",
            "Caiming Xiong",
            "Junnan Li"
        ],
        "submitted": "2025-08-20 13:28:58",
        "source": "arxiv",
        "comment": "Website: https://mcp-universe.github.io"
    },
    {
        "title": "Improving in-context learning with a better scoring function",
        "abstract": "Large language models (LLMs) exhibit a remarkable capacity to learn by\nanalogy, known as in-context learning (ICL). However, recent studies have\nrevealed limitations in this ability. In this paper, we examine these\nlimitations on tasks involving first-order quantifiers such as {\\em all} and\n{\\em some}, as well as on ICL with linear functions. We identify Softmax, the\nscoring function in attention mechanism, as a contributing factor to these\nconstraints. To address this, we propose \\textbf{scaled signed averaging\n(SSA)}, a novel alternative to Softmax. Empirical results show that SSA\ndramatically improves performance on our target tasks. Furthermore, we evaluate\nboth encoder-only and decoder-only transformers models with SSA, demonstrating\nthat they match or exceed their Softmax-based counterparts across a variety of\nlinguistic probing tasks.",
        "url": "http://arxiv.org/abs/2508.14685v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14685v1",
        "arxiv_id": "2508.14685v1",
        "authors": [
            "Omar Naim",
            "Swarnadeep Bhar",
            "Jérôme Bolte",
            "Nicholas Asher"
        ],
        "submitted": "2025-08-20 13:01:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service",
        "abstract": "Local life service is a vital scenario in Kuaishou App, where video\nrecommendation is intrinsically linked with store's location information. Thus,\nrecommendation in our scenario is challenging because we should take into\naccount user's interest and real-time location at the same time. In the face of\nsuch complex scenarios, end-to-end generative recommendation has emerged as a\nnew paradigm, such as OneRec in the short video scenario, OneSug in the search\nscenario, and EGA in the advertising scenario. However, in local life service,\nan end-to-end generative recommendation model has not yet been developed as\nthere are some key challenges to be solved. The first challenge is how to make\nfull use of geographic information. The second challenge is how to balance\nmultiple objectives, including user interests, the distance between user and\nstores, and some other business objectives. To address the challenges, we\npropose OneLoc. Specifically, we leverage geographic information from different\nperspectives: (1) geo-aware semantic ID incorporates both video and geographic\ninformation for tokenization, (2) geo-aware self-attention in the encoder\nleverages both video location similarity and user's real-time location, and (3)\nneighbor-aware prompt captures rich context information surrounding users for\ngeneration. To balance multiple objectives, we use reinforcement learning and\npropose two reward functions, i.e., geographic reward and GMV reward. With the\nabove design, OneLoc achieves outstanding offline and online performance. In\nfact, OneLoc has been deployed in local life service of Kuaishou App. It serves\n400 million active users daily, achieving 21.016% and 17.891% improvements in\nterms of gross merchandise value (GMV) and orders numbers.",
        "url": "http://arxiv.org/abs/2508.14646v1",
        "pdf_url": "http://arxiv.org/pdf/2508.14646v1",
        "arxiv_id": "2508.14646v1",
        "authors": [
            "Zhipeng Wei",
            "Kuo Cai",
            "Junda She",
            "Jie Chen",
            "Minghao Chen",
            "Yang Zeng",
            "Qiang Luo",
            "Wencong Zeng",
            "Ruiming Tang",
            "Kun Gai",
            "Guorui Zhou"
        ],
        "submitted": "2025-08-20 11:57:48",
        "source": "arxiv",
        "comment": null
    }
]