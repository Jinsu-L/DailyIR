# Daily Papers Report - 2025-07-23

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. RAVine: Reality-Aligned Evaluation for Agentic Search

- **LLM Score**: 7
- **Keyword Score**: 6
- **Authors**: Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao
- **URL**: <http://arxiv.org/abs/2507.16725v1>
- **Submitted**: 2025-07-22 16:08:12
- **Topic Keywords**: queries, retrieval, search
- **Reason**: The paper proposes a new evaluation framework for agentic search, which is a relevant topic in Information Retrieval. The framework addresses limitations in existing evaluation methods, such as noisy ground truth and focus on final answers. While the paper does not specifically focus on query understanding, ranking models, or user behavior modeling, it contributes to the development of intelligent search systems, which aligns with the user's broader interests in IR and NLP.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Evaluation frameworks for agentic search systems
- **Aim**: To propose a new evaluation framework that addresses the limitations of existing frameworks
- **Rationale**: Existing frameworks have limitations, including complex queries, noisy ground truth extraction, and neglect of iterative process
- **Ground**: Agentic search systems, which are designed to be more autonomous and adaptive
- **Experiment**: Benchmarking a series of models using the proposed RAVine framework
- **Takeaway**: The proposed RAVine framework provides a more accurate and comprehensive evaluation of agentic search systems

#### Abstract
> Agentic search, as a more autonomous and adaptive paradigm of retrieval
augmentation, is driving the evolution of intelligent search systems. However,
existing evaluation frameworks fail to align well with the goals of agentic
search. First, the complex queries commonly used in current benchmarks often
deviate from realistic user search scenarios. Second, prior approaches tend to
introduce noise when extracting ground truth for end-to-end evaluations,
leading to distorted assessments at a fine-grained level. Third, most current
frameworks focus solely on the quality of final answers, neglecting the
evaluation of the iterative process inherent to agentic search. To address
these limitations, we propose RAVine -- a Reality-Aligned eValuation framework
for agentic LLMs with search. RAVine targets multi-point queries and long-form
answers that better reflect user intents, and introduces an attributable ground
truth construction strategy to enhance the accuracy of fine-grained evaluation.
Moreover, RAVine examines model's interaction with search tools throughout the
iterative process, and accounts for factors of efficiency. We benchmark a
series of models using RAVine and derive several insights, which we hope will
contribute to advancing the development of agentic search systems. The code and
datasets are available at https://github.com/SwordFaith/RAVine.

---

### 2. Generating Search Explanations using Large Language Models

- **LLM Score**: 7
- **Keyword Score**: 3
- **Authors**: Arif Laksito, Mark Stevenson
- **URL**: <http://arxiv.org/abs/2507.16692v1>
- **Submitted**: 2025-07-22 15:29:39
- **Comment**: Extended Abstract - Workshop on Explainability in Information
  Retrieval (WExIR), SIGIR 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the application of Large Language Models (LLMs) in generating search explanations, which aligns with the user's interest in Information Retrieval and Search technologies. The focus on query understanding and ranking models is not directly addressed, but the study's emphasis on generating accurate and plausible explanations resonates with the user's interest in user behavior modeling and click models. While the paper's scope is not exclusively focused on the user's core research themes, it does contribute to the broader field of IR and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Using Large Language Models for Generating Search Explanations
- **Aim**: To explore the potential of Large Language Models (LLMs) in generating search explanations for search results
- **Rationale**: LLMs can provide more accurate and plausible explanations than traditional aspect-oriented explanations
- **Ground**: Fine-tuning LLMs for both encoder-decoder and decoder-only models using two fine-tuning methods: full fine-tuning and QLoRA
- **Experiment**: Evaluating the approach using a dataset constructed from Wikipedia article titles and section headings, comparing it to several baseline models
- **Takeaway**: Fine-tuned BART and T5 models demonstrated strong performance in terms of effectiveness and computational efficiency, making them promising options for scenarios with limited computational resources

#### Abstract
> Aspect-oriented explanations in search results are typically concise text
snippets placed alongside retrieved documents to serve as explanations that
assist users in efficiently locating relevant information. While Large Language
Models (LLMs) have demonstrated exceptional performance for a range of
problems, their potential to generate explanations for search results has not
been explored. This study addresses that gap by leveraging both encoder-decoder
and decoder-only LLMs to generate explanations for search results. The
explanations generated are consistently more accurate and plausible
explanations than those produced by a range of baseline models.

---

### 3. Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Jean Lelong, Adnane Errazine, Annabelle Blangero
- **URL**: <http://arxiv.org/abs/2507.16507v1>
- **Submitted**: 2025-07-22 12:03:10
- **Comment**: ECAI 2025 demo track, 4 pages
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) systems, which is related to my interest in Information Retrieval. The use of knowledge graphs and multi-hop reasoning is also relevant to my focus on query understanding and ranking models. However, the specific application domain of agriculture and food environment is not directly aligned with my interests in e-commerce and general information retrieval.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: INRAExplorer: A Novel Agentic Retrieval-Augmented Generation (RAG) System for Complex Multi-Hop Reasoning
- **Aim**: To design and develop a system that enhances Large Language Models (LLMs) for complex multi-hop reasoning in real-world applications
- **Rationale**: Conventional RAG systems have limitations in retrieving a limited set of semantically similar text chunks, and INRAExplorer aims to overcome these shortcomings by synergizing agentic RAG and Knowledge Graph (KG)-enhanced RAG
- **Ground**: The system is grounded in a comprehensive knowledge graph derived from open access publications of INRAE (France's National Research Institute for Agriculture, Food and Environment) and enriched with additional public sources
- **Experiment**: The system demonstrates its capabilities through two representative scenarios, illustrating its ability to handle complex information needs requiring exhaustive and structured answers
- **Takeaway**: INRAExplorer enables the LLM agent to dynamically navigate between different tools, gather evidence, evaluate intermediate findings, and plan subsequent steps, making it a novel and effective approach for complex multi-hop reasoning

#### Abstract
> Conventional Retrieval-Augmented Generation (RAG) systems enhance Large
Language Models (LLMs) but often fall short on complex queries, delivering
limited, extractive answers and struggling with multiple targeted retrievals or
navigating intricate entity relationships. This is a critical gap in
knowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system
for exploring the scientific data of INRAE (France's National Research
Institute for Agriculture, Food and Environment). INRAExplorer employs an
LLM-based agent with a multi-tool architecture to dynamically engage a rich
knowledge base, through a comprehensive knowledge graph derived from open
access INRAE publications. This design empowers INRAExplorer to conduct
iterative, targeted queries, retrieve exhaustive datasets (e.g., all
publications by an author), perform multi-hop reasoning, and deliver
structured, comprehensive answers. INRAExplorer serves as a concrete
illustration of enhancing knowledge interaction in specialized fields.

---

### 4. Enhancing patent retrieval using automated patent summarization

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Eleni Kamateri, Renukswamy Chikkamath, Michail Salampasis, Linda Andersson, Markus Endres
- **URL**: <http://arxiv.org/abs/2507.16371v1>
- **Submitted**: 2025-07-22 09:14:44
- **Comment**: This version was submitted and accepted for publication at the 6th
  Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech
  2025), held in conjunction with SIGIR 2025. A revised and polished version,
  incorporating reviewers' feedback, will follow
- **Topic Keywords**: query, queries, retrieval
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval (IR) and query understanding, as it explores the application of summarization methods for generating concise summaries of patent documents. However, the focus is on patent retrieval, which is a specific domain, and the paper does not directly address ranking models or user behavior modeling, which are key areas of interest for the user.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Patent Retrieval using Automated Summarization
- **Aim**: To improve patent retrieval performance using automatically generated summaries as surrogate queries
- **Rationale**: Effective query formulation is crucial in patent retrieval, particularly in domain-specific contexts with lengthy and complex documents
- **Ground**: The study employs a five-stage experimental workflow, involving summary generation and evaluation across multiple benchmark datasets
- **Experiment**: The authors propose a pipeline for automated patent summarization using state-of-the-art language models and semantically rich patent document segments
- **Takeaway**: Automatically generated summaries can improve retrieval performance and serve as an efficient alternative to traditional query formulation techniques

#### Abstract
> Effective query formulation is a key challenge in long-document Information
Retrieval (IR). This challenge is particularly acute in domain-specific
contexts like patent retrieval, where documents are lengthy, linguistically
complex, and encompass multiple interrelated technical topics. In this work, we
present the application of recent extractive and abstractive summarization
methods for generating concise, purpose-specific summaries of patent documents.
We further assess the utility of these automatically generated summaries as
surrogate queries across three benchmark patent datasets and compare their
retrieval performance against conventional approaches that use entire patent
sections. Experimental results show that summarization-based queries
significantly improve prior-art retrieval effectiveness, highlighting their
potential as an efficient alternative to traditional query formulation
techniques.

---

### 5. LLM-Enhanced Reranking for Complementary Product Recommendation

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Zekun Xu, Yudi Zhang
- **URL**: <http://arxiv.org/abs/2507.16237v1>
- **Submitted**: 2025-07-22 05:15:45
- **Topic Keywords**: ranking, rerank, rag, recommend, commerce, e-commerce, rank
- **Reason**: The paper focuses on complementary product recommendation, which is a specific application in e-commerce, and leverages Large Language Models (LLMs) for reranking. While it touches on aspects of search and ranking, the primary focus is on recommendation systems, which is not a central match with the user's interests in Information Retrieval and Search technologies. The paper's emphasis on LLMs and graph neural networks is also not directly related to the user's interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Complementary Product Recommendations using Large Language Models and Graph Neural Networks
- **Aim**: To balance accuracy and diversity in complementary product recommendations
- **Rationale**: Combining strengths of GNNs and LLMs to capture complex relationships between products and improve reranking
- **Ground**: GNNs capture structural and semantic complementarity patterns, while LLMs possess rich semantic understanding
- **Experiment**: Extensive experiments on four public datasets using three baseline GNN models, achieving at least 50% lift in accuracy metrics and 2% lift in diversity metrics
- **Takeaway**: The proposed approach demonstrates promising results in addressing the accuracy-diversity tradeoff and improving both accuracy and diversity metrics

#### Abstract
> Complementary product recommendation, which aims to suggest items that are
used together to enhance customer value, is a crucial yet challenging task in
e-commerce. While existing graph neural network (GNN) approaches have made
significant progress in capturing complex product relationships, they often
struggle with the accuracy-diversity tradeoff, particularly for long-tail
items. This paper introduces a model-agnostic approach that leverages Large
Language Models (LLMs) to enhance the reranking of complementary product
recommendations. Unlike previous works that use LLMs primarily for data
preprocessing and graph augmentation, our method applies LLM-based prompting
strategies directly to rerank candidate items retrieved from existing
recommendation models, eliminating the need for model retraining. Through
extensive experiments on public datasets, we demonstrate that our approach
effectively balances accuracy and diversity in complementary product
recommendations, with at least 50% lift in accuracy metrics and 2% lift in
diversity metrics on average for the top recommended items across datasets.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Time to Split: Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov
- **URL**: <http://arxiv.org/abs/2507.16289v1>
- **Submitted**: 2025-07-22 07:20:52
- **Comment**: Accepted for ACM RecSys 2025. Author's version. The final published
  version will be available at the ACM Digital Library
- **Topic Keywords**: ranking, relevance, recommend, rank
- **Reason**: The paper explores data splitting strategies for offline evaluation of sequential recommenders, which is a related topic to information retrieval and search technologies. However, the focus is on recommender systems, which is not the primary area of interest. The paper's relevance is somewhat limited due to the lack of direct connection to query understanding, ranking models, and user behavior modeling.

#### Abstract
> Modern sequential recommender systems, ranging from lightweight
transformer-based variants to large language models, have become increasingly
prominent in academia and industry due to their strong performance in the
next-item prediction task. Yet common evaluation protocols for sequential
recommendations remain insufficiently developed: they often fail to reflect the
corresponding recommendation task accurately, or are not aligned with
real-world scenarios.
  Although the widely used leave-one-out split matches next-item prediction, it
permits the overlap between training and test periods, which leads to temporal
leakage and unrealistically long test horizon, limiting real-world relevance.
Global temporal splitting addresses these issues by evaluating on distinct
future periods. However, its applications to sequential recommendations remain
loosely defined, particularly in terms of selecting target interactions and
constructing a validation subset that provides necessary consistency between
validation and test metrics.
  In this paper, we demonstrate that evaluation outcomes can vary significantly
across splitting strategies, influencing model rankings and practical
deployment decisions. To improve reproducibility in both academic and
industrial settings, we systematically compare different splitting strategies
for sequential recommendations across multiple datasets and established
baselines. Our findings show that prevalent splits, such as leave-one-out, may
be insufficiently aligned with more realistic evaluation strategies. Code:
https://github.com/monkey0head/time-to-split

### 7. Reinforce Lifelong Interaction Value of User-Author Pairs for Large-Scale Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Yisha Li, Lexi Gao, Jingxin Liu, Xiang Gao, Xin Li, Haiyang Lu, Liyin Hong
- **URL**: <http://arxiv.org/abs/2507.16253v1>
- **Submitted**: 2025-07-22 05:58:55
- **Topic Keywords**: rag, click, click-through rate, recommend, search
- **Reason**: The paper focuses on recommendation systems, which is related to information retrieval, but the emphasis is on long-term engagement and user-author pairs, which is not directly aligned with my interests in query understanding, ranking models, and user behavior modeling. While the paper mentions reinforcement learning, it is not specifically applied to search technologies or query understanding.

#### Abstract
> Recommendation systems (RS) help users find interested content and connect
authors with their target audience. Most research in RS tends to focus either
on predicting users' immediate feedback (like click-through rate) accurately or
improving users' long-term engagement. However, they ignore the influence for
authors and the lifelong interaction value (LIV) of user-author pairs, which is
particularly crucial for improving the prosperity of social community in
short-video platforms. Currently, reinforcement learning (RL) can optimize
long-term benefits and has been widely applied in RS. In this paper, we
introduce RL to Reinforce Lifelong Interaction Value of User-Author pairs
(RLIV-UA) based on each interaction of UA pairs. To address the long intervals
between UA interactions and the large scale of the UA space, we propose a novel
Sparse Cross-Request Interaction Markov Decision Process (SCRI-MDP) and
introduce an Adjacent State Approximation (ASA) method to construct RL training
samples. Additionally, we introduce Multi-Task Critic Learning (MTCL) to
capture the progressive nature of UA interactions (click -> follow -> gift),
where denser interaction signals are leveraged to compensate for the learning
of sparse labels. Finally, an auxiliary supervised learning task is designed to
enhance the convergence of the RLIV-UA model. In offline experiments and online
A/B tests, the RLIV-UA model achieves both higher user satisfaction and higher
platform profits than compared methods.

### 8. mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Hellina Hailu Nigatu, Min Li, Maartje ter Hoeve, Saloni Potdar, Sarah Chasins
- **URL**: <http://arxiv.org/abs/2507.16011v1>
- **Submitted**: 2025-07-21 19:11:31
- **Comment**: Accepted to Findings of ACL 2025
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper focuses on multilingual knowledge graph construction, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions retrieval, it is not in the context of query understanding or ranking models. The user's background in e-commerce is also not directly applicable to this paper.

#### Abstract
> Knowledge Graphs represent real-world entities and the relationships between
them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of
automatically constructing or predicting missing entities and links for
knowledge graphs in a multilingual setting. In this work, we reformulate the
mKGC task as a Question Answering (QA) task and introduce mRAKL: a
Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve
this by using the head entity and linking relation in a question, and having
our model predict the tail entity as an answer. Our experiments focus primarily
on two low-resourced languages: Tigrinya and Amharic. We experiment with using
higher-resourced languages Arabic and English for cross-lingual transfer. With
a BM25 retriever, we find that the RAG-based approach improves performance over
a no-context setting. Further, our ablation studies show that with an idealized
retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points
for Tigrinya and Amharic, respectively.

### 9. Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass
- **URL**: <http://arxiv.org/abs/2507.16784v1>
- **Submitted**: 2025-07-22 17:30:04
- **Comment**: Research preview
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper proposes a new language model architecture for long-horizon reasoning, which is somewhat related to information retrieval and search technologies. However, the focus is on large language models and reasoning trees, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the information retrieval challenges that require long-horizon reasoning and multi-hop tool use, but the connection is not strong enough to warrant a higher score.

#### Abstract
> To break the context limits of large language models (LLMs) that bottleneck
reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),
a family of LLMs trained for recursive and decompositional problem solving, and
TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond
context limits. Together, TIM hosted on TIMRUN supports virtually unlimited
working memory and multi-hop tool calls within a single language model
inference, overcoming output limits, positional-embedding constraints, and
GPU-memory bottlenecks. Performance is achieved by modeling natural language as
reasoning trees measured by both length and depth instead of linear sequences.
The reasoning trees consist of tasks with thoughts, recursive subtasks, and
conclusions based on the concept we proposed in Schroeder et al, 2025. During
generation, we maintain a working memory that retains only the key-value states
of the most relevant context tokens, selected by a rule-based subtask-pruning
mechanism, enabling reuse of positional embeddings and GPU memory pages
throughout reasoning. Experimental results show that our system sustains high
inference throughput, even when manipulating up to 90% of the KV cache in GPU
memory. It also delivers accurate reasoning on mathematical tasks and handles
information retrieval challenges that require long-horizon reasoning and
multi-hop tool use.

### 10. Deep Researcher with Test-Time Diffusion

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Rujun Han, Yanfei Chen, Zoey CuiZhu, Lesly Miculicich, Guan Sun, Yuanjun Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Sol√®ne Ma√Ætre, George Lee, Vishy Tirumalashetty, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, Chen-Yu Lee
- **URL**: <http://arxiv.org/abs/2507.16075v1>
- **Submitted**: 2025-07-21 21:23:21
- **Topic Keywords**: retrieval, search
- **Reason**: The paper proposes a novel framework for research report generation, leveraging Large Language Models and a diffusion process. While it touches on search and retrieval mechanisms, the focus is on report generation rather than information retrieval or ranking models. The paper's relevance to the user's interests is limited, but it may be of interest to those exploring NLP and data mining applications.

#### Abstract
> Deep research agents, powered by Large Language Models (LLMs), are rapidly
advancing; yet, their performance often plateaus when generating complex,
long-form research reports using generic test-time scaling algorithms. Drawing
inspiration from the iterative nature of human research, which involves cycles
of searching, reasoning, and revision, we propose the Test-Time Diffusion Deep
Researcher (TTD-DR). This novel framework conceptualizes research report
generation as a diffusion process. TTD-DR initiates this process with a
preliminary draft, an updatable skeleton that serves as an evolving foundation
to guide the research direction. The draft is then iteratively refined through
a "denoising" process, which is dynamically informed by a retrieval mechanism
that incorporates external information at each step. The core process is
further enhanced by a self-evolutionary algorithm applied to each component of
the agentic workflow, ensuring the generation of high-quality context for the
diffusion process. This draft-centric design makes the report writing process
more timely and coherent while reducing information loss during the iterative
search process. We demonstrate that our TTD-DR achieves state-of-the-art
results on a wide array of benchmarks that require intensive search and
multi-hop reasoning, significantly outperforming existing deep research agents.

### 11. LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Da-Chen Lian, Ri-Sheng Huang, Pin-Er Chen, Chunki Lim, You-Kuan Lin, Guan-Yu Tseng, Zi-Cheng Yang, Shu-Kai Hsieh
- **URL**: <http://arxiv.org/abs/2507.16809v1>
- **Submitted**: 2025-07-22 17:57:44
- **Comment**: 41 pages, 17 figures, 10 tables
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on evaluating large language models on complex linguistic tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions reasoning and inference, the context is more focused on linguistic tasks rather than user behavior modeling or ranking models. The paper's relevance to NLP and data mining is higher, but it does not specifically address the user's interests in real-time relevance optimization or deep semantic understanding.

#### Abstract
> We propose LingBench++, a linguistically-informed benchmark and reasoning
framework designed to evaluate large language models (LLMs) on complex
linguistic tasks inspired by the International Linguistics Olympiad (IOL).
Unlike prior benchmarks that focus solely on final answer accuracy, LingBench++
provides structured reasoning traces, stepwise evaluation protocols, and rich
typological metadata across over 90 low-resource and cross-cultural languages.
We further develop a multi-agent architecture integrating grammatical knowledge
retrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through
systematic comparisons of baseline and our proposed agentic models, we
demonstrate that models equipped with external knowledge sources and iterative
reasoning outperform single-pass approaches in both accuracy and
interpretability. LingBench++ offers a comprehensive foundation for advancing
linguistically grounded, culturally informed, and cognitively plausible
reasoning in LLMs.

### 12. Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas
- **URL**: <http://arxiv.org/abs/2507.16806v1>
- **Submitted**: 2025-07-22 17:56:01
- **Topic Keywords**: rag
- **Reason**: The paper focuses on training language models to reason about their uncertainty, which is related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus is on Natural Language Processing and reinforcement learning, which is not directly aligned with the user's interests in Search technologies and user behavior modeling.

#### Abstract
> When language models (LMs) are trained via reinforcement learning (RL) to
generate natural language "reasoning chains", their performance improves on a
variety of difficult question answering tasks. Today, almost all successful
applications of RL for reasoning use binary reward functions that evaluate the
correctness of LM outputs. Because such reward functions do not penalize
guessing or low-confidence outputs, they often have the unintended side-effect
of degrading calibration and increasing the rate at which LMs generate
incorrect responses (or "hallucinate") in other problem domains. This paper
describes RLCR (Reinforcement Learning with Calibration Rewards), an approach
to training reasoning models that jointly improves accuracy and calibrated
confidence estimation. During RLCR, LMs generate both predictions and numerical
confidence estimates after reasoning. They are trained to optimize a reward
function that augments a binary correctness score with a Brier score -- a
scoring rule for confidence estimates that incentivizes calibrated prediction.
We first prove that this reward function (or any analogous reward function that
uses a bounded, proper scoring rule) yields models whose predictions are both
accurate and well-calibrated. We next show that across diverse datasets, RLCR
substantially improves calibration with no loss in accuracy, on both in-domain
and out-of-domain evaluations -- outperforming both ordinary RL training and
classifiers trained to assign post-hoc confidence scores. While ordinary RL
hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized
confidence can be leveraged at test time to improve accuracy and calibration
via confidence-weighted scaling methods. Our results show that explicitly
optimizing for calibration can produce more generally reliable reasoning
models.

### 13. Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda
- **URL**: <http://arxiv.org/abs/2507.16795v1>
- **Submitted**: 2025-07-22 17:45:04
- **Topic Keywords**: rag
- **Reason**: The paper discusses fine-tuning large language models, which is related to my interests in Natural Language Processing. However, the focus on controlling out-of-distribution generalization and concept ablation fine-tuning is not directly aligned with my primary research themes in Information Retrieval, query understanding, and ranking models.

#### Abstract
> Fine-tuning large language models (LLMs) can lead to unintended
out-of-distribution generalization. Standard approaches to this problem rely on
modifying training data, for example by adding data that better specify the
intended generalization. However, this is not always practical. We introduce
Concept Ablation Fine-Tuning (CAFT), a technique that leverages
interpretability tools to control how LLMs generalize from fine-tuning, without
needing to modify the training data or otherwise use data from the target
distribution. Given a set of directions in an LLM's latent space corresponding
to undesired concepts, CAFT works by ablating these concepts with linear
projections during fine-tuning, steering the model away from unintended
generalizations. We successfully apply CAFT to three fine-tuning tasks,
including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow
task generalize to give egregiously misaligned responses to general questions.
Without any changes to the fine-tuning data, CAFT reduces misaligned responses
by 10x without degrading performance on the training distribution. Overall,
CAFT represents a novel approach for steering LLM generalization without
modifying training data.

### 14. Biases in LLM-Generated Musical Taste Profiles for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Bruno Sguerra, Elena V. Epure, Harin Lee, Manuel Moussallam
- **URL**: <http://arxiv.org/abs/2507.16708v1>
- **Submitted**: 2025-07-22 15:44:10
- **Topic Keywords**: recommend, personalization
- **Reason**: The paper explores the use of Large Language Models (LLMs) for generating user taste profiles, which is related to information retrieval and search technologies. However, the focus on music streaming and recommendation systems is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and interest in recommender systems, but it does not address the user's core research themes.

#### Abstract
> One particularly promising use case of Large Language Models (LLMs) for
recommendation is the automatic generation of Natural Language (NL) user taste
profiles from consumption data. These profiles offer interpretable and editable
alternatives to opaque collaborative filtering representations, enabling
greater transparency and user control. However, it remains unclear whether
users consider these profiles to be an accurate representation of their taste,
which is crucial for trust and usability. Moreover, because LLMs inherit
societal and data-driven biases, profile quality may systematically vary across
user and item characteristics. In this paper, we study this issue in the
context of music streaming, where personalization is challenged by a large and
culturally diverse catalog. We conduct a user study in which participants rate
NL profiles generated from their own listening histories. We analyze whether
identification with the profiles is biased by user attributes (e.g.,
mainstreamness, taste diversity) and item features (e.g., genre, country of
origin). We also compare these patterns to those observed when using the
profiles in a downstream recommendation task. Our findings highlight both the
potential and limitations of scrutable, LLM-based profiling in personalized
systems.

### 15. Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Sumit Singh, Rohit Mishra, Uma Shanker Tiwary
- **URL**: <http://arxiv.org/abs/2507.16002v1>
- **Submitted**: 2025-07-21 18:41:58
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on named entity recognition (NER) in Hindi, which is not directly related to information retrieval or search technologies. While it uses transformer-based models and retrieval augmentation, the context is specific to NLP and language processing, and the relevance to the user's interests is limited.

#### Abstract
> One major challenge in natural language processing is named entity
recognition (NER), which identifies and categorises named entities in textual
input. In order to improve NER, this study investigates a Hindi NER technique
that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and
Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf
(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments
the data with retrieved data from external relevant contexts, notably from
Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.
However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER
generation. Our investigation shows that the mentioned language models (LMs)
with Retrieval Augmentation (RA) outperform baseline methods that don't
incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69
and 0.495, respectively, without RA and increase to 0.70 and 0.71,
respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B
by a significant margin. On the other hand the generative models which are not
fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA
well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval
context. The findings show that RA significantly improves performance,
especially for low-context data. This study adds significant knowledge about
how best to use data augmentation methods and pretrained models to enhance NER
performance, particularly in languages with limited resources.

### 16. Efficient Compositional Multi-tasking for On-device Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ondrej Bohdal, Mete Ozay, Jijoong Moon, Kyeng-Hun Lee, Hyeonmok Ko, Umberto Michieli
- **URL**: <http://arxiv.org/abs/2507.16083v1>
- **Submitted**: 2025-07-21 21:39:23
- **Topic Keywords**: search
- **Reason**: The paper focuses on large language models and task merging, which is related to my interests in NLP and data mining. However, the specific context of on-device settings and compositional multi-tasking is not directly aligned with my primary focus on information retrieval and query understanding.

#### Abstract
> Adapter parameters provide a mechanism to modify the behavior of machine
learning models and have gained significant popularity in the context of large
language models (LLMs) and generative AI. These parameters can be merged to
support multiple tasks via a process known as task merging. However, prior work
on merging in LLMs, particularly in natural language processing, has been
limited to scenarios where each test example addresses only a single task. In
this paper, we focus on on-device settings and study the problem of text-based
compositional multi-tasking, where each test example involves the simultaneous
execution of multiple tasks. For instance, generating a translated summary of a
long text requires solving both translation and summarization tasks
concurrently. To facilitate research in this setting, we propose a benchmark
comprising four practically relevant compositional tasks. We also present an
efficient method (Learnable Calibration) tailored for on-device applications,
where computational resources are limited, emphasizing the need for solutions
that are both resource-efficient and high-performing. Our contributions lay the
groundwork for advancing the capabilities of LLMs in real-world multi-tasking
scenarios, expanding their applicability to complex, resource-constrained use
cases.

### 17. Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance

- **LLM Score**: 2
- **Keyword Score**: 16
- **Authors**: Lars Hillebrand, Armin Berger, Daniel Uedelhoven, David Berghaus, Ulrich Warning, Tim Dilmaghani, Bernd Kliem, Thomas Schmid, R√ºdiger Loitz, Rafet Sifa
- **URL**: <http://arxiv.org/abs/2507.16711v1>
- **Submitted**: 2025-07-22 15:46:44
- **Comment**: Accepted and published at BigData 2024, 3 pages, 3 tables, 2 figures
- **Topic Keywords**: query, queries, relevance, rag, retrieval augmented generation, retrieval, search
- **Reason**: The paper focuses on Risk and Quality assurance in regulated industries, using a chatbot to improve regulatory compliance. While it mentions retrieval and relevance, the context is quite different from the user's interests in Information Retrieval and Search technologies, particularly in query understanding, ranking models, and user behavior modeling. The paper's emphasis on regulatory compliance and policy interpretation also doesn't align with the user's background in e-commerce or broader interests in NLP, data mining, and related topics.

#### Abstract
> Risk and Quality (R&Q) assurance in highly regulated industries requires
constant navigation of complex regulatory frameworks, with employees handling
numerous daily queries demanding accurate policy interpretation. Traditional
methods relying on specialized experts create operational bottlenecks and limit
scalability. We present a novel Retrieval Augmented Generation (RAG) system
leveraging Large Language Models (LLMs), hybrid search and relevance boosting
to enhance R&Q query processing. Evaluated on 124 expert-annotated real-world
queries, our actively deployed system demonstrates substantial improvements
over traditional RAG approaches. Additionally, we perform an extensive
hyperparameter analysis to compare and evaluate multiple configuration setups,
delivering valuable insights to practitioners.

### 18. Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Paul-Andrei PogƒÉcean, Sanda-Maria Avram
- **URL**: <http://arxiv.org/abs/2507.16284v2>
- **Submitted**: 2025-07-22 07:11:01
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: The paper focuses on language detection using frequency analysis, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the scope is limited to language detection and does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest.

#### Abstract
> The debate surrounding language identification has gained renewed attention
in recent years, especially with the rapid evolution of AI-powered language
models. However, the non-AI-based approaches to language identification have
been overshadowed. This research explores a mathematical implementation of an
algorithm for language determinism by leveraging monograms and bigrams
frequency rankings derived from established linguistic research. The datasets
used comprise texts varying in length, historical period, and genre, including
short stories, fairy tales, and poems. Despite these variations, the method
achieves over 80\% accuracy on texts shorter than 150 characters and reaches
100\% accuracy for longer texts. These results demonstrate that classical
frequency-based approaches remain effective and scalable alternatives to
AI-driven models for language detection.

### 19. Step-Audio 2 Technical Report

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu
- **URL**: <http://arxiv.org/abs/2507.16632v1>
- **Submitted**: 2025-07-22 14:23:55
- **Topic Keywords**: rag, retrieval, web search, search
- **Reason**: The paper focuses on audio understanding and speech conversation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions retrieval-augmented generation, the context is different from the user's interests in IR and NLP.

#### Abstract
> This paper presents Step-Audio~2, an end-to-end multi-modal large language
model designed for industry-strength audio understanding and speech
conversation. By integrating a latent audio encoder and reasoning-centric
reinforcement learning (RL), Step-Audio 2 achieves promising performance in
automatic speech recognition (ASR) and audio understanding. To facilitate
genuine end-to-end speech conversation, Step-Audio 2 incorporates the
generation of discrete audio tokens into language modeling, significantly
enhancing its responsiveness to paralinguistic information such as speaking
styles and emotions. To effectively leverage the rich textual and acoustic
knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented
generation (RAG) and is able to call external tools such as web search to
mitigate hallucination and audio search to switch timbres. Trained on millions
of hours of speech and audio data, Step-Audio 2 delivers intelligence and
expressiveness across diverse conversational scenarios. Evaluation results
demonstrate that Step-Audio 2 achieves state-of-the-art performance on various
audio understanding and conversational benchmarks compared to other open-source
and commercial solutions. Please visit
https://github.com/stepfun-ai/Step-Audio2 for more information.

### 20. Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Tianyun Zhong, Guozhao Mo, Yanjiang Liu, Yihan Chen, Lingdi Kong, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Le Sun
- **URL**: <http://arxiv.org/abs/2507.16271v1>
- **Submitted**: 2025-07-22 06:37:51
- **Topic Keywords**: queries, rag
- **Reason**: The paper focuses on evaluating the ability of language models to extract information from complex documents and reconstruct it into organized tables, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of deep semantic understanding, it is not primarily focused on real-time relevance optimization or user behavior modeling.

#### Abstract
> With the emergence of large language models (LLMs), there is an expectation
that LLMs can effectively extract explicit information from complex real-world
documents (e.g., papers, reports). However, most LLMs generate paragraph-style
answers that are chaotic, disorganized, and untraceable. To bridge this gap, we
introduce the Arranged and Organized Extraction Benchmark (AOE), a new
bilingual benchmark with data and documents of varying lengths designed to
systematically evaluate the ability of LLMs to comprehend fragmented documents
and reconstruct isolated information into one organized table. Unlike
conventional text-to-table tasks, which rely on fixed schema and narrow task
domains, AOE includes 11 carefully crafted tasks across three diverse domains,
requiring models to generate context-specific schema tailored to varied input
queries. In the experiment, we evaluated both open-source and closed-source
state-of-the-art LLMs. The results show that even the most advanced models
struggled significantly. The benchmark is available at
https://huggingface.co/datasets/tianyumyum/AOE.

### 21. Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Guowei Lan, Kaixian Qu, Ren√© Zurbr√ºgg, Changan Chen, Christopher E. Mower, Haitham Bou-Ammar, Marco Hutter
- **URL**: <http://arxiv.org/abs/2507.16713v1>
- **Submitted**: 2025-07-22 15:48:49
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on grounding vision-language models for robotics, which is not directly related to information retrieval, search technologies, or query understanding. While it involves machine learning and data processing, the context and applications are quite different from the user's primary research interests.

#### Abstract
> Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.

### 22. EBaReT: Expert-guided Bag Reward Transformer for Auto Bidding

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kaiyuan Li, Pengyu Wang, Yunshan Peng, Pengjia Yuan, Yanxiang Zeng, Rui Xiang, Yanhua Cheng, Xialong Liu, Peng Jiang
- **URL**: <http://arxiv.org/abs/2507.16186v1>
- **Submitted**: 2025-07-22 02:56:36
- **Topic Keywords**: click, conversion rate
- **Reason**: The paper focuses on automated bidding, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions reinforcement learning, the context is different from the user's interests in ranking models and user behavior modeling.

#### Abstract
> Reinforcement learning has been widely applied in automated bidding.
Traditional approaches model bidding as a Markov Decision Process (MDP).
Recently, some studies have explored using generative reinforcement learning
methods to address long-term dependency issues in bidding environments.
Although effective, these methods typically rely on supervised learning
approaches, which are vulnerable to low data quality due to the amount of
sub-optimal bids and low probability rewards resulting from the low click and
conversion rates. Unfortunately, few studies have addressed these challenges.
  In this paper, we formalize the automated bidding as a sequence
decision-making problem and propose a novel Expert-guided Bag Reward
Transformer (EBaReT) to address concerns related to data quality and
uncertainty rewards. Specifically, to tackle data quality issues, we generate a
set of expert trajectories to serve as supplementary data in the training
process and employ a Positive-Unlabeled (PU) learning-based discriminator to
identify expert transitions. To ensure the decision also meets the expert
level, we further design a novel expert-guided inference strategy. Moreover, to
mitigate the uncertainty of rewards, we consider the transitions within a
certain period as a "bag" and carefully design a reward function that leads to
a smoother acquisition of rewards. Extensive experiments demonstrate that our
model achieves superior performance compared to state-of-the-art bidding
methods.

### 23. MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Run-Ze Fan, Zengzhi Wang, Pengfei Liu
- **URL**: <http://arxiv.org/abs/2507.16812v1>
- **Submitted**: 2025-07-22 17:59:03
- **Comment**: 39 pages; Github: https://github.com/GAIR-NLP/MegaScience; HF:
  https://huggingface.co/MegaScience
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on creating large-scale datasets for scientific reasoning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions training models, it does not specifically address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests.

#### Abstract
> Scientific reasoning is critical for developing AI scientists and supporting
human researchers in advancing the frontiers of natural science discovery.
However, the open-source community has primarily focused on mathematics and
coding while neglecting the scientific domain, largely due to the absence of
open, large-scale, high-quality, verifiable scientific reasoning datasets. To
bridge this gap, we first present TextbookReasoning, an open dataset featuring
truthful reference answers extracted from 12k university-level scientific
textbooks, comprising 650k reasoning questions spanning 7 scientific
disciplines. We further introduce MegaScience, a large-scale mixture of
high-quality open-source datasets totaling 1.25 million instances, developed
through systematic ablation studies that evaluate various data selection
methodologies to identify the optimal subset for each publicly available
scientific dataset. Meanwhile, we build a comprehensive evaluation system
covering diverse subjects and question types across 15 benchmarks,
incorporating comprehensive answer extraction strategies to ensure accurate
evaluation metrics. Our experiments demonstrate that our datasets achieve
superior performance and training efficiency with more concise response lengths
compared to existing open-source scientific datasets. Furthermore, we train
Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which
significantly outperform the corresponding official instruct models in average
performance. In addition, MegaScience exhibits greater effectiveness for larger
and stronger models, suggesting a scaling benefit for scientific tuning. We
release our data curation pipeline, evaluation system, datasets, and seven
trained models to the community to advance scientific reasoning research.

### 24. Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Lars Hillebrand, David Biesner, Christian Bauckhage, Rafet Sifa
- **URL**: <http://arxiv.org/abs/2507.16695v1>
- **Submitted**: 2025-07-22 15:30:32
- **Comment**: Accepted and published at CD-MAKE 2020, 20 pages, 8 tables, 8 figures
- **Topic Keywords**: pointwise
- **Reason**: The paper focuses on matrix factorization and topic modeling, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions word embeddings, the context is not related to ranking models or user behavior modeling, and the paper does not seem to address real-time relevance optimization or deep semantic understanding.

#### Abstract
> The DEDICOM algorithm provides a uniquely interpretable matrix factorization
method for symmetric and asymmetric square matrices. We employ a new
row-stochastic variation of DEDICOM on the pointwise mutual information
matrices of text corpora to identify latent topic clusters within the
vocabulary and simultaneously learn interpretable word embeddings. We introduce
a method to efficiently train a constrained DEDICOM algorithm and a qualitative
evaluation of its topic modeling and word embedding performance.

### 25. Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Armin Berger, Lars Hillebrand, David Leonhard, Tobias Deu√üer, Thiago Bell Felix de Oliveira, Tim Dilmaghani, Mohamed Khaled, Bernd Kliem, R√ºdiger Loitz, Christian Bauckhage, Rafet Sifa
- **URL**: <http://arxiv.org/abs/2507.16642v1>
- **Submitted**: 2025-07-22 14:39:54
- **Comment**: Accepted and published at BigData 2023, 10 pages, 3 figures, 5 tables
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on applying Large Language Models to regulatory compliance verification in financial auditing, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper does involve NLP, the context and application are quite different from the user's primary focus.

#### Abstract
> The auditing of financial documents, historically a labor-intensive process,
stands on the precipice of transformation. AI-driven solutions have made
inroads into streamlining this process by recommending pertinent text passages
from financial reports to align with the legal requirements of accounting
standards. However, a glaring limitation remains: these systems commonly fall
short in verifying if the recommended excerpts indeed comply with the specific
legal mandates. Hence, in this paper, we probe the efficiency of publicly
available Large Language Models (LLMs) in the realm of regulatory compliance
across different model configurations. We place particular emphasis on
comparing cutting-edge open-source LLMs, such as Llama-2, with their
proprietary counterparts like OpenAI's GPT models. This comparative analysis
leverages two custom datasets provided by our partner PricewaterhouseCoopers
(PwC) Germany. We find that the open-source Llama-2 70 billion model
demonstrates outstanding performance in detecting non-compliance or true
negative occurrences, beating all their proprietary counterparts. Nevertheless,
proprietary models such as GPT-4 perform the best in a broad variety of
scenarios, particularly in non-English contexts.

### 26. WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Gautam Kishore Shahi, Scot A. Hale
- **URL**: <http://arxiv.org/abs/2507.16298v1>
- **Submitted**: 2025-07-22 07:35:42
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on WhatsApp tiplines and fact-checking during the 2021 Indian assembly elections, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's methodology and findings are specific to the Indian election context and do not contribute to the user's areas of focus.

#### Abstract
> WhatsApp tiplines, first launched in 2019 to combat misinformation, enable
users to interact with fact-checkers to verify misleading content. This study
analyzes 580 unique claims (tips) from 451 users, covering both high-resource
languages (English, Hindi) and a low-resource language (Telugu) during the 2021
Indian assembly elections using a mixed-method approach. We categorize the
claims into three categories, election, COVID-19, and others, and observe
variations across languages. We compare content similarity through frequent
word analysis and clustering of neural sentence embeddings. We also investigate
user overlap across languages and fact-checking organizations. We measure the
average time required to debunk claims and inform tipline users. Results reveal
similarities in claims across languages, with some users submitting tips in
multiple languages to the same fact-checkers. Fact-checkers generally require a
couple of days to debunk a new claim and share the results with users. Notably,
no user submits claims to multiple fact-checking organizations, indicating that
each organization maintains a unique audience. We provide practical
recommendations for using tiplines during elections with ethical consideration
of users' information.

### 27. Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yujin Han, Hao Chen, Andi Han, Zhiheng Wang, Xinyu Lin, Yingya Zhang, Shiwei Zhang, Difan Zou
- **URL**: <http://arxiv.org/abs/2507.16663v1>
- **Submitted**: 2025-07-22 14:56:39
- **Comment**: 19 pages, 9 figures, 3 tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal language models (MLLMs) and their generation-understanding gap, which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on the idea of self-improvement, the context is different from the user's primary research interests.

#### Abstract
> Despite efforts to unify multimodal generation and understanding tasks in a
single model, we show these MLLMs exhibit self-contradiction where generation
produces images deemed misaligned with input prompts based on the model's own
understanding. We define a Nonunified score that quantifies such
self-contradiction. Our empirical results reveal that the self-contradiction
mainly arises from weak generation that fails to align with prompts, rather
than misunderstanding. This capability asymmetry indicates the potential of
leveraging self-contradiction for self-improvement, where the stronger model
understanding guides the weaker generation to mitigate the
generation-understanding gap. Applying standard post-training methods (e.g.,
SFT, DPO) with such internal supervision successfully improves both generation
and unification. We discover a co-improvement effect on both generation and
understanding when only fine-tuning the generation branch, a phenomenon known
in pre-training but underexplored in post-training. Our analysis shows
improvements stem from better detection of false positives that are previously
incorrectly identified as prompt-aligned. Theoretically, we show the aligned
training dynamics between generation and understanding allow reduced
prompt-misaligned generations to also improve mismatch detection in the
understanding branch. Additionally, the framework reveals a potential risk of
co-degradation under poor supervision-an overlooked phenomenon that is
empirically validated in our experiments. Notably, we find intrinsic metrics
like Nonunified score cannot distinguish co-degradation from co-improvement,
which highlights the necessity of data quality check. Finally, we propose a
curriculum-based strategy based on our findings that gradually introduces
harder samples as the model improves, leading to better unification and
improved MLLM generation and understanding.

### 28. P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dongjun Jang, Youngchae Ahn, Hyopil Shin
- **URL**: <http://arxiv.org/abs/2507.16656v1>
- **Submitted**: 2025-07-22 14:52:25
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on phonological reasoning in large language models, which is a topic in Natural Language Processing, but not directly related to your core interests.

#### Abstract
> This study explores the potential of phonological reasoning within text-based
large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess
tasks like rhyme word generation, g2p conversion, and syllable counting. Our
evaluations across 12 LLMs reveal that while few-shot learning offers
inconsistent gains, the introduction of a novel Pedagogically-motivated
Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational
theories like scaffolding and discovery learning, consistently enhances
performance. This method leverages structured guidance to activate latent
phonological abilities, achieving up to 52% improvement and even surpassing
human baselines in certain tasks. Future work could aim to optimize P-CoT
prompts for specific models or explore their application across different
linguistic domains.

### 29. Scaling Linear Attention with Sparse State Expansion

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuqi Pan, Yongqi An, Zheng Li, Yuhong Chou, Ruijie Zhu, Xiaohui Wang, Mingxuan Wang, Jinqiao Wang, Guoqi Li
- **URL**: <http://arxiv.org/abs/2507.16577v1>
- **Submitted**: 2025-07-22 13:27:31
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on improving the Transformer architecture for long-context scenarios, which is not directly related to information retrieval, query understanding, or ranking models. While it mentions retrieval and reasoning tasks, the primary focus is on the architecture and its efficiency, rather than the underlying retrieval or ranking mechanisms.

#### Abstract
> The Transformer architecture, despite its widespread success, struggles with
long-context scenarios due to quadratic computation and linear memory growth.
While various linear attention variants mitigate these efficiency constraints
by compressing context into fixed-size states, they often degrade performance
in tasks such as in-context retrieval and reasoning. To address this limitation
and achieve more effective context compression, we propose two key innovations.
First, we introduce a row-sparse update formulation for linear attention by
conceptualizing state updating as information classification. This enables
sparse state updates via softmax-based top-$k$ hard classification, thereby
extending receptive fields and reducing inter-class interference. Second, we
present Sparse State Expansion (SSE) within the sparse framework, which expands
the contextual state into multiple partitions, effectively decoupling parameter
size from state capacity while maintaining the sparse classification paradigm.
Our design, supported by efficient parallelized implementations, yields
effective classification and discriminative state representations. We
extensively validate SSE in both pure linear and hybrid (SSE-H) architectures
across language modeling, in-context retrieval, and mathematical reasoning
benchmarks. SSE demonstrates strong retrieval performance and scales favorably
with state size. Moreover, after reinforcement learning (RL) training, our 2B
SSE-H model achieves state-of-the-art mathematical reasoning performance among
small reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,
significantly outperforming similarly sized open-source Transformers. These
results highlight SSE as a promising and efficient architecture for
long-context modeling.

### 30. Learning Text Styles: A Study on Transfer, Attribution, and Verification

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhiqiang Hu
- **URL**: <http://arxiv.org/abs/2507.16530v1>
- **Submitted**: 2025-07-22 12:38:39
- **Comment**: PhD thesis
- **Topic Keywords**: rag
- **Reason**: The paper focuses on text style transfer, authorship attribution, and verification, which are not directly related to information retrieval, search technologies, or query understanding. While it involves language models, the application is not in the context of search or retrieval, and the topics are more aligned with NLP and data mining.

#### Abstract
> This thesis advances the computational understanding and manipulation of text
styles through three interconnected pillars: (1) Text Style Transfer (TST),
which alters stylistic properties (e.g., sentiment, formality) while preserving
content; (2)Authorship Attribution (AA), identifying the author of a text via
stylistic fingerprints; and (3) Authorship Verification (AV), determining
whether two texts share the same authorship. We address critical challenges in
these areas by leveraging parameter-efficient adaptation of large language
models (LLMs), contrastive disentanglement of stylistic features, and
instruction-based fine-tuning for explainable verification.

### 31. ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhenliang Zhang, Xinyu Hu, Huixuan Zhang, Junzhe Zhang, Xiaojun Wan
- **URL**: <http://arxiv.org/abs/2507.16488v1>
- **Submitted**: 2025-07-22 11:44:26
- **Comment**: Accepted to ACL 2025 (Main Conference)
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it involves natural language processing, the focus is on large language models and hallucination detection, which is not a core area of interest for you.

#### Abstract
> Large language models (LLMs) excel at various natural language processing
tasks, but their tendency to generate hallucinations undermines their
reliability. Existing hallucination detection methods leveraging hidden states
predominantly focus on static and isolated representations, overlooking their
dynamic evolution across layers, which limits efficacy. To address this
limitation, we shift the focus to the hidden state update process and introduce
a novel metric, the ICR Score (Information Contribution to Residual Stream),
which quantifies the contribution of modules to the hidden states' update. We
empirically validate that the ICR Score is effective and reliable in
distinguishing hallucinations. Building on these insights, we propose a
hallucination detection method, the ICR Probe, which captures the cross-layer
evolution of hidden states. Experimental results show that the ICR Probe
achieves superior performance with significantly fewer parameters. Furthermore,
ablation studies and case analyses offer deeper insights into the underlying
mechanism of this method, improving its interpretability.

### 32. ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yizhi Hu, Zezhao Tian, Xingqun Qi, Chen Su, Bingkun Yang, Junhui Yin, Muyi Sun, Man Zhang, Zhenan Sun
- **URL**: <http://arxiv.org/abs/2507.16877v1>
- **Submitted**: 2025-07-22 11:23:48
- **Comment**: 15 pages, 7 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Referring Expression Comprehension in images, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the context is specific to image-text pairs and does not align with the user's interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

### 33. Towards Enforcing Company Policy Adherence in Agentic Workflows

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Naama Zwerdling, David Boaz, Ella Rabinovich, Guy Uziel, David Amid, Ateret Anaby-Tavor
- **URL**: <http://arxiv.org/abs/2507.16459v1>
- **Submitted**: 2025-07-22 11:00:37
- **Comment**: 11 pages
- **Topic Keywords**: rag
- **Reason**: The paper focuses on enforcing company policy adherence in agentic workflows, using Large Language Model agents, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, and the topic is not relevant to the user's background in e-commerce or their primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large Language Model (LLM) agents hold promise for a flexible and scalable
alternative to traditional business process automation, but struggle to
reliably follow complex company policies. In this study we introduce a
deterministic, transparent, and modular framework for enforcing business policy
adherence in agentic workflows. Our method operates in two phases: (1) an
offline buildtime stage that compiles policy documents into verifiable guard
code associated with tool use, and (2) a runtime integration where these guards
ensure compliance before each agent action. We demonstrate our approach on the
challenging $\tau$-bench Airlines domain, showing encouraging preliminary
results in policy enforcement, and further outline key challenges for
real-world deployments.

### 34. PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hui Xiang, Jinqiao Shi, Ting Zhang, Xiaojie Zhao, Yong Liu, Yong Ma
- **URL**: <http://arxiv.org/abs/2507.16424v1>
- **Submitted**: 2025-07-22 10:17:42
- **Topic Keywords**: rag
- **Reason**: The paper focuses on active learning and few-shot scenarios, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions 'prompts' and 'predictive distribution', the context is different from the user's interests in NLP and IR.

#### Abstract
> Active learning (AL) aims to optimize model training and reduce annotation
costs by selecting the most informative samples for labeling. Typically, AL
methods rely on the empirical distribution of labeled data to define the
decision boundary and perform uncertainty or diversity estimation, subsequently
identifying potential high-quality samples. In few-shot scenarios, the
empirical distribution often diverges significantly from the target
distribution, causing the decision boundary to shift away from its optimal
position. However, existing methods overlook the role of unlabeled samples in
enhancing the empirical distribution to better align with the target
distribution, resulting in a suboptimal decision boundary and the selection of
samples that inadequately represent the target distribution. To address this,
we propose a hybrid AL framework, termed \textbf{PromptAL} (Sample-Aware
Dynamic Soft \textbf{Prompts} for Few-Shot \textbf{A}ctive \textbf{L}earning).
This framework accounts for the contribution of each unlabeled data point in
aligning the current empirical distribution with the target distribution,
thereby optimizing the decision boundary. Specifically, PromptAL first
leverages unlabeled data to construct sample-aware dynamic soft prompts that
adjust the model's predictive distribution and decision boundary. Subsequently,
based on the adjusted decision boundary, it integrates uncertainty estimation
with both global and local diversity to select high-quality samples that more
accurately represent the target distribution. Experimental results on six
in-domain and three out-of-domain datasets show that PromptAL achieves superior
performance over nine baselines. Our codebase is openly accessible.

### 35. SpeLLM: Character-Level Multi-Head Decoding

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Amit Ben-Artzy, Roy Schwartz
- **URL**: <http://arxiv.org/abs/2507.16323v1>
- **Submitted**: 2025-07-22 08:07:06
- **Topic Keywords**: rag
- **Reason**: The paper focuses on scaling language models (LLMs) by decoupling input and output vocabularies, which is not directly related to information retrieval, query understanding, or ranking models. Although it touches on the topic of reducing costs, it does not address user behavior modeling or click models, and its relevance to e-commerce or other domains is unclear.

#### Abstract
> Scaling LLM vocabulary is often used to reduce input sequence length and
alleviate attention's quadratic cost. Yet, current LLM architectures impose a
critical bottleneck to this procedure: the output projection layer scales
linearly with vocabulary size, rendering substantial expansion impractical. We
propose SpeLLM, a method that decouples input and output vocabularies by
predicting character-level strings through multiple output heads. In SpeLLM,
each of the $k$ linear heads predicts a single character simultaneously,
enabling the model to represent a much larger output space using smaller,
independent linear heads. We present a self-distillation approach for
converting a standard LLM to a SpeLLM. Our experiments with four pre-trained
LLMs show their SpeLLM variants achieve competitive performance on downstream
tasks while reducing runtime by 5.1% on average across models. Our approach
provides a potential avenue for reducing LLM costs, while increasing support
for underrepresented languages and domains.

### 36. iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yujian Sun, Tian Li
- **URL**: <http://arxiv.org/abs/2507.16263v1>
- **Submitted**: 2025-07-22 06:21:54
- **Topic Keywords**: rank, search
- **Reason**: The paper focuses on machine unlearning, a topic not directly related to information retrieval, search technologies, or query understanding. While it involves large language models, the primary goal is to erase sensitive information, which is not aligned with the user's interests in ranking models, user behavior modeling, or deep semantic understanding.

#### Abstract
> As the Large Language Model (LLM) gains widespread adoption, increasing
attention has been given to the challenge of making LLM forget non-compliant
data memorized during its pre-training. Machine Unlearning focuses on
efficiently erasing sensitive information from LLM under limited computational
resources. To advance research in this area, SemEval 2025 Task 4: "Unlearning
Sensitive Content from Large Language Models" introduces three unlearning
datasets and establishes a benchmark by evaluating both forgetting
effectiveness and the preservation of standard capabilities. In this work, we
propose a more controllable forgetting loss, Effective Unlearning Loss, and
explore its integration with various techniques to achieve more efficient and
controlled unlearning. Our system ultimately ranked 5th on the competition
leaderboard.

### 37. SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shuhao Mei, Yongchao Long, Shan Cao, Xiaobo Han, Shijia Geng, Jinbo Sun, Yuxi Zhou, Shenda Hong
- **URL**: <http://arxiv.org/abs/2507.16145v1>
- **Submitted**: 2025-07-22 01:44:12
- **Topic Keywords**: rag
- **Reason**: The paper focuses on applying large language models to understand spirogram time series in COPD reporting, which is not directly related to information retrieval, search technologies, or query understanding. While it involves NLP and data mining, the context is medical and clinical, which is not a primary focus of your research interests.

#### Abstract
> Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory
disease with persistent airflow limitation, is a leading global cause of
disability and mortality. Respiratory spirogram time series, routinely
collected during pulmonary function tests (PFTs), play a critical role in the
early detection of repsiratory diseases and in monitoring lung function over
time. However, most current AI models for COPD diagnosis are limited to
outputting classification results without providing a rationale for their
diagnostic process, while current Large Language Models (LLMs) cannot
understand spirograms yet, which severely limits their clinical trust and
adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals
from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large
language model that can understand spirogram. The model extracts morphological
features from respiratory curves via a SpiroEncoder and aligns them with PFT
numerical values in a unified latent space using a SpiroProjector, ultimately
empowering a large language model to generate a comprehensive diagnostic
report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC
of 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,
it maintained a 100% valid response rate, far surpassing the 13.4% of a
text-only model and showcasing the superiority of its multimodal design. This
work demonstrates the substantial potential of deeply fusing physiological
signals with large language models, establishing a new paradigm for the next
generation of interpretable and reliable clinical decision support tools.

### 38. Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum
- **URL**: <http://arxiv.org/abs/2507.16746v1>
- **Submitted**: 2025-07-22 16:35:36
- **Comment**: dataset link:
  https://huggingface.co/datasets/multimodal-reasoning-lab/Zebra-CoT
- **Topic Keywords**: search
- **Reason**: The paper focuses on a specific topic of visual language reasoning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions multimodal models, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Humans often use visual aids, for example diagrams or sketches, when solving
complex problems. Training multimodal models to do the same, known as Visual
Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf
visual CoT performance, which hinders reinforcement learning, and (2) the lack
of high-quality visual CoT training data. We introduce $\textbf{Zebra-CoT}$, a
diverse large-scale dataset with 182,384 samples, containing logically coherent
interleaved text-image reasoning traces. We focus on four categories of tasks
where sketching or visual reasoning is especially natural, spanning scientific
questions such as geometry, physics, and algorithms; 2D visual reasoning tasks
like visual search and jigsaw puzzles; 3D reasoning tasks including 3D
multi-hop inference, embodied and robot planning; visual logic problems and
strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT
training corpus results in an improvement of +12% in our test-set accuracy and
yields up to +13% performance gain on standard VLM benchmark evaluations.
Fine-tuning Bagel-7B yields a model that generates high-quality interleaved
visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing
multimodal reasoning abilities. We open-source our dataset and models to
support development and evaluation of visual CoT.

### 39. Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kristin Gnadt, David Thulke, Simone Kopeinik, Ralf Schl√ºter
- **URL**: <http://arxiv.org/abs/2507.16557v1>
- **Submitted**: 2025-07-22 13:09:41
- **Comment**: Accepted at the 6th Workshop on Gender Bias in Natural Language
  Processing (GeBNLP) at ACL 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on evaluating gender bias in large language models, which is a topic outside your primary focus.

#### Abstract
> In recent years, various methods have been proposed to evaluate gender bias
in large language models (LLMs). A key challenge lies in the transferability of
bias measurement methods initially developed for the English language when
applied to other languages. This work aims to contribute to this research
strand by presenting five German datasets for gender bias evaluation in LLMs.
The datasets are grounded in well-established concepts of gender bias and are
accessible through multiple methodologies. Our findings, reported for eight
multilingual LLM models, reveal unique challenges associated with gender bias
in German, including the ambiguous interpretation of male occupational terms
and the influence of seemingly neutral nouns on gender perception. This work
contributes to the understanding of gender bias in LLMs across languages and
underscores the necessity for tailored evaluation frameworks.

### 40. Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Siqi Liu, Guangrong Dai, Dechao Li
- **URL**: <http://arxiv.org/abs/2507.16515v1>
- **Submitted**: 2025-07-22 12:25:00
- **Comment**: 11 pages, 5 figures, 2 tables. To be published in the Proceedings of
  the 20th Machine Translation Summit (MT Summit 2025; Geneva, Switzerland)
- **Topic Keywords**: search
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Machine Translation Post-editing, Quality Estimation, and translation expertise, which are outside the user's primary research areas.

#### Abstract
> This preliminary study investigates the usefulness of sentence-level Quality
Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),
focusing on its impact on post-editing speed and student translators'
perceptions. It also explores the interaction effects between QE and MT
quality, as well as between QE and translation expertise. The findings reveal
that QE significantly reduces post-editing time. The examined interaction
effects were not significant, suggesting that QE consistently improves MTPE
efficiency across medium- and high-quality MT outputs and among student
translators with varying levels of expertise. In addition to indicating
potentially problematic segments, QE serves multiple functions in MTPE, such as
validating translators' evaluations of MT quality and enabling them to
double-check translation outputs. However, interview data suggest that
inaccurate QE may hinder post-editing processes. This research provides new
insights into the strengths and limitations of QE, facilitating its more
effective integration into MTPE workflows to enhance translators' productivity.

### 41. Knowledge-aware Diffusion-Enhanced Multimedia Recommendation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Xian Mo, Fei Liu, Rui Tang, Jintao, Gao, Hao Liu
- **URL**: <http://arxiv.org/abs/2507.16396v1>
- **Submitted**: 2025-07-22 09:47:56
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on multimedia recommendation, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions graph neural networks and contrastive learning paradigms, the context is different from the user's background in e-commerce and query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multimedia recommendations aim to use rich multimedia content to enhance
historical user-item interaction information, which can not only indicate the
content relatedness among items but also reveal finer-grained preferences of
users. In this paper, we propose a Knowledge-aware Diffusion-Enhanced
architecture using contrastive learning paradigms (KDiffE) for multimedia
recommendations. Specifically, we first utilize original user-item graphs to
build an attention-aware matrix into graph neural networks, which can learn the
importance between users and items for main view construction. The
attention-aware matrix is constructed by adopting a random walk with a restart
strategy, which can preserve the importance between users and items to generate
aggregation of attention-aware node features. Then, we propose a guided
diffusion model to generate strongly task-relevant knowledge graphs with less
noise for constructing a knowledge-aware contrastive view, which utilizes user
embeddings with an edge connected to an item to guide the generation of
strongly task-relevant knowledge graphs for enhancing the item's semantic
information. We perform comprehensive experiments on three multimedia datasets
that reveal the effectiveness of our KDiffE and its components on various
state-of-the-art methods. Our source codes are available
https://github.com/1453216158/KDiffE.

### 42. FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Run Sun, Zuo Bai, Wentao Zhang, Yuxiang Zhang, Li Zhao, Shan Sun, Zhengwen Qiu
- **URL**: <http://arxiv.org/abs/2507.16248v1>
- **Submitted**: 2025-07-22 05:40:25
- **Topic Keywords**: search
- **Reason**: The paper proposes a framework for evaluating financial research agents, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions AI agents, the focus is on financial research and does not seem to involve ranking models, user behavior modeling, or deep semantic understanding.

#### Abstract
> Recently, AI agents are rapidly evolving in intelligence and widely used in
professional research applications, such as STEM, software development,
finance, etc. Among these AI agents, deep research agent is a key category as
it can perform long-horizon tasks and solve problems of greater complexity.
However, there are few evaluation frameworks and benchmarks that systematically
and automatically investigate the capabilities of these research agents.
Furthermore, financial research problems have distinct complexity and subtlety.
To fill in the gap, we propose FinResearchBench, which is a logic tree based
Agent-as-a-Judge and targets specifically for the financial research agents. It
provides a comprehensive and automatic assessment of the research agents across
7 key types of tasks in the financial research domain. The contributions of
this work are two-folded: (1) the first and innovative Agent-as-a-Judge system
that extracts the logic tree of the research outcome and uses it as the
intermediate information to present a comprehensive, reliable and robust
evaluation; (2) finance oriented that it covers 70 typical financial research
questions, spreading across 7 frequently encountered types of tasks in the
domain.

### 43. Characterizing Online Activities Contributing to Suicide Mortality among Youth

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Aparna Ananthasubramaniam, Elyse J. Thulin, Viktoryia Kalesnikava, Silas Falde, Jonathan Kertawidjaja, Lily Johns, Alejandro Rodr√≠guez-Putnam, Emma Spring, Kara Zivin, Briana Mezuk
- **URL**: <http://arxiv.org/abs/2507.16185v1>
- **Submitted**: 2025-07-22 02:55:51
- **Comment**: Accepted at the AAAI International Conference on Web and Social Media
  (ICWSM) 2026
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of suicide mortality among youth and online activities contributing to it is outside the scope of your research areas, and the paper does not discuss query understanding, ranking models, or user behavior modeling.

#### Abstract
> The recent rise in youth suicide highlights the urgent need to understand how
online experiences contribute to this public health issue. Our mixed-methods
approach responds to this challenge by developing a set of themes focused on
risk factors for suicide mortality in online spaces among youth ages 10-24, and
a framework to model these themes at scale. Using 29,124 open text summaries of
death investigations between 2013-2022, we conducted a thematic analysis to
identify 12 types of online activities that were considered by investigators or
next of kin to be relevant in contextualizing a given suicide death. We then
develop a zero-shot learning framework to model these 12 themes at scale, and
analyze variation in these themes by decedent characteristics and over time.
Our work uncovers several online activities related to harm to self, harm to
others, interpersonal interactions, activity levels online, and life events,
which correspond to different phases of suicide risk from two prominent suicide
theories. We find an association between these themes and decedent
characteristics like age, means of death, and interpersonal problems, and many
themes became more prevalent during the 2020 COVID-19 lockdowns. While digital
spaces have taken some steps to address expressions of suicidality online, our
work illustrates the opportunities for developing interventions related to less
explicit indicators of suicide risk by combining suicide theories with
computational research.

### 44. Pixels, Patterns, but No Poetry: To See The World like Humans

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Hongcheng Gao, Zihao Huang, Lin Xu, Jingyi Tang, Xinhao Li, Yue Liu, Haoyang Li, Taihang Hu, Minhua Lin, Xinlong Yang, Ge Wu, Balong Bi, Hongyu Chen, Wentao Zhang
- **URL**: <http://arxiv.org/abs/2507.16863v1>
- **Submitted**: 2025-07-21 21:50:16
- **Topic Keywords**: search
- **Reason**: The paper focuses on Multimodal Large Language Models and their ability to perceive the world like humans, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of benchmarking, it is primarily concerned with the perception capabilities of language models, rather than ranking models or user behavior modeling.

#### Abstract
> Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

### 45. AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Simon Baeuerle, Max Radyschevski, Ulrike Pado
- **URL**: <http://arxiv.org/abs/2507.16054v1>
- **Submitted**: 2025-07-21 20:44:53
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on applying genAI to automate meetings in automotive engineering, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for you.

#### Abstract
> In large organisations, knowledge is mainly shared in meetings, which takes
up significant amounts of work time. Additionally, frequent in-person meetings
produce inconsistent documentation -- official minutes, personal notes,
presentations may or may not exist. Shared information therefore becomes hard
to retrieve outside of the meeting, necessitating lengthy updates and
high-frequency meeting schedules.
  Generative Artificial Intelligence (genAI) models like Large Language Models
(LLMs) exhibit an impressive performance on spoken and written language
processing. This motivates a practical usage of genAI for knowledge management
in engineering departments: using genAI for transcribing meetings and
integrating heterogeneous additional information sources into an easily usable
format for ad-hoc searches.
  We implement an end-to-end pipeline to automate the entire meeting
documentation workflow in a proof-of-concept state: meetings are recorded and
minutes are created by genAI. These are further made easily searchable through
a chatbot interface. The core of our work is to test this genAI-based software
tooling in a real-world engineering department and collect extensive survey
data on both ethical and technical aspects. Direct feedback from this
real-world setup points out both opportunities and risks: a) users agree that
the effort for meetings could be significantly reduced with the help of genAI
models, b) technical aspects are largely solved already, c) organizational
aspects are crucial for a successful ethical usage of such a system.

### 46. Learning without training: The implicit dynamics of in-context learning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, Javier Gonzalvo
- **URL**: <http://arxiv.org/abs/2507.16003v1>
- **Submitted**: 2025-07-21 18:44:35
- **Topic Keywords**: rank
- **Reason**: The paper focuses on the ability of Large Language Models to learn in context, which is a topic in NLP. However, it does not directly relate to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling, which are the user's primary research interests.

#### Abstract
> One of the most striking features of Large Language Models (LLM) is their
ability to learn in context. Namely at inference time an LLM is able to learn
new patterns without any additional weight update when these patterns are
presented in the form of examples in the prompt, even if these patterns were
not seen during training. The mechanisms through which this can happen are
still largely unknown. In this work, we show that the stacking of a
self-attention layer with an MLP, allows the transformer block to implicitly
modify the weights of the MLP layer according to the context. We argue through
theory and experimentation that this simple mechanism may be the reason why
LLMs can learn in context and not only during training. Specifically, we show
under mild simplifying assumptions how a transformer block implicitly
transforms a context into a low-rank weight-update of the MLP layer.

### 47. Scaling Recommender Transformers to One Billion Parameters

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kirill Khrylchenko, Artem Matveev, Sergei Makeev, Vladimir Baikalov
- **URL**: <http://arxiv.org/abs/2507.15994v1>
- **Submitted**: 2025-07-21 18:30:43
- **Comment**: To be submitted
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on scaling transformer models for recommender systems, which is only loosely related to the user's primary interest in Information Retrieval and Search technologies. Although it mentions transformers, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> While large transformer models have been successfully used in many real-world
applications such as natural language processing, computer vision, and speech
processing, scaling transformers for recommender systems remains a challenging
problem. Recently, Generative Recommenders framework was proposed to scale
beyond typical Deep Learning Recommendation Models (DLRMs). Reformulation of
recommendation as sequential transduction task led to improvement of scaling
properties in terms of compute. Nevertheless, the largest encoder configuration
reported by the HSTU authors amounts only to ~176 million parameters, which is
considerably smaller than the hundreds of billions or even trillions of
parameters common in modern language models.
  In this work, we present a recipe for training large transformer recommenders
with up to a billion parameters. We show that autoregressive learning on user
histories naturally decomposes into two subtasks, feedback prediction and
next-item prediction, and demonstrate that such a decomposition scales
effectively across a wide range of transformer sizes. Furthermore, we report a
successful deployment of our proposed architecture on a large-scale music
platform serving millions of users. According to our online A/B tests, this new
model increases total listening time by +2.26% and raises the likelihood of
user likes by +6.37%, constituting (to our knowledge) the largest improvement
in recommendation quality reported for any deep learning-based system in the
platform's history.

---

