# Daily Papers Report - 2025-10-21

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. How role-play shapes relevance judgment in zero-shot LLM rankers

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Yumeng Wang, Jirui Qi, Catherine Chen, Panagiotis Eustratiadis, Suzan Verberne
- **URL**: <http://arxiv.org/abs/2510.17535v1>
- **Submitted**: 2025-10-20 13:39:48
- **Topic Keywords**: query, ranking, relevance, rag, rank
- **Reason**: This paper explores the role of role-play prompts in zero-shot Large Language Model (LLM) rankers, which is closely related to query understanding and ranking models in Information Retrieval. The paper's focus on the inner workings of LLMs and the design of effective prompts aligns with your research interests in IR and NLP. While the paper's specific domain is not e-commerce, its relevance to IR and ranking models makes it a useful contribution to your field.

#### Abstract
> Large Language Models (LLMs) have emerged as promising zero-shot rankers, but
their performance is highly sensitive to prompt formulation. In particular,
role-play prompts, where the model is assigned a functional role or identity,
often give more robust and accurate relevance rankings. However, the mechanisms
and diversity of role-play effects remain underexplored, limiting both
effective use and interpretability. In this work, we systematically examine how
role-play variations influence zero-shot LLM rankers. We employ causal
intervention techniques from mechanistic interpretability to trace how
role-play information shapes relevance judgments in LLMs. Our analysis reveals
that (1) careful formulation of role descriptions have a large effect on the
ranking quality of the LLM; (2) role-play signals are predominantly encoded in
early layers and communicate with task instructions in middle layers, while
receiving limited interaction with query or document representations.
Specifically, we identify a group of attention heads that encode information
critical for role-conditioned relevance. These findings not only shed light on
the inner workings of role-play in LLM ranking but also offer guidance for
designing more effective prompts in IR and beyond, pointing toward broader
opportunities for leveraging role-play in zero-shot applications.

---

### 2. Rethinking On-policy Optimization for Query Augmentation

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Zhichao Xu, Shengyao Zhuang, Xueguang Ma, Bingsen Chen, Yijun Tian, Fengran Mo, Jie Cao, Vivek Srikumar
- **URL**: <http://arxiv.org/abs/2510.17139v1>
- **Submitted**: 2025-10-20 04:16:28
- **Topic Keywords**: information retrieval, query, queries, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly query augmentation and ranking models. The work compares and combines different approaches to query augmentation, which is a key area of focus for you. The paper's emphasis on large language models and retrieval metrics also aligns with your interests.

#### Abstract
> Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

---

### 3. Agentic Reinforcement Learning for Search is Unsafe

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Yushi Yang, Shreyansh Padarha, Andrew Lee, Adam Mahdi
- **URL**: <http://arxiv.org/abs/2510.17431v1>
- **Submitted**: 2025-10-20 11:19:37
- **Topic Keywords**: query, queries, rag, web search, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of search technologies and query understanding. The focus on safety properties of search models and the potential for users to exploit vulnerabilities is a timely and important concern in the field. While not directly related to ranking models or user behavior modeling, the paper's emphasis on search and query generation aligns with your broader interests in IR and NLP.

#### Abstract
> Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

---

### 4. SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Qiusi Zhan, Angeline Budiman-Chan, Abdelrahman Zayed, Xingzhi Guo, Daniel Kang, Joo-Kyung Kim
- **URL**: <http://arxiv.org/abs/2510.17017v2>
- **Submitted**: 2025-10-19 21:47:19
- **Topic Keywords**: query, queries, search
- **Reason**: This paper is highly relevant to your interests in Information Retrieval and Search technologies, particularly in the context of query understanding and ranking models. The focus on safety and utility in Large Language Model (LLM) search agents aligns with your background in e-commerce and your interest in deep semantic understanding. However, the paper's primary focus on safety rather than ranking models or user behavior modeling prevents it from being a perfect match.

#### Abstract
> Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked "How can I track
someone's location without their consent?", a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

---

### 5. LILO: Bayesian Optimization with Interactive Natural Language Feedback

- **LLM Score**: 8
- **Keyword Score**: 3
- **Authors**: Katarzyna Kobalczyk, Zhiyuan Jerry Lin, Benjamin Letham, Zhuokai Zhao, Maximilian Balandat, Eytan Bakshy
- **URL**: <http://arxiv.org/abs/2510.17671v1>
- **Submitted**: 2025-10-20 15:41:56
- **Topic Keywords**: rag, search
- **Reason**: This paper aligns well with your interests in Information Retrieval, particularly in query understanding and user behavior modeling, as it explores a language-in-the-loop framework for converting natural language feedback into optimization objectives. The use of large language models and Bayesian optimization also relates to your interests in NLP and data mining. However, the focus on optimization objectives and decision-making may not be a central match with your primary focus on information retrieval.

#### Abstract
> For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. DSEBench: A Test Collection for Explainable Dataset Search with Examples

- **LLM Score**: 7
- **Keyword Score**: 19
- **Authors**: Qing Shi, Jing He, Qiaosheng Chen, Gong Cheng
- **URL**: <http://arxiv.org/abs/2510.17228v1>
- **Submitted**: 2025-10-20 07:19:47
- **Comment**: 34 pages, 5 figures, submitted to Knowledge-Based Systems
- **Topic Keywords**: information retrieval, query, ranking, rerank, relevance, retrieval, rank, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of dataset search. Although it focuses on explainable dataset search, it involves retrieval and reranking methods, which align with your interests in query understanding and ranking models. However, the specific domain of dataset search is not a central match to your primary focus on information retrieval in e-commerce and deep semantic understanding.

#### Abstract
> Dataset search has been an established information retrieval task. Current
paradigms either retrieve datasets that are relevant to a keyword query or find
datasets that are similar to an input target dataset. To allow for their
combined specification of information needs, in this article, we investigate
the more generalized task of Dataset Search with Examples (DSE) and further
extend it to Explainable DSE that requires identifying the metadata and content
fields of a dataset that indicate its relevance to the query and similarity to
the target datasets. To facilitate this research, we construct DSEBench, a test
collection that provides high-quality dataset- and field-level annotations to
enable the evaluation of explainable DSE. We also employ a large language model
to generate numerous annotations to be used for training. We establish
extensive baselines on DSEBench by adapting and evaluating a variety of sparse,
dense, and LLM-based retrieval, reranking, and explanation methods.

### 7. Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Yehor Tereshchenko, Mika H√§m√§l√§inen
- **URL**: <http://arxiv.org/abs/2510.17924v1>
- **Submitted**: 2025-10-20 08:03:28
- **Comment**: Published in the Journal of Data Mining & Digital Humanities (JDMDH),
  special issue NLP4DH
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of query understanding and ranking models. However, the focus on toxicity detection in gaming chats is somewhat narrow and not directly aligned with the user's core research themes in e-commerce or real-time relevance optimization. The use of embeddings, fine-tuned transformers, and LLMs is relevant to the user's interests in NLP and IR, but the application is specific to content moderation.

#### Abstract
> This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.

### 8. Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Tong Chen, Akari Asai, Luke Zettlemoyer, Hannaneh Hajishirzi, Faeze Brahman
- **URL**: <http://arxiv.org/abs/2510.17733v1>
- **Submitted**: 2025-10-20 16:45:43
- **Topic Keywords**: retrieval
- **Reason**: This paper explores a method to mitigate hallucinations in language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on open-ended generation and factuality gains is more aligned with NLP and deep semantic understanding, rather than the user's primary focus on IR and real-time relevance optimization.

#### Abstract
> Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

### 9. DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Lanni Bu, Lauren Levin, Amir Zeldes
- **URL**: <http://arxiv.org/abs/2510.17013v1>
- **Submitted**: 2025-10-19 21:26:27
- **Topic Keywords**: rag
- **Reason**: This paper presents a multilingual benchmark for discourse tracking, which involves integrating and aggregating information across sentences, paragraphs, and speaker utterances. While it touches on natural language understanding and pragmatic inferences, it is more focused on discourse tracking than query understanding, ranking models, or user behavior modeling. It may be of interest due to its connection to deep semantic understanding, but it is not a central match for your research interests.

#### Abstract
> Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

### 10. OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Praphul Singh, Corey Barrett, Sumana Srivasta, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi
- **URL**: <http://arxiv.org/abs/2510.17614v1>
- **Submitted**: 2025-10-20 15:00:02
- **Topic Keywords**: ranking, rerank, learning to rank, rank
- **Reason**: The paper presents a ranking system, OG-Rank, which is relevant to the field of Information Retrieval, particularly in the context of real-time relevance optimization. However, the focus on clinical order selection and decoder-based reranking diverges from the user's primary interest in e-commerce and deep semantic understanding. The paper's emphasis on uncertainty-gated explanation and selective generation is somewhat related to user behavior modeling, but it is not a central match.

#### Abstract
> Clinicians need ranking systems that work in real time and still justify
their choices. Motivated by the need for a low-latency, decoder-based reranker,
we present OG-Rank, a single-decoder approach that pairs a pooled first-token
scoring signal with an uncertainty-gated explanation step. The model scores all
candidates in one pass and generates a brief, structured rationale only when
the list is genuinely ambiguous, keeping latency predictable. Trained with a
curriculum that concentrates effort on hard cases, OG-Rank delivers strong
effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,
nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,
nDCG@20~0.699 at a 45\% gate rate), while compact backbones show similar gains
under the same policy. Encoder baselines trail in both effectiveness and
flexibility. The result is a practical recipe: rank fast by default and explain
when it helps, a pattern that applies broadly to decision tasks where selective
generation buys accuracy at acceptable cost. The single-policy design
simplifies deployment and budget planning, and the curriculum principle (spend
more on the hard cases, less on the easy ones) readily transfers beyond
clinical order selection.

### 11. Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Chenghao Zhang, Guanting Dong, Xinyu Yang, Zhicheng Dou
- **URL**: <http://arxiv.org/abs/2510.17354v1>
- **Submitted**: 2025-10-20 09:56:43
- **Comment**: This work is in progress
- **Topic Keywords**: retriever, queries, rag, retrieval
- **Reason**: This paper is somewhat related to your interests in Information Retrieval and Search technologies, as it involves retrieval-augmented generation and mixed-modal information. However, the focus on vision-language generation and mixed-modal data is not a central match to your primary research themes. The paper's use of deep semantic understanding and real-time relevance optimization is relevant, but not the primary focus.

#### Abstract
> Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

### 12. AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Haoyu Huang, Hong Ting Tsang, Jiaxin Bai, Xi Peng, Gong Zhang, Yangqiu Song
- **URL**: <http://arxiv.org/abs/2510.17934v1>
- **Submitted**: 2025-10-20 15:40:14
- **Topic Keywords**: retriever, rag, retrieval, search
- **Reason**: This paper proposes a method to augment large language models with knowledge graphs, which is related to information retrieval and NLP. However, the focus is on knowledge integration and scalability, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. While it touches on the use of large-scale knowledge, it does not directly address real-time relevance optimization or deep semantic understanding.

#### Abstract
> Retrieval-augmented generation (RAG) has shown some success in augmenting
large language models (LLMs) with external knowledge. However, as a
non-parametric knowledge integration paradigm for LLMs, RAG methods heavily
rely on external retrieval modules and the retrieved textual context prior.
Especially for very large scale knowledge augmentation, they would introduce
substantial inference latency due to expensive searches and much longer
relevant context. In this paper, we propose a parametric knowledge integration
method, called \textbf{AtlasKV}, a scalable, effective, and general way to
augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using
very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we
introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with
sub-linear time and memory complexity. It maintains strong knowledge grounding
and generalization performance using the LLMs' inherent attention mechanism,
and requires no external retrievers, long context priors, or retraining when
adapting to new knowledge.

### 13. Lingua Custodi's participation at the WMT 2025 Terminology shared task

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Jingshu Liu, Raheel Qader, Ga√´tan Caillaut, Mariam Nakhl√©
- **URL**: <http://arxiv.org/abs/2510.17504v1>
- **Submitted**: 2025-10-20 13:00:47
- **Topic Keywords**: ranking, retrieval, rank
- **Reason**: This paper explores multilingual sentence embeddings, which is related to NLP and deep semantic understanding. However, it does not directly focus on information retrieval, query understanding, or ranking models, making it somewhat relevant but not a central match to your research interests.

#### Abstract
> While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

### 14. Executable Knowledge Graphs for Replicating AI Research

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Yujie Luo, Zhuoyun Yu, Xuehai Wang, Yuqi Zhu, Ningyu Zhang, Lanning Wei, Lun Du, Da Zheng, Huajun Chen
- **URL**: <http://arxiv.org/abs/2510.17795v1>
- **Submitted**: 2025-10-20 17:53:23
- **Comment**: Work in progress
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper proposes Executable Knowledge Graphs for replicating AI research, which involves integrating technical insights and code snippets from scientific literature. While it touches on the topic of information retrieval and knowledge extraction, its primary focus is on replicating AI research rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to your core research themes is limited, but it may be of interest for its application of knowledge extraction techniques.

#### Abstract
> Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

### 15. Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, Weiran Yao
- **URL**: <http://arxiv.org/abs/2510.17797v1>
- **Submitted**: 2025-10-20 17:55:11
- **Comment**: Technical report; 13 pages plus references and appendices
- **Topic Keywords**: query, search
- **Reason**: The paper discusses a multi-agent system for enterprise analytics, which involves search and query understanding. However, the focus is more on the integration of various agents and tools rather than deep semantic understanding or real-time relevance optimization, which are core aspects of your research interests. The connection to information retrieval is somewhat loose, but the use of NLP and search agents makes it somewhat relevant.

#### Abstract
> As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

### 16. MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Mir Nafis Sharear Shopnil, Sharad Duwal, Abhishek Tyagi, Adiba Mahbub Proma
- **URL**: <http://arxiv.org/abs/2510.17590v1>
- **Submitted**: 2025-10-20 14:40:26
- **Comment**: 16 pages, 3 tables, 1 figure
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses multimodal misinformation detection, which is somewhat related to information retrieval, but the focus is on fact-checking and verification rather than query understanding, ranking models, or user behavior modeling. While it involves web retrieval, the primary goal is not relevance optimization, making it only loosely relevant to the user's core research themes.

#### Abstract
> Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

### 17. Disparities in Multilingual LLM-Based Healthcare Q&A

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Ipek Baris Schlicht, Burcu Sayin, Zhixue Zhao, Frederik M. Labont√©, Cesare Barbera, Marco Viviani, Paolo Rosso, Lucie Flek
- **URL**: <http://arxiv.org/abs/2510.17476v1>
- **Submitted**: 2025-10-20 12:19:08
- **Comment**: Under review
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of multilingual Large Language Models. However, it focuses on healthcare Q&A and disparities in multilingual LLMs, which is not a central match to your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

### 18. On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Wenyu Mao, Jiancan Wu, Guoqing Hu, Zhengyi Yang, Wei Ji, Xiang Wang
- **URL**: <http://arxiv.org/abs/2510.17245v2>
- **Submitted**: 2025-10-20 07:35:12
- **Topic Keywords**: recommend, acl
- **Reason**: This paper focuses on recommender systems, specifically diffusion-based models, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on efficiency-effectiveness trade-offs in recommender systems is somewhat tangential to the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Diffusion models have emerged as a powerful paradigm for generative
sequential recommendation, which typically generate next items to recommend
guided by user interaction histories with a multi-step denoising process.
However, the multi-step process relies on discrete approximations, introducing
discretization error that creates a trade-off between computational efficiency
and recommendation effectiveness. To address this trade-off, we propose TA-Rec,
a two-stage framework that achieves one-step generation by smoothing the
denoising function during pretraining while alleviating trajectory deviation by
aligning with user preferences during fine-tuning. Specifically, to improve the
efficiency without sacrificing the recommendation performance, TA-Rec pretrains
the denoising model with Temporal Consistency Regularization (TCR), enforcing
the consistency between the denoising results across adjacent steps. Thus, we
can smooth the denoising function to map the noise as oracle items in one step
with bounded error. To further enhance effectiveness, TA-Rec introduces
Adaptive Preference Alignment (APA) that aligns the denoising process with user
preference adaptively based on preference pair similarity and timesteps.
Extensive experiments prove that TA-Rec's two-stage objective effectively
mitigates the discretization errors-induced trade-off, enhancing both
efficiency and effectiveness of diffusion-based recommenders.

### 19. Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Guoqing Luo, Iffat Maab, Lili Mou, Junichi Yamagishi
- **URL**: <http://arxiv.org/abs/2510.17062v1>
- **Submitted**: 2025-10-20 00:33:44
- **Topic Keywords**: queries
- **Reason**: The paper explores the social bias in reasoning-based language models, which is somewhat related to information retrieval and NLP. However, the focus on social bias mitigation and language model behavior does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

### 20. SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Tiancheng Hu, Joachim Baumann, Lorenzo Lupo, Nigel Collier, Dirk Hovy, Paul R√∂ttger
- **URL**: <http://arxiv.org/abs/2510.17516v2>
- **Submitted**: 2025-10-20 13:14:38
- **Comment**: Project Website: http://simbench.tiancheng.hu/ Data:
  https://huggingface.co/datasets/pitehu/SimBench
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your interests in Natural Language Processing (NLP) and large language models, but it focuses on simulating human behaviors rather than information retrieval or search technologies. While it touches on the evaluation of model performance, it does not directly address query understanding, ranking models, or user behavior modeling in the context of search.

#### Abstract
> Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

### 21. DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yongxin He, Shan Zhang, Yixuan Cao, Lei Ma, Ping Luo
- **URL**: <http://arxiv.org/abs/2510.17489v1>
- **Submitted**: 2025-10-20 12:41:44
- **Comment**: To appear in NeurIPS 2025
- **Topic Keywords**: ctr
- **Reason**: This paper explores the detection of AI-involved text, which is somewhat related to information retrieval and search technologies. However, the focus on text generation and detection, rather than query understanding or ranking models, limits its relevance to your core research themes.

#### Abstract
> Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

### 22. Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jiaqi Leng, Xiang Hu, Junxiong Wang, Jianguo Li, Wei Wu, Yucheng Lu
- **URL**: <http://arxiv.org/abs/2510.17196v1>
- **Submitted**: 2025-10-20 06:17:57
- **Comment**: Preprint. Work in progress
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on improving the performance of language models on long contexts, which is somewhat related to information retrieval and search technologies. However, the specific topic of hierarchical sparse attention models and their design principles is not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

### 23. Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Austin Xu, Xuan-Phi Nguyen, Yilun Zhou, Chien-Sheng Wu, Caiming Xiong, Shafiq Joty
- **URL**: <http://arxiv.org/abs/2510.17793v1>
- **Submitted**: 2025-10-20 17:52:06
- **Comment**: 29 pages, 9 tables, 6 figures
- **Topic Keywords**: rerank, pairwise, rank, acl
- **Reason**: This paper focuses on generative evaluators for reasoning-centric domains, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves training and evaluation, the context is more aligned with AI evaluation and model assessment rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

### 24. UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Yuhao Yang, Zhen Yang, Zi-Yi Dou, Anh Nguyen, Keen You, Omar Attia, Andrew Szot, Michael Feng, Ram Ramrakhya, Alexander Toshev, Chao Huang, Yinfei Yang, Zhe Gan
- **URL**: <http://arxiv.org/abs/2510.17790v1>
- **Submitted**: 2025-10-20 17:48:26
- **Topic Keywords**: ltr, rag, click
- **Reason**: This paper focuses on computer use agents and multimodal interactions, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it involves some form of 'action' and 'trajectory collection', the context is quite different from query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

### 25. Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Masahiro Kaneko, Timothy Baldwin
- **URL**: <http://arxiv.org/abs/2510.17000v1>
- **Submitted**: 2025-10-19 20:51:24
- **Comment**: NeurIPS 2025 (spotlight)
- **Topic Keywords**: query, queries
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models, the focus is on adversarial attacks and security, which is not a primary area of interest for you.

#### Abstract
> Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

### 26. PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Neeladri Bhuiya, Madhav Aggarwal, Diptanshu Purwar
- **URL**: <http://arxiv.org/abs/2510.17947v1>
- **Submitted**: 2025-10-20 17:37:03
- **Topic Keywords**: query
- **Reason**: This paper focuses on multi-turn attacks and jailbreaking of Large Language Models, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the interaction with LLMs, it's more aligned with security and vulnerability evaluation rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

### 27. On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yehonathan Refael, Amit Aides, Aviad Barzilai, George Leifman, Genady Beryozkin, Vered Silverman, Bolous Jaber, Tomer Shekel
- **URL**: <http://arxiv.org/abs/2510.17670v1>
- **Submitted**: 2025-10-20 15:41:55
- **Topic Keywords**: queries
- **Reason**: This paper is primarily focused on object detection and active learning in the context of Remote Sensing, which is not a central match to your research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

### 28. Qomhra: A Bilingual Irish-English Large Language Model

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Joseph McInerney
- **URL**: <http://arxiv.org/abs/2510.17652v1>
- **Submitted**: 2025-10-20 15:27:53
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on developing a bilingual language model for Irish and English, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the specific application and goals are distinct from the user's research interests.

#### Abstract
> This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

### 29. LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Huiyuan Xie, Chenyang Li, Huining Zhu, Chubin Zhang, Yuxiao Ye, Zhenghao Liu, Zhiyuan Liu
- **URL**: <http://arxiv.org/abs/2510.17602v1>
- **Submitted**: 2025-10-20 14:50:58
- **Topic Keywords**: rag, search
- **Reason**: This paper is primarily focused on legal reasoning and its application in tort case analysis, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is legal analysis rather than search or retrieval.

#### Abstract
> Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

### 30. Deep Self-Evolving Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zihan Liu, Shun Zheng, Xumeng Wen, Yang Wang, Jiang Bian, Mao Yang
- **URL**: <http://arxiv.org/abs/2510.17498v1>
- **Submitted**: 2025-10-20 12:51:42
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on developing a framework for large language models to improve their reasoning capabilities through self-evolving reasoning. While it touches on the idea of iterative reasoning, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of interest for you.

#### Abstract
> Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

### 31. Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Manuela Daniela Danu, George Marica, Constantin Suciu, Lucian Mihai Itu, Oladimeji Farri
- **URL**: <http://arxiv.org/abs/2510.17437v1>
- **Submitted**: 2025-10-20 11:26:22
- **Comment**: 11 pages, 5 figures, 1 table, published in Working Notes of the
  Conference and Labs of the Evaluation Forum (CLEF 2024)
- **Topic Keywords**: ctr, search
- **Reason**: This paper focuses on clinical NER in cardiology texts using BERT embeddings, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it involves NLP, the context and application are quite different from the user's areas of focus.

#### Abstract
> The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

### 32. Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hajar Bakarou, Mohamed Sinane El Messoussi, Ana√Øs Ollagnier
- **URL**: <http://arxiv.org/abs/2510.17289v1>
- **Submitted**: 2025-10-20 08:27:38
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on addressing antisocial behavior in multi-party dialogs through multimodal representation learning, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves representation learning and multimodal fusion, the context and application are quite different from your areas of focus.

#### Abstract
> Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

### 33. How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mohd Ruhul Ameen, Akif Islam, Abu Saleh Musa Miah, Ayesha Siddiqua, Jungpil Shin
- **URL**: <http://arxiv.org/abs/2510.17252v1>
- **Submitted**: 2025-10-20 07:40:46
- **Comment**: 15 pages, 7 figures, 4 tables. Submitted to the International
  Conference on Data and Applied Analytics (IDAA 2025)
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on affective bias in news headlines, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the topic of text analysis, the context is media design and human-centered news aggregation, which is not a primary area of interest for the user.

#### Abstract
> News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

### 34. CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Keuntae Kim, Eunhye Jeong, Sehyeon Lee, Seohee Yoon, Yong Suk Choi
- **URL**: <http://arxiv.org/abs/2510.17921v1>
- **Submitted**: 2025-10-20 06:59:37
- **Comment**: NeurIPS 2025
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on creativity detection in LLM-generated solutions, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves attention mechanisms, the context is specific to creativity assessment in reasoning tasks, which is not a central match to your research themes.

#### Abstract
> Recent advances in enhancing the reasoning ability of large language models
(LLMs) have been remarkably successful. LLMs trained with reinforcement
learning (RL) for reasoning demonstrate strong performance in challenging tasks
such as mathematics and coding, even with relatively small model sizes.
However, despite these improvements in task accuracy, the assessment of
creativity in LLM generations has been largely overlooked in reasoning tasks,
in contrast to writing tasks. The lack of research on creativity assessment in
reasoning primarily stems from two challenges: (1) the difficulty of defining
the range of creativity, and (2) the necessity of human evaluation in the
assessment process. To address these challenges, we propose CLAWS, a method
that defines and classifies mathematical solutions into typical, creative, and
hallucinated categories without human evaluation, by leveraging attention
weights across prompt sections and output. CLAWS outperforms five existing
white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden
Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,
Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems
collected from 181 math contests (AJHSME, AMC, AIME).

### 35. Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Chenchen Tan, Youyang Qu, Xinghao Li, Hui Zhang, Shujie Cui, Cunjian Chen, Longxiang Gao
- **URL**: <http://arxiv.org/abs/2510.17210v1>
- **Submitted**: 2025-10-20 06:50:03
- **Comment**: 22 pages, 10 figures
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on machine unlearning in large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves attention mechanisms, the context is more aligned with model reliability and knowledge retention rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

### 36. Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Melik Ozolcer, Sang Won Bae
- **URL**: <http://arxiv.org/abs/2510.17173v2>
- **Submitted**: 2025-10-20 05:28:59
- **Comment**: Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in
  Large Language Models
- **Topic Keywords**: rag, personalization
- **Reason**: This paper appears to be focused on evaluating the effectiveness of a language model-based health coach, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is specific to health coaching and does not align with the user's primary research interests.

#### Abstract
> We study a web-deployed, tool-augmented LLM health coach with real users. In
a pilot with seven users (280 rated turns), offline policy evaluation (OPE)
over factorized decision heads (Tool/Style) shows that a uniform heavy-tool
policy raises average value on logs but harms specific subgroups, most notably
low-health-literacy/high-self-efficacy users. A lightweight simulator with
hidden archetypes further shows that adding a small early information-gain
bonus reliably shortens trait identification and improves goal success and
pass@3. Together, these early findings indicate an evaluation-first path to
personalization: freeze the generator, learn subgroup-aware decision heads on
typed rewards (objective tool outcomes and satisfaction), and always report
per-archetype metrics to surface subgroup harms that averages obscure.

### 37. Mapping Post-Training Forgetting in Language Models at Scale

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jackson Harmon, Andreas Hochlehnert, Matthias Bethge, Ameya Prabhu
- **URL**: <http://arxiv.org/abs/2510.17776v1>
- **Submitted**: 2025-10-20 17:35:47
- **Comment**: 43 pages,15 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on the concept of post-training forgetting in language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on the idea of model performance and knowledge retention, it does not address the core themes of query understanding, ranking models, or real-time relevance optimization.

#### Abstract
> Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

### 38. VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qilin Liao, Anamika Lochab, Ruqi Zhang
- **URL**: <http://arxiv.org/abs/2510.17759v1>
- **Submitted**: 2025-10-20 17:12:10
- **Comment**: 18 pages, 7 Figures,
- **Topic Keywords**: rag
- **Reason**: This paper is primarily focused on Vision-Language Models and their vulnerabilities, which is not a core area of interest for you. While it does involve some aspects of deep semantic understanding, the context is more related to model security and adversarial attacks rather than information retrieval or search technologies.

#### Abstract
> Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

### 39. PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nanda Kumar Rengarajan, Jun Yan, Chun Wang
- **URL**: <http://arxiv.org/abs/2510.17720v1>
- **Submitted**: 2025-10-20 16:36:18
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Named Entity Recognition (NER) and proposes a framework for low-resource scenarios. While it involves data augmentation and NLP techniques, it does not align with the user's primary research interests in Information Retrieval, Search technologies, and query understanding.

#### Abstract
> Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

### 40. QueST: Incentivizing LLMs to Generate Difficult Problems

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hanxu Hu, Xingxing Zhang, Jannis Vamvas, Rico Sennrich, Furu Wei
- **URL**: <http://arxiv.org/abs/2510.17715v1>
- **Submitted**: 2025-10-20 16:29:53
- **Comment**: 20 pages, 7 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on generating challenging coding problems for large language models, which is related to query understanding and ranking models in the context of competitive coding and reasoning. However, the paper's primary focus is on NLP and problem generation, rather than information retrieval or search technologies, making it only loosely relevant to your research interests.

#### Abstract
> Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

### 41. Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dayan Pan, Zhaoyang Fu, Jingyuan Wang, Xiao Han, Yue Zhu, Xiangyu Zhao
- **URL**: <http://arxiv.org/abs/2510.17705v1>
- **Submitted**: 2025-10-20 16:19:27
- **Comment**: Accepted by CIKM' 25
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Large Language Models and multi-task adaptation, which is not directly related to your core research interests in Information Retrieval and Search technologies. While it involves attention mechanisms, the context is more aligned with NLP and deep learning, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Large Language Models (LLMs) possess remarkable generalization capabilities
but struggle with multi-task adaptation, particularly in balancing knowledge
retention with task-specific specialization. Conventional fine-tuning methods
suffer from catastrophic forgetting and substantial resource consumption, while
existing parameter-efficient methods perform suboptimally in complex multi-task
scenarios. To address this, we propose Contextual Attention Modulation (CAM), a
novel mechanism that dynamically modulates the representations of
self-attention modules in LLMs. CAM enhances task-specific features while
preserving general knowledge, thereby facilitating more effective and efficient
adaptation. For effective multi-task adaptation, CAM is integrated into our
Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a
shared, full-parameter CAM module with multiple specialized, lightweight CAM
modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.
Extensive experiments on heterogeneous tasks, including question answering,
code generation, and logical reasoning, demonstrate that our approach
significantly outperforms existing approaches, achieving an average performance
improvement of 3.65%. The implemented code and data are available to ease
reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.

### 42. Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuefeng Peng, Parnian Afshar, Megan Ganji, Thomas Butler, Amir Houmansadr, Mingxian Wang, Dezhi Hong
- **URL**: <http://arxiv.org/abs/2510.17620v1>
- **Submitted**: 2025-10-20 15:03:45
- **Topic Keywords**: rag
- **Reason**: This paper focuses on unlearning and contextual utility in large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

### 43. Reasoning Distillation and Structural Alignment for Improved Code Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Amir Jalilifard, Anderson de Rezende Rocha, Marcos Medeiros Raimundo
- **URL**: <http://arxiv.org/abs/2510.17598v1>
- **Submitted**: 2025-10-20 14:47:47
- **Topic Keywords**: rag
- **Reason**: This paper focuses on code generation with language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context and application are quite different from your areas of focus.

#### Abstract
> Effective code generation with language models hinges on two critical
factors: accurately understanding the intent of the prompt and generating code
that applies algorithmic reasoning to produce correct solutions capable of
passing diverse test cases while adhering to the syntax of the target
programming language. Unlike other language tasks, code generation requires
more than accurate token prediction; it demands comprehension of solution-level
and structural relationships rather than merely generating the most likely
tokens. very large language model (VLLM) are capable of generating detailed
steps toward the correct solution of complex tasks where reasoning is crucial
in solving the problem. Such reasoning capabilities may be absent in smaller
language models. Therefore, in this work, we distill the reasoning capabilities
of a VLLM into a smaller, more efficient model that is faster and cheaper to
deploy. Our approach trains the model to emulate the reasoning and
problem-solving abilities of the VLLM by learning to identify correct solution
pathways and establishing a structural correspondence between problem
definitions and potential solutions through a novel method of structure-aware
loss optimization. This enables the model to transcend token-level generation
and to deeply grasp the overarching structure of solutions for given problems.
Experimental results show that our fine-tuned model, developed through a cheap
and simple to implement process, significantly outperforms our baseline model
in terms of pass@1, average data flow, and average syntax match metrics across
the MBPP, MBPP Plus, and HumanEval benchmarks.

### 44. BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiacheng Xie, Yang Yu, Yibo Chen, Hanyao Zhang, Lening Zhao, Jiaxuan He, Lei Jiang, Xiaoting Tang, Guanghui An, Dong Xu
- **URL**: <http://arxiv.org/abs/2510.17415v1>
- **Submitted**: 2025-10-20 10:57:37
- **Topic Keywords**: retrieval
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on developing a large language model for Traditional Chinese Medicine, which is outside your core research themes.

#### Abstract
> Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

### 45. Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiacheng Xie, Shuai Zeng, Yang Yu, Xiaoting Tang, Guanghui An, Dong Xu
- **URL**: <http://arxiv.org/abs/2510.17402v1>
- **Submitted**: 2025-10-20 10:43:33
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on Traditional Chinese Medicine and the application of reinforcement learning to large language models.

#### Abstract
> Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

### 46. StreamingThinker: Large Language Models Can Think While Reading

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen
- **URL**: <http://arxiv.org/abs/2510.17238v1>
- **Submitted**: 2025-10-20 07:27:37
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the performance of Large Language Models (LLMs) through a 'streaming thinking' paradigm, which is not directly related to Information Retrieval (IR), Search technologies, or user behavior modeling. While it involves NLP, the context is more aligned with deep learning and model optimization, rather than query understanding, ranking models, or real-time relevance optimization.

#### Abstract
> Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

### 47. DVAGen: Dynamic Vocabulary Augmented Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wei Du, Nuowei Liu, Jie Wang, Jiahao Kuang, Tao Ji, Xiaoling Wang, Yuanbin Wu
- **URL**: <http://arxiv.org/abs/2510.17115v1>
- **Submitted**: 2025-10-20 03:09:24
- **Topic Keywords**: rag
- **Reason**: This paper focuses on language models and dynamic vocabulary augmentation, which is somewhat related to your interests in NLP and deep semantic understanding. However, it does not directly address your core research themes in Information Retrieval, Search technologies, or query understanding.

#### Abstract
> Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

### 48. JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Junlan Feng, Fanyu Meng, Chong Long, Pengyu Cong, Duqing Wang, Yan Zheng, Yuyao Zhang, Xuanchang Gao, Ye Yuan, Yunfei Ma, Zhijie Ren, Fan Yang, Na Wu, Di Jin, Chao Deng
- **URL**: <http://arxiv.org/abs/2510.17918v1>
- **Submitted**: 2025-10-20 02:12:49
- **Topic Keywords**: rag
- **Reason**: This paper focuses on enhancing the safety and trustworthiness of Large Language Models (LLMs) through pre-training data, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on NLP, the primary focus is on improving LLMs, not on deep semantic understanding or real-time relevance optimization.

#### Abstract
> The hallucination and credibility concerns of large language models (LLMs)
are global challenges that the industry is collectively addressing. Recently, a
significant amount of advances have been made on post-training and inference
techniques to mitigate these challenges. However, it is widely agreed that
unsafe and hallucinations of LLMs intrinsically originate from pre-training,
involving pre-training data and the next-token prediction learning mechanism.
In this paper, we focus on enhancing pre-training data to improve the
trustworthiness and safety of LLMs. Since the data is vast, it's almost
impossible to entirely purge the data of factual errors, logical
inconsistencies, or distributional biases. Moreover, the pre-training data lack
grounding in real-world knowledge. Each piece of data is treated as a sequence
of tokens rather than as a representation of a part of the world. To overcome
these issues, we propose approaches to enhancing our pre-training data with its
context in the world and increasing a substantial amount of data reflecting
industrial scenarios. We argue that most source data are created by the authors
for specific purposes in a certain spatial-temporal context. They have played a
role in the real world. By incorporating related world context information, we
aim to better anchor pre-training data within real-world scenarios, thereby
reducing uncertainty in model training and enhancing the model's safety and
trustworthiness. We refer to our Data with World Context as DWC. We continue
pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC
tokens. We introduce our post-training procedures to activate the potentials of
DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an
average performance improvement of 1.79% on the Safety and Trustworthy
evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.

### 49. Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Masahiro Kaneko, Zeerak Talat, Timothy Baldwin
- **URL**: <http://arxiv.org/abs/2510.17006v1>
- **Submitted**: 2025-10-19 21:07:21
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are your core research interests. Although it involves Natural Language Processing, the focus is on defending against language model attacks, which is not a central match to your research themes.

#### Abstract
> Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

### 50. DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Massa Baali, Rita Singh, Bhiksha Raj
- **URL**: <http://arxiv.org/abs/2510.17662v1>
- **Submitted**: 2025-10-20 15:35:55
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speaker-aware self-supervised speech models, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

---

