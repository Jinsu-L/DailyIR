[
    {
        "title": "Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing",
        "abstract": "Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce $\\textbf{Parallel-Probe}$, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to $\\textbf{35.8}$% and total token cost by over $\\textbf{25.8}$% while maintaining competitive accuracy.",
        "url": "http://arxiv.org/abs/2602.03845v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03845v1",
        "arxiv_id": "2602.03845v1",
        "authors": [
            "Tong Zheng",
            "Chengsong Huang",
            "Runpeng Dai",
            "Yun He",
            "Rui Liu",
            "Xin Ni",
            "Huiwen Bao",
            "Kaishen Wang",
            "Hongtu Zhu",
            "Jiaxin Huang",
            "Furong Huang",
            "Heng Huang"
        ],
        "submitted": "2026-02-03 18:59:41",
        "source": "arxiv",
        "comment": "14 pages"
    },
    {
        "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
        "abstract": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a \"neuro-symbolic\" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.",
        "url": "http://arxiv.org/abs/2602.03837v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03837v1",
        "arxiv_id": "2602.03837v1",
        "authors": [
            "David P. Woodruff",
            "Vincent Cohen-Addad",
            "Lalit Jain",
            "Jieming Mao",
            "Song Zuo",
            "MohammadHossein Bateni",
            "Simina Branzei",
            "Michael P. Brenner",
            "Lin Chen",
            "Ying Feng",
            "Lance Fortnow",
            "Gang Fu",
            "Ziyi Guan",
            "Zahra Hadizadeh",
            "Mohammad T. Hajiaghayi",
            "Mahdi JafariRaviz",
            "Adel Javanmard",
            "Karthik C. S.",
            "Ken-ichi Kawarabayashi",
            "Ravi Kumar",
            "Silvio Lattanzi",
            "Euiwoong Lee",
            "Yi Li",
            "Ioannis Panageas",
            "Dimitris Paparas",
            "Benjamin Przybocki",
            "Bernardo Subercaseaux",
            "Ola Svensson",
            "Shayan Taherijam",
            "Xuan Wu",
            "Eylon Yogev",
            "Morteza Zadimoghaddam",
            "Samson Zhou",
            "Vahab Mirrokni"
        ],
        "submitted": "2026-02-03 18:56:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
        "abstract": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.",
        "url": "http://arxiv.org/abs/2602.03828v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03828v1",
        "arxiv_id": "2602.03828v1",
        "authors": [
            "Minjun Zhu",
            "Zhen Lin",
            "Yixuan Weng",
            "Panzhong Lu",
            "Qiujie Xie",
            "Yifan Wei",
            "Sifan Liu",
            "Qiyao Sun",
            "Yue Zhang"
        ],
        "submitted": "2026-02-03 18:41:43",
        "source": "arxiv",
        "comment": "Accepted at the ICLR 2026"
    },
    {
        "title": "They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References",
        "abstract": "Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.",
        "url": "http://arxiv.org/abs/2602.03822v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03822v1",
        "arxiv_id": "2602.03822v1",
        "authors": [
            "Sahil Tripathi",
            "Gautam Siddharth Kashyap",
            "Mehwish Nasim",
            "Jian Yang",
            "Jiechao Gao",
            "Usman Naseem"
        ],
        "submitted": "2026-02-03 18:29:46",
        "source": "arxiv",
        "comment": "Accepted at the The Web Conference 2026 (Research Track)"
    },
    {
        "title": "Antidistillation Fingerprinting",
        "abstract": "Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.",
        "url": "http://arxiv.org/abs/2602.03812v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03812v1",
        "arxiv_id": "2602.03812v1",
        "authors": [
            "Yixuan Even Xu",
            "John Kirchenbauer",
            "Yash Savani",
            "Asher Trockman",
            "Alexander Robey",
            "Tom Goldstein",
            "Fei Fang",
            "J. Zico Kolter"
        ],
        "submitted": "2026-02-03 18:15:50",
        "source": "arxiv",
        "comment": "26 pages, 11 figures"
    },
    {
        "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
        "abstract": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
        "url": "http://arxiv.org/abs/2602.03806v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03806v1",
        "arxiv_id": "2602.03806v1",
        "authors": [
            "Ziru Chen",
            "Dongdong Chen",
            "Ruinan Jin",
            "Yingbin Liang",
            "Yujia Xie",
            "Huan Sun"
        ],
        "submitted": "2026-02-03 18:08:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation",
        "abstract": "Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.",
        "url": "http://arxiv.org/abs/2602.03798v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03798v1",
        "arxiv_id": "2602.03798v1",
        "authors": [
            "Zimu Lu",
            "Houxing Ren",
            "Yunqiao Yang",
            "Ke Wang",
            "Zhuofan Zong",
            "Mingjie Zhan",
            "Hongsheng Li"
        ],
        "submitted": "2026-02-03 18:01:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents",
        "abstract": "Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \\emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: https://github.com/wxl-lxw/WebSentinel.",
        "url": "http://arxiv.org/abs/2602.03792v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03792v1",
        "arxiv_id": "2602.03792v1",
        "authors": [
            "Xilong Wang",
            "Yinuo Liu",
            "Zhun Wang",
            "Dawn Song",
            "Neil Gong"
        ],
        "submitted": "2026-02-03 17:55:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?",
        "abstract": "Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.",
        "url": "http://arxiv.org/abs/2602.03916v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03916v1",
        "arxiv_id": "2602.03916v1",
        "authors": [
            "Azmine Toushik Wasi",
            "Wahid Faisal",
            "Abdur Rahman",
            "Mahfuz Ahmed Anik",
            "Munem Shahriar",
            "Mohsin Mahmud Topu",
            "Sadia Tasnim Meem",
            "Rahatun Nesa Priti",
            "Sabrina Afroz Mitu",
            "Md. Iqramul Hoque",
            "Shahriyar Zaman Ridoy",
            "Mohammed Eunus Ali",
            "Majd Hawasly",
            "Mohammad Raza",
            "Md Rizwan Parvez"
        ],
        "submitted": "2026-02-03 17:52:02",
        "source": "arxiv",
        "comment": "Accepted to ICLR 2026. 92 Pages. 42 Figures and 29 Tables"
    },
    {
        "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
        "abstract": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple Instruction, Context, Tools, Model. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand. Building on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation. Such designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance-cost trade-off, allowing the system to approach Pareto-efficient. Across three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra",
        "url": "http://arxiv.org/abs/2602.03786v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03786v1",
        "arxiv_id": "2602.03786v1",
        "authors": [
            "Jianhao Ruan",
            "Zhihao Xu",
            "Yiran Peng",
            "Fashen Ren",
            "Zhaoyang Yu",
            "Xinbing Liang",
            "Jinyu Xiang",
            "Bang Liu",
            "Chenglin Wu",
            "Yuyu Luo",
            "Jiayi Zhang"
        ],
        "submitted": "2026-02-03 17:46:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context Compression via Explicit Information Transmission",
        "abstract": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
        "url": "http://arxiv.org/abs/2602.03784v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03784v1",
        "arxiv_id": "2602.03784v1",
        "authors": [
            "Jiangnan Ye",
            "Hanqi Yan",
            "Zhenyi Shen",
            "Heng Chang",
            "Ye Mao",
            "Yulan He"
        ],
        "submitted": "2026-02-03 17:44:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution",
        "abstract": "Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.",
        "url": "http://arxiv.org/abs/2602.03783v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03783v1",
        "arxiv_id": "2602.03783v1",
        "authors": [
            "Zhenshuo Zhang",
            "Minxuan Duan",
            "Hongyang R. Zhang"
        ],
        "submitted": "2026-02-03 17:43:48",
        "source": "arxiv",
        "comment": "27 pages. To appear in ICLR 2026"
    },
    {
        "title": "CUBO: Self-Contained Retrieval-Augmented Generation on Consumer Laptops 10 GB Corpora, 16 GB RAM, Single-Device Deployment",
        "abstract": "Organizations handling sensitive documents face a tension: cloud-based AI risks GDPR violations, while local systems typically require 18-32 GB RAM. This paper presents CUBO, a systems-oriented RAG platform for consumer laptops with 16 GB shared memory. CUBO's novelty lies in engineering integration of streaming ingestion (O(1) buffer overhead), tiered hybrid retrieval, and hardware-aware orchestration that enables competitive Recall@10 (0.48-0.97 across BEIR domains) within a hard 15.5 GB RAM ceiling. The 37,000-line codebase achieves retrieval latencies of 185 ms (p50) on C1,300 laptops while maintaining data minimization through local-only processing aligned with GDPR Art. 5(1)(c). Evaluation on BEIR benchmarks validates practical deployability for small-to-medium professional archives. The codebase is publicly available at https://github.com/PaoloAstrino/CUBO.",
        "url": "http://arxiv.org/abs/2602.03731v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03731v1",
        "arxiv_id": "2602.03731v1",
        "authors": [
            "Paolo Astrino"
        ],
        "submitted": "2026-02-03 16:50:58",
        "source": "arxiv",
        "comment": "24 pages, 2 figures, 6 tables"
    },
    {
        "title": "Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling",
        "abstract": "Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \\href{https://github.com/YubaoZhao/BranPO}{code}.",
        "url": "http://arxiv.org/abs/2602.03719v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03719v1",
        "arxiv_id": "2602.03719v1",
        "authors": [
            "Yubao Zhao",
            "Weiquan Huang",
            "Sudong Wang",
            "Ruochen Zhao",
            "Chen Chen",
            "Yao Shu",
            "Chengwei Qin"
        ],
        "submitted": "2026-02-03 16:43:09",
        "source": "arxiv",
        "comment": "24 pages, 5 figures"
    },
    {
        "title": "Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals",
        "abstract": "Sequential recommender systems rank relevant items by modeling a user's interaction history and computing the inner product between the resulting user representation and stored item embeddings. To avoid the significant memory overhead of storing large item sets, the generative recommendation paradigm instead models each item as a series of discrete semantic codes. Here, the next item is predicted by an autoregressive model that generates the code sequence corresponding to the predicted item. However, despite promising ranking capabilities on small datasets, these methods have yet to surpass traditional sequential recommenders on large item sets, limiting their adoption in the very scenarios they were designed to address. To resolve this, we propose MSCGRec, a Multimodal Semantic and Collaborative Generative Recommender. MSCGRec incorporates multiple semantic modalities and introduces a novel self-supervised quantization learning approach for images based on the DINO framework. Additionally, MSCGRec fuses collaborative and semantic signals by extracting collaborative features from sequential recommenders and treating them as a separate modality. Finally, we propose constrained sequence learning that restricts the large output space during training to the set of permissible tokens. We empirically demonstrate on three large real-world datasets that MSCGRec outperforms both sequential and generative recommendation baselines and provide an extensive ablation study to validate the impact of each component.",
        "url": "http://arxiv.org/abs/2602.03713v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03713v1",
        "arxiv_id": "2602.03713v1",
        "authors": [
            "Moritz Vandenhirtz",
            "Kaveh Hassani",
            "Shervin Ghasemlou",
            "Shuai Shao",
            "Hamid Eghbalzadeh",
            "Fuchun Peng",
            "Jun Liu",
            "Michael Louis Iuzzolino"
        ],
        "submitted": "2026-02-03 16:39:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding",
        "abstract": "Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.",
        "url": "http://arxiv.org/abs/2602.03709v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03709v1",
        "arxiv_id": "2602.03709v1",
        "authors": [
            "Vynska Amalia Permadi",
            "Xingwei Tan",
            "Nafise Sadat Moosavi",
            "Nikos Aletras"
        ],
        "submitted": "2026-02-03 16:32:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States",
        "abstract": "Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific meanings. Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.",
        "url": "http://arxiv.org/abs/2602.03708v2",
        "pdf_url": "https://arxiv.org/pdf/2602.03708v2",
        "arxiv_id": "2602.03708v2",
        "authors": [
            "Ximing Dong",
            "Shaowei Wang",
            "Dayi Lin",
            "Boyuan Chen",
            "Ahmed E. Hassan"
        ],
        "submitted": "2026-02-03 16:30:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering",
        "abstract": "Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.",
        "url": "http://arxiv.org/abs/2602.03707v2",
        "pdf_url": "https://arxiv.org/pdf/2602.03707v2",
        "arxiv_id": "2602.03707v2",
        "authors": [
            "Yifan Zhu",
            "Xinyu Mu",
            "Tao Feng",
            "Zhonghong Ou",
            "Yuning Gong",
            "Haoran Luo"
        ],
        "submitted": "2026-02-03 16:28:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models",
        "abstract": "Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.",
        "url": "http://arxiv.org/abs/2602.03704v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03704v1",
        "arxiv_id": "2602.03704v1",
        "authors": [
            "Yu Tian",
            "Linh Huynh",
            "Katerina Christhilf",
            "Shubham Chakraborty",
            "Micah Watanabe",
            "Tracy Arner",
            "Danielle McNamara"
        ],
        "submitted": "2026-02-03 16:26:47",
        "source": "arxiv",
        "comment": "This manuscript is under review at Electronics"
    },
    {
        "title": "Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates",
        "abstract": "Large language models (LLMs) rely on internal knowledge to solve many downstream tasks, making it crucial to keep them up to date. Since full retraining is expensive, prior work has explored efficient alternatives such as model editing and parameter-efficient fine-tuning. However, these approaches often break down in practice due to poor generalization across inputs, limited stability, and knowledge conflict. To address these limitations, we propose the CoRSA (Conflict-Resolving and Sharpness-Aware Minimization) training framework, a parameter-efficient, holistic approach for knowledge editing with multiple updates. CoRSA tackles multiple challenges simultaneously: it improves generalization to different input forms and enhances stability across multiple updates by minimizing loss curvature, and resolves conflicts by maximizing the margin between new and prior knowledge. Across three widely used fact editing benchmarks, CoRSA achieves significant gains in generalization, outperforming baselines with average absolute improvements of 12.42% over LoRA and 10% over model editing methods. With multiple updates, it maintains high update efficacy while reducing catastrophic forgetting by 27.82% compared to LoRA. CoRSA also generalizes to the code domain, outperforming the strongest baseline by 5.48% Pass@5 in update efficacy.",
        "url": "http://arxiv.org/abs/2602.03696v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03696v1",
        "arxiv_id": "2602.03696v1",
        "authors": [
            "Duy Nguyen",
            "Hanqi Xiao",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Hyunji Lee",
            "Mohit Bansal"
        ],
        "submitted": "2026-02-03 16:18:06",
        "source": "arxiv",
        "comment": "22 pages, 8 figures. Code link: https://github.com/duykhuongnguyen/CoRSA"
    },
    {
        "title": "Agent Primitives: Reusable Latent Building Blocks for Multi-Agent Systems",
        "abstract": "While existing multi-agent systems (MAS) can handle complex problems by enabling collaboration among multiple agents, they are often highly task-specific, relying on manually crafted agent roles and interaction prompts, which leads to increased architectural complexity and limited reusability across tasks. Moreover, most MAS communicate primarily through natural language, making them vulnerable to error accumulation and instability in long-context, multi-stage interactions within internal agent histories.\n  In this work, we propose \\textbf{Agent Primitives}, a set of reusable latent building blocks for LLM-based MAS. Inspired by neural network design, where complex models are built from reusable components, we observe that many existing MAS architectures can be decomposed into a small number of recurring internal computation patterns. Based on this observation, we instantiate three primitives: Review, Voting and Selection, and Planning and Execution. All primitives communicate internally via key-value (KV) cache, which improves both robustness and efficiency by mitigating information degradation across multi-stage interactions. To enable automatic system construction, an Organizer agent selects and composes primitives for each query, guided by a lightweight knowledge pool of previously successful configurations, forming a primitive-based MAS.\n  Experiments show that primitives-based MAS improve average accuracy by 12.0-16.5\\% over single-agent baselines, reduce token usage and inference latency by approximately 3$\\times$-4$\\times$ compared to text-based MAS, while incurring only 1.3$\\times$-1.6$\\times$ overhead relative to single-agent inference and providing more stable performance across model backbones.",
        "url": "http://arxiv.org/abs/2602.03695v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03695v1",
        "arxiv_id": "2602.03695v1",
        "authors": [
            "Haibo Jin",
            "Kuang Peng",
            "Ye Yu",
            "Xiaopeng Yuan",
            "Haohan Wang"
        ],
        "submitted": "2026-02-03 16:17:53",
        "source": "arxiv",
        "comment": "16 pages"
    },
    {
        "title": "OCRTurk: A Comprehensive OCR Benchmark for Turkish",
        "abstract": "Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.",
        "url": "http://arxiv.org/abs/2602.03693v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03693v1",
        "arxiv_id": "2602.03693v1",
        "authors": [
            "Deniz Yılmaz",
            "Evren Ayberk Munis",
            "Çağrı Toraman",
            "Süha Kağan Köse",
            "Burak Aktaş",
            "Mehmet Can Baytekin",
            "Bilge Kaan Görür"
        ],
        "submitted": "2026-02-03 16:11:25",
        "source": "arxiv",
        "comment": "Accepted by EACL 2026 SIGTURK"
    },
    {
        "title": "Bringing Reasoning to Generative Recommendation Through the Lens of Cascaded Ranking",
        "abstract": "Generative Recommendation (GR) has become a promising end-to-end approach with high FLOPS utilization for resource-efficient recommendation. Despite the effectiveness, we show that current GR models suffer from a critical \\textbf{bias amplification} issue, where token-level bias escalates as token generation progresses, ultimately limiting the recommendation diversity and hurting the user experience. By comparing against the key factor behind the success of traditional multi-stage pipelines, we reveal two limitations in GR that can amplify the bias: homogeneous reliance on the encoded history, and fixed computational budgets that prevent deeper user preference understanding.\n  To combat the bias amplification issue, it is crucial for GR to 1) incorporate more heterogeneous information, and 2) allocate greater computational resources at each token generation step. To this end, we propose CARE, a simple yet effective cascaded reasoning framework for debiased GR. To incorporate heterogeneous information, we introduce a progressive history encoding mechanism, which progressively incorporates increasingly fine-grained history information as the generation process advances. To allocate more computations, we propose a query-anchored reasoning mechanism, which seeks to perform a deeper understanding of historical information through parallel reasoning steps. We instantiate CARE on three GR backbones. Empirical results on four datasets show the superiority of CARE in recommendation accuracy, diversity, efficiency, and promising scalability. The codes and datasets are available at https://github.com/Linxyhaha/CARE.",
        "url": "http://arxiv.org/abs/2602.03692v2",
        "pdf_url": "https://arxiv.org/pdf/2602.03692v2",
        "arxiv_id": "2602.03692v2",
        "authors": [
            "Xinyu Lin",
            "Pengyuan Liu",
            "Wenjie Wang",
            "Yicheng Hu",
            "Chen Xu",
            "Fuli Feng",
            "Qifan Wang",
            "Tat-Seng Chua"
        ],
        "submitted": "2026-02-03 16:10:54",
        "source": "arxiv",
        "comment": "Accepted by WWW2026"
    },
    {
        "title": "Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.",
        "url": "http://arxiv.org/abs/2602.03689v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03689v1",
        "arxiv_id": "2602.03689v1",
        "authors": [
            "Jiashuo Sun",
            "Pengcheng Jiang",
            "Saizhuo Wang",
            "Jiajun Fan",
            "Heng Wang",
            "Siru Ouyang",
            "Ming Zhong",
            "Yizhu Jiao",
            "Chengsong Huang",
            "Xueqiang Xu",
            "Pengrui Han",
            "Peiran Li",
            "Jiaxin Huang",
            "Ge Liu",
            "Heng Ji",
            "Jiawei Han"
        ],
        "submitted": "2026-02-03 16:08:23",
        "source": "arxiv",
        "comment": "19 pages, 8 tables, 5 figures"
    },
    {
        "title": "Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models",
        "abstract": "The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.",
        "url": "http://arxiv.org/abs/2602.03681v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03681v1",
        "arxiv_id": "2602.03681v1",
        "authors": [
            "Difan Deng",
            "Andreas Bentzen Winje",
            "Lukas Fehring",
            "Marius Lindauer"
        ],
        "submitted": "2026-02-03 16:02:50",
        "source": "arxiv",
        "comment": "17 pages, 8 figures"
    },
    {
        "title": "Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration",
        "abstract": "Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\\%$ of these critical heads can decrease the modality-following ratio by $60\\%$ through blocking, or increase it by $60\\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.",
        "url": "http://arxiv.org/abs/2602.03677v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03677v1",
        "arxiv_id": "2602.03677v1",
        "authors": [
            "Yu Zhang",
            "Mufan Xu",
            "Xuefeng Bai",
            "Kehai chen",
            "Pengfei Zhang",
            "Yang Xiang",
            "Min Zhang"
        ],
        "submitted": "2026-02-03 15:59:24",
        "source": "arxiv",
        "comment": "Modality Following"
    },
    {
        "title": "RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.",
        "url": "http://arxiv.org/abs/2602.03652v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03652v1",
        "arxiv_id": "2602.03652v1",
        "authors": [
            "Süha Kağan Köse",
            "Mehmet Can Baytekin",
            "Burak Aktaş",
            "Bilge Kaan Görür",
            "Evren Ayberk Munis",
            "Deniz Yılmaz",
            "Muhammed Yusuf Kartal",
            "Çağrı Toraman"
        ],
        "submitted": "2026-02-03 15:35:11",
        "source": "arxiv",
        "comment": "Accepted by EACL 2026 SIGTURK"
    },
    {
        "title": "Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration",
        "abstract": "Search-integrated reasoning enables language agents to transcend static parametric knowledge by actively querying external sources. However, training these agents via reinforcement learning is hindered by the multi-scale credit assignment problem: existing methods typically rely on sparse, trajectory-level rewards that fail to distinguish between high-quality reasoning and fortuitous guesses, leading to redundant or misleading search behaviors. To address this, we propose Search-R2, a novel Actor-Refiner collaboration framework that enhances reasoning through targeted intervention, with both components jointly optimized during training. Our approach decomposes the generation process into an Actor, which produces initial reasoning trajectories, and a Meta-Refiner, which selectively diagnoses and repairs flawed steps via a 'cut-and-regenerate' mechanism. To provide fine-grained supervision, we introduce a hybrid reward design that couples outcome correctness with a dense process reward quantifying the information density of retrieved evidence. Theoretically, we formalize the Actor-Refiner interaction as a smoothed mixture policy, proving that selective correction yields strict performance gains over strong baselines. Extensive experiments across various general and multi-hop QA datasets demonstrate that Search-R2 consistently outperforms strong RAG and RL-based baselines across model scales, achieving superior reasoning accuracy with minimal overhead.",
        "url": "http://arxiv.org/abs/2602.03647v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03647v1",
        "arxiv_id": "2602.03647v1",
        "authors": [
            "Bowei He",
            "Minda Hu",
            "Zenan Xu",
            "Hongru Wang",
            "Licheng Zong",
            "Yankai Chen",
            "Chen Ma",
            "Xue Liu",
            "Pluto Zhou",
            "Irwin King"
        ],
        "submitted": "2026-02-03 15:32:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tutorial on Reasoning for IR & IR for Reasoning",
        "abstract": "Information retrieval has long focused on ranking documents by semantic relatedness. Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. Across AI communities, researchers are developing diverse solutions for the problem of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, Bayesian and probabilistic frameworks, geometric representations, and energy-based models. These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities. To help navigate the fragmented landscape of research in reasoning, this tutorial first articulates a working definition of reasoning within the context of information retrieval and derives from it a unified analytical framework. The framework maps existing approaches along axes that reflect the core components of the definition. By providing a comprehensive overview of recent approaches and mapping current methods onto the defined axes, we expose their trade-offs and complementarities, highlight where IR can benefit from cross-disciplinary advances, and illustrate how retrieval process itself can play a central role in broader reasoning systems. The tutorial will equip participants with both a conceptual framework and practical guidance for enhancing reasoning-capable IR systems, while situating IR as a domain that both benefits and contributes to the broader development of reasoning methodologies.",
        "url": "http://arxiv.org/abs/2602.03640v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03640v1",
        "arxiv_id": "2602.03640v1",
        "authors": [
            "Mohanna Hoveyda",
            "Panagiotis Efstratiadis",
            "Arjen de Vries",
            "Maarten de Rijke"
        ],
        "submitted": "2026-02-03 15:24:36",
        "source": "arxiv",
        "comment": "Accepted to ECIR 2026"
    },
    {
        "title": "TRE: Encouraging Exploration in the Trust Region",
        "abstract": "Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.",
        "url": "http://arxiv.org/abs/2602.03635v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03635v1",
        "arxiv_id": "2602.03635v1",
        "authors": [
            "Chao Huang",
            "Yujing Lu",
            "Quangang Li",
            "Shenghe Wang",
            "Yan Wang",
            "Yueyang Zhang",
            "Long Xia",
            "Jiashu Zhao",
            "Zhiyuan Sun",
            "Daiting Shi",
            "Tingwen Liu"
        ],
        "submitted": "2026-02-03 15:21:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish",
        "abstract": "Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.",
        "url": "http://arxiv.org/abs/2602.03633v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03633v1",
        "arxiv_id": "2602.03633v1",
        "authors": [
            "Burak Aktaş",
            "Mehmet Can Baytekin",
            "Süha Kağan Köse",
            "Ömer İlbilgi",
            "Elif Özge Yılmaz",
            "Çağrı Toraman",
            "Bilge Kaan Görür"
        ],
        "submitted": "2026-02-03 15:21:00",
        "source": "arxiv",
        "comment": "Accepted by EACL 2026 SIGTURK"
    },
    {
        "title": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation",
        "abstract": "Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.",
        "url": "http://arxiv.org/abs/2602.03619v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03619v1",
        "arxiv_id": "2602.03619v1",
        "authors": [
            "Changze Lv",
            "Jie Zhou",
            "Wentao Zhao",
            "Jingwen Xu",
            "Zisu Huang",
            "Muzhao Tian",
            "Shihan Dou",
            "Tao Gui",
            "Le Tian",
            "Xiao Zhou",
            "Xiaoqing Zheng",
            "Xuanjing Huang",
            "Jie Zhou"
        ],
        "submitted": "2026-02-03 15:09:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Controlling Output Rankings in Generative Engines for LLM-based Search",
        "abstract": "The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.\n  In this work, we propose CORE, an optimization method that \\textbf{C}ontrols \\textbf{O}utput \\textbf{R}ankings in g\\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.\n  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \\textbf{91.4\\% @Top-5}, \\textbf{86.6\\% @Top-3}, and \\textbf{80.3\\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.",
        "url": "http://arxiv.org/abs/2602.03608v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03608v1",
        "arxiv_id": "2602.03608v1",
        "authors": [
            "Haibo Jin",
            "Ruoxi Chen",
            "Peiyan Zhang",
            "Yifeng Luo",
            "Huimin Zeng",
            "Man Luo",
            "Haohan Wang"
        ],
        "submitted": "2026-02-03 14:59:48",
        "source": "arxiv",
        "comment": "23 pages"
    },
    {
        "title": "Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs",
        "abstract": "In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain $D$ that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by~\\cite{RegisterAllocation}. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of \\(O(|G| \\cdot |D|^6)\\), where \\(|G|\\) represents the size of the control-flow graph. Note that for any fixed domain $D,$ this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art.",
        "url": "http://arxiv.org/abs/2602.03588v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03588v1",
        "arxiv_id": "2602.03588v1",
        "authors": [
            "Xuran Cai",
            "Amir Goharshady"
        ],
        "submitted": "2026-02-03 14:38:10",
        "source": "arxiv",
        "comment": "Already accepted by SETTA'25. https://www.setta2025.uk/accepted-papers. arXiv admin note: substantial text overlap with arXiv:2507.16660"
    },
    {
        "title": "CL-bench: A Benchmark for Context Learning",
        "abstract": "Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.",
        "url": "http://arxiv.org/abs/2602.03587v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03587v1",
        "arxiv_id": "2602.03587v1",
        "authors": [
            "Shihan Dou",
            "Ming Zhang",
            "Zhangyue Yin",
            "Chenhao Huang",
            "Yujiong Shen",
            "Junzhe Wang",
            "Jiayi Chen",
            "Yuchen Ni",
            "Junjie Ye",
            "Cheng Zhang",
            "Huaibing Xie",
            "Jianglu Hu",
            "Shaolei Wang",
            "Weichao Wang",
            "Yanling Xiao",
            "Yiting Liu",
            "Zenan Xu",
            "Zhen Guo",
            "Pluto Zhou",
            "Tao Gui",
            "Zuxuan Wu",
            "Xipeng Qiu",
            "Qi Zhang",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Di Wang",
            "Shunyu Yao"
        ],
        "submitted": "2026-02-03 14:37:47",
        "source": "arxiv",
        "comment": "78 pages, 17 figures"
    },
    {
        "title": "$V_0$: A Generalist Value Model for Any Policy at State Zero",
        "abstract": "Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.",
        "url": "http://arxiv.org/abs/2602.03584v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03584v1",
        "arxiv_id": "2602.03584v1",
        "authors": [
            "Yi-Kai Zhang",
            "Zhiyuan Yao",
            "Hongyan Hao",
            "Yueqing Sun",
            "Qi Gu",
            "Hui Su",
            "Xunliang Cai",
            "De-Chuan Zhan",
            "Han-Jia Ye"
        ],
        "submitted": "2026-02-03 14:35:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs",
        "abstract": "Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.",
        "url": "http://arxiv.org/abs/2602.03578v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03578v1",
        "arxiv_id": "2602.03578v1",
        "authors": [
            "Su Dong",
            "Qinggang Zhang",
            "Yilin Xiao",
            "Shengyuan Chen",
            "Chuang Zhou",
            "Xiao Huang"
        ],
        "submitted": "2026-02-03 14:26:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning",
        "abstract": "Despite its success in self-supervised learning, contrastive learning is less studied in the supervised setting. In this work, we first use a set of pilot experiments to show that in the supervised setting, the cross-entropy loss objective (CE) and the contrastive learning objective often conflict with each other, thus hindering the applications of CL in supervised settings. To resolve this problem, we introduce a novel \\underline{A}ligned \\underline{C}ontrastive \\underline{L}earning (ACL) framework. First, ACL-Embed regards label embeddings as extra augmented samples with different labels and employs contrastive learning to align the label embeddings with its samples' representations. Second, to facilitate the optimization of ACL-Embed objective combined with the CE loss, we propose ACL-Grad, which will discard the ACL-Embed term if the two objectives are in conflict. To further enhance the performances of intermediate exits of multi-exit BERT, we further propose cross-layer ACL (ACL-CL), which is to ask the teacher exit to guide the optimization of student shallow exits. Extensive experiments on the GLUE benchmark results in the following takeaways: (a) ACL-BRT outperforms or performs comparably with CE and CE+SCL on the GLUE tasks; (b) ACL, especially CL-ACL, significantly surpasses the baseline methods on the fine-tuning of multi-exit BERT, thus providing better quality-speed tradeoffs for low-latency applications.",
        "url": "http://arxiv.org/abs/2602.03563v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03563v1",
        "arxiv_id": "2602.03563v1",
        "authors": [
            "Wei Zhu"
        ],
        "submitted": "2026-02-03 14:08:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing",
        "abstract": "This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.",
        "url": "http://arxiv.org/abs/2602.03560v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03560v1",
        "arxiv_id": "2602.03560v1",
        "authors": [
            "Yizhao Gao",
            "Jianyu Wei",
            "Qihao Zhang",
            "Yu Cheng",
            "Shimao Chen",
            "Zhengju Tang",
            "Zihan Jiang",
            "Yifan Song",
            "Hailin Zhang",
            "Liang Zhao",
            "Bo Yang",
            "Gang Wang",
            "Shijie Cao",
            "Fuli Luo"
        ],
        "submitted": "2026-02-03 14:05:57",
        "source": "arxiv",
        "comment": "17 pages, 2 figures"
    },
    {
        "title": "When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs",
        "abstract": "Recent progress has expanded the use of large language models (LLMs) in drug discovery, including synthesis planning. However, objective evaluation of retrosynthesis performance remains limited. Existing benchmarks and metrics typically rely on published synthetic procedures and Top-K accuracy based on single ground-truth, which does not capture the open-ended nature of real-world synthesis planning. We propose a new benchmarking framework for single-step retrosynthesis that evaluates both general-purpose and chemistry-specialized LLMs using ChemCensor, a novel metric for chemical plausibility. By emphasizing plausibility over exact match, this approach better aligns with human synthesis planning practices. We also introduce CREED, a novel dataset comprising millions of ChemCensor-validated reaction records for LLM training, and use it to train a model that improves over the LLM baselines under this benchmark.",
        "url": "http://arxiv.org/abs/2602.03554v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03554v1",
        "arxiv_id": "2602.03554v1",
        "authors": [
            "Bogdan Zagribelnyy",
            "Ivan Ilin",
            "Maksim Kuznetsov",
            "Nikita Bondarev",
            "Roman Schutski",
            "Thomas MacDougall",
            "Rim Shayakhmetov",
            "Zulfat Miftakhutdinov",
            "Mikolaj Mizera",
            "Vladimir Aladinskiy",
            "Alex Aliper",
            "Alex Zhavoronkov"
        ],
        "submitted": "2026-02-03 14:03:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models",
        "abstract": "Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious impact of uneven training resources, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of both models, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could profit from alternative decoding strategies beyond the standard left-to-right beam search. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.",
        "url": "http://arxiv.org/abs/2602.03551v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03551v1",
        "arxiv_id": "2602.03551v1",
        "authors": [
            "Vitalii Hirak",
            "Jaap Jumelet",
            "Arianna Bisazza"
        ],
        "submitted": "2026-02-03 14:02:06",
        "source": "arxiv",
        "comment": "19 pages, 11 figures, EACL 2026"
    },
    {
        "title": "SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue",
        "abstract": "Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.",
        "url": "http://arxiv.org/abs/2602.03548v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03548v1",
        "arxiv_id": "2602.03548v1",
        "authors": [
            "Yuqin Dai",
            "Ning Gao",
            "Wei Zhang",
            "Jie Wang",
            "Zichen Luo",
            "Jinpeng Wang",
            "Yujie Wang",
            "Ruiyuan Wu",
            "Chaozheng Wang"
        ],
        "submitted": "2026-02-03 14:01:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can Large Language Models Generalize Procedures Across Representations?",
        "abstract": "Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.",
        "url": "http://arxiv.org/abs/2602.03542v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03542v1",
        "arxiv_id": "2602.03542v1",
        "authors": [
            "Fangru Lin",
            "Valentin Hofmann",
            "Xingchen Wan",
            "Weixing Wang",
            "Zifeng Ding",
            "Anthony G. Cohn",
            "Janet B. Pierrehumbert"
        ],
        "submitted": "2026-02-03 13:56:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning to Reason Faithfully through Step-Level Faithfulness Maximization",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.",
        "url": "http://arxiv.org/abs/2602.03507v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03507v1",
        "arxiv_id": "2602.03507v1",
        "authors": [
            "Runquan Gui",
            "Yafu Li",
            "Xiaoye Qu",
            "Ziyan Liu",
            "Yeqiu Cheng",
            "Yu Cheng"
        ],
        "submitted": "2026-02-03 13:28:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance",
        "abstract": "Reasoning over table images remains challenging for Large Vision-Language Models (LVLMs) due to complex layouts and tightly coupled structure-content information. Existing solutions often depend on expensive supervised training, reinforcement learning, or external tools, limiting efficiency and scalability. This work addresses a key question: how to adapt LVLMs to table reasoning with minimal annotation and no external tools? Specifically, we first introduce DiSCo, a Disentangled Structure-Content alignment framework that explicitly separates structural abstraction from semantic grounding during multimodal alignment, efficiently adapting LVLMs to tables structures. Building on DiSCo, we further present Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs table reasoning via structured exploration and evidence-grounded inference. Extensive experiments across diverse benchmarks demonstrate that our framework efficiently enhances LVLM's table understanding and reasoning capabilities, particularly generalizing to unseen table structures.",
        "url": "http://arxiv.org/abs/2602.03491v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03491v1",
        "arxiv_id": "2602.03491v1",
        "authors": [
            "Yingjie Zhu",
            "Xuefeng Bai",
            "Kehai Chen",
            "Yang Xiang",
            "Youcheng Pan",
            "Xiaoqiang Zhou",
            "Min Zhang"
        ],
        "submitted": "2026-02-03 13:08:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning",
        "abstract": "Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.",
        "url": "http://arxiv.org/abs/2602.03485v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03485v1",
        "arxiv_id": "2602.03485v1",
        "authors": [
            "Quanyu Long",
            "Kai Jie Jiang",
            "Jianda Chen",
            "Xu Guo",
            "Leilei Gan",
            "Wenya Wang"
        ],
        "submitted": "2026-02-03 12:58:23",
        "source": "arxiv",
        "comment": "19 pages, 8 figures"
    },
    {
        "title": "Preferences for Idiomatic Language are Acquired Slowly -- and Forgotten Quickly: A Case Study on Swedish",
        "abstract": "In this study, we investigate how language models develop preferences for \\textit{idiomatic} as compared to \\textit{linguistically acceptable} Swedish, both during pretraining and when adapting a model from English to Swedish. To do so, we train models on Swedish from scratch and by fine-tuning English-pretrained models, probing their preferences at various checkpoints using minimal pairs that differ in linguistic acceptability or idiomaticity. For linguistic acceptability, we adapt existing benchmarks into a minimal-pair format. To assess idiomaticity, we introduce two novel datasets: one contrasting conventionalized idioms with plausible variants, and another contrasting idiomatic Swedish with Translationese. Our findings suggest that idiomatic competence emerges more slowly than other linguistic abilities, including grammatical and lexical correctness. While longer training yields diminishing returns for most tasks, idiom-related performance continues to improve, particularly in the largest model tested (8B). However, instruction tuning on data machine-translated from English -- the common approach for languages with little or no native instruction data -- causes models to rapidly lose their preference for idiomatic language.",
        "url": "http://arxiv.org/abs/2602.03484v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03484v1",
        "arxiv_id": "2602.03484v1",
        "authors": [
            "Jenny Kunz"
        ],
        "submitted": "2026-02-03 12:57:39",
        "source": "arxiv",
        "comment": "Accepted to TACL. Note that the arXiv version is a pre-MIT Press publication version"
    },
    {
        "title": "A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces",
        "abstract": "Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.",
        "url": "http://arxiv.org/abs/2602.03442v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03442v1",
        "arxiv_id": "2602.03442v1",
        "authors": [
            "Mingxuan Du",
            "Benfeng Xu",
            "Chiwei Zhu",
            "Shaohan Wang",
            "Pengyu Wang",
            "Xiaorui Wang",
            "Zhendong Mao"
        ],
        "submitted": "2026-02-03 12:07:21",
        "source": "arxiv",
        "comment": "18 pages, 8 figures"
    },
    {
        "title": "Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents",
        "abstract": "We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA's semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.",
        "url": "http://arxiv.org/abs/2602.03439v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03439v1",
        "arxiv_id": "2602.03439v1",
        "authors": [
            "Xiaochi Zhou",
            "Patrick Bulter",
            "Changxuan Yang",
            "Simon D. Rihm",
            "Thitikarn Angkanaporn",
            "Jethro Akroyd",
            "Sebastian Mosbach",
            "Markus Kraft"
        ],
        "submitted": "2026-02-03 12:03:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Failure is Feedback: History-Aware Backtracking for Agentic Traversal in Multimodal Graphs",
        "abstract": "Open-domain multimodal document retrieval aims to retrieve specific components (paragraphs, tables, or images) from large and interconnected document corpora. Existing graph-based retrieval approaches typically rely on a uniform similarity metric that overlooks hop-specific semantics, and their rigid pre-defined plans hinder dynamic error correction. These limitations suggest that a retriever should adapt its reasoning to the evolving context and recover intelligently from dead ends. To address these needs, we propose Failure is Feedback (FiF), which casts subgraph retrieval as a sequential decision process and introduces two key innovations. (i) We introduce a history-aware backtracking mechanism; unlike standard backtracking that simply reverts the state, our approach piggybacks on the context of failed traversals, leveraging insights from previous failures. (ii) We implement an economically-rational agentic workflow. Unlike conventional agents with static strategies, our orchestrator employs a cost-aware traversal method to dynamically manage the trade-off between retrieval accuracy and inference costs, escalating to intensive LLM-based reasoning only when the prior failure justifies the additional computational investment. Extensive experiments show that FiF achieves state-of-the-art retrieval on the benchmarks of MultimodalQA, MMCoQA and WebQA.",
        "url": "http://arxiv.org/abs/2602.03432v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03432v1",
        "arxiv_id": "2602.03432v1",
        "authors": [
            "Joohyung Yun",
            "Doyup Lee",
            "Wook-Shin Han"
        ],
        "submitted": "2026-02-03 11:54:38",
        "source": "arxiv",
        "comment": "Project page: https://failureisfeedback.github.io/"
    },
    {
        "title": "DiscoverLLM: From Executing Intents to Discovering Them",
        "abstract": "To handle ambiguous and open-ended requests, Large Language Models (LLMs) are increasingly trained to interact with users to surface intents they have not yet expressed (e.g., ask clarification questions). However, users are often ambiguous because they have not yet formed their intents: they must observe and explore outcomes to discover what they want. Simply asking \"what kind of tone do you want?\" fails when users themselves do not know. We introduce DiscoverLLM, a novel and generalizable framework that trains LLMs to help users form and discover their intents. Central to our approach is a novel user simulator that models cognitive state with a hierarchy of intents that progressively concretize as the model surfaces relevant options -- where the degree of concretization serves as a reward signal that models can be trained to optimize. Resulting models learn to collaborate with users by adaptively diverging (i.e., explore options) when intents are unclear, and converging (i.e., refine and implement) when intents concretize. Across proposed interactive benchmarks in creative writing, technical writing, and SVG drawing, DiscoverLLM achieves over 10% higher task performance while reducing conversation length by up to 40%. In a user study with 75 human participants, DiscoverLLM improved conversation satisfaction and efficiency compared to baselines.",
        "url": "http://arxiv.org/abs/2602.03429v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03429v1",
        "arxiv_id": "2602.03429v1",
        "authors": [
            "Tae Soo Kim",
            "Yoonjoo Lee",
            "Jaesang Yu",
            "John Joon Young Chung",
            "Juho Kim"
        ],
        "submitted": "2026-02-03 11:51:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RankSteer: Activation Steering for Pointwise LLM Ranking",
        "abstract": "Large language models (LLMs) have recently shown strong performance as zero-shot rankers, yet their effectiveness is highly sensitive to prompt formulation, particularly role-play instructions. Prior analyses suggest that role-related signals are encoded along activation channels that are largely separate from query-document representations, raising the possibility of steering ranking behavior directly at the activation level rather than through brittle prompt engineering. In this work, we propose RankSteer, a post-hoc activation steering framework for zero-shot pointwise LLM ranking. We characterize ranking behavior through three disentangled and steerable directions in representation space: a \\textbf{decision direction} that maps hidden states to relevance scores, an \\textbf{evidence direction} that captures relevance signals not directly exploited by the decision head, and a \\textbf{role direction} that modulates model behavior without injecting relevance information. Using projection-based interventions at inference time, RankSteer jointly controls these directions to calibrate ranking behavior without modifying model weights or introducing explicit cross-document comparisons. Experiments on TREC DL 20 and multiple BEIR benchmarks show that RankSteer consistently improves ranking quality using only a small number of anchor queries, demonstrating that substantial ranking capacity remains under-utilized in pointwise LLM rankers. We further provide a geometric analysis revealing that steering improves ranking by stabilizing ranking geometry and reducing dispersion, offering new insight into how LLMs internally represent and calibrate relevance judgments.",
        "url": "http://arxiv.org/abs/2602.03422v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03422v1",
        "arxiv_id": "2602.03422v1",
        "authors": [
            "Yumeng Wang",
            "Catherine Chen",
            "Suzan Verberne"
        ],
        "submitted": "2026-02-03 11:49:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
        "abstract": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
        "url": "http://arxiv.org/abs/2602.03419v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03419v1",
        "arxiv_id": "2602.03419v1",
        "authors": [
            "Shuang Sun",
            "Huatong Song",
            "Lisheng Huang",
            "Jinhao Jiang",
            "Ran Le",
            "Zhihao Lv",
            "Zongchao Chen",
            "Yiwen Hu",
            "Wenyang Luo",
            "Wayne Xin Zhao",
            "Yang Song",
            "Hongteng Xu",
            "Tao Zhang",
            "Ji-Rong Wen"
        ],
        "submitted": "2026-02-03 11:44:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding",
        "abstract": "While LLMs exhibit remarkable fluency, their utility is often compromised by factual hallucinations and a lack of traceable provenance. Existing resources for grounding mitigate this but typically enforce a dichotomy: they offer either structured knowledge without textual context (e.g., knowledge bases) or grounded text with limited scale and linguistic coverage. To bridge this gap, we introduce FactNet, a massive, open-source resource designed to unify 1.7 billion atomic assertions with 3.01 billion auditable evidence pointers derived exclusively from 316 Wikipedia editions. Unlike recent synthetic approaches, FactNet employs a strictly deterministic construction pipeline, ensuring that every evidence unit is recoverable with byte-level precision. Extensive auditing confirms a high grounding precision of 92.1%, even in long-tail languages. Furthermore, we establish FactNet-Bench, a comprehensive evaluation suite for Knowledge Graph Completion, Question Answering, and Fact Checking. FactNet provides the community with a foundational, reproducible resource for training and evaluating trustworthy, verifiable multilingual systems.",
        "url": "http://arxiv.org/abs/2602.03417v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03417v1",
        "arxiv_id": "2602.03417v1",
        "authors": [
            "Yingli Shen",
            "Wen Lai",
            "Jie Zhou",
            "Xueren Zhang",
            "Yudong Wang",
            "Kangyang Luo",
            "Shuo Wang",
            "Ge Gao",
            "Alexander Fraser",
            "Maosong Sun"
        ],
        "submitted": "2026-02-03 11:44:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation",
        "abstract": "Clothing recommendation extends beyond merely generating personalized outfits; it serves as a crucial medium for aesthetic guidance. However, existing methods predominantly rely on user-item-outfit interaction behaviors while overlooking explicit representations of clothing aesthetics. To bridge this gap, we present the AesRec benchmark dataset featuring systematic quantitative aesthetic annotations, thereby enabling the development of aesthetics-aligned recommendation systems. Grounded in professional apparel quality standards and fashion aesthetic principles, we define a multidimensional set of indicators. At the item level, six dimensions are independently assessed: silhouette, chromaticity, materiality, craftsmanship, wearability, and item-level impression. Transitioning to the outfit level, the evaluation retains the first five core attributes while introducing stylistic synergy, visual harmony, and outfit-level impression as distinct metrics to capture the collective aesthetic impact. Given the increasing human-like proficiency of Vision-Language Models in multimodal understanding and interaction, we leverage them for large-scale aesthetic scoring. We conduct rigorous human-machine consistency validation on a fashion dataset, confirming the reliability of the generated ratings. Experimental results based on AesRec further demonstrate that integrating quantified aesthetic information into clothing recommendation models can provide aesthetic guidance for users while fulfilling their personalized requirements.",
        "url": "http://arxiv.org/abs/2602.03416v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03416v1",
        "arxiv_id": "2602.03416v1",
        "authors": [
            "Wenxin Ye",
            "Lin Li",
            "Ming Li",
            "Yang Shen",
            "Kanghong Wang",
            "Jimmy Xiangji Huang"
        ],
        "submitted": "2026-02-03 11:44:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Verified Critical Step Optimization for LLM Agents",
        "abstract": "As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.",
        "url": "http://arxiv.org/abs/2602.03412v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03412v1",
        "arxiv_id": "2602.03412v1",
        "authors": [
            "Mukai Li",
            "Qingcheng Zeng",
            "Tianqing Fang",
            "Zhenwen Liang",
            "Linfeng Song",
            "Qi Liu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "submitted": "2026-02-03 11:41:02",
        "source": "arxiv",
        "comment": "Working in progress"
    },
    {
        "title": "SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training",
        "abstract": "In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.",
        "url": "http://arxiv.org/abs/2602.03411v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03411v1",
        "arxiv_id": "2602.03411v1",
        "authors": [
            "Huatong Song",
            "Lisheng Huang",
            "Shuang Sun",
            "Jinhao Jiang",
            "Ran Le",
            "Daixuan Cheng",
            "Guoxin Chen",
            "Yiwen Hu",
            "Zongchao Chen",
            "Wayne Xin Zhao",
            "Yang Song",
            "Tao Zhang",
            "Ji-Rong Wen"
        ],
        "submitted": "2026-02-03 11:38:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective",
        "abstract": "Proprietary large language models (LLMs) embody substantial economic value and are generally exposed only as black-box APIs, yet adversaries can still exploit their outputs to extract knowledge via distillation. Existing defenses focus exclusively on text-based distillation, leaving the important logit-based distillation largely unexplored. In this work, we analyze this problem and present an effective solution from an information-theoretic perspective. We characterize distillation-relevant information in teacher outputs using the conditional mutual information (CMI) between teacher logits and input queries conditioned on ground-truth labels. This quantity captures contextual information beneficial for model extraction, motivating us to defend distillation via CMI minimization. Guided by our theoretical analysis, we propose learning a transformation matrix that purifies the original outputs to enhance distillation resistance. We further derive a CMI-inspired anti-distillation objective to optimize this transformation, which effectively removes distillation-relevant information while preserving output utility. Extensive experiments across multiple LLMs and strong distillation algorithms demonstrate that the proposed method significantly degrades distillation performance while preserving task accuracy, effectively protecting models' intellectual property.",
        "url": "http://arxiv.org/abs/2602.03396v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03396v1",
        "arxiv_id": "2602.03396v1",
        "authors": [
            "Hao Fang",
            "Tianyi Zhang",
            "Tianqu Zhuang",
            "Jiawei Kong",
            "Kuofeng Gao",
            "Bin Chen",
            "Leqi Liang",
            "Shu-Tao Xia",
            "Ke Xu"
        ],
        "submitted": "2026-02-03 11:16:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain",
        "abstract": "While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.",
        "url": "http://arxiv.org/abs/2602.03368v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03368v1",
        "arxiv_id": "2602.03368v1",
        "authors": [
            "Wei Zhu"
        ],
        "submitted": "2026-02-03 10:37:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling",
        "abstract": "Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at https://github.com/ningding-o/MeKi.",
        "url": "http://arxiv.org/abs/2602.03359v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03359v1",
        "arxiv_id": "2602.03359v1",
        "authors": [
            "Ning Ding",
            "Fangcheng Liu",
            "Kyungrae Kim",
            "Linji Hao",
            "Kyeng-Hun Lee",
            "Hyeonmok Ko",
            "Yehui Tang"
        ],
        "submitted": "2026-02-03 10:32:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GFlowPO: Generative Flow Network as a Language Model Prompt Optimizer",
        "abstract": "Finding effective prompts for language models (LMs) is critical yet notoriously difficult: the prompt space is combinatorially large, rewards are sparse due to expensive target-LM evaluation. Yet, existing RL-based prompt optimizers often rely on on-policy updates and a meta-prompt sampled from a fixed distribution, leading to poor sample efficiency. We propose GFlowPO, a probabilistic prompt optimization framework that casts prompt search as a posterior inference problem over latent prompts regularized by a meta-prompted reference-LM prior. In the first step, we fine-tune a lightweight prompt-LM with an off-policy Generative Flow Network (GFlowNet) objective, using a replay-based training policy that reuses past prompt evaluations to enable sample-efficient exploration. In the second step, we introduce Dynamic Memory Update (DMU), a training-free mechanism that updates the meta-prompt by injecting both (i) diverse prompts from a replay buffer and (ii) top-performing prompts from a small priority queue, thereby progressively concentrating the search process on high-reward regions. Across few-shot text classification, instruction induction benchmarks, and question answering tasks, GFlowPO consistently outperforms recent discrete prompt optimization baselines.",
        "url": "http://arxiv.org/abs/2602.03358v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03358v1",
        "arxiv_id": "2602.03358v1",
        "authors": [
            "Junmo Cho",
            "Suhan Kim",
            "Sangjune An",
            "Minsu Kim",
            "Dong Bok Lee",
            "Heejun Lee",
            "Sung Ju Hwang",
            "Hae Beom Lee"
        ],
        "submitted": "2026-02-03 10:30:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \\textbf{PEGRL}, a \\textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\\to$Finnish, English$\\to$Turkish, and English$\\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).",
        "url": "http://arxiv.org/abs/2602.03352v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03352v1",
        "arxiv_id": "2602.03352v1",
        "authors": [
            "Yunzhi Shen",
            "Hao Zhou",
            "Xin Huang",
            "Xue Han",
            "Junlan Feng",
            "Shujian Huang"
        ],
        "submitted": "2026-02-03 10:22:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Exposure: Optimizing Ranking Fairness with Non-linear Time-Income Functions",
        "abstract": "Ranking is central to information distribution in web search and recommendation. Nowadays, in ranking optimization, the fairness to item providers is viewed as a crucial factor alongside ranking relevance for users. There are currently numerous concepts of fairness and one widely recognized fairness concept is Exposure Fairness. However, it relies primarily on exposure determined solely by position, overlooking other factors that significantly influence income, such as time. To address this limitation, we propose to study ranking fairness when the provider utility is influenced by other contextual factors and is neither equal to nor proportional to item exposure. We give a formal definition of Income Fairness and develop a corresponding measurement metric. Simulated experiments show that existing-exposure-fairness-based ranking algorithms fail to optimize the proposed income fairness. Therefore, we propose the Dynamic-Income-Derivative-aware Ranking Fairness algorithm, which, based on the marginal income gain at the present timestep, uses Taylor-expansion-based gradients to simultaneously optimize effectiveness and income fairness. In both offline and online settings with diverse time-income functions, DIDRF consistently outperforms state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2602.03345v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03345v1",
        "arxiv_id": "2602.03345v1",
        "authors": [
            "Xuancheng Li",
            "Tao Yang",
            "Yujia Zhou",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "submitted": "2026-02-03 10:11:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Robustness as an Emergent Property of Task Performance",
        "abstract": "Robustness is often regarded as a critical future challenge for real-world applications, where stability is essential. However, as models often learn tasks in a similar order, we hypothesize that easier tasks will be easier regardless of how they are presented to the model. Indeed, in this paper, we show that as models approach high performance on a task, robustness is effectively achieved. Through an empirical analysis of multiple models across diverse datasets and configurations (e.g., paraphrases, different temperatures), we find a strong positive correlation. Moreover, we find that robustness is primarily driven by task-specific competence rather than inherent model-level properties, challenging current approaches that treat robustness as an independent capability. Thus, from a high-level perspective, we may expect that as new tasks saturate, model robustness on these tasks will emerge accordingly. For researchers, this implies that explicit efforts to measure and improve robustness may warrant reduced emphasis, as such robustness is likely to develop alongside performance gains. For practitioners, it acts as a sign that indeed the tasks that the literature deals with are unreliable, but on easier past tasks, the models are reliable and ready for real-world deployment.",
        "url": "http://arxiv.org/abs/2602.03344v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03344v1",
        "arxiv_id": "2602.03344v1",
        "authors": [
            "Shir Ashury-Tahan",
            "Ariel Gera",
            "Elron Bandel",
            "Michal Shmueli-Scheuer",
            "Leshem Choshen"
        ],
        "submitted": "2026-02-03 10:10:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention",
        "abstract": "Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.\n  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.",
        "url": "http://arxiv.org/abs/2602.03338v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03338v1",
        "arxiv_id": "2602.03338v1",
        "authors": [
            "Rakshith Vasudev",
            "Melisa Russak",
            "Dan Bikel",
            "Waseem Alshikh"
        ],
        "submitted": "2026-02-03 10:02:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SCASRec: A Self-Correcting and Auto-Stopping Model for Generative Route List Recommendation",
        "abstract": "Route recommendation systems commonly adopt a multi-stage pipeline involving fine-ranking and re-ranking to produce high-quality ordered recommendations. However, this paradigm faces three critical limitations. First, there is a misalignment between offline training objectives and online metrics. Offline gains do not necessarily translate to online improvements. Actual performance must be validated through A/B testing, which may potentially compromise the user experience. Second, redundancy elimination relies on rigid, handcrafted rules that lack adaptability to the high variance in user intent and the unstructured complexity of real-world scenarios. Third, the strict separation between fine-ranking and re-ranking stages leads to sub-optimal performance. Since each module is optimized in isolation, the fine-ranking stage remains oblivious to the list-level objectives (e.g., diversity) targeted by the re-ranker, thereby preventing the system from achieving a jointly optimized global optimum. To overcome these intertwined challenges, we propose SCASRec (Self-Correcting and Auto-Stopping Recommendation), a unified generative framework that integrates ranking and redundancy elimination into a single end-to-end process. SCASRec introduces a stepwise corrective reward (SCR) to guide list-wise refinement by focusing on hard samples, and employs a learnable End-of-Recommendation (EOR) token to terminate generation adaptively when no further improvement is expected. Experiments on two large-scale, open-sourced route recommendation datasets demonstrate that SCASRec establishes an SOTA in offline and online settings. SCASRec has been fully deployed in a real-world navigation app, demonstrating its effectiveness.",
        "url": "http://arxiv.org/abs/2602.03324v2",
        "pdf_url": "https://arxiv.org/pdf/2602.03324v2",
        "arxiv_id": "2602.03324v2",
        "authors": [
            "Chao Chen",
            "Longfei Xu",
            "Daohan Su",
            "Tengfei Liu",
            "Hanyu Guo",
            "Yihai Duan",
            "Kaikui Liu",
            "Xiangxiang Chu"
        ],
        "submitted": "2026-02-03 09:51:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research",
        "abstract": "Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.",
        "url": "http://arxiv.org/abs/2602.03318v2",
        "pdf_url": "https://arxiv.org/pdf/2602.03318v2",
        "arxiv_id": "2602.03318v2",
        "authors": [
            "Yifan Shi",
            "Jialong Shi",
            "Jiayi Wang",
            "Ye Fan",
            "Jianyong Sun"
        ],
        "submitted": "2026-02-03 09:46:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning to Select: Query-Aware Adaptive Dimension Selection for Dense Retrieval",
        "abstract": "Dense retrieval represents queries and docu-002 ments as high-dimensional embeddings, but003 these representations can be redundant at the004 query level: for a given information need, only005 a subset of dimensions is consistently help-006 ful for ranking. Prior work addresses this via007 pseudo-relevance feedback (PRF) based dimen-008 sion importance estimation, which can produce009 query-aware masks without labeled data but010 often relies on noisy pseudo signals and heuris-011 tic test-time procedures. In contrast, super-012 vised adapter methods leverage relevance labels013 to improve embedding quality, yet they learn014 global transformations shared across queries015 and do not explicitly model query-aware di-016 mension importance. We propose a Query-017 Aware Adaptive Dimension Selection frame-018 work that learns to predict per-dimension im-019 portance directly from query embedding. We020 first construct oracle dimension importance dis-021 tributions over embedding dimensions using022 supervised relevance labels, and then train a023 predictor to map a query embedding to these024 label-distilled importance scores. At inference,025 the predictor selects a query-aware subset of026 dimensions for similarity computation based027 solely on the query embedding, without pseudo-028 relevance feedback. Experiments across multi-029 ple dense retrievers and benchmarks show that030 our learned dimension selector improves re-031 trieval effectiveness over the full-dimensional032 baseline as well as PRF-based masking and033 supervised adapter baselines.",
        "url": "http://arxiv.org/abs/2602.03306v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03306v1",
        "arxiv_id": "2602.03306v1",
        "authors": [
            "Zhanyu Wu",
            "Richong Zhang",
            "Zhijie Nie"
        ],
        "submitted": "2026-02-03 09:32:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "To Search or Not to Search: Aligning the Decision Boundary of Deep Search Agents via Causal Intervention",
        "abstract": "Deep search agents, which autonomously iterate through multi-turn web-based reasoning, represent a promising paradigm for complex information-seeking tasks. However, current agents suffer from critical inefficiency: they conduct excessive searches as they cannot accurately judge when to stop searching and start answering. This stems from outcome-centric training that prioritize final results over the search process itself. We identify the root cause as misaligned decision boundaries, the threshold determining when accumulated information suffices to answer. This causes over-search (redundant searching despite sufficient knowledge) and under-search (premature termination yielding incorrect answers). To address these errors, we propose a comprehensive framework comprising two key components. First, we introduce causal intervention-based diagnosis that identifies boundary errors by comparing factual and counterfactual trajectories at each decision point. Second, we develop Decision Boundary Alignment for Deep Search agents (DAS), which constructs preference datasets from causal feedback and aligns policies via preference optimization. Experiments on public datasets demonstrate that decision boundary errors are pervasive across state-of-the-art agents. Our DAS method effectively calibrates these boundaries, mitigating both over-search and under-search to achieve substantial gains in accuracy and efficiency. Our code and data are publicly available at: https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS.",
        "url": "http://arxiv.org/abs/2602.03304v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03304v1",
        "arxiv_id": "2602.03304v1",
        "authors": [
            "Wenlin Zhang",
            "Kuicai Dong",
            "Junyi Li",
            "Yingyi Zhang",
            "Xiaopeng Li",
            "Pengyue Jia",
            "Yi Wen",
            "Derong Xu",
            "Maolin Wang",
            "Yichao Wang",
            "Yong Liu",
            "Xiangyu Zhao"
        ],
        "submitted": "2026-02-03 09:29:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?",
        "abstract": "In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.",
        "url": "http://arxiv.org/abs/2602.03300v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03300v1",
        "arxiv_id": "2602.03300v1",
        "authors": [
            "Jingyi Zhang",
            "Tianyi Lin",
            "Huanjin Yao",
            "Xiang Lan",
            "Shunyu Liu",
            "Jiaxing Huang"
        ],
        "submitted": "2026-02-03 09:26:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "POP: Prefill-Only Pruning for Efficient Large Model Inference",
        "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.",
        "url": "http://arxiv.org/abs/2602.03295v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03295v1",
        "arxiv_id": "2602.03295v1",
        "authors": [
            "Junhui He",
            "Zhihui Fu",
            "Jun Wang",
            "Qingan Li"
        ],
        "submitted": "2026-02-03 09:22:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mići Princ -- A Little Boy Teaching Speech Technologies the Chakavian Dialect",
        "abstract": "This paper documents our efforts in releasing the printed and audio book of the translation of the famous novel The Little Prince into the Chakavian dialect, as a computer-readable, AI-ready dataset, with the textual and the audio components of the two releases now aligned on the level of each written and spoken word. Our motivation for working on this release is multiple. The first one is our wish to preserve the highly valuable and specific content beyond the small editions of the printed and the audio book. With the dataset published in the CLARIN.SI repository, this content is from now on at the fingertips of any interested individual. The second motivation is to make the data available for various artificial-intelligence-related usage scenarios, such as the one we follow upon inside this paper already -- adapting the Whisper-large-v3 open automatic speech recognition model, with decent performance on standard Croatian, to Chakavian dialectal speech. We can happily report that with adapting the model, the word error rate on the selected test data has being reduced to a half, while we managed to remove up to two thirds of the error on character level. We envision many more usages of this dataset beyond the set of experiments we have already performed, both on tasks of artificial intelligence research and application, as well as dialectal research. The third motivation for this release is our hope that this, now highly structured dataset, will be transformed into a digital online edition of this work, allowing individuals beyond the research and technology communities to enjoy the beauty of the message of the little boy in the desert, told through the spectacular prism of the Chakavian dialect.",
        "url": "http://arxiv.org/abs/2602.03245v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03245v1",
        "arxiv_id": "2602.03245v1",
        "authors": [
            "Nikola Ljubešić",
            "Peter Rupnik",
            "Tea Perinčić"
        ],
        "submitted": "2026-02-03 08:24:58",
        "source": "arxiv",
        "comment": "2 figures, 14 pages, accepted and presented at JTDH 2024"
    },
    {
        "title": "Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations",
        "abstract": "The escalating scale of Large Language Models (LLMs) necessitates efficient adaptation techniques. Model merging has gained prominence for its efficiency and controllability. However, existing merging techniques typically serve as post-hoc refinements or focus on mitigating task interference, often failing to capture the dynamic optimization benefits of supervised fine-tuning (SFT). In this work, we propose Streaming Merging, an innovative model updating paradigm that conceptualizes merging as an iterative optimization process. Central to this paradigm is \\textbf{ARM} (\\textbf{A}ctivation-guided \\textbf{R}otation-aware \\textbf{M}erging), a strategy designed to approximate gradient descent dynamics. By treating merging coefficients as learning rates and deriving rotation vectors from activation subspaces, ARM effectively steers parameter updates along data-driven trajectories. Unlike conventional linear interpolation, ARM aligns semantic subspaces to preserve the geometric structure of high-dimensional parameter evolution. Remarkably, ARM requires only early SFT checkpoints and, through iterative merging, surpasses the fully converged SFT model. Experimental results across model scales (1.7B to 14B) and diverse domains (e.g., math, code) demonstrate that ARM can transcend converged checkpoints. Extensive experiments show that ARM provides a scalable and lightweight framework for efficient model adaptation.",
        "url": "http://arxiv.org/abs/2602.03237v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03237v1",
        "arxiv_id": "2602.03237v1",
        "authors": [
            "Yuxuan Yao",
            "Haonan Sheng",
            "Qingsong Lv",
            "Han Wu",
            "Shuqi Liu",
            "Zehua Liu",
            "Zengyan Liu",
            "Jiahui Gao",
            "Haochen Tan",
            "Xiaojin Fu",
            "Haoli Bai",
            "Hing Cheung So",
            "Zhijiang Guo",
            "Linqi Song"
        ],
        "submitted": "2026-02-03 08:15:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ATACompressor: Adaptive Task-Aware Compression for Efficient Long-Context Processing in LLMs",
        "abstract": "Long-context inputs in large language models (LLMs) often suffer from the \"lost in the middle\" problem, where critical information becomes diluted or ignored due to excessive length. Context compression methods aim to address this by reducing input size, but existing approaches struggle with balancing information preservation and compression efficiency. We propose Adaptive Task-Aware Compressor (ATACompressor), which dynamically adjusts compression based on the specific requirements of the task. ATACompressor employs a selective encoder that compresses only the task-relevant portions of long contexts, ensuring that essential information is preserved while reducing unnecessary content. Its adaptive allocation controller perceives the length of relevant content and adjusts the compression rate accordingly, optimizing resource utilization. We evaluate ATACompressor on three QA datasets: HotpotQA, MSMARCO, and SQUAD-showing that it outperforms existing methods in terms of both compression efficiency and task performance. Our approach provides a scalable solution for long-context processing in LLMs. Furthermore, we perform a range of ablation studies and analysis experiments to gain deeper insights into the key components of ATACompressor.",
        "url": "http://arxiv.org/abs/2602.03226v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03226v1",
        "arxiv_id": "2602.03226v1",
        "authors": [
            "Xuancheng Li",
            "Haitao Li",
            "Yujia Zhou",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "submitted": "2026-02-03 07:53:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Distribution-Aware End-to-End Embedding for Streaming Numerical Features in Click-Through Rate Prediction",
        "abstract": "This paper explores effective numerical feature embedding for Click-Through Rate prediction in streaming environments. Conventional static binning methods rely on offline statistics of numerical distributions; however, this inherently two-stage process often triggers semantic drift during bin boundary updates. While neural embedding methods enable end-to-end learning, they often discard explicit distributional information. Integrating such information end-to-end is challenging because streaming features often violate the i.i.d. assumption, precluding unbiased estimation of the population distribution via the expectation of order statistics. Furthermore, the critical context dependency of numerical distributions is often neglected. To this end, we propose DAES, an end-to-end framework designed to tackle numerical feature embedding in streaming training scenarios by integrating distributional information with an adaptive modulation mechanism. Specifically, we introduce an efficient reservoir-sampling-based distribution estimation method and two field-aware distribution modulation strategies to capture streaming distributions and field-dependent semantics. DAES significantly outperforms existing approaches as demonstrated by extensive offline and online experiments and has been fully deployed on a leading short-video platform with hundreds of millions of daily active users.",
        "url": "http://arxiv.org/abs/2602.03223v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03223v1",
        "arxiv_id": "2602.03223v1",
        "authors": [
            "Jiahao Liu",
            "Hongji Ruan",
            "Weimin Zhang",
            "Ziye Tong",
            "Derick Tang",
            "Zhanpeng Zeng",
            "Qinsong Zeng",
            "Peng Zhang",
            "Tun Lu",
            "Ning Gu"
        ],
        "submitted": "2026-02-03 07:50:54",
        "source": "arxiv",
        "comment": "Under review"
    },
    {
        "title": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection",
        "abstract": "The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.",
        "url": "http://arxiv.org/abs/2602.03216v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03216v1",
        "arxiv_id": "2602.03216v1",
        "authors": [
            "Dongwon Jo",
            "Beomseok Kang",
            "Jiwon Song",
            "Jae-Joon Kim"
        ],
        "submitted": "2026-02-03 07:31:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution",
        "abstract": "Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.",
        "url": "http://arxiv.org/abs/2602.03203v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03203v1",
        "arxiv_id": "2602.03203v1",
        "authors": [
            "Zican Dong",
            "Peiyu Liu",
            "Junyi Li",
            "Zhipeng Chen",
            "Han Peng",
            "Shuo Wang",
            "Wayne Xin Zhao"
        ],
        "submitted": "2026-02-03 07:16:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning",
        "abstract": "Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at https://github.com/wenquanlu/prompt-augmentation-GRPO.",
        "url": "http://arxiv.org/abs/2602.03190v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03190v1",
        "arxiv_id": "2602.03190v1",
        "authors": [
            "Wenquan Lu",
            "Hai Huang",
            "Randall Balestriero"
        ],
        "submitted": "2026-02-03 06:59:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference",
        "abstract": "Although Key-Value (KV) Cache is essential for efficient large language models (LLMs) inference, its growing memory footprint in long-context scenarios poses a significant bottleneck, making KVCache compression crucial. Current compression methods rely on rigid splitting strategies, such as fixed intervals or pre-defined delimiters. We observe that rigid splitting suffers from significant accuracy degradation (ranging from 5.5% to 55.1%) across different scenarios, owing to the scenario-dependent nature of the semantic boundaries. This highlights the necessity of dynamic semantic splitting to match semantics. To achieve this, we face two challenges. (1) Improper delimiter selection misaligns semantics with the KVCache, resulting in 28.6% accuracy loss. (2) Variable-length blocks after splitting introduce over 73.1% additional inference overhead. To address the above challenges, we propose DynSplit-KV, a KVCache compression method that dynamically identifies delimiters for splitting. We propose: (1) a dynamic importance-aware delimiter selection strategy, improving accuracy by 49.9%. (2) A uniform mapping strategy that transforms variable-length semantic blocks into a fixed-length format, reducing inference overhead by 4.9x. Experiments show that DynSplit-KV achieves the highest accuracy, 2.2x speedup compared with FlashAttention and 2.6x peak memory reduction in long-context scenarios.",
        "url": "http://arxiv.org/abs/2602.03184v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03184v1",
        "arxiv_id": "2602.03184v1",
        "authors": [
            "Jiancai Ye",
            "Jun Liu",
            "Qingchen Li",
            "Tianlang Zhao",
            "Hanbin Zhang",
            "Jiayi Pan",
            "Ningyi Xu",
            "Guohao Dai"
        ],
        "submitted": "2026-02-03 06:54:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Privasis: Synthesizing the Largest \"Public\" Private Dataset from Scratch",
        "abstract": "Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.",
        "url": "http://arxiv.org/abs/2602.03183v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03183v1",
        "arxiv_id": "2602.03183v1",
        "authors": [
            "Hyunwoo Kim",
            "Niloofar Mireshghallah",
            "Michael Duan",
            "Rui Xin",
            "Shuyue Stella Li",
            "Jaehun Jung",
            "David Acuna",
            "Qi Pang",
            "Hanshen Xiao",
            "G. Edward Suh",
            "Sewoong Oh",
            "Yulia Tsvetkov",
            "Pang Wei Koh",
            "Yejin Choi"
        ],
        "submitted": "2026-02-03 06:54:46",
        "source": "arxiv",
        "comment": "For code and data, see https://privasis.github.io"
    },
    {
        "title": "VALUEFLOW: Toward Pluralistic and Steerable Value-based Alignment in Large Language Models",
        "abstract": "Aligning Large Language Models (LLMs) with the diverse spectrum of human values remains a central challenge: preference-based methods often fail to capture deeper motivational principles. Value-based approaches offer a more principled path, yet three gaps persist: extraction often ignores hierarchical structure, evaluation detects presence but not calibrated intensity, and the steerability of LLMs at controlled intensities remains insufficiently understood. To address these limitations, we introduce VALUEFLOW, the first unified framework that spans extraction, evaluation, and steering with calibrated intensity control. The framework integrates three components: (i) HIVES, a hierarchical value embedding space that captures intra- and cross-theory value structure; (ii) the Value Intensity DataBase (VIDB), a large-scale resource of value-labeled texts with intensity estimates derived from ranking-based aggregation; and (iii) an anchor-based evaluator that produces consistent intensity scores for model outputs by ranking them against VIDB panels. Using VALUEFLOW, we conduct a comprehensive large-scale study across ten models and four value theories, identifying asymmetries in steerability and composition laws for multi-value control. This paper establishes a scalable infrastructure for evaluating and controlling value intensity, advancing pluralistic alignment of LLMs.",
        "url": "http://arxiv.org/abs/2602.03160v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03160v1",
        "arxiv_id": "2602.03160v1",
        "authors": [
            "Woojin Kim",
            "Sieun Hyeon",
            "Jusang Oh",
            "Jaeyoung Do"
        ],
        "submitted": "2026-02-03 06:19:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PAMAS: Self-Adaptive Multi-Agent System with Perspective Aggregation for Misinformation Detection",
        "abstract": "Misinformation on social media poses a critical threat to information credibility, as its diverse and context-dependent nature complicates detection. Large language model-empowered multi-agent systems (MAS) present a promising paradigm that enables cooperative reasoning and collective intelligence to combat this threat. However, conventional MAS suffer from an information-drowning problem, where abundant truthful content overwhelms sparse and weak deceptive cues. With full input access, agents tend to focus on dominant patterns, and inter-agent communication further amplifies this bias. To tackle this issue, we propose PAMAS, a multi-agent framework with perspective aggregation, which employs hierarchical, perspective-aware aggregation to highlight anomaly cues and alleviate information drowning. PAMAS organizes agents into three roles: Auditors, Coordinators, and a Decision-Maker. Auditors capture anomaly cues from specialized feature subsets; Coordinators aggregate their perspectives to enhance coverage while maintaining diversity; and the Decision-Maker, equipped with evolving memory and full contextual access, synthesizes all subordinate insights to produce the final judgment. Furthermore, to improve efficiency in multi-agent collaboration, PAMAS incorporates self-adaptive mechanisms for dynamic topology optimization and routing-based inference, enhancing both efficiency and scalability. Extensive experiments on multiple benchmark datasets demonstrate that PAMAS achieves superior accuracy and efficiency, offering a scalable and trustworthy way for misinformation detection.",
        "url": "http://arxiv.org/abs/2602.03158v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03158v1",
        "arxiv_id": "2602.03158v1",
        "authors": [
            "Zongwei Wang",
            "Min Gao",
            "Junliang Yu",
            "Tong Chen",
            "Chenghua Lin"
        ],
        "submitted": "2026-02-03 06:18:39",
        "source": "arxiv",
        "comment": "12 pages"
    },
    {
        "title": "FASA: Frequency-aware Sparse Attention",
        "abstract": "The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of \"dominant\" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\\% of full-KV performance when only keeping 256 tokens, and achieves 2.56$\\times$ speedup using just 18.9\\% of the cache on AIME24.",
        "url": "http://arxiv.org/abs/2602.03152v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03152v1",
        "arxiv_id": "2602.03152v1",
        "authors": [
            "Yifei Wang",
            "Yueqi Wang",
            "Zhenrui Yue",
            "Huimin Zeng",
            "Yong Wang",
            "Ismini Lourentzou",
            "Zhengzhong Tu",
            "Xiangxiang Chu",
            "Julian McAuley"
        ],
        "submitted": "2026-02-03 06:09:06",
        "source": "arxiv",
        "comment": "Accepted by ICLR 2026"
    },
    {
        "title": "Self-Hinting Language Models Enhance Reinforcement Learning",
        "abstract": "Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $τ$ conditioned on $(x,h)$. Crucially, the task reward $R(x,τ)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at https://github.com/BaohaoLiao/SAGE.",
        "url": "http://arxiv.org/abs/2602.03143v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03143v1",
        "arxiv_id": "2602.03143v1",
        "authors": [
            "Baohao Liao",
            "Hanze Dong",
            "Xinxing Xu",
            "Christof Monz",
            "Jiang Bian"
        ],
        "submitted": "2026-02-03 05:56:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Short Chains, Deep Thoughts: Balancing Reasoning Efficiency and Intra-Segment Capability via Split-Merge Optimization",
        "abstract": "While Large Reasoning Models (LRMs) have demonstrated impressive capabilities in solving complex tasks through the generation of long reasoning chains, this reliance on verbose generation results in significant latency and computational overhead. To address these challenges, we propose \\textbf{CoSMo} (\\textbf{Co}nsistency-Guided \\textbf{S}plit-\\textbf{M}erge \\textbf{O}ptimization), a framework designed to eliminate structural redundancy rather than indiscriminately restricting token volume. Specifically, CoSMo utilizes a split-merge algorithm that dynamically refines reasoning chains by merging redundant segments and splitting logical gaps to ensure coherence. We then employ structure-aligned reinforcement learning with a novel segment-level budget to supervise the model in maintaining efficient reasoning structures throughout training. Extensive experiments across multiple benchmarks and backbones demonstrate that CoSMo achieves superior performance, improving accuracy by \\textbf{3.3} points while reducing segment usage by \\textbf{28.7\\%} on average compared to reasoning efficiency baselines.",
        "url": "http://arxiv.org/abs/2602.03141v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03141v1",
        "arxiv_id": "2602.03141v1",
        "authors": [
            "Runquan Gui",
            "Jie Wang",
            "Zhihai Wang",
            "Chi Ma",
            "Jianye Hao",
            "Feng Wu"
        ],
        "submitted": "2026-02-03 05:54:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "One Model, All Roles: Multi-Turn, Multi-Agent Self-Play Reinforcement Learning for Conversational Social Intelligence",
        "abstract": "This paper introduces OMAR: One Model, All Roles, a reinforcement learning framework that enables AI to develop social intelligence through multi-turn, multi-agent conversational self-play. Unlike traditional paradigms that rely on static, single-turn optimizations, OMAR allows a single model to role-play all participants in a conversation simultaneously, learning to achieve long-term goals and complex social norms directly from dynamic social interaction. To ensure training stability across long dialogues, we implement a hierarchical advantage estimation that calculates turn-level and token-level advantages. Evaluations in the SOTOPIA social environment and Werewolf strategy games show that our trained models develop fine-grained, emergent social intelligence, such as empathy, persuasion, and compromise seeking, demonstrating the effectiveness of learning collaboration even under competitive scenarios. While we identify practical challenges like reward hacking, our results show that rich social intelligence can emerge without human supervision. We hope this work incentivizes further research on AI social intelligence in group conversations.",
        "url": "http://arxiv.org/abs/2602.03109v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03109v1",
        "arxiv_id": "2602.03109v1",
        "authors": [
            "Bowen Jiang",
            "Taiwei Shi",
            "Ryo Kamoi",
            "Yuan Yuan",
            "Camillo J. Taylor",
            "Longqi Yang",
            "Pei Zhou",
            "Sihao Chen"
        ],
        "submitted": "2026-02-03 05:09:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ChemPro: A Progressive Chemistry Benchmark for Large Language Models",
        "abstract": "We introduce ChemPro, a progressive benchmark with 4100 natural language question-answer pairs in Chemistry, across 4 coherent sections of difficulty designed to assess the proficiency of Large Language Models (LLMs) in a broad spectrum of general chemistry topics. We include Multiple Choice Questions and Numerical Questions spread across fine-grained information recall, long-horizon reasoning, multi-concept questions, problem-solving with nuanced articulation, and straightforward questions in a balanced ratio, effectively covering Bio-Chemistry, Inorganic-Chemistry, Organic-Chemistry and Physical-Chemistry. ChemPro is carefully designed analogous to a student's academic evaluation for basic to high-school chemistry. A gradual increase in the question difficulty rigorously tests the ability of LLMs to progress from solving basic problems to solving more sophisticated challenges.\n  We evaluate 45+7 state-of-the-art LLMs, spanning both open-source and proprietary variants, and our analysis reveals that while LLMs perform well on basic chemistry questions, their accuracy declines with different types and levels of complexity. These findings highlight the critical limitations of LLMs in general scientific reasoning and understanding and point towards understudied dimensions of difficulty, emphasizing the need for more robust methodologies to improve LLMs.",
        "url": "http://arxiv.org/abs/2602.03108v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03108v1",
        "arxiv_id": "2602.03108v1",
        "authors": [
            "Aaditya Baranwal",
            "Shruti Vyas"
        ],
        "submitted": "2026-02-03 05:08:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models",
        "abstract": "From a pragmatic perspective, this study systematically evaluates the differences in performance among representative large language models (LLMs) in recognizing politeness, impoliteness, and mock politeness phenomena in Chinese. Addressing the existing gaps in pragmatic comprehension, the research adopts the frameworks of Rapport Management Theory and the Model of Mock Politeness to construct a three-category dataset combining authentic and simulated Chinese discourse. Six representative models, including GPT-5.1 and DeepSeek, were selected as test subjects and evaluated under four prompting conditions: zero-shot, few-shot, knowledge-enhanced, and hybrid strategies. This study serves as a meaningful attempt within the paradigm of ``Great Linguistics,'' offering a novel approach to applying pragmatic theory in the age of technological transformation. It also responds to the contemporary question of how technology and the humanities may coexist, representing an interdisciplinary endeavor that bridges linguistic technology and humanistic reflection.",
        "url": "http://arxiv.org/abs/2602.03107v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03107v1",
        "arxiv_id": "2602.03107v1",
        "authors": [
            "Yitong Zhang",
            "Yuhan Xiang",
            "Mingxuan Liu"
        ],
        "submitted": "2026-02-03 05:07:25",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision",
        "abstract": "Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \\emph{does the instruction uniquely determine the target output?}\n  We propose the \\textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \\textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\\textsc{Alpaca}, \\textsc{Dolly-15k}, \\textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.",
        "url": "http://arxiv.org/abs/2602.03103v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03103v1",
        "arxiv_id": "2602.03103v1",
        "authors": [
            "Pritam Kadasi",
            "Abhishek Upperwal",
            "Mayank Singh"
        ],
        "submitted": "2026-02-03 04:57:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Test-time Recursive Thinking: Self-Improvement without External Feedback",
        "abstract": "Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.",
        "url": "http://arxiv.org/abs/2602.03094v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03094v1",
        "arxiv_id": "2602.03094v1",
        "authors": [
            "Yufan Zhuang",
            "Chandan Singh",
            "Liyuan Liu",
            "Yelong Shen",
            "Dinghuai Zhang",
            "Jingbo Shang",
            "Jianfeng Gao",
            "Weizhu Chen"
        ],
        "submitted": "2026-02-03 04:37:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback",
        "abstract": "Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \\underline{A}utonomous \\underline{E}volutionary \\underline{R}easoning \\underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \\textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\\% on Qwen3-4B-Base and 5.10\\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.",
        "url": "http://arxiv.org/abs/2602.03084v2",
        "pdf_url": "https://arxiv.org/pdf/2602.03084v2",
        "arxiv_id": "2602.03084v2",
        "authors": [
            "Zhitao Gao",
            "Jie Ma",
            "Xuhong Li",
            "Pengyu Li",
            "Ning Qu",
            "Yaqiang Wu",
            "Hui Liu",
            "Jun Liu"
        ],
        "submitted": "2026-02-03 04:14:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution",
        "abstract": "Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.",
        "url": "http://arxiv.org/abs/2602.03075v1",
        "pdf_url": "https://arxiv.org/pdf/2602.03075v1",
        "arxiv_id": "2602.03075v1",
        "authors": [
            "Junjie Huang",
            "Jiarui Qin",
            "Di Yin",
            "Weiwen Liu",
            "Yong Yu",
            "Xing Sun",
            "Weinan Zhang"
        ],
        "submitted": "2026-02-03 04:04:41",
        "source": "arxiv",
        "comment": "25 pages"
    }
]