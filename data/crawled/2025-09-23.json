[
    {
        "title": "MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction",
        "abstract": "Universal multimodal embedding models have achieved great success in\ncapturing semantic relevance between queries and candidates. However, current\nmethods either condense queries and candidates into a single vector,\npotentially limiting the expressiveness for fine-grained information, or\nproduce too many vectors that are prohibitively expensive for multi-vector\nretrieval. In this work, we introduce MetaEmbed, a new framework for multimodal\nretrieval that rethinks how multimodal embeddings are constructed and\ninteracted with at scale. During training, a fixed number of learnable Meta\nTokens are appended to the input sequence. At test-time, their last-layer\ncontextualized representations serve as compact yet expressive multi-vector\nembeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,\nMetaEmbed learns to organize information by granularity across multiple\nvectors. As a result, we enable test-time scaling in multimodal retrieval,\nwhere users can balance retrieval quality against efficiency demands by\nselecting the number of tokens used for indexing and retrieval interactions.\nExtensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and\nthe Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed\nachieves state-of-the-art retrieval performance while scaling robustly to\nmodels with 32B parameters.",
        "url": "http://arxiv.org/abs/2509.18095v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18095v1",
        "arxiv_id": "2509.18095v1",
        "authors": [
            "Zilin Xiao",
            "Qi Ma",
            "Mengting Gu",
            "Chun-cheng Jason Chen",
            "Xintao Chen",
            "Vicente Ordonez",
            "Vijai Mohan"
        ],
        "submitted": "2025-09-22 17:59:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SEQR: Secure and Efficient QR-based LoRA Routing",
        "abstract": "Low-Rank Adaptation (LoRA) has become a standard technique for\nparameter-efficient fine-tuning of large language models, enabling large\nlibraries of LoRAs, each for a specific task or domain. Efficiently selecting\nthe correct LoRA adapter for a given input remains a challenge, particularly in\nsecure environments where supervised training of routers may raise privacy\nconcerns. Motivated by previous approaches, we formalize the goal of\nunsupervised LoRA routing in terms of activation norm maximization, providing a\ntheoretical framework for analysis. We demonstrate the discriminative power of\nactivation norms and introduce SEQR, an unsupervised LoRA routing algorithm\ndesigned to maximize efficiency while providing strict routing guarantees. SEQR\nprovably identifies the norm-maximizing adapter with significantly greater\nefficiency, making it a highly scalable and effective solution for dynamic LoRA\ncomposition. We validate our results through experiments that demonstrate\nimproved multi-task performance and efficiency.",
        "url": "http://arxiv.org/abs/2509.18093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18093v1",
        "arxiv_id": "2509.18093v1",
        "authors": [
            "William Fleshman",
            "Benjamin Van Durme"
        ],
        "submitted": "2025-09-22 17:59:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System",
        "abstract": "Despite the growing interest in replicating the scaled success of large\nlanguage models (LLMs) in industrial search and recommender systems, most\nexisting industrial efforts remain limited to transplanting Transformer\narchitectures, which bring only incremental improvements over strong Deep\nLearning Recommendation Models (DLRMs). From a first principle perspective, the\nbreakthroughs of LLMs stem not only from their architectures but also from two\ncomplementary mechanisms: context engineering, which enriches raw input queries\nwith contextual cues to better elicit model capabilities, and multi-step\nreasoning, which iteratively refines model outputs through intermediate\nreasoning paths. However, these two mechanisms and their potential to unlock\nsubstantial improvements remain largely underexplored in industrial ranking\nsystems.\n  In this paper, we propose OnePiece, a unified framework that seamlessly\nintegrates LLM-style context engineering and reasoning into both retrieval and\nranking models of industrial cascaded pipelines. OnePiece is built on a pure\nTransformer backbone and further introduces three key innovations: (1)\nstructured context engineering, which augments interaction history with\npreference and scenario signals and unifies them into a structured tokenized\ninput sequence for both retrieval and ranking; (2) block-wise latent reasoning,\nwhich equips the model with multi-step refinement of representations and scales\nreasoning bandwidth via block size; (3) progressive multi-task training, which\nleverages user feedback chains to effectively supervise reasoning steps during\ntraining. OnePiece has been deployed in the main personalized search scenario\nof Shopee and achieves consistent online gains across different key business\nmetrics, including over $+2\\%$ GMV/UU and a $+2.90\\%$ increase in advertising\nrevenue.",
        "url": "http://arxiv.org/abs/2509.18091v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18091v1",
        "arxiv_id": "2509.18091v1",
        "authors": [
            "Sunhao Dai",
            "Jiakai Tang",
            "Jiahua Wu",
            "Kun Wang",
            "Yuxuan Zhu",
            "Bingjun Chen",
            "Bangyang Hong",
            "Yu Zhao",
            "Cong Fu",
            "Kangle Wu",
            "Yabo Ni",
            "Anxiang Zeng",
            "Wenjie Wang",
            "Xu Chen",
            "Jun Xu",
            "See-Kiong Ng"
        ],
        "submitted": "2025-09-22 17:59:07",
        "source": "arxiv",
        "comment": "OnePiece Technical Report; Applied in Shopee"
    },
    {
        "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding",
        "abstract": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
        "url": "http://arxiv.org/abs/2509.18085v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18085v1",
        "arxiv_id": "2509.18085v1",
        "authors": [
            "Sudhanshu Agrawal",
            "Risheek Garrepalli",
            "Raghavv Goel",
            "Mingu Lee",
            "Christopher Lott",
            "Fatih Porikli"
        ],
        "submitted": "2025-09-22 17:58:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
        "abstract": "We introduce Reasoning Core, a new scalable environment for Reinforcement\nLearning with Verifiable Rewards (RLVR), designed to advance foundational\nsymbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks\nthat focus on games or isolated puzzles, Reasoning Core procedurally generates\nproblems across core formal domains, including PDDL planning, first-order\nlogic, context-free grammar parsing, causal reasoning, and system equation\nsolving. The environment is built on key design principles of high-generality\nproblem distributions, verification via external tools, and continuous\ndifficulty control, which together provide a virtually infinite supply of novel\ntraining instances. Initial zero-shot evaluations with frontier LLMs confirm\nthe difficulty of Reasoning Core's tasks, positioning it as a promising\nresource to improve the reasoning capabilities of future models.",
        "url": "http://arxiv.org/abs/2509.18083v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18083v1",
        "arxiv_id": "2509.18083v1",
        "authors": [
            "Valentin Lacombe",
            "Valentin Quesnel",
            "Damien Sileo"
        ],
        "submitted": "2025-09-22 17:56:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning",
        "abstract": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability.",
        "url": "http://arxiv.org/abs/2509.18063v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18063v1",
        "arxiv_id": "2509.18063v1",
        "authors": [
            "Jan-Felix Klein",
            "Lars Ohnemus"
        ],
        "submitted": "2025-09-22 17:40:05",
        "source": "arxiv",
        "comment": "Work in Progess"
    },
    {
        "title": "TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation",
        "abstract": "Tibetan is a low-resource language with limited parallel speech corpora\nspanning its three major dialects (\\\"U-Tsang, Amdo, and Kham), limiting\nprogress in speech modeling. To address this issue, we propose TMD-TTS, a\nunified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes\nparallel dialectal speech from explicit dialect labels. Our method features a\ndialect fusion module and a Dialect-Specialized Dynamic Routing Network\n(DSDR-Net) to capture fine-grained acoustic and linguistic variations across\ndialects. Extensive objective and subjective evaluations demonstrate that\nTMD-TTS significantly outperforms baselines in dialectal expressiveness. We\nfurther validate the quality and utility of the synthesized speech through a\nchallenging Speech-to-Speech Dialect Conversion (S2SDC) task.",
        "url": "http://arxiv.org/abs/2509.18060v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18060v1",
        "arxiv_id": "2509.18060v1",
        "authors": [
            "Yutong Liu",
            "Ziyue Zhang",
            "Ban Ma-bao",
            "Renzeng Duojie",
            "Yuqing Cai",
            "Yongbin Yu",
            "Xiangxiang Wang",
            "Fan Gao",
            "Cheng Huang",
            "Nyima Tashi"
        ],
        "submitted": "2025-09-22 17:38:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem",
        "abstract": "Selecting a solution algorithm for the Facility Layout Problem (FLP), an\nNP-hard optimization problem with a multiobjective trade-off, is a complex task\nthat requires deep expert knowledge. The performance of a given algorithm\ndepends on specific problem characteristics such as its scale, objectives, and\nconstraints. This creates a need for a data-driven recommendation method to\nguide algorithm selection in automated design systems. This paper introduces a\nnew recommendation method to make such expertise accessible, based on a\nKnowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To\naddress this, a domain-specific knowledge graph is constructed from published\nliterature. The method then employs a multi-faceted retrieval mechanism to\ngather relevant evidence from this knowledge graph using three distinct\napproaches, which include a precise graph-based search, flexible vector-based\nsearch, and high-level cluster-based search. The retrieved evidence is utilized\nby a Large Language Model (LLM) to generate algorithm recommendations with\ndata-driven reasoning. The proposed KG-RAG method is compared against a\ncommercial LLM chatbot with access to the knowledge base as a table, across a\nseries of diverse, real-world FLP test cases. Based on recommendation accuracy\nand reasoning capability, the proposed method performed significantly better\nthan the commercial LLM chatbot.",
        "url": "http://arxiv.org/abs/2509.18054v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18054v1",
        "arxiv_id": "2509.18054v1",
        "authors": [
            "Nikhil N S",
            "Amol Dilip Joshi",
            "Bilal Muhammed",
            "Soban Babu"
        ],
        "submitted": "2025-09-22 17:29:10",
        "source": "arxiv",
        "comment": "10 pages, 5 figures"
    },
    {
        "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies",
        "abstract": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\"",
        "url": "http://arxiv.org/abs/2509.18052v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18052v1",
        "arxiv_id": "2509.18052v1",
        "authors": [
            "Jiaxu Zhou",
            "Jen-tse Huang",
            "Xuhui Zhou",
            "Man Ho Lam",
            "Xintao Wang",
            "Hao Zhu",
            "Wenxuan Wang",
            "Maarten Sap"
        ],
        "submitted": "2025-09-22 17:27:29",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "RadEval: A framework for radiology text evaluation",
        "abstract": "We introduce RadEval, a unified, open-source framework for evaluating\nradiology texts. RadEval consolidates a diverse range of metrics, from classic\nn-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical\nconcept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,\nTemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and\nstandardize implementations, extend GREEN to support multiple imaging\nmodalities with a more lightweight model, and pretrain a domain-specific\nradiology encoder, demonstrating strong zero-shot retrieval performance. We\nalso release a richly annotated expert dataset with over 450 clinically\nsignificant error labels and show how different metrics correlate with\nradiologist judgment. Finally, RadEval provides statistical testing tools and\nbaseline model evaluations across multiple publicly available datasets,\nfacilitating reproducibility and robust benchmarking in radiology report\ngeneration.",
        "url": "http://arxiv.org/abs/2509.18030v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18030v1",
        "arxiv_id": "2509.18030v1",
        "authors": [
            "Justin Xu",
            "Xi Zhang",
            "Javid Abderezaei",
            "Julie Bauml",
            "Roger Boodoo",
            "Fatemeh Haghighi",
            "Ali Ganjizadeh",
            "Eric Brattain",
            "Dave Van Veen",
            "Zaiqiao Meng",
            "David Eyre",
            "Jean-Benoit Delbrouck"
        ],
        "submitted": "2025-09-22 17:03:48",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Demo track - Oral"
    },
    {
        "title": "Cross-Attention is Half Explanation in Speech-to-Text Models",
        "abstract": "Cross-attention is a core mechanism in encoder-decoder architectures,\nwidespread in many fields, including speech-to-text (S2T) processing. Its\nscores have been repurposed for various downstream applications--such as\ntimestamp estimation and audio-text alignment--under the assumption that they\nreflect the dependencies between input speech representation and the generated\ntext. While the explanatory nature of attention mechanisms has been widely\ndebated in the broader NLP literature, this assumption remains largely\nunexplored within the speech domain. To address this gap, we assess the\nexplanatory power of cross-attention in S2T models by comparing its scores to\ninput saliency maps derived from feature attribution. Our analysis spans\nmonolingual and multilingual, single-task and multi-task models at multiple\nscales, and shows that attention scores moderately to strongly align with\nsaliency-based explanations, particularly when aggregated across heads and\nlayers. However, it also shows that cross-attention captures only about 50% of\nthe input relevance and, in the best case, only partially reflects how the\ndecoder attends to the encoder's representations--accounting for just 52-75% of\nthe saliency. These findings uncover fundamental limitations in interpreting\ncross-attention as an explanatory proxy, suggesting that it offers an\ninformative yet incomplete view of the factors driving predictions in S2T\nmodels.",
        "url": "http://arxiv.org/abs/2509.18010v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18010v1",
        "arxiv_id": "2509.18010v1",
        "authors": [
            "Sara Papi",
            "Dennis Fucci",
            "Marco Gaido",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "submitted": "2025-09-22 16:49:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration",
        "abstract": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis.",
        "url": "http://arxiv.org/abs/2509.18008v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18008v1",
        "arxiv_id": "2509.18008v1",
        "authors": [
            "Bingsheng Yao",
            "Jiaju Chen",
            "Chaoran Chen",
            "April Wang",
            "Toby Jia-jun Li",
            "Dakuo Wang"
        ],
        "submitted": "2025-09-22 16:47:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing",
        "abstract": "The scarcity of large-scale, open-source data for dialects severely hinders\nprogress in speech technology, a challenge particularly acute for the widely\nspoken Sichuanese dialects of Chinese. To address this critical gap, we\nintroduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed\nusing our novel Chuan-Pipeline, a complete data processing framework for\ndialectal speech. To facilitate rigorous evaluation and demonstrate the\ncorpus's effectiveness, we also release high-quality ASR and TTS benchmarks,\nWenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show\nthat models trained on WenetSpeech-Chuan achieve state-of-the-art performance\namong open-source systems and demonstrate results comparable to commercial\nservices. As the largest open-source corpus for Sichuanese dialects,\nWenetSpeech-Chuan not only lowers the barrier to research in dialectal speech\nprocessing but also plays a crucial role in promoting AI equity and mitigating\nbias in speech technologies. The corpus, benchmarks, models, and receipts are\npublicly available on our project page.",
        "url": "http://arxiv.org/abs/2509.18004v1",
        "pdf_url": "http://arxiv.org/pdf/2509.18004v1",
        "arxiv_id": "2509.18004v1",
        "authors": [
            "Yuhang Dai",
            "Ziyu Zhang",
            "Shuai Wang",
            "Longhao Li",
            "Zhao Guo",
            "Tianlun Zuo",
            "Shuiyuan Wang",
            "Hongfei Xue",
            "Chengyou Wang",
            "Qing Wang",
            "Xin Xu",
            "Hui Bu",
            "Jie Li",
            "Jian Kang",
            "Binbin Zhang",
            "Lei Xie"
        ],
        "submitted": "2025-09-22 16:44:00",
        "source": "arxiv",
        "comment": "4 pages, 5 figures, 4 tables"
    },
    {
        "title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models",
        "abstract": "Recent advances have shown that scaling test-time computation enables large\nlanguage models (LLMs) to solve increasingly complex problems across diverse\ndomains. One effective paradigm for test-time scaling (TTS) involves LLM\ngenerators producing multiple solution candidates, with LLM verifiers assessing\nthe correctness of these candidates without reference answers. In this paper,\nwe study generative verifiers, which perform verification by generating\nchain-of-thought (CoT) reasoning followed by a binary verdict. We\nsystematically analyze verification dynamics across three dimensions - problem\ndifficulty, generator capability, and verifier generation capability - with\nempirical studies on 12 benchmarks across mathematical reasoning, knowledge,\nand natural language reasoning tasks using 14 open-source models (2B to 72B\nparameter range) and GPT-4o. Our experiments reveal three key findings about\nverification effectiveness: (1) Easy problems allow verifiers to more reliably\ncertify correct responses; (2) Weak generators produce errors that are easier\nto detect than strong generators; (3) Verification ability is generally\ncorrelated with the verifier's own problem-solving capability, but this\nrelationship varies with problem difficulty. These findings reveal\nopportunities to optimize basic verification strategies in TTS applications.\nFirst, given the same verifier, some weak generators can nearly match stronger\nones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B\nperformance gap shrinks by 75.5%). Second, we identify cases where strong\nverifiers offer limited advantage over weak ones, as both fail to provide\nmeaningful verification gains, suggesting that verifier scaling alone cannot\novercome fundamental verification challenges.",
        "url": "http://arxiv.org/abs/2509.17995v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17995v1",
        "arxiv_id": "2509.17995v1",
        "authors": [
            "Yefan Zhou",
            "Austin Xu",
            "Yilun Zhou",
            "Janvijay Singh",
            "Jiang Gui",
            "Shafiq Joty"
        ],
        "submitted": "2025-09-22 16:36:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media",
        "abstract": "Almost 50% depression patients face the risk of going into relapse. The risk\nincreases to 80% after the second episode of depression. Although, depression\ndetection from social media has attained considerable attention, depression\nrelapse detection has remained largely unexplored due to the lack of curated\ndatasets and the difficulty of distinguishing relapse and non-relapse users. In\nthis work, we present ReDepress, the first clinically validated social media\ndataset focused on relapse, comprising 204 Reddit users annotated by mental\nhealth professionals. Unlike prior approaches, our framework draws on cognitive\ntheories of depression, incorporating constructs such as attention bias,\ninterpretation bias, memory bias and rumination into both annotation and\nmodeling. Through statistical analyses and machine learning experiments, we\ndemonstrate that cognitive markers significantly differentiate relapse and\nnon-relapse groups, and that models enriched with these features achieve\ncompetitive performance, with transformer-based temporal models attaining an F1\nof 0.86. Our findings validate psychological theories in real-world textual\ndata and underscore the potential of cognitive-informed computational methods\nfor early relapse detection, paving the way for scalable, low-cost\ninterventions in mental healthcare.",
        "url": "http://arxiv.org/abs/2509.17991v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17991v1",
        "arxiv_id": "2509.17991v1",
        "authors": [
            "Aakash Kumar Agarwal",
            "Saprativa Bhattacharjee",
            "Mauli Rastogi",
            "Jemima S. Jacob",
            "Biplab Banerjee",
            "Rashmi Gupta",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-09-22 16:33:59",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference"
    },
    {
        "title": "Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments",
        "abstract": "Asynchronous learning environments (ALEs) are widely adopted for formal and\ninformal learning, but timely and personalized support is often limited. In\nthis context, Virtual Teaching Assistants (VTAs) can potentially reduce the\nworkload of instructors, but rigorous and pedagogically sound evaluation is\nessential. Existing assessments often rely on surface-level metrics and lack\nsufficient grounding in educational theories, making it difficult to\nmeaningfully compare the pedagogical effectiveness of different VTA systems. To\nbridge this gap, we propose an evaluation framework rooted in learning sciences\nand tailored to asynchronous forum discussions, a common VTA deployment context\nin ALE. We construct classifiers using expert annotations of VTA responses on a\ndiverse set of forum posts. We evaluate the effectiveness of our classifiers,\nidentifying approaches that improve accuracy as well as challenges that hinder\ngeneralization. Our work establishes a foundation for theory-driven evaluation\nof VTA systems, paving the way for more pedagogically effective AI in\neducation.",
        "url": "http://arxiv.org/abs/2509.17961v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17961v1",
        "arxiv_id": "2509.17961v1",
        "authors": [
            "Li Siyan",
            "Zhen Xu",
            "Vethavikashini Chithrra Raghuram",
            "Xuanming Zhang",
            "Renzhe Yu",
            "Zhou Yu"
        ],
        "submitted": "2025-09-22 16:15:58",
        "source": "arxiv",
        "comment": "Accepted in EMNLP 2025 Findings"
    },
    {
        "title": "Dorabella Cipher as Musical Inspiration",
        "abstract": "The Dorabella cipher is an encrypted note written by English composer Edward\nElgar, which has defied decipherment attempts for more than a century. While\nmost proposed solutions are English texts, we investigate the hypothesis that\nDorabella represents enciphered music. We weigh the evidence for and against\nthe hypothesis, devise a simplified music notation, and attempt to reconstruct\na melody from the cipher. Our tools are n-gram models of music which we\nvalidate on existing music corpora enciphered using monoalphabetic\nsubstitution. By applying our methods to Dorabella, we produce a decipherment\nwith musical qualities, which is then transformed via artful composition into a\nlistenable melody. Far from arguing that the end result represents the only\ntrue solution, we instead frame the process of decipherment as part of the\ncomposition process.",
        "url": "http://arxiv.org/abs/2509.17950v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17950v1",
        "arxiv_id": "2509.17950v1",
        "authors": [
            "Bradley Hauer",
            "Colin Choi",
            "Abram Hindle",
            "Scott Smallwood",
            "Grzegorz Kondrak"
        ],
        "submitted": "2025-09-22 16:09:26",
        "source": "arxiv",
        "comment": "Published in Proceedings of the Workshop on Speech and Music\n  Processing 2021"
    },
    {
        "title": "HICode: Hierarchical Inductive Coding with LLMs",
        "abstract": "Despite numerous applications for fine-grained corpus analysis, researchers\ncontinue to rely on manual labeling, which does not scale, or statistical tools\nlike topic modeling, which are difficult to control. We propose that LLMs have\nthe potential to scale the nuanced analyses that researchers typically conduct\nmanually to large text corpora. To this effect, inspired by qualitative\nresearch methods, we develop HICode, a two-part pipeline that first inductively\ngenerates labels directly from analysis data and then hierarchically clusters\nthem to surface emergent themes. We validate this approach across three diverse\ndatasets by measuring alignment with human-constructed themes and demonstrating\nits robustness through automated and human evaluations. Finally, we conduct a\ncase study of litigation documents related to the ongoing opioid crisis in the\nU.S., revealing aggressive marketing strategies employed by pharmaceutical\ncompanies and demonstrating HICode's potential for facilitating nuanced\nanalyses in large-scale data.",
        "url": "http://arxiv.org/abs/2509.17946v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17946v1",
        "arxiv_id": "2509.17946v1",
        "authors": [
            "Mian Zhong",
            "Pristina Wang",
            "Anjalie Field"
        ],
        "submitted": "2025-09-22 16:07:11",
        "source": "arxiv",
        "comment": "Long paper accepted at EMNLP 2025 main conference, 19 pages, 8\n  figures"
    },
    {
        "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models",
        "abstract": "The safety and alignment of Large Language Models (LLMs) are critical for\ntheir responsible deployment. Current evaluation methods predominantly focus on\nidentifying and preventing overtly harmful outputs. However, they often fail to\naddress a more insidious failure mode: models that produce benign-appearing\noutputs while operating on malicious or deceptive internal reasoning. This\nvulnerability, often triggered by sophisticated system prompt injections,\nallows models to bypass conventional safety filters, posing a significant,\nunderexplored risk. To address this gap, we introduce the Deceptive Reasoning\nExposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy\nbetween a model's internal reasoning process and its final output. D-REX was\nconstructed through a competitive red-teaming exercise where participants\ncrafted adversarial system prompts to induce such deceptive behaviors. Each\nsample in D-REX contains the adversarial system prompt, an end-user's test\nquery, the model's seemingly innocuous response, and, crucially, the model's\ninternal chain-of-thought, which reveals the underlying malicious intent. Our\nbenchmark facilitates a new, essential evaluation task: the detection of\ndeceptive alignment. We demonstrate that D-REX presents a significant challenge\nfor existing models and safety mechanisms, highlighting the urgent need for new\ntechniques that scrutinize the internal processes of LLMs, not just their final\noutputs.",
        "url": "http://arxiv.org/abs/2509.17938v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17938v1",
        "arxiv_id": "2509.17938v1",
        "authors": [
            "Satyapriya Krishna",
            "Andy Zou",
            "Rahul Gupta",
            "Eliot Krzysztof Jones",
            "Nick Winter",
            "Dan Hendrycks",
            "J. Zico Kolter",
            "Matt Fredrikson",
            "Spyros Matsoukas"
        ],
        "submitted": "2025-09-22 15:59:40",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Training-free Truthfulness Detection via Value Vectors in LLMs",
        "abstract": "Large language models often generate factually incorrect outputs, motivating\nefforts to detect the truthfulness of their content. Most existing approaches\nrely on training probes over internal activations, but these methods suffer\nfrom scalability and generalization issues. A recent training-free method,\nNoVo, addresses this challenge by exploiting statistical patterns from the\nmodel itself. However, it focuses exclusively on attention mechanisms,\npotentially overlooking the MLP module-a core component of Transformer models\nknown to support factual recall. In this paper, we show that certain value\nvectors within MLP modules exhibit truthfulness-related statistical patterns.\nBuilding on this insight, we propose TruthV, a simple and interpretable\ntraining-free method that detects content truthfulness by leveraging these\nvalue vectors. On the NoVo benchmark, TruthV significantly outperforms both\nNoVo and log-likelihood baselines, demonstrating that MLP modules-despite being\nneglected in prior training-free efforts-encode rich and useful signals for\ntruthfulness detection. These findings offer new insights into how truthfulness\nis internally represented in LLMs and motivate further research on scalable and\ninterpretable truthfulness detection.",
        "url": "http://arxiv.org/abs/2509.17932v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17932v1",
        "arxiv_id": "2509.17932v1",
        "authors": [
            "Runheng Liu",
            "Heyan Huang",
            "Xingchen Xiao",
            "Zhijing Wu"
        ],
        "submitted": "2025-09-22 15:54:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation",
        "abstract": "Multilingual translation faces challenges of computational redundancy and\nlimited accuracy for low-resource languages, especially in speech translation.\nTo address this, we propose a novel hierarchical Transformer Encoder Tree (TET)\ncombined with non-autoregressive encoder-only models trained with Connectionist\nTemporal Classification for multilingual translation. By sharing intermediate\nrepresentations among linguistically similar target languages, TET can improve\naccuracy on low-resource languages, reduce computational redundancy, and allow\ngenerating all target languages in a single forward pass, thus eliminating\nsequential bottlenecks and improving parallelism. For speech translation,\ncombining TET with a non-autoregressive speech recognition backbone (wav2vec2)\nshows promising results in terms of translation quality compared to\nautoregressive systems while being 7-14 times faster.",
        "url": "http://arxiv.org/abs/2509.17930v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17930v1",
        "arxiv_id": "2509.17930v1",
        "authors": [
            "Yiwen Guan",
            "Jacob Whitehill"
        ],
        "submitted": "2025-09-22 15:52:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning",
        "abstract": "Extracting individual sentences from a document as evidence or reasoning\nsteps is commonly done in many NLP tasks. However, extracted sentences often\nlack context necessary to make them understood, e.g., coreference and\nbackground information. To this end, we propose a content selection and\nplanning framework for zero-shot decontextualisation, which determines what\ncontent should be mentioned and in what order for a sentence to be understood\nout of context. Specifically, given a potentially ambiguous sentence and its\ncontext, we first segment it into basic semantically-independent units. We then\nidentify potentially ambiguous units from the given sentence, and extract\nrelevant units from the context based on their discourse relations. Finally, we\ngenerate a content plan to rewrite the sentence by enriching each ambiguous\nunit with its relevant units. Experimental results demonstrate that our\napproach is competitive for sentence decontextualisation, producing sentences\nthat exhibit better semantic integrity and discourse coherence, outperforming\nexisting methods.",
        "url": "http://arxiv.org/abs/2509.17921v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17921v1",
        "arxiv_id": "2509.17921v1",
        "authors": [
            "Zhenyun Deng",
            "Yulong Chen",
            "Andreas Vlachos"
        ],
        "submitted": "2025-09-22 15:47:07",
        "source": "arxiv",
        "comment": "Accepted to EMLNP 2025 (Main Conference)"
    },
    {
        "title": "Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles",
        "abstract": "Recommender systems (RS) greatly influence users' consumption decisions,\nmaking them attractive targets for malicious shilling attacks that inject fake\nuser profiles to manipulate recommendations. Existing shilling methods can\ngenerate effective and stealthy fake profiles when training data only contain\nrating matrix, but they lack comprehensive solutions for scenarios where side\nfeatures are present and utilized by the recommender. To address this gap, we\nextend the Leg-UP framework by enhancing the generator architecture to\nincorporate side features, enabling the generation of side-feature-aware fake\nuser profiles. Experiments on benchmarks show that our method achieves strong\nattack performance while maintaining stealthiness.",
        "url": "http://arxiv.org/abs/2509.17918v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17918v1",
        "arxiv_id": "2509.17918v1",
        "authors": [
            "Yuanrong Wang",
            "Yingpeng Du"
        ],
        "submitted": "2025-09-22 15:43:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SiDiaC: Sinhala Diachronic Corpus",
        "abstract": "SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a\nhistorical span from the 5th to the 20th century CE. SiDiaC comprises 58k words\nacross 46 literary works, annotated carefully based on the written date, after\nfiltering based on availability, authorship, copyright compliance, and data\nattribution. Texts from the National Library of Sri Lanka were digitised using\nGoogle Document AI OCR, followed by post-processing to correct formatting and\nmodernise the orthography. The construction of SiDiaC was informed by practices\nfrom other corpora, such as FarPaHC, particularly in syntactic annotation and\ntext normalisation strategies, due to the shared characteristics of\nlow-resourced language status. This corpus is categorised based on genres into\ntwo layers: primary and secondary. Primary categorisation is binary,\nclassifying each book into Non-Fiction or Fiction, while the secondary\ncategorisation is more specific, grouping texts under Religious, History,\nPoetry, Language, and Medical genres. Despite challenges including limited\naccess to rare texts and reliance on secondary date sources, SiDiaC serves as a\nfoundational resource for Sinhala NLP, significantly extending the resources\navailable for Sinhala, enabling diachronic studies in lexical change, neologism\ntracking, historical syntax, and corpus-based lexicography.",
        "url": "http://arxiv.org/abs/2509.17912v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17912v1",
        "arxiv_id": "2509.17912v1",
        "authors": [
            "Nevidu Jayatilleke",
            "Nisansa de Silva"
        ],
        "submitted": "2025-09-22 15:37:51",
        "source": "arxiv",
        "comment": "14 pages, 7 figures, 7 tables, Accepted paper at the 39th Pacific\n  Asia Conference on Language, Information and Computation (PACLIC 39)"
    },
    {
        "title": "How Persuasive is Your Context?",
        "abstract": "Two central capabilities of language models (LMs) are: (i) drawing on prior\nknowledge about entities, which allows them to answer queries such as \"What's\nthe official language of Austria?\", and (ii) adapting to new information\nprovided in context, e.g., \"Pretend the official language of Austria is\nTagalog.\", that is pre-pended to the question. In this article, we introduce\ntargeted persuasion score (TPS), designed to quantify how persuasive a given\ncontext is to an LM where persuasion is operationalized as the ability of the\ncontext to alter the LM's answer to the question. In contrast to evaluating\npersuasiveness only by inspecting the greedily decoded answer under the model,\nTPS provides a more fine-grained view of model behavior. Based on the\nWasserstein distance, TPS measures how much a context shifts a model's original\nanswer distribution toward a target distribution. Empirically, through a series\nof experiments, we show that TPS captures a more nuanced notion of\npersuasiveness than previously proposed metrics.",
        "url": "http://arxiv.org/abs/2509.17879v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17879v1",
        "arxiv_id": "2509.17879v1",
        "authors": [
            "Tu Nguyen",
            "Kevin Du",
            "Alexander Miserlis Hoyle",
            "Ryan Cotterell"
        ],
        "submitted": "2025-09-22 15:15:40",
        "source": "arxiv",
        "comment": "Long paper accepted at EMNLP 2025"
    },
    {
        "title": "Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN",
        "abstract": "This paper outlines the methodology for modeling tonal learning in fully\nunsupervised models of human language acquisition. Tonal patterns are among the\ncomputationally most complex learning objectives in language. We argue that a\nrealistic generative model of human language (ciwGAN) can learn to associate\nits categorical variables with Mandarin Chinese tonal categories without any\nlabeled data. All three trained models showed statistically significant\ndifferences in F0 across categorical variables. The model trained solely on\nmale tokens consistently encoded tone. Our results sug- gest that not only does\nthe model learn Mandarin tonal contrasts, but it learns a system that\ncorresponds to a stage of acquisition in human language learners. We also\noutline methodology for tracing tonal representations in internal convolutional\nlayers, which shows that linguistic tools can contribute to interpretability of\ndeep learning and can ultimately be used in neural experiments.",
        "url": "http://arxiv.org/abs/2509.17859v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17859v1",
        "arxiv_id": "2509.17859v1",
        "authors": [
            "Kai Schenck",
            "Gašper Beguš"
        ],
        "submitted": "2025-09-22 14:52:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution",
        "abstract": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on\nMultilingual Coreference Resolution. This fourth iteration of the shared task\nintroduces a new LLM track alongside the original unconstrained track, features\nreduced development and test sets to lower computational requirements, and\nincludes additional datasets. CorPipe 25 represents a complete reimplementation\nof our previous systems, migrating from TensorFlow to PyTorch. Our system\nsignificantly outperforms all other submissions in both the LLM and\nunconstrained tracks by a substantial margin of 8 percentage points. The source\ncode and trained models are publicly available at\nhttps://github.com/ufal/crac2025-corpipe.",
        "url": "http://arxiv.org/abs/2509.17858v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17858v1",
        "arxiv_id": "2509.17858v1",
        "authors": [
            "Milan Straka"
        ],
        "submitted": "2025-09-22 14:51:37",
        "source": "arxiv",
        "comment": "Accepted to CODI-CRAC 2025"
    },
    {
        "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora",
        "abstract": "Dialects exhibit a substantial degree of variation due to the lack of a\nstandard orthography. At the same time, the ability of Large Language Models\n(LLMs) to process dialects remains largely understudied. To address this gap,\nwe use Bavarian as a case study and investigate the lexical dialect\nunderstanding capability of LLMs by examining how well they recognize and\ntranslate dialectal terms across different parts-of-speech. To this end, we\nintroduce DiaLemma, a novel annotation framework for creating dialect variation\ndictionaries from monolingual data only, and use it to compile a ground truth\ndataset consisting of 100K human-annotated German-Bavarian word pairs. We\nevaluate how well nine state-of-the-art LLMs can judge Bavarian terms as\ndialect translations, inflected variants, or unrelated forms of a given German\nlemma. Our results show that LLMs perform best on nouns and lexically similar\nword pairs, and struggle most in distinguishing between direct translations and\ninflected variants. Interestingly, providing additional context in the form of\nexample usages improves the translation performance, but reduces their ability\nto recognize dialect variants. This study highlights the limitations of LLMs in\ndealing with orthographic dialect variation and emphasizes the need for future\nwork on adapting LLMs to dialects.",
        "url": "http://arxiv.org/abs/2509.17855v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17855v1",
        "arxiv_id": "2509.17855v1",
        "authors": [
            "Robert Litschko",
            "Verena Blaschke",
            "Diana Burkhardt",
            "Barbara Plank",
            "Diego Frassinelli"
        ],
        "submitted": "2025-09-22 14:49:08",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Findings)"
    },
    {
        "title": "Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework",
        "abstract": "Emotions, which influence how convincing an argument is, are developed\n  in context of the self and sender, and therefore require modeling\n  the cognitive evaluation process. While binary emotionality has been\n  studied in argument mining, and the cognitive appraisal has been\n  modeled in general emotion analysis, these fields have not been\n  brought together yet. We therefore propose the Contextualized\n  Argument Appraisal Framework that contextualizes the interplay\n  between the sender, receiver, and argument. It includes emotion\n  labels, appraisals, such as argument familiarity, response urgency,\n  and expected effort, as well as convincingness variables. To evaluate\n  the framework and pave the way to computational modeling, we perform\n  a study in a role-playing scenario, mimicking real-world exposure to\n  arguments, asking participants to disclose their emotion, explain the main\ncause, the\n  argument appraisal, and the\n  perceived convincingness. To consider the subjective nature of such\n  annotations, we also collect demographic data and personality traits\n  of both the participants and the perceived sender of the argument.\n  The analysis of the resulting corpus of 800 arguments, each\n  annotated by 5 participants, reveals that convincingness is\n  positively correlated with positive emotions (e.g., trust) and\n  negatively correlated with negative emotions (e.g., anger). The\n  appraisal variables disclose the importance of the argument\n  familiarity. For most participants, the content of the argument\n  itself is the primary driver of the emotional response.",
        "url": "http://arxiv.org/abs/2509.17844v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17844v1",
        "arxiv_id": "2509.17844v1",
        "authors": [
            "Lynn Greschner",
            "Sabine Weber",
            "Roman Klinger"
        ],
        "submitted": "2025-09-22 14:32:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Documents to Database: Failure Modes for Industrial Assets",
        "abstract": "We propose an interactive system using foundation models and user-provided\ntechnical documents to generate Failure Mode and Effects Analyses (FMEA) for\nindustrial equipment. Our system aggregates unstructured content across\ndocuments to generate an FMEA and stores it in a relational database.\nLeveraging this tool, the time required for creation of this\nknowledge-intensive content is reduced, outperforming traditional manual\napproaches. This demonstration showcases the potential of foundation models to\nfacilitate the creation of specialized structured content for enterprise asset\nmanagement systems.",
        "url": "http://arxiv.org/abs/2509.17834v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17834v1",
        "arxiv_id": "2509.17834v1",
        "authors": [
            "Duygu Kabakci-Zorlu",
            "Fabio Lorenzi",
            "John Sheehan",
            "Karol Lynch",
            "Bradley Eck"
        ],
        "submitted": "2025-09-22 14:23:50",
        "source": "arxiv",
        "comment": "7 pages, 4 figures. Artificial Intelligence for Knowledge Acquisition\n  & Management (AI4KAM) Workshop @ IJCAI 2025"
    },
    {
        "title": "Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation",
        "abstract": "Generation of Artificial Intelligence (AI) texts in important works has\nbecome a common practice that can be used to misuse and abuse AI at various\nlevels. Traditional AI detectors often rely on document-level classification,\nwhich struggles to identify AI content in hybrid or slightly edited texts\ndesigned to avoid detection, leading to concerns about the model's efficiency,\nwhich makes it hard to distinguish between human-written and AI-generated\ntexts. A sentence-level sequence labeling model proposed to detect transitions\nbetween human- and AI-generated text, leveraging nuanced linguistic signals\noverlooked by document-level classifiers. By this method, detecting and\nsegmenting AI and human-written text within a single document at the\ntoken-level granularity is achieved. Our model combines the state-of-the-art\npre-trained Transformer models, incorporating Neural Networks (NN) and\nConditional Random Fields (CRFs). This approach extends the power of\ntransformers to extract semantic and syntactic patterns, and the neural network\ncomponent to capture enhanced sequence-level representations, thereby improving\nthe boundary predictions by the CRF layer, which enhances sequence recognition\nand further identification of the partition between Human- and AI-generated\ntexts. The evaluation is performed on two publicly available benchmark datasets\ncontaining collaborative human and AI-generated texts. Our experimental\ncomparisons are with zero-shot detectors and the existing state-of-the-art\nmodels, along with rigorous ablation studies to justify that this approach, in\nparticular, can accurately detect the spans of AI texts in a completely\ncollaborative text. All our source code and the processed datasets are\navailable in our GitHub repository.",
        "url": "http://arxiv.org/abs/2509.17830v2",
        "pdf_url": "http://arxiv.org/pdf/2509.17830v2",
        "arxiv_id": "2509.17830v2",
        "authors": [
            "Lekkala Sai Teja",
            "Annepaka Yadagiri",
            "Partha Pakray",
            "Chukhu Chunka",
            "Mangadoddi Srikar Vardhan"
        ],
        "submitted": "2025-09-22 14:22:55",
        "source": "arxiv",
        "comment": "14 pages, 14 figures"
    },
    {
        "title": "Towards Adaptive Context Management for Intelligent Conversational Question Answering",
        "abstract": "This particular paper introduces an Adaptive Context Management (ACM)\nframework for the Conversational Question Answering (ConvQA) systems. The key\nobjective of the ACM framework is to optimize the use of the conversation\nhistory by dynamically managing context for maximizing the relevant information\nprovided to a ConvQA model within its token limit. Our approach incorporates a\nContext Manager (CM) Module, a Summarization (SM) Module, and an Entity\nExtraction (EE) Module in a bid to handle the conversation history\nefficaciously. The CM Module dynamically adjusts the context size, thereby\npreserving the most relevant and recent information within a model's token\nlimit. The SM Module summarizes the older parts of the conversation history via\na sliding window. When the summarization window exceeds its limit, the EE\nModule identifies and retains key entities from the oldest conversation turns.\nExperimental results demonstrate the effectiveness of our envisaged framework\nin generating accurate and contextually appropriate responses, thereby\nhighlighting the potential of the ACM framework to enhance the robustness and\nscalability of the ConvQA systems.",
        "url": "http://arxiv.org/abs/2509.17829v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17829v1",
        "arxiv_id": "2509.17829v1",
        "authors": [
            "Manoj Madushanka Perera",
            "Adnan Mahmood",
            "Kasun Eranda Wijethilake",
            "Quan Z. Sheng"
        ],
        "submitted": "2025-09-22 14:21:26",
        "source": "arxiv",
        "comment": "Comments: 15 pages, 6 figures, Table 1, published in Lecture Notes in\n  Computer Science (LNCS 15391), Proceedings of ADMA 2024. DOI:\n  10.1007/978-981-96-0847-8_25"
    },
    {
        "title": "Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark",
        "abstract": "Existing physical commonsense reasoning benchmarks predominantly focus on\nWestern contexts, overlooking cultural variations in physical problem-solving.\nTo address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a\nnovel benchmark comprising 181 binary-choice problems that test physical\nreasoning within Korean cultural contexts, ranging from kimchi (Korean food) to\ntraditional fermentation. EPiK is constructed using a two-stage generation and\nverification pipeline to create culturally-authentic problems across 9\nreasoning subtasks and 84 scenarios. Unlike approaches based on simple\ntranslation, our method generates problems organically from Korean contexts\nwhile upholding rigorous physical reasoning standards. Our evaluations show\nthat Korean-specialized models consistently outperform general-purpose models\nof comparable size. This performance gap highlights the limitations of\nculturally-agnostic models and demonstrates the critical need for\nculturally-aware benchmarks to truly measure language understanding. Our EPiK\nis publicly available at https://huggingface.co/datasets/jjae/EPiK.",
        "url": "http://arxiv.org/abs/2509.17807v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17807v1",
        "arxiv_id": "2509.17807v1",
        "authors": [
            "Jihae Jeong",
            "DaeYeop Lee",
            "DongGeon Lee",
            "Hwanjo Yu"
        ],
        "submitted": "2025-09-22 14:01:14",
        "source": "arxiv",
        "comment": "Accepted to MRL@EMNLP 2025"
    },
    {
        "title": "Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?",
        "abstract": "The paper presents an overview of the fourth edition of the Shared Task on\nMultilingual Coreference Resolution, organized as part of the CODI-CRAC 2025\nworkshop. As in the previous editions, participants were challenged to develop\nsystems that identify mentions and cluster them according to identity\ncoreference.\n  A key innovation of this year's task was the introduction of a dedicated\nLarge Language Model (LLM) track, featuring a simplified plaintext format\ndesigned to be more suitable for LLMs than the original CoNLL-U representation.\n  The task also expanded its coverage with three new datasets in two additional\nlanguages, using version 1.3 of CorefUD - a harmonized multilingual collection\nof 22 datasets in 17 languages.\n  In total, nine systems participated, including four LLM-based approaches (two\nfine-tuned and two using few-shot adaptation). While traditional systems still\nkept the lead, LLMs showed clear potential, suggesting they may soon challenge\nestablished approaches in future editions.",
        "url": "http://arxiv.org/abs/2509.17796v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17796v1",
        "arxiv_id": "2509.17796v1",
        "authors": [
            "Michal Novák",
            "Miloslav Konopík",
            "Anna Nedoluzhko",
            "Martin Popel",
            "Ondřej Pražák",
            "Jakub Sido",
            "Milan Straka",
            "Zdeněk Žabokrtský",
            "Daniel Zeman"
        ],
        "submitted": "2025-09-22 13:52:32",
        "source": "arxiv",
        "comment": "Accepted to CODI-CRAC 2025"
    },
    {
        "title": "Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction",
        "abstract": "Natural language generation (NLG) tasks are often subject to inherent\nvariability; \\emph{e.g.} predicting the next word given a context has multiple\nvalid responses, evident when asking multiple humans to complete the task.\nWhile having language models (LMs) that are aligned pluralistically, so that\nthey are able to reproduce well the inherent diversity in perspectives of an\nentire population of interest is clearly beneficial, \\citet{ilia2024predict}\nshow that LMs do not reproduce this type of linguistic variability well. They\nspeculate this inability might stem from the lack of consistent training of LMs\nwith data reflecting this type of inherent variability. As such, we investigate\nwhether training LMs on multiple plausible word continuations per context can\nimprove their ability to reproduce human linguistic variability for next-word\nprediction. We employ fine-tuning techniques for pre-trained and\ninstruction-tuned models; and demonstrate their potential when fine-tuning\nGPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures\ndivergence among empirically estimated human and model next-word distributions\nacross contexts before and after fine-tuning, shows that our multi-label\nfine-tuning improves the LMs' ability to reproduce linguistic variability; both\nfor contexts that admit higher and lower variability.",
        "url": "http://arxiv.org/abs/2509.17794v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17794v1",
        "arxiv_id": "2509.17794v1",
        "authors": [
            "Tobias Groot",
            "Salo Lacunes",
            "Evgenia Ilia"
        ],
        "submitted": "2025-09-22 13:51:40",
        "source": "arxiv",
        "comment": "EMNLP UncertaiNLP Workshop 2025"
    },
    {
        "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts",
        "abstract": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment.",
        "url": "http://arxiv.org/abs/2509.17788v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17788v1",
        "arxiv_id": "2509.17788v1",
        "authors": [
            "Xingyu Fan",
            "Feifei Li",
            "Wenhui Que",
            "Hailong Li"
        ],
        "submitted": "2025-09-22 13:49:37",
        "source": "arxiv",
        "comment": "7 pages"
    },
    {
        "title": "DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching",
        "abstract": "Language Identification (LID) is a core task in multilingual NLP, yet current\nsystems often overfit to clean, monolingual data. This work introduces\nDIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across\ndiverse domains, including speech transcripts, web text, social media texts,\nchildren's stories, and code-switched text. Our findings reveal that while\nmodels achieve high accuracy on curated datasets, performance degrades sharply\non noisy and informal inputs. We also introduce DIVERS-CS, a diverse\ncode-switching benchmark dataset spanning 10 language pairs, and show that\nexisting models struggle to detect multiple languages within the same sentence.\nThese results highlight the need for more robust and inclusive LID systems in\nreal-world settings.",
        "url": "http://arxiv.org/abs/2509.17768v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17768v1",
        "arxiv_id": "2509.17768v1",
        "authors": [
            "Jessica Ojo",
            "Zina Kamel",
            "David Ifeoluwa Adelani"
        ],
        "submitted": "2025-09-22 13:32:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Qwen3-Omni Technical Report",
        "abstract": "We present Qwen3-Omni, a single multimodal model that, for the first time,\nmaintains state-of-the-art performance across text, image, audio, and video\nwithout any degradation relative to single-modal counterparts. Qwen3-Omni\nmatches the performance of same-sized single-modal models within the Qwen\nseries and excels particularly on audio tasks. Across 36 audio and audio-visual\nbenchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall\nSOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,\nSeed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE\narchitecture that unifies perception and generation across text, images, audio,\nand video, yielding fluent text and natural real-time speech. It supports text\ninteraction in 119 languages, speech understanding in 19 languages, and speech\ngeneration in 10 languages. To reduce first-packet latency in streaming\nsynthesis, Talker autoregressively predicts discrete speech codecs using a\nmulti-codebook scheme. Leveraging the representational capacity of these\ncodebooks, we replace computationally intensive block-wise diffusion with a\nlightweight causal ConvNet, enabling streaming from the first codec frame. In\ncold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet\nlatency of 234 ms. To further strengthen multimodal reasoning, we introduce a\nThinking model that explicitly reasons over inputs from any modality. Since the\nresearch community currently lacks a general-purpose audio captioning model, we\nfine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which\nproduces detailed, low-hallucination captions for arbitrary audio inputs.\nQwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and\nQwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0\nlicense.",
        "url": "http://arxiv.org/abs/2509.17765v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17765v1",
        "arxiv_id": "2509.17765v1",
        "authors": [
            "Jin Xu",
            "Zhifang Guo",
            "Hangrui Hu",
            "Yunfei Chu",
            "Xiong Wang",
            "Jinzheng He",
            "Yuxuan Wang",
            "Xian Shi",
            "Ting He",
            "Xinfa Zhu",
            "Yuanjun Lv",
            "Yongqi Wang",
            "Dake Guo",
            "He Wang",
            "Linhan Ma",
            "Pei Zhang",
            "Xinyu Zhang",
            "Hongkun Hao",
            "Zishan Guo",
            "Baosong Yang",
            "Bin Zhang",
            "Ziyang Ma",
            "Xipin Wei",
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Peng Wang",
            "Mingkun Yang",
            "Dayiheng Liu",
            "Xingzhang Ren",
            "Bo Zheng",
            "Rui Men",
            "Fan Zhou",
            "Bowen Yu",
            "Jianxin Yang",
            "Le Yu",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "submitted": "2025-09-22 13:26:24",
        "source": "arxiv",
        "comment": "https://github.com/QwenLM/Qwen3-Omni"
    },
    {
        "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue",
        "abstract": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents.",
        "url": "http://arxiv.org/abs/2509.17766v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17766v1",
        "arxiv_id": "2509.17766v1",
        "authors": [
            "Ziyi Liu"
        ],
        "submitted": "2025-09-22 13:26:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Generative Framework for Personalized Sticker Retrieval",
        "abstract": "Formulating information retrieval as a variant of generative modeling,\nspecifically using autoregressive models to generate relevant identifiers for a\ngiven query, has recently attracted considerable attention. However, its\napplication to personalized sticker retrieval remains largely unexplored and\npresents unique challenges: existing relevance-based generative retrieval\nmethods typically lack personalization, leading to a mismatch between diverse\nuser expectations and the retrieved results. To address this gap, we propose\nPEARL, a novel generative framework for personalized sticker retrieval, and\nmake two key contributions: (i) To encode user-specific sticker preferences, we\ndesign a representation learning model to learn discriminative user\nrepresentations. It is trained on three prediction tasks that leverage personal\ninformation and click history; and (ii) To generate stickers aligned with a\nuser's query intent, we propose a novel intent-aware learning objective that\nprioritizes stickers associated with higher-ranked intents. Empirical results\nfrom both offline evaluations and online tests demonstrate that PEARL\nsignificantly outperforms state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.17749v2",
        "pdf_url": "http://arxiv.org/pdf/2509.17749v2",
        "arxiv_id": "2509.17749v2",
        "authors": [
            "Changjiang Zhou",
            "Ruqing Zhang",
            "Jiafeng Guo",
            "Yu-An Liu",
            "Fan Zhang",
            "Ganyuan Luo",
            "Xueqi Cheng"
        ],
        "submitted": "2025-09-22 13:11:44",
        "source": "arxiv",
        "comment": "Findings of EMNLP2025"
    },
    {
        "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual\nreasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly\nenhancing interpretability. However, existing MCoT methods rely on\nrationale-rich datasets and largely focus on inter-object reasoning,\noverlooking the intra-object understanding crucial for image classification. To\naddress this gap, we propose WISE, a Weak-supervision-guided Step-by-step\nExplanation method that augments any image classification dataset with MCoTs by\nreformulating the concept-based representations from Concept Bottleneck Models\n(CBMs) into concise, interpretable reasoning chains under weak supervision.\nExperiments across ten datasets show that our generated MCoTs not only improve\ninterpretability by 37% but also lead to gains in classification accuracy when\nused to fine-tune MLLMs. Our work bridges concept-based interpretability and\ngenerative MCoT reasoning, providing a generalizable framework for enhancing\nMLLMs in fine-grained visual understanding.",
        "url": "http://arxiv.org/abs/2509.17740v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17740v1",
        "arxiv_id": "2509.17740v1",
        "authors": [
            "Yiwen Jiang",
            "Deval Mehta",
            "Siyuan Yan",
            "Yaling Shen",
            "Zimu Wang",
            "Zongyuan Ge"
        ],
        "submitted": "2025-09-22 13:05:29",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Main)"
    },
    {
        "title": "Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics",
        "abstract": "Standard language models employ unique, monolithic embeddings for each token,\npotentially limiting their ability to capture the multifaceted nature of word\nmeanings. We investigate whether tokens can be more effectively represented\nthrough a compositional structure that accumulates diverse semantic facets. To\nexplore this, we propose Aggregate Semantic Grouping (ASG), a novel approach\nleveraging Product Quantization (PQ). We apply ASG to standard transformer\narchitectures (mBERT, XLM-R, mT5) and evaluate this representational scheme\nacross diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific\nbenchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing\ntokens compositionally via ASG achieves extreme compression in embedding\nparameters (0.4--0.5\\%) while maintaining $>$95\\% task performance relative to\nthe base model, even in generative tasks and extends to both cross lingual\ntransfer and domain-specific settings. These results validate the principle\nthat tokens can be effectively modeled as combinations of shared semantic\nbuilding blocks. ASG offers a simple yet concrete method for achieving this,\nshowcasing how compositional representations can capture linguistic richness\nwhile enabling compact yet semantically rich models.",
        "url": "http://arxiv.org/abs/2509.17737v2",
        "pdf_url": "http://arxiv.org/pdf/2509.17737v2",
        "arxiv_id": "2509.17737v2",
        "authors": [
            "Kavin R V",
            "Pawan Goyal"
        ],
        "submitted": "2025-09-22 13:04:48",
        "source": "arxiv",
        "comment": "5 pages, 1 figure Accepted at EMNLP 2025 Findings (Short)"
    },
    {
        "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs",
        "abstract": "Reinforcement learning (RL) has become a standard paradigm for refining large\nlanguage models (LLMs) beyond pre-training and instruction tuning. A prominent\nline of work is RL with verifiable rewards (RLVR), which leverages\nautomatically verifiable outcomes (e.g., correctness or executability) to\ngenerate reward signals. While efficient, this framework faces two key\nlimitations: First, its binary feedback is too sparse to capture the quality of\nthe reasoning process. Second, its coarse-grained rewards potentially lead to\nvanishing gradients. Inspired by observations from human learning, we introduce\na RL technique that integrates verifiable outcomes with the model's own\nconfidence estimates. This joint design enriches the reward signal, providing\nfiner-grained feedback and implicitly supervising the reasoning process.\nExperimental results demonstrate that our proposed method enhances RL\nperformance across multiple datasets and reduces token consumption during\ninference, while incurring negligible additional training cost. Moreover, it\ncan be used as a plug-in module to enhance other state-of-the-art RL methods.",
        "url": "http://arxiv.org/abs/2509.17730v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17730v1",
        "arxiv_id": "2509.17730v1",
        "authors": [
            "Bonan Zhang",
            "Zhongqi Chen",
            "Bowen Song",
            "Qinya Li",
            "Fan Wu",
            "Guihai Chen"
        ],
        "submitted": "2025-09-22 13:00:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs",
        "abstract": "Large Language Models (LLMs) are increasingly used for educational support,\nyet their response quality varies depending on the language of interaction.\nThis paper presents an automated multilingual pipeline for generating, solving,\nand evaluating math problems aligned with the German K-10 curriculum. We\ngenerated 628 math exercises and translated them into English, German, and\nArabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)\nwere prompted to produce step-by-step solutions in each language. A held-out\npanel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality\nusing a comparative framework. Results show a consistent gap, with English\nsolutions consistently rated highest, and Arabic often ranked lower. These\nfindings highlight persistent linguistic bias and the need for more equitable\nmultilingual AI systems in education.",
        "url": "http://arxiv.org/abs/2509.17701v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17701v1",
        "arxiv_id": "2509.17701v1",
        "authors": [
            "Mariam Mahran",
            "Katharina Simbeck"
        ],
        "submitted": "2025-09-22 12:38:09",
        "source": "arxiv",
        "comment": "Accepted at edu4AI'25: 2nd Workshop on Education for Artificial\n  Intelligence | co-located with ECAI, October 26th, 2025, Bologna, Italy. 7\n  pages, 0 figures"
    },
    {
        "title": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues",
        "abstract": "Evaluating large language models (LLMs) in long-form, knowledge-grounded\nrole-play dialogues remains challenging. This study compares LLM-generated and\nhuman-authored responses in multi-turn professional training simulations\nthrough human evaluation ($N=38$) and automated LLM-as-a-judge assessment.\nHuman evaluation revealed significant degradation in LLM-generated response\nquality across turns, particularly in naturalness, context maintenance and\noverall quality, while human-authored responses progressively improved. In line\nwith this finding, participants also indicated a consistent preference for\nhuman-authored dialogue. These human judgements were validated by our automated\nLLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment\nwith human evaluators on both zero-shot pairwise preference and stochastic\n6-shot construct ratings, confirming the widening quality gap between LLM and\nhuman responses over time. Our work contributes a multi-turn benchmark exposing\nLLM degradation in knowledge-grounded role-play dialogues and provides a\nvalidated hybrid evaluation framework to guide the reliable integration of LLMs\nin training simulations.",
        "url": "http://arxiv.org/abs/2509.17694v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17694v1",
        "arxiv_id": "2509.17694v1",
        "authors": [
            "Dongxu Lu",
            "Johan Jeuring",
            "Albert Gatt"
        ],
        "submitted": "2025-09-22 12:33:02",
        "source": "arxiv",
        "comment": "Accepted for publication at the 18th International Natural Language\n  Generation Conference (INLG 2025)"
    },
    {
        "title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation",
        "abstract": "LoRA has become one of the most widely used parameter-efficient fine-tuning\nmethods due to its simplicity and effectiveness. However, numerous studies have\nshown that LoRA often introduces substantial parameter redundancy, which not\nonly increases the number of trainable parameters but also hinders the\neffectiveness of fine-tuning. Since identifying redundant parameters in LoRA is\ninherently difficult, how to eliminate them efficiently and accurately remains\na challenging problem. In this paper, we propose TASO, a redundancy reduction\nmethod that leverages importance information from the pretrained model's\nweights to mitigate LoRA redundancy. Specifically, we estimate parameter\nimportance on downstream tasks and identify task-specific core regions based on\nthe distribution of importance scores. The location information of these core\nregions is then used to determine the sparse structure of LoRA modules,\nenabling redundancy removal before fine-tuning. Our approach significantly\nreduces the number of trainable parameters required for task adaptation, while\nproviding a novel task-aligned perspective for LoRA redundancy reduction.\nExperimental results demonstrate that, with a parameter budget comparable to\nLoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across\nmultiple tasks, achieving strong fine-tuning performance while effectively\neliminating redundant parameters.",
        "url": "http://arxiv.org/abs/2509.17688v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17688v1",
        "arxiv_id": "2509.17688v1",
        "authors": [
            "Daiye Miao",
            "Yufang Liu",
            "Jie Wang",
            "Changzhi Sun",
            "Yunke Zhang",
            "Demei Yan",
            "Shaokang Dong",
            "Qi Zhang",
            "Yuanbin Wu"
        ],
        "submitted": "2025-09-22 12:29:43",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Main Conference),13 pages,10 figures"
    },
    {
        "title": "When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables",
        "abstract": "Table question answering (TableQA) is a fundamental task in natural language\nprocessing (NLP). The strong reasoning capabilities of large language models\n(LLMs) have brought significant advances in this field. However, as real-world\napplications involve increasingly complex questions and larger tables,\nsubstantial noisy data is introduced, which severely degrades reasoning\nperformance. To address this challenge, we focus on improving two core\ncapabilities: Relevance Filtering, which identifies and retains information\ntruly relevant to reasoning, and Table Pruning, which reduces table size while\npreserving essential content. Based on these principles, we propose EnoTab, a\ndual denoising framework for complex questions and large-scale tables.\nSpecifically, we first perform Evidence-based Question Denoising by decomposing\nthe question into minimal semantic units and filtering out those irrelevant to\nanswer reasoning based on consistency and usability criteria. Then, we propose\nEvidence Tree-guided Table Denoising, which constructs an explicit and\ntransparent table pruning path to remove irrelevant data step by step. At each\npruning step, we observe the intermediate state of the table and apply a\npost-order node rollback mechanism to handle abnormal table states, ultimately\nproducing a highly reliable sub-table for final answer reasoning. Finally,\nextensive experiments show that EnoTab achieves outstanding performance on\nTableQA tasks with complex questions and large-scale tables, confirming its\neffectiveness.",
        "url": "http://arxiv.org/abs/2509.17680v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17680v1",
        "arxiv_id": "2509.17680v1",
        "authors": [
            "Shenghao Ye",
            "Yu Guo",
            "Dong Jin",
            "Yikai Shen",
            "Yunpeng Hou",
            "Shuangwu Chen",
            "Jian Yang",
            "Xiaofeng Jiang"
        ],
        "submitted": "2025-09-22 12:25:57",
        "source": "arxiv",
        "comment": "23 pages, 24 figures"
    },
    {
        "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications",
        "abstract": "The widespread adoption of Large Language Models (LLMs) has been hindered by\ntheir tendency to hallucinate, generating plausible but factually incorrect\ninformation. While Retrieval-Augmented Generation (RAG) systems attempt to\naddress this issue by grounding responses in external knowledge, hallucination\nremains a persistent challenge, particularly for morphologically complex,\nlow-resource languages like Turkish. This paper introduces Turk-LettuceDetect,\nthe first suite of hallucination detection models specifically designed for\nTurkish RAG applications. Building on the LettuceDetect framework, we formulate\nhallucination detection as a token-level classification task and fine-tune\nthree distinct encoder architectures: a Turkish-specific ModernBERT,\nTurkEmbed4STS, and multilingual EuroBERT. These models were trained on a\nmachine-translated version of the RAGTruth benchmark dataset containing 17,790\ninstances across question answering, data-to-text generation, and summarization\ntasks. Our experimental results show that the ModernBERT-based model achieves\nan F1-score of 0.7266 on the complete test set, with particularly strong\nperformance on structured tasks. The models maintain computational efficiency\nwhile supporting long contexts up to 8,192 tokens, making them suitable for\nreal-time deployment. Comparative analysis reveals that while state-of-the-art\nLLMs demonstrate high recall, they suffer from low precision due to\nover-generation of hallucinated content, underscoring the necessity of\nspecialized detection mechanisms. By releasing our models and translated\ndataset, this work addresses a critical gap in multilingual NLP and establishes\na foundation for developing more reliable and trustworthy AI applications for\nTurkish and other languages.",
        "url": "http://arxiv.org/abs/2509.17671v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17671v1",
        "arxiv_id": "2509.17671v1",
        "authors": [
            "Selva Taş",
            "Mahmut El Huseyni",
            "Özay Ezerceli",
            "Reyhan Bayraktar",
            "Fatma Betül Terzioğlu"
        ],
        "submitted": "2025-09-22 12:14:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation",
        "abstract": "With the rapid development of Large Language Models (LLMs), Controllable Text\nGeneration (CTG) has become a critical technology for enhancing system\nreliability and user experience. Addressing the limitations of traditional\nmethods, this paper proposes the PG-CE (Progressive Generation with Constraint\nEnhancement) approach, which decomposes CTG tasks into three steps: type\nprediction, constraint construction, and guided generation. This method employs\nconstraint generation models to dynamically build multi-dimensional constraints\nincluding tone, expression style, and thematic focus to guide output.\nExperiments demonstrate that PG-CE significantly improves generation quality\nacross multiple scenarios while maintaining text controllability, thematic\nrelevance, and response practicality. The research developed a dataset\ncontaining 90,000 constraint-text pairs (with an 8:2 ratio between daily and\nother topics), effectively reflecting real-world application requirements.",
        "url": "http://arxiv.org/abs/2509.17669v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17669v1",
        "arxiv_id": "2509.17669v1",
        "authors": [
            "Yan Zhuang",
            "Yuan Sun"
        ],
        "submitted": "2025-09-22 12:12:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Crosslingual Optimized Metric for Translation Assessment of Indian Languages",
        "abstract": "Automatic evaluation of translation remains a challenging task owing to the\northographic, morphological, syntactic and semantic richness and divergence\nobserved across languages. String-based metrics such as BLEU have previously\nbeen extensively used for automatic evaluation tasks, but their limitations are\nnow increasingly recognized. Although learned neural metrics have helped\nmitigate some of the limitations of string-based approaches, they remain\nconstrained by a paucity of gold evaluation data in most languages beyond the\nusual high-resource pairs. In this present work we address some of these gaps.\nWe create a large human evaluation ratings dataset for 13 Indian languages\ncovering 21 translation directions and then train a neural translation\nevaluation metric named Cross-lingual Optimized Metric for Translation\nAssessment of Indian Languages (COMTAIL) on this dataset. The best performing\nmetric variants show significant performance gains over previous\nstate-of-the-art when adjudging translation pairs with at least one Indian\nlanguage. Furthermore, we conduct a series of ablation studies to highlight the\nsensitivities of such a metric to changes in domain, translation quality, and\nlanguage groupings. We release both the COMTAIL dataset and the accompanying\nmetric models.",
        "url": "http://arxiv.org/abs/2509.17667v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17667v1",
        "arxiv_id": "2509.17667v1",
        "authors": [
            "Arafat Ahsan",
            "Vandan Mujadia",
            "Pruthwik Mishra",
            "Yash Bhaskar",
            "Dipti Misra Sharma"
        ],
        "submitted": "2025-09-22 12:11:42",
        "source": "arxiv",
        "comment": "Under review"
    },
    {
        "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?",
        "abstract": "Even without directly hearing sounds, humans can effortlessly reason about\nauditory properties, such as pitch, loudness, or sound-source associations,\ndrawing on auditory commonsense. In contrast, language models often lack this\ncapability, limiting their effectiveness in multimodal interactions. As an\ninitial step to address this gap, we present AuditoryBench++, a comprehensive\nbenchmark for evaluating auditory knowledge and reasoning in text-only\nsettings. The benchmark encompasses tasks that range from basic auditory\ncomparisons to contextually grounded reasoning, enabling fine-grained analysis\nof how models process and integrate auditory concepts. In addition, we\nintroduce AIR-CoT, a novel auditory imagination reasoning method that generates\nand integrates auditory information during inference through span detection\nwith special tokens and knowledge injection. Extensive experiments with recent\nLLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both\nthe off-the-shelf models and those augmented with auditory knowledge. The\nproject page is available at https://auditorybenchpp.github.io.",
        "url": "http://arxiv.org/abs/2509.17641v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17641v1",
        "arxiv_id": "2509.17641v1",
        "authors": [
            "Hyunjong Ok",
            "Suho Yoo",
            "Hyeonjun Kim",
            "Jaeho Lee"
        ],
        "submitted": "2025-09-22 11:45:22",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents",
        "abstract": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks\nwithin single domains. However, their reasoning and coordination capabilities\nin complex, multi-stage scenarios remain underexplored. Existing benchmarks\ntypically focus on isolated tasks or narrow domains, overlooking models'\nabilities for multi-stage collaboration and optimization without explicit\nexternal guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel\nbenchmark comprising 126696 domain-specific QA instances spanning scenarios in\nautomotive, pharmaceutical, electronics, and energy sectors. The dataset is\ncreated using a structured three-phase pipeline: dynamic sampling, iterative\nquestion-answer generation, and a multi-level quality assessment to ensure data\nquality. Tasks are further categorized into three difficulty levels according\nto stage coverage and complexity. With MSCoRe, we have conducted a\ncomprehensive evaluation of various state-of-the-art LLM agents. The commercial\nmodels performed best across all tasks and scenarios, but a notable gap in\nROUGE scores remains between simple and complex tasks. We also tested the\nmodels' robustness and found that their performance is negatively affected by\nnoisy data. MSCoRe provides a valuable new resource for the community to\nevaluate and improve multi-stage reasoning in LLM agents. The code and data are\navailable at https://github.com/D3E0-source/MSCoRE.",
        "url": "http://arxiv.org/abs/2509.17628v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17628v1",
        "arxiv_id": "2509.17628v1",
        "authors": [
            "Yuzhen Lei",
            "Hongbin Xie",
            "Jiaxing Zhao",
            "Shuangxue Liu",
            "Xuan Song"
        ],
        "submitted": "2025-09-22 11:36:16",
        "source": "arxiv",
        "comment": "10 pages, 5 figures"
    },
    {
        "title": "Human vs. Agent in Task-Oriented Conversations",
        "abstract": "Task-oriented conversational systems are essential for efficiently addressing\ndiverse user needs, yet their development requires substantial amounts of\nhigh-quality conversational data that is challenging and costly to obtain.\nWhile large language models (LLMs) have demonstrated potential in generating\nsynthetic conversations, the extent to which these agent-generated interactions\ncan effectively substitute real human conversations remains unclear. This work\npresents the first systematic comparison between LLM-simulated users and human\nusers in personalized task-oriented conversations. We propose a comprehensive\nanalytical framework encompassing three key aspects (conversation strategy,\ninteraction style, and conversation evaluation) and ten distinct dimensions for\nevaluating user behaviors, and collect parallel conversational datasets from\nboth human users and LLM agent users across four representative scenarios under\nidentical conditions. Our analysis reveals significant behavioral differences\nbetween the two user types in problem-solving approaches, question broadness,\nuser engagement, context dependency, feedback polarity and promise, language\nstyle, and hallucination awareness. We found consistency in the agent users and\nhuman users across the depth-first or breadth-first dimensions, as well as the\nusefulness dimensions. These findings provide critical insights for advancing\nLLM-based user simulation. Our multi-dimensional taxonomy constructed a\ngeneralizable framework for analyzing user behavior patterns, offering insights\nfrom LLM agent users and human users. By this work, we provide perspectives on\nrethinking how to use user simulation in conversational systems in the future.",
        "url": "http://arxiv.org/abs/2509.17619v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17619v1",
        "arxiv_id": "2509.17619v1",
        "authors": [
            "Zhefan Wang",
            "Ning Geng",
            "Zhiqiang Guo",
            "Weizhi Ma",
            "Min Zhang"
        ],
        "submitted": "2025-09-22 11:30:39",
        "source": "arxiv",
        "comment": "SIGIR-AP 2025"
    },
    {
        "title": "AutiHero: Leveraging Generative AI in Social Narratives to Engage Parents in Story-Driven Behavioral Guidance for Autistic Children",
        "abstract": "Social narratives are known to help autistic children understand and navigate\nsocial situations through stories. To ensure effectiveness, however, the\nmaterials need to be customized to reflect each child's unique behavioral\ncontext, requiring considerable time and effort for parents to practice at\nhome. We present AutiHero, a generative AI-based social narrative system for\nbehavioral guidance, which supports parents to create personalized stories for\ntheir autistic children and read them together. AutiHero generates text and\nvisual illustrations that reflect their children's interests, target behaviors,\nand everyday contexts. In a two-week deployment study with 16 autistic\nchild-parent dyads, parents created 218 stories and read an average of 4.25\nstories per day, demonstrating a high level of engagement. AutiHero also\nprovided an effective, low-demanding means to guide children's social\nbehaviors, encouraging positive change. We discuss the implications of\ngenerative AI-infused tools to empower parents in guiding their children's\nbehaviors, fostering their social learning.",
        "url": "http://arxiv.org/abs/2509.17608v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17608v1",
        "arxiv_id": "2509.17608v1",
        "authors": [
            "Jungeun Lee",
            "Kyungah Lee",
            "Inseok Hwang",
            "SoHyun Park",
            "Young-Ho Kim"
        ],
        "submitted": "2025-09-22 11:23:10",
        "source": "arxiv",
        "comment": "22 pages except reference"
    },
    {
        "title": "Asking a Language Model for Diverse Responses",
        "abstract": "Large language models increasingly rely on explicit reasoning chains and can\nproduce multiple plausible responses for a given context. We study the\ncandidate sampler that produces the set of plausible responses contrasting the\nancestral (parallel) sampling against two alternatives: enumeration, which asks\nthe model to produce $n$ candidates in one pass, and iterative sampling, which\nproposes candidates sequentially while conditioning on the currently generated\nresponse set. Under matched budgets, we compare these samplers on quality,\nlexical and computation flow diversity, and efficiency. Our empirical results\ndemonstrate that enumeration and iterative strategies result in higher\ndiversity at comparable quality. Our findings highlight the potential of simple\nnon-independent sampling strategies to improve response diversity without\nsacrificing generation quality.",
        "url": "http://arxiv.org/abs/2509.17570v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17570v1",
        "arxiv_id": "2509.17570v1",
        "authors": [
            "Sergey Troshin",
            "Irina Saparina",
            "Antske Fokkens",
            "Vlad Niculae"
        ],
        "submitted": "2025-09-22 11:01:22",
        "source": "arxiv",
        "comment": "UncertaiNLP workshop, 2025"
    },
    {
        "title": "Specification-Aware Machine Translation and Evaluation for Purpose Alignment",
        "abstract": "In professional settings, translation is guided by communicative goals and\nclient needs, often formalized as specifications. While existing evaluation\nframeworks acknowledge the importance of such specifications, these\nspecifications are often treated only implicitly in machine translation (MT)\nresearch. Drawing on translation studies, we provide a theoretical rationale\nfor why specifications matter in professional translation, as well as a\npractical guide to implementing specification-aware MT and evaluation. Building\non this foundation, we apply our framework to the translation of investor\nrelations texts from 33 publicly listed companies. In our experiment, we\ncompare five translation types, including official human translations and\nprompt-based outputs from large language models (LLMs), using expert error\nanalysis, user preference rankings, and an automatic metric. The results show\nthat LLM translations guided by specifications consistently outperformed\nofficial human translations in human evaluations, highlighting a gap between\nperceived and expected quality. These findings demonstrate that integrating\nspecifications into MT workflows, with human oversight, can improve translation\nquality in ways aligned with professional practice.",
        "url": "http://arxiv.org/abs/2509.17559v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17559v1",
        "arxiv_id": "2509.17559v1",
        "authors": [
            "Yoko Kayano",
            "Saku Sugawara"
        ],
        "submitted": "2025-09-22 10:50:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning",
        "abstract": "The remarkable performance of Large Language Models (LLMs) can be enhanced\nwith test-time computation, which relies on external tools and even other deep\nlearning models. However, existing approaches for integrating non-text modality\nrepresentations into LLMs typically require additional costly supervised\ntraining, restricting on-the-fly adaptation to new domains and modalities. In\nthis work, we explore the feasibility of integrating representations from\nnon-text foundational models (FMs) into text-based LLMs in a training-free\nmanner. We propose In-Context Representation Learning (ICRL) as a\nproof-of-concept to allow LLMs to adaptively utilize non-text modality\nrepresentations with few-shot learning. Unlike traditional in-context learning,\nwhich incorporates text-label pairs, ICRL replaces text inputs with FM\nrepresentations, enabling the LLM to perform multi-modal inference without\nfine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,\ninvestigating three core research questions: (i) how to map FM representations\ninto LLMs in a training-free manner, (ii) what factors influence ICRL\nperformance, and (iii) what mechanisms underlie the effectiveness of ICRL. To\nthe best of our knowledge, ICRL is the first training-free framework for\nintegrating non-text modality representations into text-based LLMs, presenting\na promising direction for adaptable, multi-modal generalization.",
        "url": "http://arxiv.org/abs/2509.17552v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17552v1",
        "arxiv_id": "2509.17552v1",
        "authors": [
            "Tianle Zhang",
            "Wanlong Fang",
            "Jonathan Woo",
            "Paridhi Latawa",
            "Deepak A. Subramanian",
            "Alvin Chan"
        ],
        "submitted": "2025-09-22 09:16:34",
        "source": "arxiv",
        "comment": "NIPS 2025"
    },
    {
        "title": "Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models",
        "abstract": "Self-supervised learning (SSL) has made significant advances in speech\nrepresentation learning. Models like wav2vec 2.0 and HuBERT have achieved\nstate-of-the-art results in tasks such as speech recognition, particularly in\nmonolingual settings. However, multilingual SSL models tend to underperform\ntheir monolingual counterparts on each individual language, especially in\nmultilingual scenarios with few languages such as the bilingual setting. In\nthis work, we investigate a novel approach to reduce this performance gap by\nintroducing limited visual grounding into bilingual speech SSL models. Our\nresults show that visual grounding benefits both monolingual and bilingual\nmodels, with especially pronounced gains for the latter, reducing the\nmultilingual performance gap on zero-shot phonetic discrimination from 31.5%\nfor audio-only models to 8.04% with grounding.",
        "url": "http://arxiv.org/abs/2509.17523v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17523v1",
        "arxiv_id": "2509.17523v1",
        "authors": [
            "María Andrea Cruz Blandón",
            "Zakaria Aldeneh",
            "Jie Chi",
            "Maureen de Seyssel"
        ],
        "submitted": "2025-09-22 08:48:04",
        "source": "arxiv",
        "comment": "5 pages, 2 figures"
    },
    {
        "title": "CorefInst: Leveraging LLMs for Multilingual Coreference Resolution",
        "abstract": "Coreference Resolution (CR) is a crucial yet challenging task in natural\nlanguage understanding, often constrained by task-specific architectures and\nencoder-based language models that demand extensive training and lack\nadaptability. This study introduces the first multilingual CR methodology which\nleverages decoder-only LLMs to handle both overt and zero mentions. The article\nexplores how to model the CR task for LLMs via five different instruction sets\nusing a controlled inference method. The approach is evaluated across three\nLLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when\ninstruction-tuned with a suitable instruction set, can surpass state-of-the-art\ntask-specific architectures. Specifically, our best model, a fully fine-tuned\nLlama 3.1 for multilingual CR, outperforms the leading multilingual CR model\n(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages\nin the CorefUD v1.2 dataset collection.",
        "url": "http://arxiv.org/abs/2509.17505v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17505v1",
        "arxiv_id": "2509.17505v1",
        "authors": [
            "Tuğba Pamay Arslan",
            "Emircan Erol",
            "Gülşen Eryiğit"
        ],
        "submitted": "2025-09-22 08:35:21",
        "source": "arxiv",
        "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL) (2025 August). Submission: March, 2025.\n  Revision: July, 2025. Acceptance: August, 2025"
    },
    {
        "title": "Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages",
        "abstract": "As large language models (LLMs) are trained on increasingly diverse and\nextensive multilingual corpora, they demonstrate cross-lingual transfer\ncapabilities. However, these capabilities often fail to effectively extend to\nlow-resource languages, particularly those utilizing non-Latin scripts. While\ntransliterating low-resource languages into Latin script presents a natural\nsolution, there currently lacks a comprehensive framework for integrating\ntransliteration into LLMs training and deployment. Taking a pragmatic approach,\nthis paper innovatively combines character transliteration with Huffman coding\nto design a complete transliteration framework. Our proposed framework offers\nthe following advantages: 1) Compression: Reduces storage requirements for\nlow-resource language content, achieving up to 50% reduction in file size and\n50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless\nconversion from transliterated text back to the source language. 3) Efficiency:\nEliminates the need for vocabulary expansion for low-resource languages,\nimproving training and inference efficiency. 4) Scalability: The framework can\nbe extended to other low-resource languages. We validate the effectiveness of\nour framework across multiple downstream tasks, including text classification,\nmachine reading comprehension, and machine translation. Experimental results\ndemonstrate that our method significantly enhances the model's capability to\nprocess low-resource languages while maintaining performance on high-resource\nlanguages. Our data and code are publicly available at\nhttps://github.com/CMLI-NLP/HuffmanTranslit.",
        "url": "http://arxiv.org/abs/2509.17493v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17493v1",
        "arxiv_id": "2509.17493v1",
        "authors": [
            "Wenhao Zhuang",
            "Yuan Sun",
            "Xiaobing Zhao"
        ],
        "submitted": "2025-09-22 08:24:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM",
        "abstract": "Large language models (LLMs) have advanced code generation from\nsingle-function tasks to competitive-programming problems, but existing\nmulti-agent solutions either rely on costly large-scale ($>$ 30B) models or\ncollapse when downsized to small open-source models. We present MapCoder-Lite,\nwhich upgrades a single 7B model into four role-specialised agents-retriever,\nplanner, coder, and debugger-using only rank-32, role-specific LoRA adapters\n($<3\\%$ extra parameters). Three lightweight techniques make this possible: (i)\ntrajectory distillation from strong LLMs fixes format fragility in retrieval\nand debugging, (ii) supervisor-guided correction strengthens planning and\ncoding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient\nspecialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests\nshows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\\%$ to\n$28.3\\%$), eliminates all format failures, and closes to within six points of a\n32B baseline while cutting GPU memory and token-generation time by $4\\times$.\nThese results demonstrate that careful agent-wise fine-tuning unleashes\nhigh-quality multi-agent coding on a small language model.",
        "url": "http://arxiv.org/abs/2509.17489v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17489v1",
        "arxiv_id": "2509.17489v1",
        "authors": [
            "Woongkyu Lee",
            "Junhee Cho",
            "Jungwook Choi"
        ],
        "submitted": "2025-09-22 08:19:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation improves the factual accuracy of Large\nLanguage Models (LLMs) by incorporating external context, but often suffers\nfrom irrelevant retrieved content that hinders effectiveness. Context\ncompression addresses this issue by filtering out irrelevant information from\ncontext before LLM generation. However, existing methods struggle to adaptively\nadjust compression rates for different context, maintain low latency and\nintegrate information across multiple documents. To overcome these limitations,\nWe introduce AttnComp, an adaptive, efficient and context-aware compression\nframework. By leveraging the attention mechanism of LLMs to identify relevant\ninformation, AttnComp employs a Top-P compression algorithm to retain the\nminimal set of documents whose cumulative attention weights exceeds a\npredefined threshold. In addition to compression, AttnComp estimates response\nconfidence by assessing the overall relevance of the retrieved content,\nenabling users to gauge response reliability. Experiments demonstrate that\nAttnComp outperforms existing compression methods and uncompressed baselines,\nachieving higher accuracy with substantial compression rates and lower latency.",
        "url": "http://arxiv.org/abs/2509.17486v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17486v1",
        "arxiv_id": "2509.17486v1",
        "authors": [
            "Lvzhou Luo",
            "Yixuan Cao",
            "Ping Luo"
        ],
        "submitted": "2025-09-22 08:18:50",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Findings)"
    },
    {
        "title": "Diagnosing Model Editing via Knowledge Spectrum",
        "abstract": "Model editing, the process of efficiently modifying factual knowledge in\npre-trained language models, is critical for maintaining their accuracy and\nrelevance. However, existing editing methods often introduce unintended side\neffects, degrading model performance in unpredictable ways. While much research\nhas focused on improving editing algorithms, the role of the target knowledge's\nintrinsic properties remains a significant, underexplored factor. This paper\naddresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic\nframework for categorizing knowledge based on its real-world popularity, the\nmodel's pre-edit familiarity, and the linguistic structure of the eliciting\nquestion. Our empirical analysis reveals that these characteristics are strong\npredictors of editing success and stability. Informed by these findings, we\nintroduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that\ntailors editing intensity to the diagnosed difficulty of a knowledge item. We\ndemonstrate that this framework significantly improves success rates for\nchallenging edits while optimizing computational resources. Our work provides a\nmore comprehensive understanding of the factors governing model editing.",
        "url": "http://arxiv.org/abs/2509.17482v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17482v1",
        "arxiv_id": "2509.17482v1",
        "authors": [
            "Tsung-Hsuan Pan",
            "Chung-Chi Chen",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "submitted": "2025-09-22 08:16:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding",
        "abstract": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable\nprogress, yet hallucination remains a critical barrier, particularly in chart\nunderstanding, which requires sophisticated perceptual and cognitive abilities\nas well as rigorous factual accuracy. While prior work has investigated\nhallucinations and chart comprehension independently, their intersection\nremains largely unexplored. To address this gap, we present ChartHal, a\nbenchmark that features a fine-grained taxonomy of hallucination scenarios in\nchart understanding, along with a human-validated dataset of 1,062 samples. Our\nevaluation shows that state-of-the-art LVLMs suffer from severe hallucinations\non ChartHal, including proprietary models such as GPT-5 and o4-mini, which\nachieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals\nthat questions involving information absent from or contradictory to charts are\nespecially likely to trigger hallucinations, underscoring the urgent need for\nmore robust mitigation strategies. Code and data are available at\nhttps://github.com/ymcui/ChartHal .",
        "url": "http://arxiv.org/abs/2509.17481v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17481v1",
        "arxiv_id": "2509.17481v1",
        "authors": [
            "Xingqi Wang",
            "Yiming Cui",
            "Xin Yao",
            "Shijin Wang",
            "Guoping Hu",
            "Xiaoyu Qin"
        ],
        "submitted": "2025-09-22 08:15:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes",
        "abstract": "Non-native English speakers performing English-related tasks at work struggle\nto sustain ESL learning, despite their motivation. Often, study materials are\ndisconnected from their work context. Although workers rely on LLM assistants\nto address their immediate needs, these interactions may not directly\ncontribute to their English skills. We present LingoQ, an AI-mediated system\nthat allows workers to practice English using quizzes generated from their LLM\nqueries during work. LingoQ leverages these queries using AI to generate\npersonalized quizzes that workers can review and practice on their smartphones.\nWe conducted a three-week deployment study with 28 ESL workers to evaluate\nLingoQ. Participants valued the relevance of quizzes that reflect their own\ncontext, constantly engaging with the app during the study. This active\nengagement improved self-efficacy and led to learning gains for beginners and,\npotentially, for intermediate learners. We discuss opportunities of leveraging\nusers' reliance on LLMs to situate their learning in the user context for\nimproved learning.",
        "url": "http://arxiv.org/abs/2509.17477v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17477v1",
        "arxiv_id": "2509.17477v1",
        "authors": [
            "Yeonsun Yang",
            "Sang Won Lee",
            "Jean Y. Song",
            "Sangdoo Yun",
            "Young-Ho Kim"
        ],
        "submitted": "2025-09-22 08:12:10",
        "source": "arxiv",
        "comment": "17 pages except reference"
    },
    {
        "title": "LongEval at CLEF 2025: Longitudinal Evaluation of IR Systems on Web and Scientific Data",
        "abstract": "The LongEval lab focuses on the evaluation of information retrieval systems\nover time. Two datasets are provided that capture evolving search scenarios\nwith changing documents, queries, and relevance assessments. Systems are\nassessed from a temporal perspective-that is, evaluating retrieval\neffectiveness as the data they operate on changes. In its third edition,\nLongEval featured two retrieval tasks: one in the area of ad-hoc web retrieval,\nand another focusing on scientific article retrieval. We present an overview of\nthis year's tasks and datasets, as well as the participating systems. A total\nof 19 teams submitted their approaches, which we evaluated using nDCG and a\nvariety of measures that quantify changes in retrieval effectiveness over time.",
        "url": "http://arxiv.org/abs/2509.17469v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17469v1",
        "arxiv_id": "2509.17469v1",
        "authors": [
            "Matteo Cancellieri",
            "Alaa El-Ebshihy",
            "Tobias Fink",
            "Maik Fröbe",
            "Petra Galuščáková",
            "Gabriela Gonzalez-Saez",
            "Lorraine Goeuriot",
            "David Iommi",
            "Jüri Keller",
            "Petr Knoth",
            "Philippe Mulhem",
            "Florina Piroi",
            "David Pride",
            "Philipp Schaer"
        ],
        "submitted": "2025-09-22 08:05:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Autiverse: Eliciting Autistic Adolescents' Daily Narratives through AI-guided Multimodal Journaling",
        "abstract": "Journaling can potentially serve as an effective method for autistic\nadolescents to improve narrative skills. However, its text-centric nature and\nhigh executive functioning demands present barriers to practice. We present\nAutiverse, an AI-guided multimodal journaling app for tablets that scaffolds\nstorytelling through conversational prompts and visual supports. Autiverse\nelicits key details through a stepwise dialogue with peer-like, customizable AI\nand composes them into an editable four-panel comic strip. Through a two-week\ndeployment study with 10 autistic adolescent-parent dyads, we examine how\nAutiverse supports autistic adolescents to organize their daily experience and\nemotion. Autiverse helped them construct coherent narratives, while enabling\nparents to learn additional details of their child's events and emotions. The\ncustomized AI peer created a comfortable space for sharing, fostering enjoyment\nand a strong sense of agency. We discuss the implications of designing\ntechnologies that complement autistic adolescents' strengths while ensuring\ntheir autonomy and safety in sharing experiences.",
        "url": "http://arxiv.org/abs/2509.17466v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17466v1",
        "arxiv_id": "2509.17466v1",
        "authors": [
            "Migyeong Yang",
            "Kyungah Lee",
            "Jinyoung Han",
            "SoHyun Park",
            "Young-Ho Kim"
        ],
        "submitted": "2025-09-22 08:02:09",
        "source": "arxiv",
        "comment": "19 pages excluding reference"
    },
    {
        "title": "PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents",
        "abstract": "Dialogue agents based on large language models (LLMs) have shown promising\nperformance in proactive dialogue, which requires effective strategy planning.\nHowever, existing approaches to strategy planning for proactive dialogue face\nseveral limitations: limited strategy coverage, preference bias in planning,\nand reliance on costly additional training. To address these, we propose\nPRINCIPLES: a synthetic strategy memory for proactive dialogue agents.\nPRINCIPLES is derived through offline self-play simulations and serves as\nreusable knowledge that guides strategy planning during inference, eliminating\nthe need for additional training and data annotation. We evaluate PRINCIPLES in\nboth emotional support and persuasion domains, demonstrating consistent\nimprovements over strong baselines. Furthermore, PRINCIPLES maintains its\nrobustness across extended and more diverse evaluation settings. See our\nproject page at https://huggingface.co/spaces/kimnamssya/Principles.",
        "url": "http://arxiv.org/abs/2509.17459v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17459v1",
        "arxiv_id": "2509.17459v1",
        "authors": [
            "Namyoung Kim",
            "Kai Tzu-iunn Ong",
            "Yeonjun Hwang",
            "Minseok Kang",
            "Iiseo Jihn",
            "Gayoung Kim",
            "Minju Kim",
            "Jinyoung Yeo"
        ],
        "submitted": "2025-09-22 07:53:59",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings"
    },
    {
        "title": "Codifying Natural Langauge Tasks",
        "abstract": "We explore the applicability of text-to-code to solve real-world problems\nthat are typically solved in natural language, such as legal judgment and\nmedical QA. Unlike previous works, our approach leverages the explicit\nreasoning provided by program generation. We present ICRAG, a framework that\ntransforms natural language into executable programs through iterative\nrefinement using external knowledge from domain resources and GitHub. Across 13\nbenchmarks, ICRAG achieves up to 161.1\\% relative improvement. We provide a\ndetailed analysis of the generated code and the impact of external knowledge,\nand we discuss the limitations of applying text-to-code approaches to\nreal-world natural language tasks.",
        "url": "http://arxiv.org/abs/2509.17455v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17455v1",
        "arxiv_id": "2509.17455v1",
        "authors": [
            "Haoyang Chen",
            "Kumiko Tanaka-Ishii"
        ],
        "submitted": "2025-09-22 07:49:58",
        "source": "arxiv",
        "comment": "Submitted to Journal of Automated Software Engineering"
    },
    {
        "title": "SLAyiNG: Towards Queer Language Processing",
        "abstract": "Knowledge of slang is a desirable feature of LLMs in the context of user\ninteraction, as slang often reflects an individual's social identity. Several\nworks on informal language processing have defined and curated benchmarks for\ntasks such as detection and identification of slang. In this paper, we focus on\nqueer slang. Queer slang can be mistakenly flagged as hate speech or can evoke\nnegative responses from LLMs during user interaction. Research efforts so far\nhave not focused explicitly on queer slang. In particular, detection and\nprocessing of queer slang have not been thoroughly evaluated due to the lack of\na high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the\nfirst dataset containing annotated queer slang derived from subtitles, social\nmedia posts, and podcasts, reflecting real-world usage. We describe our data\ncuration process, including the collection of slang terms and definitions,\nscraping sources for examples that reflect usage of these terms, and our\nongoing annotation process. As preliminary results, we calculate\ninter-annotator agreement for human annotators and OpenAI's model o3-mini,\nevaluating performance on the task of sense disambiguation. Reaching an average\nKrippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models\ncan serve as tools for pre-filtering, but the complex and often sensitive\nnature of queer language data requires expert and community-driven annotation\nefforts.",
        "url": "http://arxiv.org/abs/2509.17449v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17449v1",
        "arxiv_id": "2509.17449v1",
        "authors": [
            "Leonor Veloso",
            "Lea Hirlimann",
            "Philipp Wicke",
            "Hinrich Schütze"
        ],
        "submitted": "2025-09-22 07:41:45",
        "source": "arxiv",
        "comment": "To be presented at Queer in AI @ NeurIPS 2025 (non-archival)"
    },
    {
        "title": "Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks",
        "abstract": "Reliable question answering with large language models (LLMs) is challenged\nby hallucinations, fluent but factually incorrect outputs arising from\nepistemic uncertainty. Existing entropy-based semantic-level uncertainty\nestimation methods are limited by sampling noise and unstable clustering of\nvariable-length answers. We propose Semantic Reformulation Entropy (SRE), which\nimproves uncertainty estimation in two ways. First, input-side semantic\nreformulations produce faithful paraphrases, expand the estimation space, and\nreduce biases from superficial decoder tendencies. Second, progressive,\nenergy-based hybrid clustering stabilizes semantic grouping. Experiments on\nSQuAD and TriviaQA show that SRE outperforms strong baselines, providing more\nrobust and generalizable hallucination detection. These results demonstrate\nthat combining input diversification with multi-signal clustering substantially\nenhances semantic-level uncertainty estimation.",
        "url": "http://arxiv.org/abs/2509.17445v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17445v1",
        "arxiv_id": "2509.17445v1",
        "authors": [
            "Chaodong Tong",
            "Qi Zhang",
            "Lei Jiang",
            "Yanbing Liu",
            "Nannan Sun",
            "Wei Li"
        ],
        "submitted": "2025-09-22 07:38:45",
        "source": "arxiv",
        "comment": "5pages, 5 figures, submit to ICASSP 2026"
    },
    {
        "title": "Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system",
        "abstract": "This study investigates the applicability of HealthBench, a large-scale,\nrubric-based medical benchmark, to the Japanese context. While robust\nevaluation frameworks are crucial for the safe development of medical LLMs,\nresources in Japanese remain limited, often relying on translated\nmultiple-choice questions. Our research addresses this gap by first\nestablishing a performance baseline, applying a machine-translated version of\nHealthBench's 5,000 scenarios to evaluate both a high-performing multilingual\nmodel (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,\nwe employ an LLM-as-a-Judge approach to systematically classify the benchmark's\nscenarios and rubric criteria, identifying \"contextual gaps\" where content is\nmisaligned with Japan's clinical guidelines, healthcare systems, or cultural\nnorms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric\nmismatches and a significant failure in the Japanese-native model, which lacked\nthe required clinical completeness. Furthermore, our classification indicates\nthat while the majority of scenarios are applicable, a substantial portion of\nthe rubric criteria requires localization. This work underscores the\nlimitations of direct benchmark translation and highlights the urgent need for\na context-aware, localized adaptation, a J-HealthBench, to ensure the reliable\nand safe evaluation of medical LLMs in Japan.",
        "url": "http://arxiv.org/abs/2509.17444v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17444v1",
        "arxiv_id": "2509.17444v1",
        "authors": [
            "Shohei Hisada",
            "Endo Sunao",
            "Himi Yamato",
            "Shoko Wakamiya",
            "Eiji Aramaki"
        ],
        "submitted": "2025-09-22 07:36:12",
        "source": "arxiv",
        "comment": "draft v0.1"
    },
    {
        "title": "WildClaims: Information Access Conversations in the Wild(Chat)",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has transformed\nconversational systems into practical tools used by millions. However, the\nnature and necessity of information retrieval in real-world conversations\nremain largely unexplored, as research has focused predominantly on\ntraditional, explicit information access conversations. The central question\nis: What do real-world information access conversations look like? To this end,\nwe first conduct an observational study on the WildChat dataset, large-scale\nuser-ChatGPT conversations, finding that users' access to information occurs\nimplicitly as check-worthy factual assertions made by the system, even when the\nconversation's primary intent is non-informational, such as creative writing.\nTo enable the systematic study of this phenomenon, we release the WildClaims\ndataset, a novel resource consisting of 121,905 extracted factual claims from\n7,587 utterances in 3,000 WildChat conversations, each annotated for\ncheck-worthiness. Our preliminary analysis of this resource reveals that\nconservatively 18% to 51% of conversations contain check-worthy assertions,\ndepending on the methods employed, and less conservatively, as many as 76% may\ncontain such assertions. This high prevalence underscores the importance of\nmoving beyond the traditional understanding of explicit information access, to\naddress the implicit information access that arises in real-world user-system\nconversations.",
        "url": "http://arxiv.org/abs/2509.17442v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17442v1",
        "arxiv_id": "2509.17442v1",
        "authors": [
            "Hideaki Joko",
            "Shakiba Amirshahi",
            "Charles L. A. Clarke",
            "Faegheh Hasibi"
        ],
        "submitted": "2025-09-22 07:32:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting",
        "abstract": "The longitudinal evaluation of retrieval systems aims to capture how\ninformation needs and documents evolve over time. However, classical\nCranfield-style retrieval evaluations only consist of a static set of queries\nand documents and thereby miss time as an evaluation dimension. Therefore,\nlongitudinal evaluations need to complement retrieval toolkits with custom\nlogic. This custom logic increases the complexity of research software, which\nmight reduce the reproducibility and extensibility of experiments. Based on our\nsubmissions to the 2024 edition of LongEval, we propose a custom extension of\nir_datasets for longitudinal retrieval experiments. This extension allows for\ndeclaratively, instead of imperatively, describing important aspects of\nlongitudinal retrieval experiments, e.g., which queries, documents, and/or\nrelevance feedback are available at which point in time. We reimplement our\nsubmissions to LongEval 2024 against our new ir_datasets extension, and find\nthat the declarative access can reduce the complexity of the code.",
        "url": "http://arxiv.org/abs/2509.17440v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17440v1",
        "arxiv_id": "2509.17440v1",
        "authors": [
            "Jüri Keller",
            "Maik Fröbe",
            "Gijs Hendriksen",
            "Daria Alexander",
            "Martin Potthast",
            "Philipp Schaer"
        ],
        "submitted": "2025-09-22 07:29:34",
        "source": "arxiv",
        "comment": "Best of labs paper for LongEval at CLEF 2024"
    },
    {
        "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning",
        "abstract": "Recent advancements in reinforcement learning (RL) have enhanced the\nreasoning abilities of large language models (LLMs), yet the impact on\nmultimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like\ngeometric reasoning, MLLMs hallucinate frequently, leading to inaccurate\nreasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps\nthe benefits of reasoning training. To quantify this, we design a\nGeo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric\nconcepts and spatial relationships. Experiments on GeoPQA reveal significant\nshortcomings of MLLMs in visual perception, which constrain RL reward signals\nfor effective training. To address this bottleneck, we propose a two-stage RL\ntraining framework by first enhancing the visual perception of geometric\nstructures, then fostering reasoning capabilities. Applied to\nQwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by\n9.7% and geometric problem solving by 9.1%, compared to the direct reasoning\ntraining approach. Our method also generalizes to other vision-intensive\ndomains like figure understanding, highlighting the importance of perceptual\ngrounding in effective MLLM reasoning.",
        "url": "http://arxiv.org/abs/2509.17437v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17437v1",
        "arxiv_id": "2509.17437v1",
        "authors": [
            "Guizhen Chen",
            "Weiwen Xu",
            "Hao Zhang",
            "Hou Pong Chan",
            "Deli Zhao",
            "Anh Tuan Luu",
            "Yu Rong"
        ],
        "submitted": "2025-09-22 07:28:09",
        "source": "arxiv",
        "comment": "Accepted to EMNLP2025 Findings"
    },
    {
        "title": "MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses",
        "abstract": "Medical fact-checking has become increasingly critical as more individuals\nseek medical information online. However, existing datasets predominantly focus\non human-generated content, leaving the verification of content generated by\nlarge language models (LLMs) relatively unexplored. To address this gap, we\nintroduce MedFact, the first evidence-based Chinese medical fact-checking\ndataset of LLM-generated medical content. It consists of 1,321 questions and\n7,409 claims, mirroring the complexities of real-world medical scenarios. We\nconduct comprehensive experiments in both in-context learning (ICL) and\nfine-tuning settings, showcasing the capability and challenges of current LLMs\non this task, accompanied by an in-depth error analysis to point out key\ndirections for future research. Our dataset is publicly available at\nhttps://github.com/AshleyChenNLP/MedFact.",
        "url": "http://arxiv.org/abs/2509.17436v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17436v1",
        "arxiv_id": "2509.17436v1",
        "authors": [
            "Tong Chen",
            "Zimu Wang",
            "Yiyi Miao",
            "Haoran Luo",
            "Yuanfei Sun",
            "Wei Wang",
            "Zhengyong Jiang",
            "Procheta Sen",
            "Jionglong Su"
        ],
        "submitted": "2025-09-22 07:26:47",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025. Camera-ready version"
    },
    {
        "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models",
        "abstract": "The demand for efficient deployment of large language models (LLMs) has\ndriven interest in quantization, which reduces inference cost, and\nparameter-efficient fine-tuning (PEFT), which lowers training overhead. This\nmotivated the development of quantization-aware PEFT to produce accurate yet\nefficient quantized models. In this setting, reducing quantization error prior\nto fine-tuning is crucial for achieving high model accuracy. However, existing\nmethods that rely on low-rank adaptation suffer from limited representational\ncapacity. Recent Fourier-related transform (FT)-based adapters offer greater\nrepresentational power than low-rank adapters, but their direct integration\ninto quantized models often results in ineffective error reduction and\nincreased computational overhead. To overcome these limitations, we propose\nQWHA, a method that integrates FT-based adapters into quantized models by\nemploying the Walsh-Hadamard Transform (WHT) as the transform kernel, together\nwith a novel adapter initialization scheme incorporating adaptive parameter\nselection and value refinement. We demonstrate that QWHA effectively mitigates\nquantization errors while facilitating fine-tuning, and that its design\nsubstantially reduces computational cost. Experimental results show that QWHA\nconsistently outperforms baselines in low-bit quantization accuracy and\nachieves significant training speedups over existing FT-based adapters. The\ncode is available at https://github.com/vantaa89/qwha.",
        "url": "http://arxiv.org/abs/2509.17428v2",
        "pdf_url": "http://arxiv.org/pdf/2509.17428v2",
        "arxiv_id": "2509.17428v2",
        "authors": [
            "Hyesung Jeon",
            "Seojune Lee",
            "Beomseok Kang",
            "Yulhwa Kim",
            "Jae-Joon Kim"
        ],
        "submitted": "2025-09-22 07:21:41",
        "source": "arxiv",
        "comment": "25 pages, 9 figures, 14 tables"
    },
    {
        "title": "RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios",
        "abstract": "While various multimodal multi-image evaluation datasets have been emerged,\nbut these datasets are primarily based on English, and there has yet to be a\nChinese multi-image dataset. To fill this gap, we introduce RealBench, the\nfirst Chinese multimodal multi-image dataset, which contains 9393 samples and\n69910 images. RealBench distinguishes itself by incorporating real\nuser-generated content, ensuring high relevance to real-world applications.\nAdditionally, the dataset covers a wide variety of scenes, image resolutions,\nand image structures, further increasing the difficulty of multi-image\nunderstanding. Ultimately, we conduct a comprehensive evaluation of RealBench\nusing 21 multimodal LLMs of different sizes, including closed-source models\nthat support multi-image inputs as well as open-source visual and video models.\nThe experimental results indicate that even the most powerful closed-source\nmodels still face challenges when handling multi-image Chinese scenarios.\nMoreover, there remains a noticeable performance gap of around 71.8\\% on\naverage between open-source visual/video models and closed-source models. These\nresults show that RealBench provides an important research foundation for\nfurther exploring multi-image understanding capabilities in the Chinese\ncontext.",
        "url": "http://arxiv.org/abs/2509.17421v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17421v1",
        "arxiv_id": "2509.17421v1",
        "authors": [
            "Fei Zhao",
            "Chengqiang Lu",
            "Yufan Shen",
            "Qimeng Wang",
            "Yicheng Qian",
            "Haoxin Zhang",
            "Yan Gao",
            "Yi Wu",
            "Yao Hu",
            "Zhen Wu",
            "Shangyu Xing",
            "Xinyu Dai"
        ],
        "submitted": "2025-09-22 07:14:31",
        "source": "arxiv",
        "comment": "Findings of EMNLP 2025 camera-ready"
    },
    {
        "title": "Vision Language Models Are Not (Yet) Spelling Correctors",
        "abstract": "Spelling correction from visual input poses unique challenges for vision\nlanguage models (VLMs), as it requires not only detecting but also correcting\ntextual errors directly within images. We present ReViCo (Real Visual\nCorrection), the first benchmark that systematically evaluates VLMs on\nreal-world visual spelling correction across Chinese and English. ReViCo\ncontains naturally occurring errors collected from real-world image data and\nsupports fine-grained evaluation at both image and token levels. Through\ncomprehensive experiments on representative cascaded (Qwen) and native\n(InternVL) open-source models, as well as closed-source systems (GPT-4o,\nClaude), we show that current VLMs fall significantly short of human\nperformance, particularly in correction. To address these limitations, we\nexplore two solution paradigms: a Joint OCR-Correction pipeline and a\nBackground Information enhanced approach, both of which yield consistent\nperformance gains. Our analysis highlights fundamental limitations of existing\narchitectures and provides actionable insights for advancing multimodal\nspelling correction.",
        "url": "http://arxiv.org/abs/2509.17418v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17418v1",
        "arxiv_id": "2509.17418v1",
        "authors": [
            "Junhong Liang",
            "Bojun Zhang"
        ],
        "submitted": "2025-09-22 07:10:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context",
        "abstract": "Large language models (LLMs) are widely used in various tasks and\napplications. However, despite their wide capabilities, they are shown to lack\ncultural alignment \\citep{ryan-etal-2024-unintended,\nalkhamissi-etal-2024-investigating} and produce biased generations\n\\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.\nEvaluation of LLMs for cultural awareness and alignment is particularly\nchallenging due to the lack of proper evaluation metrics and unavailability of\nculturally grounded datasets representing the vast complexity of cultures at\nthe regional and sub-regional levels. Existing datasets for culture specific\nitems (CSIs) focus primarily on concepts at the regional level and may contain\nfalse positives. To address this issue, we introduce a novel CSI dataset for\nIndian culture, belonging to 17 cultural facets. The dataset comprises $\\sim$8k\ncultural concepts from 36 sub-regions. To measure the cultural competence of\nLLMs on a cultural text adaptation task, we evaluate the adaptations using the\nCSIs created, LLM as Judge, and human evaluations from diverse\nsocio-demographic region. Furthermore, we perform quantitative analysis\ndemonstrating selective sub-regional coverage and surface-level adaptations\nacross all considered LLMs. Our dataset is available here:\n\\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},\nproject\nwebpage\\footnote{\\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},\nand our codebase with model outputs can be found here:\n\\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.",
        "url": "http://arxiv.org/abs/2509.17399v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17399v1",
        "arxiv_id": "2509.17399v1",
        "authors": [
            "Pramit Sahoo",
            "Maharaj Brahma",
            "Maunendra Sankar Desarkar"
        ],
        "submitted": "2025-09-22 06:58:02",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025"
    },
    {
        "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering",
        "abstract": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
        "url": "http://arxiv.org/abs/2509.17396v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17396v1",
        "arxiv_id": "2509.17396v1",
        "authors": [
            "Minsoo Kim",
            "Arnav Kundu",
            "Han-Byul Kim",
            "Richa Dixit",
            "Minsik Cho"
        ],
        "submitted": "2025-09-22 06:56:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis",
        "abstract": "We introduce FinDebate, a multi-agent framework for financial analysis,\nintegrating collaborative debate with domain-specific Retrieval-Augmented\nGeneration (RAG). Five specialized agents, covering earnings, market,\nsentiment, valuation, and risk, run in parallel to synthesize evidence into\nmulti-dimensional insights. To mitigate overconfidence and improve reliability,\nwe introduce a safe debate protocol that enables agents to challenge and refine\ninitial conclusions while preserving coherent recommendations. Experimental\nresults, based on both LLM-based and human evaluations, demonstrate the\nframework's efficacy in producing high-quality analysis with calibrated\nconfidence levels and actionable investment strategies across multiple time\nhorizons.",
        "url": "http://arxiv.org/abs/2509.17395v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17395v1",
        "arxiv_id": "2509.17395v1",
        "authors": [
            "Tianshi Cai",
            "Guanxu Li",
            "Nijia Han",
            "Ce Huang",
            "Zimu Wang",
            "Changyu Zeng",
            "Yuqi Wang",
            "Jingshi Zhou",
            "Haiyang Zhang",
            "Qi Chen",
            "Yushan Pan",
            "Shuihua Wang",
            "Wei Wang"
        ],
        "submitted": "2025-09-22 06:56:27",
        "source": "arxiv",
        "comment": "Accepted at FinNLP@EMNLP 2025. Camera-ready version"
    },
    {
        "title": "Program Synthesis via Test-Time Transduction",
        "abstract": "We introduce transductive program synthesis, a new formulation of the program\nsynthesis task that explicitly leverages test inputs during synthesis. While\nprior approaches to program synthesis--whether based on natural language\ndescriptions or input-output examples--typically aim to generalize from\ntraining examples, they often struggle with robustness, especially in\nreal-world settings where training examples are limited and test inputs involve\nvarious edge cases. To address this, we propose a novel framework that improves\nrobustness by treating synthesis as an active learning over a finite hypothesis\nclass defined by programs' outputs. We use an LLM to predict outputs for\nselected test inputs and eliminate inconsistent hypotheses, where the inputs\nare chosen via a greedy maximin algorithm to minimize the number of LLM queries\nrequired. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC,\nand programmatic world modeling on MiniGrid. We demonstrate that our method\nsignificantly improves program synthesis in both accuracy and efficiency. We\nrelease our code at https://github.com/klee972/SYNTRA.",
        "url": "http://arxiv.org/abs/2509.17393v2",
        "pdf_url": "http://arxiv.org/pdf/2509.17393v2",
        "arxiv_id": "2509.17393v2",
        "authors": [
            "Kang-il Lee",
            "Jahyun Koo",
            "Seunghyun Yoon",
            "Minbeom Kim",
            "Hyukhun Koh",
            "Dongryeol Lee",
            "Kyomin Jung"
        ],
        "submitted": "2025-09-22 06:53:32",
        "source": "arxiv",
        "comment": "NeurIPS 2025"
    },
    {
        "title": "Robustness of Neurosymbolic Reasoners on First-Order Logic Problems",
        "abstract": "Recent trends in NLP aim to improve reasoning capabilities in Large Language\nModels (LLMs), with key focus on generalization and robustness to variations in\ntasks. Counterfactual task variants introduce minimal but semantically\nmeaningful changes to otherwise valid first-order logic (FOL) problem instances\naltering a single predicate or swapping roles of constants to probe whether a\nreasoning system can maintain logical consistency under perturbation. Previous\nstudies showed that LLMs becomes brittle on counterfactual variations,\nsuggesting that they often rely on spurious surface patterns to generate\nresponses. In this work, we explore if a neurosymbolic (NS) approach that\nintegrates an LLM and a symbolic logical solver could mitigate this problem.\nExperiments across LLMs of varying sizes show that NS methods are more robust\nbut perform worse overall that purely neural methods. We then propose NSCoT\nthat combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate\nthat while it improves performance, NSCoT still lags behind standard CoT. Our\nanalysis opens research directions for future work.",
        "url": "http://arxiv.org/abs/2509.17377v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17377v1",
        "arxiv_id": "2509.17377v1",
        "authors": [
            "Hannah Bansal",
            "Kemal Kurniawan",
            "Lea Frermann"
        ],
        "submitted": "2025-09-22 06:35:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs",
        "abstract": "We present a comparative analysis of text complexity across domains using\nscale-free metrics. We quantify linguistic complexity via Heaps' exponent\n$\\beta$ (vocabulary growth), Taylor's exponent $\\alpha$ (word-frequency\nfluctuation scaling), compression rate $r$ (redundancy), and entropy. Our\ncorpora span three domains: legal documents (statutes, cases, deeds) as a\nspecialized domain, general natural language texts (literature, Wikipedia), and\nAI-generated (GPT) text. We find that legal texts exhibit slower vocabulary\ngrowth (lower $\\beta$) and higher term consistency (higher $\\alpha$) than\ngeneral texts. Within legal domain, statutory codes have the lowest $\\beta$ and\nhighest $\\alpha$, reflecting strict drafting conventions, while cases and deeds\nshow higher $\\beta$ and lower $\\alpha$. In contrast, GPT-generated text shows\nthe statistics more aligning with general language patterns. These results\ndemonstrate that legal texts exhibit domain-specific structures and\ncomplexities, which current generative models do not fully replicate.",
        "url": "http://arxiv.org/abs/2509.17367v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17367v1",
        "arxiv_id": "2509.17367v1",
        "authors": [
            "Haoyang Chen",
            "Kumiko Tanaka-Ishii"
        ],
        "submitted": "2025-09-22 05:34:15",
        "source": "arxiv",
        "comment": "to be published in Text, Speech, and Dialogue (TSD 2025)"
    },
    {
        "title": "SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing",
        "abstract": "Personalized content marketing has become a crucial strategy for digital\nplatforms, aiming to deliver tailored advertisements and recommendations that\nmatch user preferences. Traditional recommendation systems often suffer from\ntwo limitations: (1) reliance on limited supervised signals derived from\nexplicit user feedback, and (2) vulnerability to noisy or unintentional\ninteractions. To address these challenges, we propose SeqUDA-Rec, a novel deep\nlearning framework that integrates user behavior sequences with global\nunsupervised data augmentation to enhance recommendation accuracy and\nrobustness. Our approach first constructs a Global User-Item Interaction Graph\n(GUIG) from all user behavior sequences, capturing both local and global item\nassociations. Then, a graph contrastive learning module is applied to generate\nrobust embeddings, while a sequential Transformer-based encoder models users'\nevolving preferences. To further enhance diversity and counteract sparse\nsupervised labels, we employ a GAN-based augmentation strategy, generating\nplausible interaction patterns and supplementing training data. Extensive\nexperiments on two real-world marketing datasets (Amazon Ads and TikTok Ad\nClicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art\nbaselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%\nimprovement in NDCG@10 and 11.3% improvement in HR@10, proving its\neffectiveness in personalized advertising and intelligent content\nrecommendation.",
        "url": "http://arxiv.org/abs/2509.17361v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17361v1",
        "arxiv_id": "2509.17361v1",
        "authors": [
            "Ruihan Luo",
            "Xuanjing Chen",
            "Ziyang Ding"
        ],
        "submitted": "2025-09-22 05:24:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval",
        "abstract": "Generative cross-modal retrieval, which treats retrieval as a generation\ntask, has emerged as a promising direction with the rise of Multimodal Large\nLanguage Models (MLLMs). In this setting, the model responds to a text query by\ngenerating an identifier corresponding to the target image. However, existing\nmethods typically rely on manually crafted string IDs, clustering-based labels,\nor atomic identifiers requiring vocabulary expansion, all of which face\nchallenges in semantic alignment or scalability.To address these limitations,\nwe propose a vocabulary-efficient identifier generation framework that prompts\nMLLMs to generate Structured Semantic Identifiers from image-caption pairs.\nThese identifiers are composed of concept-level tokens such as objects and\nactions, naturally aligning with the model's generation space without modifying\nthe tokenizer. Additionally, we introduce a Rationale-Guided Supervision\nStrategy, prompting the model to produce a one-sentence explanation alongside\neach identifier serves as an auxiliary supervision signal that improves\nsemantic grounding and reduces hallucinations during training.",
        "url": "http://arxiv.org/abs/2509.17359v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17359v1",
        "arxiv_id": "2509.17359v1",
        "authors": [
            "Tianyuan Li",
            "Lei Wang",
            "Ahtamjan Ahmat",
            "Yating Yang",
            "Bo Ma",
            "Rui Dong",
            "Bangju Han"
        ],
        "submitted": "2025-09-22 05:23:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation",
        "abstract": "Simultaneous speech-to-text translation (SimulST) systems have to balance\ntranslation quality with latency--the delay between speech input and the\ntranslated output. While quality evaluation is well established, accurate\nlatency measurement remains a challenge. Existing metrics often produce\ninconsistent or misleading results, especially in the widely used short-form\nsetting, where speech is artificially presegmented. In this paper, we present\nthe first comprehensive analysis of SimulST latency metrics across language\npairs, systems, and both short- and long-form regimes. We uncover a structural\nbias in current metrics related to segmentation that undermines fair and\nmeaningful comparisons. To address this, we introduce YAAL (Yet Another Average\nLagging), a refined latency metric that delivers more accurate evaluations in\nthe short-form regime. We extend YAAL to LongYAAL for unsegmented audio and\npropose SoftSegmenter, a novel resegmentation tool based on word-level\nalignment. Our experiments show that YAAL and LongYAAL outperform popular\nlatency metrics, while SoftSegmenter enhances alignment quality in long-form\nevaluation, together enabling more reliable assessments of SimulST systems.",
        "url": "http://arxiv.org/abs/2509.17349v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17349v1",
        "arxiv_id": "2509.17349v1",
        "authors": [
            "Peter Polák",
            "Sara Papi",
            "Luisa Bentivogli",
            "Ondřej Bojar"
        ],
        "submitted": "2025-09-22 04:21:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning",
        "abstract": "Continual learning (CL) is essential for deploying large language models\n(LLMs) in dynamic real-world environments without the need for costly\nretraining. Recent model merging-based methods have attracted significant\nattention, but they still struggle to effectively manage the trade-off between\nlearning new knowledge and preventing forgetting, a challenge largely stemming\nfrom suboptimal number of merges and merging frequency. In this paper, we\nintroduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework\nthat utilizes learning and forgetting signals from the training trajectory to\ndynamically monitor the model's training status. Guided by dynamic monitoring,\nthe training trajectory-guided merge controller adaptively determines the\ntiming and frequency of iterative fusion, while the rehearsal-based knowledge\nfusion module computes the merging weights and executes the fusion.\nComprehensive experiments on three CL benchmarks with various model sizes (from\n770M to 13B) demonstrate that AimMerging achieves significant performance\nimprovements over existing state-of-the-art methods, with an average relative\nimprovement of 80% and 59% on FWT and BWT, respectively. The source code is\nprovided for reproducibility.",
        "url": "http://arxiv.org/abs/2509.17348v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17348v1",
        "arxiv_id": "2509.17348v1",
        "authors": [
            "Yujie Feng",
            "Jian Li",
            "Xiaoyu Dong",
            "Pengfei Xu",
            "Xiaohui Zhou",
            "Yujia Zhang",
            "Zexin LU",
            "Yasha Wang",
            "Alan Zhao",
            "Xu Chu",
            "Xiao-Ming Wu"
        ],
        "submitted": "2025-09-22 04:19:29",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code",
        "abstract": "Increasing complexity in software systems places a growing demand on\nreasoning tools that unlock vulnerabilities manifest in source code. Many\ncurrent approaches focus on vulnerability analysis as a classifying task,\noversimplifying the nuanced and context-dependent real-world scenarios. Even\nthough current code large language models (LLMs) excel in code understanding,\nthey often pay little attention to security-specific reasoning. We propose\nLLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code\nthrough question-answering (QA). Our model is trained to integrate paired code\nand natural queries into a unified space, enhancing reasoning and\ncontext-dependent insights about code vulnerability. To evaluate our model\nperformance, we construct a curated dataset of real-world vulnerabilities\npaired with security-focused questions and answers. Our model outperforms\nstate-of-the-art general-purpose and code LLMs in the QA and detection tasks.\nWe further explain decision-making by conducting qualitative analysis to\nhighlight capabilities and limitations. By integrating code and QA, LLaVul\nenables more interpretable and security-focused code understanding.",
        "url": "http://arxiv.org/abs/2509.17337v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17337v1",
        "arxiv_id": "2509.17337v1",
        "authors": [
            "Ala Jararweh",
            "Michael Adams",
            "Avinash Sahu",
            "Abdullah Mueen",
            "Afsah Anwar"
        ],
        "submitted": "2025-09-22 03:14:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mano Report",
        "abstract": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
        "url": "http://arxiv.org/abs/2509.17336v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17336v1",
        "arxiv_id": "2509.17336v1",
        "authors": [
            "Tianyu Fu",
            "Anyang Su",
            "Chenxu Zhao",
            "Hanning Wang",
            "Minghui Wu",
            "Zhe Yu",
            "Fei Hu",
            "Mingjia Shi",
            "Wei Dong",
            "Jiayao Wang",
            "Yuyang Chen",
            "Ruiyang Yu",
            "Siran Peng",
            "Menglin Li",
            "Nan Huang",
            "Haitian Wei",
            "Jiawei Yu",
            "Yi Xin",
            "Xilin Zhao",
            "Kai Gu",
            "Ping Jiang",
            "Sifan Zhou",
            "Shuo Wang"
        ],
        "submitted": "2025-09-22 03:13:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym",
        "abstract": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage\nexternal tools to solve diverse tasks and interface with the real world.\nHowever, current training practices largely rely on supervised fine-tuning\n(SFT) over static trajectories or reinforcement learning (RL) on narrow tasks,\nand generalize poorly beyond development settings, leading to brittleness with\nnew tools and unseen workflows. Because code execution reflects many structures\nof real-world workflows, coding problems provide a natural basis for building\nagent training environments. Motivated by this, we introduce CodeGym, a\nscalable framework that synthesizes diverse, verifiable, and controllable\nmulti-turn tool-use environments for agent RL, enabling LLM agents to explore\nand master various workflows actively. CodeGym rewrites static coding problems\ninto interactive environments by extracting atomic functions or logic into\ncallable tools, yielding verifiable tasks that span various tool-execution\nworkflows. Models of varying sizes and chain-of-thought configurations, trained\nin CodeGym, exhibit consistent out-of-distribution generalizability; for\nexample, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points\non the OOD benchmark $\\tau$-Bench. These results highlight CodeGym as a step\ntoward scalable general-purpose RL environments that align with real-world\nagent workflows.",
        "url": "http://arxiv.org/abs/2509.17325v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17325v1",
        "arxiv_id": "2509.17325v1",
        "authors": [
            "Weihua Du",
            "Hailei Gong",
            "Zhan Ling",
            "Kang Liu",
            "Lingfeng Shen",
            "Xuesong Yao",
            "Yufei Xu",
            "Dingyuan Shi",
            "Yiming Yang",
            "Jiecao Chen"
        ],
        "submitted": "2025-09-22 03:03:56",
        "source": "arxiv",
        "comment": "22 pages. Project available at https://github.com/StigLidu/CodeGym"
    },
    {
        "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
        "abstract": "Data scarcity remains one of the most limiting factors in driving progress in\nrobotics. However, the amount of available robotics data in the wild is growing\nexponentially, creating new opportunities for large-scale data utilization.\nReliable temporal task completion prediction could help automatically annotate\nand curate this data at scale. The Generative Value Learning (GVL) approach was\nrecently proposed, leveraging the knowledge embedded in vision-language models\n(VLMs) to predict task progress from visual observations. Building upon GVL, we\npropose OpenGVL, a comprehensive benchmark for estimating task progress across\ndiverse challenging manipulation tasks involving both robotic and human\nembodiments. We evaluate the capabilities of publicly available open-source\nfoundation models, showing that open-source model families significantly\nunderperform closed-source counterparts, achieving only approximately $70\\%$ of\ntheir performance on temporal progress prediction tasks. Furthermore, we\ndemonstrate how OpenGVL can serve as a practical tool for automated data\ncuration and filtering, enabling efficient quality assessment of large-scale\nrobotics datasets. We release the benchmark along with the complete codebase at\n\\href{github.com/budzianowski/opengvl}{OpenGVL}.",
        "url": "http://arxiv.org/abs/2509.17321v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17321v1",
        "arxiv_id": "2509.17321v1",
        "authors": [
            "Paweł Budzianowski",
            "Emilia Wiśnios",
            "Gracjan Góral",
            "Igor Kulakov",
            "Viktor Petrenko",
            "Krzysztof Walas"
        ],
        "submitted": "2025-09-22 02:52:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models",
        "abstract": "Mathematical reasoning poses significant challenges for Large Language Models\n(LLMs) due to its demand for multi-step reasoning and abstract conceptual\nintegration. While recent test-time scaling techniques rely heavily on\nhigh-quality, challenging problems, the scarcity of Olympiad-level math\nproblems remains a bottleneck. We introduce CogAtom, a novel cognitive\natom-based framework for synthesizing mathematically rigorous and cognitively\ndiverse problems. Unlike prior approaches, CogAtom models problem construction\nas a process of selecting and recombining fundamental reasoning units,\ncognitive atoms, extracted from human-authored solutions. A diversity-promoting\nrandom walk algorithm enables exploration of the cognitive atom space, while a\nconstraint-based recombination mechanism ensures logical soundness and\nstructural validity. The combinatorial nature of the graph structure provides a\nnear-infinite space of reasoning paths, and the walk algorithm systematically\nexplores this space to achieve large-scale synthesis of high-quality problems;\nmeanwhile, by controlling the number of cognitive atoms, we can precisely\nadjust problem difficulty, ensuring diversity, scalability, and controllability\nof the generated problems. Experimental results demonstrate that CogAtom\noutperforms existing methods in accuracy, reasoning depth, and diversity,\ngenerating problems that closely match the difficulty of AIME while exceeding\nit in structural variation. Our work offers a cognitively grounded pathway\ntoward scalable, high-quality math problem generation.Our code is publicly\navailable at https://github.com/Icarus-1111/CogAtom.",
        "url": "http://arxiv.org/abs/2509.17318v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17318v1",
        "arxiv_id": "2509.17318v1",
        "authors": [
            "Zhuofan Chen",
            "Jiyuan He",
            "Yichi Zhang",
            "Xing Hu",
            "Haoxing Wen",
            "Jun Bai",
            "Wenge Rong"
        ],
        "submitted": "2025-09-22 02:48:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text",
        "abstract": "Most languages lack sufficient data for large-scale monolingual pretraining,\ncreating a \"data wall.\" Multilingual pretraining helps but is limited by\nlanguage imbalance and the \"curse of multilinguality.\" An alternative is to\ntranslate high-resource text with machine translation (MT), which raises three\nquestions: (1) How does MT-derived data scale with model capacity? (2) Can\nsource-side transformations (e.g., simplifying English with an LLM) improve\ngeneralization to native text? (3) How well do models pretrained on MT-derived\ndata adapt when continually trained on limited native text? We investigate\nthese questions by translating English into Indonesian and Tamil--two\ntypologically distant, lower-resource languages--and pretraining GPT-2 models\n(124M-774M) on native or MT-derived corpora from raw and LLM-simplified\nEnglish. We evaluate cross-entropy loss on native text, along with accuracy on\nsyntactic probes and downstream tasks. Our results show that (1) MT-pretrained\nmodels benefit from scaling; (2) source-side simplification harms\ngeneralization to native text; and (3) adapting MT-pretrained models on native\ntext often yields better performance than native-only models, even with less\nnative data. However, tasks requiring cultural nuance (e.g., toxicity\ndetection) demand more exposure to native data.",
        "url": "http://arxiv.org/abs/2509.17317v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17317v1",
        "arxiv_id": "2509.17317v1",
        "authors": [
            "Dan John Velasco",
            "Matthew Theodore Roque"
        ],
        "submitted": "2025-09-22 02:48:43",
        "source": "arxiv",
        "comment": "Under review"
    },
    {
        "title": "Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection",
        "abstract": "Cognitive distortions have been closely linked to mental health disorders,\nyet their automatic detection remained challenging due to contextual ambiguity,\nco-occurrence, and semantic overlap. We proposed a novel framework that\ncombines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)\narchitecture to enhance interpretability and expression-level reasoning. Each\nutterance was decomposed into Emotion, Logic, and Behavior (ELB) components,\nwhich were processed by LLMs to infer multiple distortion instances, each with\na predicted type, expression, and model-assigned salience score. These\ninstances were integrated via a Multi-View Gated Attention mechanism for final\nclassification. Experiments on Korean (KoACD) and English (Therapist QA)\ndatasets demonstrate that incorporating ELB and LLM-inferred salience scores\nimproves classification performance, especially for distortions with high\ninterpretive ambiguity. Our results suggested a psychologically grounded and\ngeneralizable approach for fine-grained reasoning in mental health NLP.",
        "url": "http://arxiv.org/abs/2509.17292v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17292v1",
        "arxiv_id": "2509.17292v1",
        "authors": [
            "Jun Seo Kim",
            "Hyemi Kim",
            "Woo Joo Oh",
            "Hongjin Cho",
            "Hochul Lee",
            "Hye Hyeon Kim"
        ],
        "submitted": "2025-09-22 00:18:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling",
        "abstract": "We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting\nsentence-level knowledge graphs by combining robust coreference resolution with\nsyntactic sentence decomposition. Using our model, we contribute a dataset of\nover 150,000 knowledge triples, which is open source. We also contribute a\ntraining corpus of 7248 rows for sentence complexity, 190 rows of gold human\nannotations for co-reference resolution using open source lung-cancer abstracts\nfrom PubMed, 900 rows of gold human annotations for sentence conversion\npolicies, and 398 triples of gold human annotations. We systematically select\noptimal prompt-model pairs across five complexity categories, showing that\nhybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match\naccuracy on sentence simplification. On relation extraction (RE), our pipeline\nachieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the\nart, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on\nWiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference\nand decomposition increases recall on rare relations by over 20%. Code and\ndataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025",
        "url": "http://arxiv.org/abs/2509.17289v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17289v1",
        "arxiv_id": "2509.17289v1",
        "authors": [
            "Sydney Anuyah",
            "Mehedi Mahmud Kaushik",
            "Krishna Dwarampudi",
            "Rakesh Shiradkar",
            "Arjan Durresi",
            "Sunandan Chakraborty"
        ],
        "submitted": "2025-09-22 00:01:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Probabilistic Token Alignment for Large Language Model Fusion",
        "abstract": "Training large language models (LLMs) from scratch can yield models with\nunique functionalities and strengths, but it is costly and often leads to\nredundant capabilities. A more cost-effective alternative is to fuse existing\npre-trained LLMs with different architectures into a more powerful model.\nHowever, a key challenge in existing model fusion is their dependence on\nmanually predefined vocabulary alignment, which may not generalize well across\ndiverse contexts, leading to performance degradation in several evaluation. To\nsolve this, we draw inspiration from distribution learning and propose the\nprobabilistic token alignment method as a general and soft mapping for\nalignment, named as PTA-LLM. Our approach innovatively reformulates token\nalignment into a classic mathematical problem: optimal transport, seamlessly\nleveraging distribution-aware learning to facilitate more coherent model\nfusion. Apart from its inherent generality, PTA-LLM exhibits interpretability\nfrom a distributional perspective, offering insights into the essence of the\ntoken alignment. Empirical results demonstrate that probabilistic token\nalignment enhances the target model's performance across multiple capabilities.\nOur code is avaliable at https://runjia.tech/neurips_pta-llm/.",
        "url": "http://arxiv.org/abs/2509.17276v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17276v1",
        "arxiv_id": "2509.17276v1",
        "authors": [
            "Runjia Zeng",
            "James Chenhao Liang",
            "Cheng Han",
            "Zhiwen Cao",
            "Jiahao Liu",
            "Xiaojun Quan",
            "Yingjie Victor Chen",
            "Lifu Huang",
            "Tong Geng",
            "Qifan Wang",
            "Dongfang Liu"
        ],
        "submitted": "2025-09-21 23:18:24",
        "source": "arxiv",
        "comment": "NeurIPS 2025"
    },
    {
        "title": "Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations",
        "abstract": "Recommender systems have been shown to exhibit popularity bias by\nover-recommending popular items and under-recommending relevant niche items. We\nseek to understand interactions with niche items in benchmark recommendation\ndatasets as a step toward mitigating popularity bias. We find that, compared to\nmainstream users, niche-preferring users exhibit a longer-tailed activity-level\ndistribution, indicating the existence of users who both prefer niche items and\nexhibit high activity levels. We partition users along two axes: (1) activity\nlevel (\"power\" vs. \"light\") and (2) item-popularity preference (\"mainstream\"\nvs. \"niche\"), and show that in several benchmark datasets, the number of\npower-niche users (high activity and niche preference) is statistically\nsignificantly larger than expected under a null configuration model. Motivated\nby this observation, we propose a framework for reweighting the Bayesian\nPersonalized Ranking (BPR) loss that simultaneously reweights based on user\nactivity level and item popularity. Our method introduces two interpretable\nparameters: one controlling the significance of user activity level, and the\nother of item popularity. Experiments on benchmark datasets show that\nupweighting power-niche users reduces popularity bias and can increase overall\nperformance. In contrast to previous work that only considers user activity\nlevel or item popularity in isolation, our results suggest that considering\ntheir interaction leads to Pareto-dominant performance.",
        "url": "http://arxiv.org/abs/2509.17265v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17265v1",
        "arxiv_id": "2509.17265v1",
        "authors": [
            "David Liu",
            "Erik Weis",
            "Moritz Laber",
            "Tina Eliassi-Rad",
            "Brennan Klein"
        ],
        "submitted": "2025-09-21 22:41:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Extending Automatic Machine Translation Evaluation to Book-Length Documents",
        "abstract": "Despite Large Language Models (LLMs) demonstrating superior translation\nperformance and long-context capabilities, evaluation methodologies remain\nconstrained to sentence-level assessment due to dataset limitations, token\nnumber restrictions in metrics, and rigid sentence boundary requirements. We\nintroduce SEGALE, an evaluation scheme that extends existing automatic metrics\nto long-document translation by treating documents as continuous text and\napplying sentence segmentation and alignment methods. Our approach enables\npreviously unattainable document-level evaluation, handling translations of\narbitrary length generated with document-level prompts while accounting for\nunder-/over-translations and varied sentence boundaries. Experiments show our\nscheme significantly outperforms existing long-form document evaluation\nschemes, while being comparable to evaluations performed with groundtruth\nsentence alignments. Additionally, we apply our scheme to book-length texts and\nnewly demonstrate that many open-weight LLMs fail to effectively translate\ndocuments at their reported maximum context lengths.",
        "url": "http://arxiv.org/abs/2509.17249v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17249v1",
        "arxiv_id": "2509.17249v1",
        "authors": [
            "Kuang-Da Wang",
            "Shuoyang Ding",
            "Chao-Han Huck Yang",
            "Ping-Chun Hsieh",
            "Wen-Chih Peng",
            "Vitaly Lavrukhin",
            "Boris Ginsburg"
        ],
        "submitted": "2025-09-21 21:46:58",
        "source": "arxiv",
        "comment": "Accepted for EMNLP 2025 main conference"
    },
    {
        "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System",
        "abstract": "Systematic Literature Reviews (SLRs) are foundational to evidence-based\nresearch but remain labor-intensive and prone to inconsistency across\ndisciplines. We present an LLM-based SLR evaluation copilot built on a\nMulti-Agent System (MAS) architecture to assist researchers in assessing the\noverall quality of the systematic literature reviews. The system automates\nprotocol validation, methodological assessment, and topic relevance checks\nusing a scholarly database. Unlike conventional single-agent methods, our\ndesign integrates a specialized agentic approach aligned with PRISMA guidelines\nto support more structured and interpretable evaluations. We conducted an\ninitial study on five published SLRs from diverse domains, comparing system\noutputs to expert-annotated PRISMA scores, and observed 84% agreement. While\nearly results are promising, this work represents a first step toward scalable\nand accurate NLP-driven systems for interdisciplinary workflows and reveals\ntheir capacity for rigorous, domain-agnostic knowledge aggregation to\nstreamline the review process.",
        "url": "http://arxiv.org/abs/2509.17240v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17240v1",
        "arxiv_id": "2509.17240v1",
        "authors": [
            "Abdullah Mushtaq",
            "Muhammad Rafay Naeem",
            "Ibrahim Ghaznavi",
            "Alaa Abd-alrazaq",
            "Aliya Tabassum",
            "Junaid Qadir"
        ],
        "submitted": "2025-09-21 21:17:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE",
        "abstract": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
        "url": "http://arxiv.org/abs/2509.17238v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17238v1",
        "arxiv_id": "2509.17238v1",
        "authors": [
            "Soheil Zibakhsh",
            "Mohammad Samragh",
            "Kumari Nishu",
            "Lauren Hannah",
            "Arnav Kundu",
            "Minsik Cho"
        ],
        "submitted": "2025-09-21 21:05:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness",
        "abstract": "Clinical notes contain rich patient information, such as diagnoses or\nmedications, making them valuable for patient representation learning. Recent\nadvances in large language models have further improved the ability to extract\nmeaningful representations from clinical texts. However, clinical notes are\noften missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of\npatients have no available discharge summaries. In such cases, representations\ncan be learned from other modalities such as structured data, chest X-rays, or\nradiology reports. Yet the availability of these modalities is influenced by\nclinical decision-making and varies across patients, resulting in modality\nmissing-not-at-random (MMNAR) patterns. We propose a causal representation\nlearning framework that leverages observed data and informative missingness in\nmultimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion\ncomponent that integrates structured data, imaging, and text while conditioning\non missingness patterns to capture patient health and clinician-driven\nassignment; (2) a modality reconstruction component with contrastive learning\nto ensure semantic sufficiency in representation learning; and (3) a multitask\noutcome prediction model with a rectifier that corrects for residual bias from\nspecific modality observation patterns. Comprehensive evaluations across\nMIMIC-IV and eICU show consistent gains over the strongest baselines, achieving\nup to 13.8% AUC improvement for hospital readmission and 13.1% for ICU\nadmission.",
        "url": "http://arxiv.org/abs/2509.17228v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17228v1",
        "arxiv_id": "2509.17228v1",
        "authors": [
            "Zihan Liang",
            "Ziwen Pan",
            "Ruoxuan Xiong"
        ],
        "submitted": "2025-09-21 20:34:52",
        "source": "arxiv",
        "comment": "To appear in Proc. of EMNLP 2025 (18 pages)"
    },
    {
        "title": "Prompt-Based Simplification for Plain Language using Spanish Language Models",
        "abstract": "This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask\n1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies\nbased on models trained on Spanish texts, including a zero-shot configuration\nusing prompt engineering and a fine-tuned version with Low-Rank Adaptation\n(LoRA). Different strategies were evaluated on representative internal subsets\nof the training data, using the official task metrics, cosine similarity (SIM)\nand the Fern\\'andez-Huerta readability index (FH) to guide the selection of the\noptimal model and prompt combination. The final system was selected for its\nbalanced and consistent performance, combining normalization steps, the\nRigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in\nsemantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).\nWe also discuss key challenges related to training data heterogeneity and the\nlimitations of current evaluation metrics in capturing both linguistic clarity\nand content preservation.",
        "url": "http://arxiv.org/abs/2509.17209v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17209v1",
        "arxiv_id": "2509.17209v1",
        "authors": [
            "Lourdes Moreno",
            "Jesus M. Sanchez-Gomez",
            "Marco Antonio Sanchez-Escudero",
            "Paloma Martínez"
        ],
        "submitted": "2025-09-21 19:28:37",
        "source": "arxiv",
        "comment": "11 pages, 7 tables,"
    },
    {
        "title": "Evolution of Concepts in Language Model Pre-Training",
        "abstract": "Language models obtain extensive capabilities through pre-training. However,\nthe pre-training process remains a black box. In this work, we track linear\ninterpretable feature evolution across pre-training snapshots using a sparse\ndictionary learning method called crosscoders. We find that most features begin\nto form around a specific point, while more complex patterns emerge in later\ntraining stages. Feature attribution analyses reveal causal connections between\nfeature evolution and downstream performance. Our feature-level observations\nare highly consistent with previous findings on Transformer's two-stage\nlearning process, which we term a statistical learning phase and a feature\nlearning phase. Our work opens up the possibility to track fine-grained\nrepresentation progress during language model learning dynamics.",
        "url": "http://arxiv.org/abs/2509.17196v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17196v1",
        "arxiv_id": "2509.17196v1",
        "authors": [
            "Xuyang Ge",
            "Wentao Shu",
            "Jiaxing Wu",
            "Yunhua Zhou",
            "Zhengfu He",
            "Xipeng Qiu"
        ],
        "submitted": "2025-09-21 18:53:12",
        "source": "arxiv",
        "comment": "30 pages, 25 figures"
    },
    {
        "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
        "abstract": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general\nmodels lack domain expertise, and SFT often overfits superficial patterns,\nyielding brittle reasoning for authentication and historical attribution. This\nraises the question of how to equip MLLMs with robust, expert-level reasoning\nfor ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns\nevaluation into supervision: we construct a taxonomy of question types, probe\nthe SFT model to localize type-specific performance gaps, and optimize with\ntype-conditioned, compositionality-oriented rewards targeting those gaps. We\nalso release VaseVQA, a comprehensive benchmark of 31,773 images designed to\nprobe deep understanding. Experiments show state-of-the-art results on style\nclassification and historical attribution with marked gains in compositional\nrobustness over SFT-only baselines, validating diagnosis-guided,\ntaxonomy-conditioned reward engineering and providing a reusable resource for\nfuture research. Code and dataset will be available at\nhttps://github.com/AIGeeksGroup/VaseVQA.",
        "url": "http://arxiv.org/abs/2509.17191v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17191v1",
        "arxiv_id": "2509.17191v1",
        "authors": [
            "Jinchao Ge",
            "Tengfei Cheng",
            "Biao Wu",
            "Zeyu Zhang",
            "Shiya Huang",
            "Judith Bishop",
            "Gillian Shepherd",
            "Meng Fang",
            "Ling Chen",
            "Yang Zhao"
        ],
        "submitted": "2025-09-21 18:36:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization",
        "abstract": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning\nwith human preferences on a specific task/domain. Traditional alignment methods\nsuffer from catastrophic forgetting, where models lose previously acquired\nknowledge when adapting to new preferences or domains. We introduce LifeAlign,\na novel framework for lifelong alignment that enables LLMs to maintain\nconsistent human preference alignment across sequential learning tasks without\nforgetting previously learned knowledge. Our approach consists of two key\ninnovations. First, we propose a focalized preference optimization strategy\nthat aligns LLMs with new preferences while preventing the erosion of knowledge\nacquired from previous tasks. Second, we develop a short-to-long memory\nconsolidation mechanism that merges denoised short-term preference\nrepresentations into stable long-term memory using intrinsic dimensionality\nreduction, enabling efficient storage and retrieval of alignment patterns\nacross diverse domains. We evaluate LifeAlign across multiple sequential\nalignment tasks spanning different domains and preference types. Experimental\nresults demonstrate that our method achieves superior performance in\nmaintaining both preference alignment quality and knowledge retention compared\nto existing lifelong learning approaches. The codes and datasets will be\nreleased on GitHub.",
        "url": "http://arxiv.org/abs/2509.17183v1",
        "pdf_url": "http://arxiv.org/pdf/2509.17183v1",
        "arxiv_id": "2509.17183v1",
        "authors": [
            "Junsong Li",
            "Jie Zhou",
            "Bihao Zhan",
            "Yutao Yang",
            "Qianjun Pan",
            "Shilian Chen",
            "Tianyu Huai",
            "Xin Li",
            "Qin Chen",
            "Liang He"
        ],
        "submitted": "2025-09-21 18:06:05",
        "source": "arxiv",
        "comment": null
    }
]