[
    {
        "title": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study on Query-Category and Query-Item Relevance",
        "abstract": "Multilingual e-commerce search suffers from severe data imbalance across\nlanguages, label noise, and limited supervision for low-resource\nlanguages--challenges that impede the cross-lingual generalization of relevance\nmodels despite the strong capabilities of large language models (LLMs). In this\nwork, we present a practical, architecture-agnostic, data-centric framework to\nenhance performance on two core tasks: Query-Category (QC) relevance (matching\nqueries to product categories) and Query-Item (QI) relevance (matching queries\nto product titles). Rather than altering the model, we redesign the training\ndata through three complementary strategies: (1) translation-based augmentation\nto synthesize examples for languages absent in training, (2) semantic negative\nsampling to generate hard negatives and mitigate class imbalance, and (3)\nself-validation filtering to detect and remove likely mislabeled instances.\nEvaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields\nsubstantial F1 score improvements over strong LLM baselines, achieving\ncompetitive results in the official competition. Our findings demonstrate that\nsystematic data engineering can be as impactful as--and often more deployable\nthan--complex model modifications, offering actionable guidance for building\nrobust multilingual search systems in the real-world e-commerce settings.",
        "url": "http://arxiv.org/abs/2510.21671v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21671v1",
        "arxiv_id": "2510.21671v1",
        "authors": [
            "Yabo Yin",
            "Yang Xi",
            "Jialong Wang",
            "Shanqi Wang",
            "Jiateng Hu"
        ],
        "submitted": "2025-10-24 17:27:35",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'cikm' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of multilingual e-commerce search. The focus on query understanding, relevance models, and data-centric approaches aligns with your expertise. However, the specific application to e-commerce and multilingual search is somewhat narrower than your broader interests in IR and NLP."
    },
    {
        "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research",
        "abstract": "Deep Research systems have revolutionized how LLMs solve complex questions\nthrough iterative reasoning and evidence gathering. However, current systems\nremain fundamentally constrained to textual web data, overlooking the vast\nknowledge embedded in multimodal documents Processing such documents demands\nsophisticated parsing to preserve visual semantics (figures, tables, charts,\nand equations), intelligent chunking to maintain structural coherence, and\nadaptive retrieval across modalities, which are capabilities absent in existing\nsystems. In response, we present Doc-Researcher, a unified system that bridges\nthis gap through three integrated components: (i) deep multimodal parsing that\npreserves layout structure and visual semantics while creating multi-granular\nrepresentations from chunk to document level, (ii) systematic retrieval\narchitecture supporting text-only, vision-only, and hybrid paradigms with\ndynamic granularity selection, and (iii) iterative multi-agent workflows that\ndecompose complex queries, progressively accumulate evidence, and synthesize\ncomprehensive answers across documents and modalities. To enable rigorous\nevaluation, we introduce M4DocBench, the first benchmark for Multi-modal,\nMulti-hop, Multi-document, and Multi-turn deep research. Featuring 158\nexpert-annotated questions with complete evidence chains across 304 documents,\nM4DocBench tests capabilities that existing benchmarks cannot assess.\nExperiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter\nthan state-of-the-art baselines, validating that effective document research\nrequires not just better retrieval, but fundamentally deep parsing that\npreserve multimodal integrity and support iterative research. Our work\nestablishes a new paradigm for conducting deep research on multimodal document\ncollections.",
        "url": "http://arxiv.org/abs/2510.21603v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21603v1",
        "arxiv_id": "2510.21603v1",
        "authors": [
            "Kuicai Dong",
            "Shurui Huang",
            "Fangda Ye",
            "Wei Han",
            "Zhi Zhang",
            "Dexun Li",
            "Wenjun Li",
            "Qu Yang",
            "Gang Wang",
            "Yichao Wang",
            "Chen Zhang",
            "Yong Liu"
        ],
        "submitted": "2025-10-24 16:07:54",
        "source": "arxiv",
        "comment": "preprint",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns with your interests in Information Retrieval, particularly in the context of multimodal document parsing and deep research. The proposed system, Doc-Researcher, addresses the need for sophisticated parsing and retrieval capabilities, which is relevant to your focus on query understanding and ranking models. However, the paper's primary focus on multimodal documents and deep research may not be directly related to your background in e-commerce, but it is still a strong match for your broader interests in IR and NLP."
    },
    {
        "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
        "abstract": "Large reasoning models have demonstrated strong problem-solving abilities,\nyet real-world tasks often require external tools and long-horizon\ninteractions. Existing agent frameworks typically follow predefined workflows,\nwhich limit autonomous and global task completion. In this paper, we introduce\nDeepAgent, an end-to-end deep reasoning agent that performs autonomous\nthinking, tool discovery, and action execution within a single, coherent\nreasoning process. To address the challenges of long-horizon interactions,\nparticularly the context length explosion from multiple tool calls and the\naccumulation of interaction history, we introduce an autonomous memory folding\nmechanism that compresses past interactions into structured episodic, working,\nand tool memories, reducing error accumulation while preserving critical\ninformation. To teach general-purpose tool use efficiently and stably, we\ndevelop an end-to-end reinforcement learning strategy, namely ToolPO, that\nleverages LLM-simulated APIs and applies tool-call advantage attribution to\nassign fine-grained credit to the tool invocation tokens. Extensive experiments\non eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,\nTMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,\nHLE), demonstrate that DeepAgent consistently outperforms baselines across both\nlabeled-tool and open-set tool retrieval scenarios. This work takes a step\ntoward more general and capable agents for real-world applications. The code\nand demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
        "url": "http://arxiv.org/abs/2510.21618v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21618v1",
        "arxiv_id": "2510.21618v1",
        "authors": [
            "Xiaoxi Li",
            "Wenxiang Jiao",
            "Jiarui Jin",
            "Guanting Dong",
            "Jiajie Jin",
            "Yinuo Wang",
            "Hao Wang",
            "Yutao Zhu",
            "Ji-Rong Wen",
            "Yuan Lu",
            "Zhicheng Dou"
        ],
        "submitted": "2025-10-24 16:24:01",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves deep learning and reinforcement learning, the focus is on general reasoning agents and tool discovery, which is not a central match to your core research themes."
    },
    {
        "title": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene",
        "abstract": "Large language models are demonstrating increasing capabilities, excelling at\nbenchmarks once considered very difficult. As their capabilities grow, there is\na need for more challenging evaluations that go beyond surface-level linguistic\ncompetence. Namely, language competence involves not only syntax and semantics\nbut also pragmatics, i.e., understanding situational meaning as shaped by\ncontext as well as linguistic and cultural norms. To contribute to this line of\nresearch, we introduce SloPragEval and SloPragMega, the first pragmatics\nunderstanding benchmarks for Slovene that contain altogether 405\nmultiple-choice questions. We discuss the difficulties of translation, describe\nthe campaign to establish a human baseline, and report pilot evaluations with\nLLMs. Our results indicate that current models have greatly improved in\nunderstanding nuanced language but may still fail to infer implied speaker\nmeaning in non-literal utterances, especially those that are culture-specific.\nWe also observe a significant gap between proprietary and open-source models.\nFinally, we argue that benchmarks targeting nuanced language understanding and\nknowledge of the target culture must be designed with care, preferably\nconstructed from native data, and validated with human responses.",
        "url": "http://arxiv.org/abs/2510.21575v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21575v1",
        "arxiv_id": "2510.21575v1",
        "authors": [
            "Mojca Brglez",
            "Å pela Vintar"
        ],
        "submitted": "2025-10-24 15:43:42",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper primarily focuses on developing pragmatics understanding benchmarks for Slovene, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on language understanding, it is more focused on pragmatics and cultural norms, which is a specific and narrow area of NLP."
    },
    {
        "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
        "abstract": "Knowledge distillation is a promising approach to transfer capabilities from\ncomplex teacher models to smaller, resource-efficient student models that can\nbe deployed easily, particularly in task-aware scenarios. However, existing\nmethods of task-aware distillation typically require substantial quantities of\ndata which may be unavailable or expensive to obtain in many practical\nscenarios. In this paper, we address this challenge by introducing a novel\nstrategy called Counterfactual-explanation-infused Distillation CoD for\nfew-shot task-aware knowledge distillation by systematically infusing\ncounterfactual explanations. Counterfactual explanations (CFEs) refer to inputs\nthat can flip the output prediction of the teacher model with minimum\nperturbation. Our strategy CoD leverages these CFEs to precisely map the\nteacher's decision boundary with significantly fewer samples. We provide\ntheoretical guarantees for motivating the role of CFEs in distillation, from\nboth statistical and geometric perspectives. We mathematically show that CFEs\ncan improve parameter estimation by providing more informative examples near\nthe teacher's decision boundary. We also derive geometric insights on how CFEs\neffectively act as knowledge probes, helping the students mimic the teacher's\ndecision boundaries more effectively than standard data. We perform experiments\nacross various datasets and LLMs to show that CoD outperforms standard\ndistillation approaches in few-shot regimes (as low as 8-512 samples). Notably,\nCoD only uses half of the original samples used by the baselines, paired with\ntheir corresponding CFEs and still improves performance.",
        "url": "http://arxiv.org/abs/2510.21631v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21631v1",
        "arxiv_id": "2510.21631v1",
        "authors": [
            "Faisal Hamman",
            "Pasan Dissanayake",
            "Yanjun Fu",
            "Sanghamitra Dutta"
        ],
        "submitted": "2025-10-24 16:36:34",
        "source": "arxiv",
        "comment": "NeurIPS 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on knowledge distillation and few-shot learning, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning models, the context and application are not aligned with your primary focus on IR and real-time relevance optimization."
    },
    {
        "title": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models",
        "abstract": "Recently, large language models (LLMs) have demonstrated outstanding\nreasoning capabilities on mathematical and coding tasks. However, their\napplication to financial tasks-especially the most fundamental task of stock\nmovement prediction-remains underexplored. We study a three-class\nclassification problem (up, hold, down) and, by analyzing existing reasoning\nresponses, observe that: (1) LLMs follow analysts' opinions rather than exhibit\na systematic, independent analytical logic (CoTs). (2) LLMs list summaries from\ndifferent sources without weighing adversarial evidence, yet such\ncounterevidence is crucial for reliable prediction. It shows that the model\ndoes not make good use of its reasoning ability to complete the task. To\naddress this, we propose Reflective Evidence Tuning (RETuning), a cold-start\nmethod prior to reinforcement learning, to enhance prediction ability. While\ngenerating CoT, RETuning encourages dynamically constructing an analytical\nframework from diverse information sources, organizing and scoring evidence for\nprice up or down based on that framework-rather than on contextual\nviewpoints-and finally reflecting to derive the prediction. This approach\nmaximally aligns the model with its learned analytical framework, ensuring\nindependent logical reasoning and reducing undue influence from context. We\nalso build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,\nwith long contexts (32K tokens) and over 200K samples. In addition to price and\nnews, it incorporates analysts' opinions, quantitative reports, fundamental\ndata, macroeconomic indicators, and similar stocks. Experiments show that\nRETuning successfully unlocks the model's reasoning ability in the financial\ndomain. Inference-time scaling still works even after 6 months or on\nout-of-distribution stocks, since the models gain valuable insights about stock\nmovement prediction.",
        "url": "http://arxiv.org/abs/2510.21604v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21604v1",
        "arxiv_id": "2510.21604v1",
        "authors": [
            "Xueyuan Lin",
            "Cehao Yang",
            "Ye Ma",
            "Ming Li",
            "Rongjunchen Zhang",
            "Yang Ni",
            "Xiaojun Wu",
            "Chengjin Xu",
            "Jian Guo",
            "Hui Xiong"
        ],
        "submitted": "2025-10-24 16:08:33",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on stock movement prediction using large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the application domain and specific task are quite different from your areas of focus."
    },
    {
        "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite",
        "abstract": "AI agents hold the potential to revolutionize scientific productivity by\nautomating literature reviews, replicating experiments, analyzing data, and\neven proposing new directions of inquiry; indeed, there are now many such\nagents, ranging from general-purpose \"deep research\" systems to specialized\nscience-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of\nthese agents is critical for progress. Yet existing benchmarks fall short on\nseveral fronts: they (1) fail to provide holistic, product-informed measures of\nreal-world use cases such as science research; (2) lack reproducible agent\ntools necessary for a controlled comparison of core agentic capabilities; (3)\ndo not account for confounding variables such as model cost and tool access;\n(4) do not provide standardized interfaces for quick agent prototyping and\nevaluation; and (5) lack comprehensive baseline agents necessary to identify\ntrue advances. In response, we define principles and tooling for more\nrigorously benchmarking agents. Using these, we present AstaBench, a suite that\nprovides the first holistic measure of agentic ability to perform scientific\nresearch, comprising 2400+ problems spanning the entire scientific discovery\nprocess and multiple scientific domains, and including many problems inspired\nby actual user requests to deployed Asta agents. Our suite comes with the first\nscientific research environment with production-grade search tools that enable\ncontrolled, reproducible evaluation, better accounting for confounders.\nAlongside, we provide a comprehensive suite of nine science-optimized classes\nof Asta agents and numerous baselines. Our extensive evaluation of 57 agents\nacross 22 agent classes reveals several interesting findings, most importantly\nthat despite meaningful progress on certain individual aspects, AI remains far\nfrom solving the challenge of science research assistance.",
        "url": "http://arxiv.org/abs/2510.21652v1",
        "pdf_url": "http://arxiv.org/pdf/2510.21652v1",
        "arxiv_id": "2510.21652v1",
        "authors": [
            "Jonathan Bragg",
            "Mike D'Arcy",
            "Nishant Balepur",
            "Dan Bareket",
            "Bhavana Dalvi",
            "Sergey Feldman",
            "Dany Haddad",
            "Jena D. Hwang",
            "Peter Jansen",
            "Varsha Kishore",
            "Bodhisattwa Prasad Majumder",
            "Aakanksha Naik",
            "Sigal Rahamimov",
            "Kyle Richardson",
            "Amanpreet Singh",
            "Harshit Surana",
            "Aryeh Tiktinsky",
            "Rosni Vasu",
            "Guy Wiener",
            "Chloe Anastasiades",
            "Stefan Candra",
            "Jason Dunkelberger",
            "Dan Emery",
            "Rob Evans",
            "Malachi Hamada",
            "Regan Huff",
            "Rodney Kinney",
            "Matt Latzke",
            "Jaron Lochner",
            "Ruben Lozano-Aguilera",
            "Cecile Nguyen",
            "Smita Rao",
            "Amber Tanaka",
            "Brooke Vlahos",
            "Peter Clark",
            "Doug Downey",
            "Yoav Goldberg",
            "Ashish Sabharwal",
            "Daniel S. Weld"
        ],
        "submitted": "2025-10-24 17:10:26",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on benchmarking AI agents for scientific research, which is not directly related to your core research interests in Information Retrieval and Search technologies. While it touches on the concept of search tools, the context is specific to scientific research and does not align with your broader interests in e-commerce, NLP, and data mining."
    }
]