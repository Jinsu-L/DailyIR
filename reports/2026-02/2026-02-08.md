# Daily Papers Report - 2026-02-08

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Completing Missing Annotation: Multi-Agent Debate for Accurate and Scalable Relevant Assessment for IR Benchmarks

- **LLM Score**: 8
- **Keyword Score**: 18
- **Authors**: Minjeong Ban, Jeonghwan Choi, Hyangsuk Min, Nicole Hee-Yeon Kim, Minseok Kim, Jae-Gil Lee, Hwanjun Song
- **URL**: <http://arxiv.org/abs/2602.06526v1>
- **Submitted**: 2026-02-06 09:27:03
- **Comment**: Accepted at ICLR 2026
- **Topic Keywords**: information retrieval, retriever, ranking, relevance, rag, retrieval, rank, iclr
- **Reason**: This paper is highly relevant to Information Retrieval (IR) and Search technologies, particularly in the area of query understanding and evaluation benchmarks. The proposed framework, DREAM, addresses a significant challenge in IR evaluation by providing a more accurate and scalable method for labeling relevant chunks. The paper's focus on real-time relevance optimization and its impact on retriever comparison and retrieval-generation misalignment aligns with the user's research interests.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Incomplete IR benchmark datasets and labeling
- **Aim**: Introduce a multi-round debate framework (DREAM) to improve labeling accuracy and reduce human effort, and construct a refined benchmark (BRIDGE) that corrects evaluation bias
- **Rationale**: Existing LLM‚Äëbased or hybrid LLM‚Äëhuman methods suffer from LLM over‚Äëconfidence and poor escalation to human reviewers, leading to inaccurate labels and hidden relevant chunks
- **Ground**: Many relevant chunks remain unlabeled in IR benchmarks; current methods cannot reliably identify and correct these gaps
- **Experiment**: DREAM employs LLM agents with opposing stances and iterative reciprocal critique, achieving 95.2‚ÄØ% labeling accuracy while requiring only 3.5‚ÄØ% human effort; the framework flags uncertain cases for human review. Using DREAM, the authors built BRIDGE, uncovering 29,824 missing relevant chunks and re‚Äëbenchmarking IR systems to reveal distortions in retriever rankings and retrieval‚Äëgeneration misalignment
- **Takeaway**: DREAM enhances labeling accuracy, minimizes human effort, and provides a systematic escalation mechanism; BRIDGE corrects evaluation bias, exposing many missing relevant chunks and highlighting the impact on IR system evaluation and RAG performance. Both the framework and dataset are publicly released.

#### Abstract
> Information retrieval (IR) evaluation remains challenging due to incomplete IR benchmark datasets that contain unlabeled relevant chunks. While LLMs and LLM-human hybrid strategies reduce costly human effort, they remain prone to LLM overconfidence and ineffective AI-to-human escalation. To address this, we propose DREAM, a multi-round debate-based relevance assessment framework with LLM agents, built on opposing initial stances and iterative reciprocal critique. Through our agreement-based debate, it yields more accurate labeling for certain cases and more reliable AI-to-human escalation for uncertain ones, achieving 95.2% labeling accuracy with only 3.5% human involvement. Using DREAM, we build BRIDGE, a refined benchmark that mitigates evaluation bias and enables fairer retriever comparison by uncovering 29,824 missing relevant chunks. We then re-benchmark IR systems and extend evaluation to RAG, showing that unaddressed holes not only distort retriever rankings but also drive retrieval-generation misalignment. The relevance assessment framework is available at https: //github.com/DISL-Lab/DREAM-ICLR-26; and the BRIDGE dataset is available at https://github.com/DISL-Lab/BRIDGE-Benchmark.

---

### 2. Table-as-Search: Formulate Long-Horizon Agentic Information Seeking as Table Completion

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Tian Lan, Felix Henry, Bin Zhu, Qianghuai Jia, Junyang Ren, Qihang Pu, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo
- **URL**: <http://arxiv.org/abs/2602.06724v1>
- **Submitted**: 2026-02-06 14:18:26
- **Topic Keywords**: query, rag, search
- **Reason**: This paper introduces a novel structured planning framework, Table-as-Search, which reformulates the Information Seeking task as a Table Completion task. Although it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it does address a related challenge in Information Retrieval, specifically in long-horizon exploration and search state management. The paper's emphasis on structured planning and its experimental results make it relevant to the broader field of Information Retrieval.

#### Abstract
> Current Information Seeking (InfoSeeking) agents struggle to maintain focus and coherence during long-horizon exploration, as tracking search states, including planning procedure and massive search results, within one plain-text context is inherently fragile. To address this, we introduce \textbf{Table-as-Search (TaS)}, a structured planning framework that reformulates the InfoSeeking task as a Table Completion task. TaS maps each query into a structured table schema maintained in an external database, where rows represent search candidates and columns denote constraints or required information. This table precisely manages the search states: filled cells strictly record the history and search results, while empty cells serve as an explicit search plan. Crucially, TaS unifies three distinct InfoSeeking tasks: Deep Search, Wide Search, and the challenging DeepWide Search. Extensive experiments demonstrate that TaS significantly outperforms numerous state-of-the-art baselines across three kinds of benchmarks, including multi-agent framework and commercial systems. Furthermore, our analysis validates the TaS's superior robustness in long-horizon InfoSeeking, alongside its efficiency, scalability and flexibility. Code and datasets are publicly released at https://github.com/AIDC-AI/Marco-Search-Agent.

---

### 3. R2LED: Equipping Retrieval and Refinement in Lifelong User Modeling with Semantic IDs for CTR Prediction

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Qidong Liu, Gengnan Wang, Zhichen Liu, Moranxin Wang, Zijian Zhang, Xiao Han, Ni Zhang, Tao Qin, Chen Li
- **URL**: <http://arxiv.org/abs/2602.06622v1>
- **Submitted**: 2026-02-06 11:27:20
- **Topic Keywords**: rag, ctr, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on lifelong user modeling, semantic understanding, and real-time relevance optimization aligns with your core themes. However, the specific application to CTR prediction and user behavior modeling is somewhat niche, preventing a perfect match.

#### Abstract
> Lifelong user modeling, which leverages users' long-term behavior sequences for CTR prediction, has been widely applied in personalized services. Existing methods generally adopted a two-stage "retrieval-refinement" strategy to balance effectiveness and efficiency. However, they still suffer from (i) noisy retrieval due to skewed data distribution and (ii) lack of semantic understanding in refinement. While semantic enhancement, e.g., LLMs modeling or semantic embeddings, offers potential solutions to these two challenges, these approaches face impractical inference costs or insufficient representation granularity. Obsorbing multi-granularity and lightness merits of semantic identity (SID), we propose a novel paradigm that equips retrieval and refinement in Lifelong User Modeling with SEmantic IDs (R2LED) to address these issues. First, we introduce a Multi-route Mixed Retrieval for the retrieval stage. On the one hand, it captures users' interests from various granularities by several parallel recall routes. On the other hand, a mixed retrieval mechanism is proposed to efficiently retrieve candidates from both collaborative and semantic views, reducing noise. Then, for refinement, we design a Bi-level Fusion Refinement, including a target-aware cross-attention for route-level fusion and a gate mechanism for SID-level fusion. It can bridge the gap between semantic and collaborative spaces, exerting the merits of SID. The comprehensive experimental results on two public datasets demonstrate the superiority of our method in both performance and efficiency. To facilitate the reproduction, we have released the code online https://github.com/abananbao/R2LED.

---

### 4. Improve Large Language Model Systems with User Logs

- **LLM Score**: 6
- **Keyword Score**: 11
- **Authors**: Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu
- **URL**: <http://arxiv.org/abs/2602.06470v1>
- **Submitted**: 2026-02-06 07:55:26
- **Topic Keywords**: query, rag, retrieval augmented generation, user behavior, retrieval
- **Reason**: The paper explores the use of user logs to improve large language model systems, which is somewhat related to information retrieval and query understanding. However, the focus is more on NLP and model optimization rather than search technologies or ranking models, limiting its relevance to your core research themes.

#### Abstract
> Scaling training data and model parameters has long driven progress in large language models (LLMs), but this paradigm is increasingly constrained by the scarcity of high-quality data and diminishing returns from rising computational costs. As a result, recent work is increasing the focus on continual learning from real-world deployment, where user interaction logs provide a rich source of authentic human feedback and procedural knowledge. However, learning from user logs is challenging due to their unstructured and noisy nature. Vanilla LLM systems often struggle to distinguish useful feedback signals from noisy user behavior, and the disparity between user log collection and model optimization (e.g., the off-policy optimization problem) further strengthens the problem. To this end, we propose UNO (User log-driveN Optimization), a unified framework for improving LLM systems (LLMsys) with user logs. UNO first distills logs into semi-structured rules and preference pairs, then employs query-and-feedback-driven clustering to manage data heterogeneity, and finally quantifies the cognitive gap between the model's prior knowledge and the log data. This assessment guides the LLMsys to adaptively filter out noisy feedback and construct different modules for primary and reflective experiences extracted from user logs, thereby improving future responses. Extensive experiments show that UNO achieves state-of-the-art effectiveness and efficiency, significantly outperforming Retrieval Augmented Generation (RAG) and memory-based baselines. We have open-sourced our code at https://github.com/bebr2/UNO .

---

### 5. MuCo: Multi-turn Contrastive Learning for Multimodal Embedding Model

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Geonmo Gu, Byeongho Heo, Jaemyung Yu, Jaehui Hwang, Taekyung Kim, Sangmin Lee, HeeJae Jun, Yoohoon Kang, Sangdoo Yun, Dongyoon Han
- **URL**: <http://arxiv.org/abs/2602.06393v1>
- **Submitted**: 2026-02-06 05:18:33
- **Comment**: 22 pages
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: This paper introduces a new framework for multimodal embedding models, focusing on contrastive learning and dialogue-inspired processing of multiple query-target pairs. While it touches on aspects of information retrieval and multimodal learning, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat related but not a central match for your research interests.

#### Abstract
> Universal Multimodal embedding models built on Multimodal Large Language Models (MLLMs) have traditionally employed contrastive learning, which aligns representations of query-target pairs across different modalities. Yet, despite its empirical success, they are primarily built on a "single-turn" formulation where each query-target pair is treated as an independent data point. This paradigm leads to computational inefficiency when scaling, as it requires a separate forward pass for each pair and overlooks potential contextual relationships between multiple queries that can relate to the same context. In this work, we introduce Multi-Turn Contrastive Learning (MuCo), a dialogue-inspired framework that revisits this process. MuCo leverages the conversational nature of MLLMs to process multiple, related query-target pairs associated with a single image within a single forward pass. This allows us to extract a set of multiple query and target embeddings simultaneously, conditioned on a shared context representation, amplifying the effective batch size and overall training efficiency. Experiments exhibit MuCo with a newly curated 5M multimodal multi-turn dataset (M3T), which yields state-of-the-art retrieval performance on MMEB and M-BEIR benchmarks, while markedly enhancing both training efficiency and representation coherence across modalities. Code and M3T are available at https://github.com/naver-ai/muco

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Multimodal Generative Retrieval Model with Staged Pretraining for Food Delivery on Meituan

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Boyu Chen, Tai Guo, Weiyu Cui, Yuqing Li, Xingxing Wang, Chuan Shi, Cheng Yang
- **URL**: <http://arxiv.org/abs/2602.06654v1>
- **Submitted**: 2026-02-06 12:29:13
- **Topic Keywords**: queries, click, click-through rate, retrieval
- **Reason**: The paper focuses on multimodal retrieval models for food delivery, which is somewhat related to information retrieval and search technologies. However, the emphasis on multimodal generative retrieval and staged pretraining is not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multimodal retrieval models are becoming increasingly important in scenarios such as food delivery, where rich multimodal features can meet diverse user needs and enable precise retrieval. Mainstream approaches typically employ a dual-tower architecture between queries and items, and perform joint optimization of intra-tower and inter-tower tasks. However, we observe that joint optimization often leads to certain modalities dominating the training process, while other modalities are neglected. In addition, inconsistent training speeds across modalities can easily result in the one-epoch problem. To address these challenges, we propose a staged pretraining strategy, which guides the model to focus on specialized tasks at each stage, enabling it to effectively attend to and utilize multimodal features, and allowing flexible control over the training process at each stage to avoid the one-epoch problem. Furthermore, to better utilize the semantic IDs that compress high-dimensional multimodal embeddings, we design both generative and discriminative tasks to help the model understand the associations between SIDs, queries, and item features, thereby improving overall performance. Extensive experiments on large-scale real-world Meituan data demonstrate that our method achieves improvements of 3.80%, 2.64%, and 2.17% on R@5, R@10, and R@20, and 5.10%, 4.22%, and 2.09% on N@5, N@10, and N@20 compared to mainstream baselines. Online A/B testing on the Meituan platform shows that our approach achieves a 1.12% increase in revenue and a 1.02% increase in click-through rate, validating the effectiveness and superiority of our method in practical applications.

### 7. FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Bo Yang, Lanfei Feng, Yunkui Chen, Yu Zhang, Xiao Xu, Shijian Li
- **URL**: <http://arxiv.org/abs/2602.06625v1>
- **Submitted**: 2026-02-06 11:35:32
- **Topic Keywords**: pointwise, pairwise, search
- **Reason**: The paper explores the use of Large Language Models (LLMs) as judges, addressing limitations such as adaptivity, bias, and evaluation inconsistency. While it touches on aspects of query understanding and ranking models, its primary focus is on LLMs and debiasing, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the paper's emphasis on LLMs and evaluation consistency is not a central match for your core research themes.

#### Abstract
> Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

### 8. TokenMixer-Large: Scaling Up Large Ranking Models in Industrial Recommenders

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Yuchen Jiang, Jie Zhu, Xintian Han, Hui Lu, Kunmin Bai, Mingyu Yang, Shikang Wu, Ruihao Zhang, Wenlin Zhao, Shipeng Bai, Sijin Zhou, Huizhi Yang, Tianyi Liu, Wenda Liu, Ziyan Gong, Haoran Ding, Zheng Chai, Deping Xie, Zhe Chen, Yuchao Zheng, Peng Xu
- **URL**: <http://arxiv.org/abs/2602.06563v1>
- **Submitted**: 2026-02-06 10:04:33
- **Topic Keywords**: ranking, rag, recommend, rank
- **Reason**: The paper is somewhat related to your research interests in Information Retrieval, specifically in ranking models and scaling laws. However, the focus on recommender systems and industrial applications, while tangentially related to your e-commerce background, does not strongly align with your primary interests in deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> In recent years, the study of scaling laws for large recommendation models has gradually gained attention. Works such as Wukong, HiFormer, and DHEN have attempted to increase the complexity of interaction structures in ranking models and validate scaling laws between performance and parameters/FLOPs by stacking multiple layers. However, their experimental scale remains relatively limited. Our previous work introduced the TokenMixer architecture, an efficient variant of the standard Transformer where the self-attention mechanism is replaced by a simple reshape operation, and the feed-forward network is adapted to a pertoken FFN. The effectiveness of this architecture was demonstrated in the ranking stage by the model presented in the RankMixer paper. However, this foundational TokenMixer architecture itself has several design limitations. In this paper, we propose TokenMixer-Large, which systematically addresses these core issues: sub-optimal residual design, insufficient gradient updates in deep models, incomplete MoE sparsification, and limited exploration of scalability. By leveraging a mixing-and-reverting operation, inter-layer residuals, the auxiliary loss and a novel Sparse-Pertoken MoE architecture, TokenMixer-Large successfully scales its parameters to 7-billion and 15-billion on online traffic and offline experiments, respectively. Currently deployed in multiple scenarios at ByteDance, TokenMixer -Large has achieved significant offline and online performance gains.

### 9. Beyond Static Alignment: Hierarchical Policy Control for LLM Safety via Risk-Aware Chain-of-Thought

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Jianfeng Si, Lin Sun, Weihong Lin, Xiangzheng Zhang
- **URL**: <http://arxiv.org/abs/2602.06650v1>
- **Submitted**: 2026-02-06 12:20:01
- **Comment**: 13 pages, 5 tables, 2 figures
- **Topic Keywords**: queries, search
- **Reason**: This paper explores the safety of Large Language Models (LLMs) through a hierarchical policy control framework called PACT. While it touches on aspects of query understanding and decision-making, its primary focus is on safety and controllability, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the paper's emphasis on safety and risk-aware reasoning is not a central match for your research themes.

#### Abstract
> Large Language Models (LLMs) face a fundamental safety-helpfulness trade-off due to static, one-size-fits-all safety policies that lack runtime controllabilityxf, making it difficult to tailor responses to diverse application needs. %As a result, models may over-refuse benign requests or under-constrain harmful ones. We present \textbf{PACT} (Prompt-configured Action via Chain-of-Thought), a framework for dynamic safety control through explicit, risk-aware reasoning. PACT operates under a hierarchical policy architecture: a non-overridable global safety policy establishes immutable boundaries for critical risks (e.g., child safety, violent extremism), while user-defined policies can introduce domain-specific (non-global) risk categories and specify label-to-action behaviors to improve utility in real-world deployment settings. The framework decomposes safety decisions into structured Classify$\rightarrow$Act paths that route queries to the appropriate action (comply, guide, or reject) and render the decision-making process transparent.
  Extensive experiments demonstrate that PACT achieves near state-of-the-art safety performance under global policy evaluation while attaining the best controllability under user-specific policy evaluation, effectively mitigating the safety-helpfulness trade-off. We will release the PACT model suite, training data, and evaluation protocols to facilitate reproducible research in controllable safety alignment.

### 10. Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Duygu Altinok
- **URL**: <http://arxiv.org/abs/2602.06942v1>
- **Submitted**: 2026-02-06 18:41:14
- **Comment**: Submitted to Cambridge NLP journal, all rights belong to them
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of tokenization and subword strategies for morphologically rich languages. However, the focus on Turkish language and subword tokenization is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a "subwords manifest", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this "subwords manifest" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.

### 11. compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Lucie Termignon, Simonas Zilinskas, Hadrien P√©lissier, Aur√©lien Barrot, Nicolas Chesnais, Elie Gavoty
- **URL**: <http://arxiv.org/abs/2602.06669v1>
- **Submitted**: 2026-02-06 12:53:44
- **Comment**: 18 pages, 7 figures, preprint
- **Topic Keywords**: pairwise
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves the collection and analysis of human preference data for language models. However, the focus on large language models and human-AI interaction is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to your research is limited to the broader context of NLP and data mining.

#### Abstract
> Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction.

### 12. A methodology for analyzing financial needs hierarchy from social discussions using LLM

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Abhishek Jangra, Sachin Thukral, Arnab Chatterjee, Jayasree Raveendran
- **URL**: <http://arxiv.org/abs/2602.06431v1>
- **Submitted**: 2026-02-06 06:58:25
- **Comment**: 15 pages, 5 figures, 4 tables
- **Topic Keywords**: ctr, search
- **Reason**: The paper explores the application of LLMs in analyzing financial needs from social media discourse, which is somewhat related to the user's interests in Information Retrieval and NLP. However, the focus on financial behavior and needs hierarchy is not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> This study examines the hierarchical structure of financial needs as articulated in social media discourse, employing generative AI techniques to analyze large-scale textual data. While human needs encompass a broad spectrum from fundamental survival to psychological fulfillment financial needs are particularly critical, influencing both individual well-being and day-to-day decision-making. Our research advances the understanding of financial behavior by utilizing large language models (LLMs) to extract and analyze expressions of financial needs from social media posts. We hypothesize that financial needs are organized hierarchically, progressing from short-term essentials to long-term aspirations, consistent with theoretical frameworks established in the behavioral sciences. Through computational analysis, we demonstrate the feasibility of identifying these needs and validate the presence of a hierarchical structure within them. In addition to confirming this structure, our findings provide novel insights into the content and themes of financial discussions online. By inferring underlying needs from naturally occurring language, this approach offers a scalable and data-driven alternative to conventional survey methodologies, enabling a more dynamic and nuanced understanding of financial behavior in real-world contexts.

### 13. Steering Safely or Off a Cliff? Rethinking Specificity and Robustness in Inference-Time Interventions

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Navita Goyal, Hal Daum√©
- **URL**: <http://arxiv.org/abs/2602.06256v1>
- **Submitted**: 2026-02-05 23:14:05
- **Comment**: EACL 2026 Main, Long Paper
- **Topic Keywords**: queries
- **Reason**: This paper explores model steering in large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on model safety and robustness specificity is not directly aligned with the user's core research themes, and the paper's relevance to search technologies and user behavior modeling is limited.

#### Abstract
> Model steering, which involves intervening on hidden representations at inference time, has emerged as a lightweight alternative to finetuning for precisely controlling large language models. While steering efficacy has been widely studied, evaluations of whether interventions alter only the intended property remain limited, especially with respect to unintended changes in behaviors related to the target property. We call this notion specificity. We propose a framework that distinguishes three dimensions of specificity: general (preserving fluency and unrelated abilities), control (preserving related control properties), and robustness (preserving control properties under distribution shifts). We study two safety-critical use cases: steering models to reduce overrefusal and faithfulness hallucinations, and show that while steering achieves high efficacy and largely maintains general and control specificity, it consistently fails to preserve robustness specificity. In the case of overrefusal steering, for example, all steering methods reduce overrefusal without harming general abilities and refusal on harmful queries; however, they substantially increase vulnerability to jailbreaks. Our work provides the first systematic evaluation of specificity in model steering, showing that standard efficacy and specificity checks are insufficient, because without robustness evaluation, steering methods may appear reliable even when they compromise model safety.

### 14. Visual Word Sense Disambiguation with CLIP through Dual-Channel Text Prompting and Image Augmentations

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shamik Bhattacharya, Daniel Perkins, Yaren Dogan, Vineeth Konjeti, Sudarshan Srinivasan, Edmon Begoli
- **URL**: <http://arxiv.org/abs/2602.06799v1>
- **Submitted**: 2026-02-06 15:53:10
- **Comment**: 9 pages, 6 figures, pending journal/workshop submission
- **Topic Keywords**: rag
- **Reason**: This paper explores Visual Word Sense Disambiguation using CLIP, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on visual domain and word sense disambiguation is not a central match to the user's core research themes. The paper's relevance is somewhat enhanced by its use of multimodal embeddings and cosine similarity, which are related to user behavior modeling and click models.

#### Abstract
> Ambiguity poses persistent challenges in natural language understanding for large language models (LLMs). To better understand how lexical ambiguity can be resolved through the visual domain, we develop an interpretable Visual Word Sense Disambiguation (VWSD) framework. The model leverages CLIP to project ambiguous language and candidate images into a shared multimodal space. We enrich textual embeddings using a dual-channel ensemble of semantic and photo-based prompts with WordNet synonyms, while image embeddings are refined through robust test-time augmentations. We then use cosine similarity to determine the image that best aligns with the ambiguous text. When evaluated on the SemEval-2023 VWSD dataset, enriching the embeddings raises the MRR from 0.7227 to 0.7590 and the Hit Rate from 0.5810 to 0.6220. Ablation studies reveal that dual-channel prompting provides strong, low-latency performance, whereas aggressive image augmentation yields only marginal gains. Additional experiments with WordNet definitions and multilingual prompt ensembles further suggest that noisy external signals tend to dilute semantic specificity, reinforcing the effectiveness of precise, CLIP-aligned prompts for visual word sense disambiguation.

### 15. RoPE-LIME: RoPE-Space Locality + Sparse-K Sampling for Efficient LLM Attribution

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Isaac Picov, Ritesh Goru
- **URL**: <http://arxiv.org/abs/2602.06275v1>
- **Submitted**: 2026-02-06 00:26:14
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding, as it involves explaining LLM outputs and computing token-level attributions. However, it is not directly related to your primary focus on Information Retrieval (IR), query understanding, ranking models, or user behavior modeling.

#### Abstract
> Explaining closed-source LLM outputs is challenging because API access prevents gradient-based attribution, while perturbation methods are costly and noisy when they depend on regenerated text. We introduce RoPE-LIME, an open-source extension of gSMILE that decouples reasoning from explanation: given a fixed output from a closed model, a smaller open-source surrogate computes token-level attributions from probability-based objectives (negative log-likelihood and divergence targets) under input perturbations. RoPE-LIME incorporates (i) a locality kernel based on Relaxed Word Mover's Distance computed in RoPE embedding space for stable similarity under masking, and (ii) Sparse-K sampling, an efficient perturbation strategy that improves interaction coverage under limited budgets. Experiments on HotpotQA (sentence features) and a hand-labeled MMLU subset (word features) show that RoPE-LIME produces more informative attributions than leave-one-out sampling and improves over gSMILE while substantially reducing closed-model API calls.

### 16. PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Cheng Liang, Chaoyi Wu, Weike Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie
- **URL**: <http://arxiv.org/abs/2602.06184v1>
- **Submitted**: 2026-02-05 20:44:07
- **Topic Keywords**: retrieval
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and data mining, as it involves integrating ontology knowledge into a vision-language model. However, it is not directly related to the user's core research themes in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's focus on medical image analysis and phenotype recognition is also outside the user's primary domain of e-commerce.

#### Abstract
> Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.

### 17. On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Shankar Veludandi, Gulrukh Kurdistan, Uzma Mushtaque
- **URL**: <http://arxiv.org/abs/2602.06935v1>
- **Submitted**: 2026-02-06 18:30:23
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, but it focuses on recommender systems rather than query understanding or ranking models. The use of deep semantic understanding is not a primary focus, and the paper is more concerned with efficiency and computational resources. While it touches on sequential recommendation, which is a related topic, it is not a central match for your core research themes.

#### Abstract
> Sequential recommendation (SR) models predict a user's next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec's viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.

### 18. TrailBlazer: History-Guided Reinforcement Learning for Black-Box LLM Jailbreaking

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Sung-Hoon Yoon, Ruizhi Qian, Minda Zhao, Weiyue Li, Mengyu Wang
- **URL**: <http://arxiv.org/abs/2602.06440v1>
- **Submitted**: 2026-02-06 07:11:10
- **Topic Keywords**: query, queries, rag, search
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves reinforcement learning, the context is focused on 'jailbreaking' large language models, which is not a central match to the user's interests.

#### Abstract
> Large Language Models (LLMs) have become integral to many domains, making their safety a critical priority. Prior jailbreaking research has explored diverse approaches, including prompt optimization, automated red teaming, obfuscation, and reinforcement learning (RL) based methods. However, most existing techniques fail to effectively leverage vulnerabilities revealed in earlier interaction turns, resulting in inefficient and unstable attacks. Since jailbreaking involves sequential interactions in which each response influences future actions, reinforcement learning provides a natural framework for this problem. Motivated by this, we propose a history-aware RL-based jailbreak framework that analyzes and reweights vulnerability signals from prior steps to guide future decisions. We show that incorporating historical information alone improves jailbreak success rates. Building on this insight, we introduce an attention-based reweighting mechanism that highlights critical vulnerabilities within the interaction history, enabling more efficient exploration with fewer queries. Extensive experiments on AdvBench and HarmBench demonstrate that our method achieves state-of-the-art jailbreak performance while significantly improving query efficiency. These results underscore the importance of historical vulnerability signals in reinforcement learning-driven jailbreak strategies and offer a principled pathway for advancing adversarial research on LLM safeguards.

### 19. MPIB: A Benchmark for Medical Prompt Injection Attacks and Clinical Safety in LLMs

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Junhyeok Lee, Han Jang, Kyu Sung Choi
- **URL**: <http://arxiv.org/abs/2602.06268v1>
- **Submitted**: 2026-02-06 00:03:09
- **Comment**: 13 pages, 7 figures
- **Topic Keywords**: query, rag, retrieval, search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves Large Language Models and Retrieval-Augmented Generation, the focus is on clinical safety and prompt injection attacks, which is not a central match to your research themes.

#### Abstract
> Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems are increasingly integrated into clinical workflows; however, prompt injection attacks can steer these systems toward clinically unsafe or misleading outputs. We introduce the Medical Prompt Injection Benchmark (MPIB), a dataset-and-benchmark suite for evaluating clinical safety under both direct prompt injection and indirect, RAG-mediated injection across clinically grounded tasks. MPIB emphasizes outcome-level risk via the Clinical Harm Event Rate (CHER), which measures high-severity clinical harm events under a clinically grounded taxonomy, and reports CHER alongside Attack Success Rate (ASR) to disentangle instruction compliance from downstream patient risk. The benchmark comprises 9,697 curated instances constructed through multi-stage quality gates and clinical safety linting. Evaluating MPIB across a diverse set of baseline LLMs and defense configurations, we find that ASR and CHER can diverge substantially, and that robustness depends critically on whether adversarial instructions appear in the user query or in retrieved context. We release MPIB with evaluation code, adversarial baselines, and comprehensive documentation to support reproducible and systematic research on clinical prompt injection. Code and data are available at GitHub (code) and Hugging Face (data).

### 20. Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, Hyunwoo Ko, Amit Agarwal, Sunghee Ahn, Kyong-Ha Lee, Youngjae Yu
- **URL**: <http://arxiv.org/abs/2602.06291v1>
- **Submitted**: 2026-02-06 01:10:28
- **Comment**: Preprint
- **Topic Keywords**: ranking, rank, search, acl
- **Reason**: This paper focuses on a consequence-based approach for evaluating research-level math solutions, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves LLMs, the context and application are distinct from the user's areas of focus.

#### Abstract
> Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

### 21. BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Nishant Balepur, Bhavya Rajasekaran, Jane Oh, Michael Xie, Atrey Desai, Vipul Gupta, Steven James Moore, Eunsol Choi, Rachel Rudinger, Jordan Lee Boyd-Graber
- **URL**: <http://arxiv.org/abs/2602.06221v1>
- **Submitted**: 2026-02-05 21:57:50
- **Comment**: In-progress preprint
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, which are core areas of your research interests. While it touches on Natural Language Processing, its focus on multiple-choice question answering benchmarks and education research does not align closely with your primary areas of focus.

#### Abstract
> Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

### 22. Personality as Relational Infrastructure: User Perceptions of Personality-Trait-Infused LLM Messaging

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Dominik P. Hofer, David Haag, Rania Islambouli, Jan D. Smeddinck
- **URL**: <http://arxiv.org/abs/2602.06596v1>
- **Submitted**: 2026-02-06 10:47:47
- **Comment**: Currently under review
- **Topic Keywords**: retrieval augmented generation, retrieval
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models (LLMs), the focus is on personality-infused messaging for behavior change, which is not a central theme in your research.

#### Abstract
> Digital behaviour change systems increasingly rely on repeated, system-initiated messages to support users in everyday contexts. LLMs enable these messages to be personalised consistently across interactions, yet it remains unclear whether such personalisation improves individual messages or instead shapes users' perceptions through patterns of exposure. We explore this question in the context of LLM-generated JITAIs, which are short, context-aware messages delivered at moments deemed appropriate to support behaviour change, using physical activity as an application domain. In a controlled retrospective study, 90 participants evaluated messages generated using four LLM strategies: baseline prompting, few-shot prompting, fine-tuned models, and retrieval augmented generation, each implemented with and without Big Five Personality Traits to produce personality-aligned communication across multiple scenarios. Using ordinal multilevel models with within-between decomposition, we distinguish trial-level effects, whether personality information improves evaluations of individual messages, from person-level exposure effects, whether participants receiving higher proportions of personality-informed messages exhibit systematically different overall perceptions. Results showed no trial-level associations, but participants who received higher proportions of BFPT-informed messages rated the messages as more personalised, appropriate, and reported less negative affect. We use Communication Accommodation Theory for post-hoc analysis. These results suggest that personality-based personalisation in behaviour change systems may operate primarily through aggregate exposure rather than per-message optimisation, with implications for how adaptive systems are designed and evaluated in sustained human-AI interaction. In-situ longitudinal studies are needed to validate these findings in real-world contexts.

### 23. Stopping Computation for Converged Tokens in Masked Diffusion-LM Decoding

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Daisuke Oba, Danushka Bollegala, Masahiro Kaneko, Naoaki Okazaki
- **URL**: <http://arxiv.org/abs/2602.06412v1>
- **Submitted**: 2026-02-06 06:08:51
- **Comment**: Accepted at ICLR 2026
- **Topic Keywords**: query
- **Reason**: This paper appears to be primarily focused on improving the efficiency of a specific type of language model, which is not directly related to information retrieval or search technologies. While it does involve some aspects of deep semantic understanding, the context is more aligned with NLP and deep learning optimizations rather than IR or search.

#### Abstract
> Masked Diffusion Language Models generate sequences via iterative sampling that progressively unmasks tokens. However, they still recompute the attention and feed-forward blocks for every token position at every step -- even when many unmasked tokens are essentially fixed, resulting in substantial waste in compute. We propose SureLock: when the posterior at an unmasked position has stabilized across steps (our sure condition), we lock that position -- thereafter skipping its query projection and feed-forward sublayers -- while caching its attention keys and values so other positions can continue to attend to it. This reduces the dominant per-iteration computational cost from $O(N^2d)$ to $O(MNd)$ where $N$ is the sequence length, $M$ is the number of unlocked token positions, and $d$ is the model dimension. In practice, $M$ decreases as the iteration progresses, yielding substantial savings. On LLaDA-8B, SureLock reduces algorithmic FLOPs by 30--50% relative to the same sampler without locking, while maintaining comparable generation quality. We also provide a theoretical analysis to justify the design rationale of SureLock: monitoring only the local KL at the lock step suffices to bound the deviation in final token probabilities. Our code will be available at https://daioba.github.io/surelock .

### 24. The Condensate Theorem: Transformers are O(n), Not $O(n^2)$

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jorge L. Ruiz Williams
- **URL**: <http://arxiv.org/abs/2602.06317v1>
- **Submitted**: 2026-02-06 02:32:42
- **Comment**: 13 pages, 4 figures, 8 tables
- **Topic Keywords**: query
- **Reason**: This paper appears to be focused on optimizing transformer architectures for efficiency, specifically by exploiting attention sparsity. While it touches on deep learning and NLP, its primary contribution is in improving the scalability of transformer models, which is not a central match to your research interests in IR, query understanding, and ranking models.

#### Abstract
> We present the Condensate Theorem: attention sparsity is a learned topological property, not an architectural constraint. Through empirical analysis of trained language models, we find that attention mass concentrates on a distinct topological manifold -- and this manifold can be identified dynamically without checking every position. We prove a general result: for any query, projecting attention onto the Condensate Manifold (Anchor + Window + Dynamic Top-k) achieves 100% output equivalence with full $O(n^2)$ attention. This is not an approximation -- it is lossless parity. We validate this across GPT-2, Pythia, Qwen2, TinyLlama, and Mistral, demonstrating bit-exact token matching on 1,500+ generated tokens. By mapping this topology to hardware, our Topological Attention kernel achieves a 159x measured speedup at 131K tokens (3.94ms vs 628ms) and a projected >1,200x speedup at 1M tokens, reducing inference costs by >99.9% compared to Flash Attention. We conclude that the quadratic bottleneck is an artifact of naive implementation, not intelligence.

### 25. Learning Rate Scaling across LoRA Ranks and Transfer to Full Finetuning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Nan Chen, Soledad Villar, Soufiane Hayou
- **URL**: <http://arxiv.org/abs/2602.06204v1>
- **Submitted**: 2026-02-05 21:28:59
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on learning rate scaling in the context of Low-Rank Adaptation (LoRA) for parameter-efficient finetuning of large models. While it touches on model adaptation and hyperparameter transfer, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Low-Rank Adaptation (LoRA) is a standard tool for parameter-efficient finetuning of large models. While it induces a small memory footprint, its training dynamics can be surprisingly complex as they depend on several hyperparameters such as initialization, adapter rank, and learning rate. In particular, it is unclear how the optimal learning rate scales with adapter rank, which forces practitioners to re-tune the learning rate whenever the rank is changed. In this paper, we introduce Maximal-Update Adaptation ($Œº$A), a theoretical framework that characterizes how the "optimal" learning rate should scale with model width and adapter rank to produce stable, non-vanishing feature updates under standard configurations. $Œº$A is inspired from the Maximal-Update Parametrization ($Œº$P) in pretraining. Our analysis leverages techniques from hyperparameter transfer and reveals that the optimal learning rate exhibits different scaling patterns depending on initialization and LoRA scaling factor. Specifically, we identify two regimes: one where the optimal learning rate remains roughly invariant across ranks, and another where it scales inversely with rank. We further identify a configuration that allows learning rate transfer from LoRA to full finetuning, drastically reducing the cost of learning rate tuning for full finetuning. Experiments across language, vision, vision--language, image generation, and reinforcement learning tasks validate our scaling rules and show that learning rates tuned on LoRA transfer reliably to full finetuning.

### 26. Large Language Model Reasoning Failures

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Peiyang Song, Pengrui Han, Noah Goodman
- **URL**: <http://arxiv.org/abs/2602.06176v1>
- **Submitted**: 2026-02-05 20:29:26
- **Comment**: Repository: https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures. Published at TMLR 2026 with Survey Certification
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on Large Language Models (LLMs) and their reasoning failures, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it touches on a related topic, the paper's emphasis on LLMs and their limitations does not directly align with your core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) have exhibited remarkable reasoning capabilities, achieving impressive results across a wide range of tasks. Despite these advances, significant reasoning failures persist, occurring even in seemingly simple scenarios. To systematically understand and address these shortcomings, we present the first comprehensive survey dedicated to reasoning failures in LLMs. We introduce a novel categorization framework that distinguishes reasoning into embodied and non-embodied types, with the latter further subdivided into informal (intuitive) and formal (logical) reasoning. In parallel, we classify reasoning failures along a complementary axis into three types: fundamental failures intrinsic to LLM architectures that broadly affect downstream tasks; application-specific limitations that manifest in particular domains; and robustness issues characterized by inconsistent performance across minor variations. For each reasoning failure, we provide a clear definition, analyze existing studies, explore root causes, and present mitigation strategies. By unifying fragmented research efforts, our survey provides a structured perspective on systemic weaknesses in LLM reasoning, offering valuable insights and guiding future research towards building stronger, more reliable, and robust reasoning capabilities. We additionally release a comprehensive collection of research works on LLM reasoning failures, as a GitHub repository at https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures, to provide an easy entry point to this area.

### 27. Stop the Flip-Flop: Context-Preserving Verification for Fast Revocable Diffusion Decoding

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yanzheng Xiang, Lan Wei, Yizhen Yao, Qinglin Zhu, Hanqi Yan, Chen Jin, Philip Alexander Teare, Dandan Zhang, Lin Gui, Amrutha Saseendran, Yulan He
- **URL**: <http://arxiv.org/abs/2602.06161v1>
- **Submitted**: 2026-02-05 19:58:48
- **Topic Keywords**: queries
- **Reason**: This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on diffusion language model inference and decoding, which is not a central match to your areas of interest.

#### Abstract
> Parallel diffusion decoding can accelerate diffusion language model inference by unmasking multiple tokens per step, but aggressive parallelism often harms quality. Revocable decoding mitigates this by rechecking earlier tokens, yet we observe that existing verification schemes frequently trigger flip-flop oscillations, where tokens are remasked and later restored unchanged. This behaviour slows inference in two ways: remasking verified positions weakens the conditioning context for parallel drafting, and repeated remask cycles consume the revision budget with little net progress. We propose COVER (Cache Override Verification for Efficient Revision), which performs leave-one-out verification and stable drafting within a single forward pass. COVER constructs two attention views via KV cache override: selected seeds are masked for verification, while their cached key value states are injected for all other queries to preserve contextual information, with a closed form diagonal correction preventing self leakage at the seed positions. COVER further prioritises seeds using a stability aware score that balances uncertainty, downstream influence, and cache drift, and it adapts the number of verified seeds per step. Across benchmarks, COVER markedly reduces unnecessary revisions and yields faster decoding while preserving output quality.

### 28. DAWN: Dependency-Aware Fast Inference for Diffusion LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lizhuo Luo, Zhuoran Shi, Jiajun Luo, Zhi Wang, Shen Ren, Wenya Wang, Tianwei Zhang
- **URL**: <http://arxiv.org/abs/2602.06953v1>
- **Submitted**: 2026-02-06 18:51:29
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the efficiency of large language models through a dependency-aware decoding method, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the specific application and goals of the paper do not align with the user's core research interests.

#### Abstract
> Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.

### 29. SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mingqian Feng, Xiaodong Liu, Weiwei Yang, Jialin Song, Xuekai Zhu, Chenliang Xu, Jianfeng Gao
- **URL**: <http://arxiv.org/abs/2602.06854v1>
- **Submitted**: 2026-02-06 16:44:57
- **Comment**: ICLR 2026, 37 pages, 13 tables, 7 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on multi-turn jailbreak attacks for safety-aligned chatbots, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's emphasis on adversarial attacks and reinforcement learning does not align with your core research themes.

#### Abstract
> Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.

### 30. Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kate Sanders, Nathaniel Weir, Sapana Chaudhary, Kaj Bostrom, Huzefa Rangwala
- **URL**: <http://arxiv.org/abs/2602.06795v1>
- **Submitted**: 2026-02-06 15:51:52
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Large Language Models (LLMs) for reasoning output verification and reward modeling, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on related topics, the paper's primary focus on LLMs and reward modeling makes it less relevant to your research areas.

#### Abstract
> An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or "rubrics", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.

### 31. Reading Between the Waves: Robust Topic Segmentation Using Inter-Sentence Audio Features

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Steffen Freisinger, Philipp Seeberger, Tobias Bocklet, Korbinian Riedhammer
- **URL**: <http://arxiv.org/abs/2602.06647v1>
- **Submitted**: 2026-02-06 12:16:51
- **Comment**: Accepted to IEEE ICASSP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on spoken content topic segmentation using audio features, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text encoders and multi-modal approaches, the primary focus is on audio features and topic segmentation, making it somewhat tangential to the user's interests.

#### Abstract
> Spoken content, such as online videos and podcasts, often spans multiple topics, which makes automatic topic segmentation essential for user navigation and downstream applications. However, current methods do not fully leverage acoustic features, leaving room for improvement. We propose a multi-modal approach that fine-tunes both a text encoder and a Siamese audio encoder, capturing acoustic cues around sentence boundaries. Experiments on a large-scale dataset of YouTube videos show substantial gains over text-only and multi-modal baselines. Our model also proves more resilient to ASR noise and outperforms a larger text-only baseline on three additional datasets in Portuguese, German, and English, underscoring the value of learned acoustic features for robust topic segmentation.

### 32. LogicSkills: A Structured Benchmark for Formal Reasoning in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Brian Rabern, Philipp Mondorf, Barbara Plank
- **URL**: <http://arxiv.org/abs/2602.06533v1>
- **Submitted**: 2026-02-06 09:38:44
- **Comment**: 13 pages, 5 figures
- **Topic Keywords**: rag
- **Reason**: This paper is primarily focused on evaluating the formal reasoning capabilities of large language models, which is a topic related to NLP. However, it does not directly address information retrieval, query understanding, or ranking models, making it less relevant to your core research interests.

#### Abstract
> Large language models have demonstrated notable performance across various logical reasoning benchmarks. However, it remains unclear which core logical skills they truly master. To address this, we introduce LogicSkills, a unified benchmark designed to isolate three fundamental skills in formal reasoning: (i) $\textit{formal symbolization}\unicode{x2014}$translating premises into first-order logic; (ii) $\textit{countermodel construction}\unicode{x2014}$formulating a finite structure in which all premises are true while the conclusion is false; and (iii) $\textit{validity assessment}\unicode{x2014}$deciding whether a conclusion follows from a given set of premises. Items are drawn from the two-variable fragment of first-order logic (without identity) and are presented in both natural English and a Carroll-style language with nonce words. All examples are verified for correctness and non-triviality using the SMT solver Z3. Across leading models, performance is high on validity but substantially lower on symbolization and countermodel construction, suggesting reliance on surface-level patterns rather than genuine symbolic or rule-based reasoning.

### 33. On the Wings of Imagination: Conflicting Script-based Multi-role Framework for Humor Caption Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wenbo Shang, Yuxi Sun, Jing Ma, Xin Huang
- **URL**: <http://arxiv.org/abs/2602.06423v1>
- **Submitted**: 2026-02-06 06:41:33
- **Comment**: Paper accepted as a conference paper at ICLR 2026
- **Topic Keywords**: retrieval
- **Reason**: This paper is not relevant to your research interests as it focuses on humor caption generation using large language models, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing, particularly in the context of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Humor is a commonly used and intricate human language in daily life. Humor generation, especially in multi-modal scenarios, is a challenging task for large language models (LLMs), which is typically as funny caption generation for images, requiring visual understanding, humor reasoning, creative imagination, and so on. Existing LLM-based approaches rely on reasoning chains or self-improvement, which suffer from limited creativity and interpretability. To address these bottlenecks, we develop a novel LLM-based humor generation mechanism based on a fundamental humor theory, GTVH. To produce funny and script-opposite captions, we introduce a humor-theory-driven multi-role LLM collaboration framework augmented with humor retrieval (HOMER). The framework consists of three LLM-based roles: (1) conflicting-script extractor that grounds humor in key script oppositions, forming the basis of caption generation; (2) retrieval-augmented hierarchical imaginator that identifies key humor targets and expands the creative space of them through diverse associations structured as imagination trees; and (3) caption generator that produces funny and diverse captions conditioned on the obtained knowledge. Extensive experiments on two New Yorker Cartoon benchmarking datasets show that HOMER outperforms state-of-the-art baselines and powerful LLM reasoning strategies on multi-modal humor captioning.

### 34. MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nurbek Tastan, Stefanos Laskaridis, Karthik Nandakumar, Samuel Horvath
- **URL**: <http://arxiv.org/abs/2602.06154v1>
- **Submitted**: 2026-02-05 19:48:41
- **Topic Keywords**: ctr
- **Reason**: This paper focuses on efficient and adaptive language models, which is not directly related to your primary research interests in Information Retrieval and Search technologies. While it touches on the concept of 'expert' models, the context is different from your work on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.

### 35. Self-Improving World Modelling with Latent Actions

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yifu Qiu, Zheng Zhao, Waylon Li, Yftah Ziser, Anna Korhonen, Shay B. Cohen, Edoardo M. Ponti
- **URL**: <http://arxiv.org/abs/2602.06130v1>
- **Submitted**: 2026-02-05 19:04:41
- **Topic Keywords**: rag
- **Reason**: This paper focuses on self-improving world modeling for Large Language Models (LLMs) and Vision-Language Models (VLMs), which is not directly related to Information Retrieval or Search technologies. While it involves learning and optimization, the context is different from query understanding, ranking models, and user behavior modeling.

#### Abstract
> Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_Œ∏(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_œÜ(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.

### 36. Halluverse-M^3: A multitask multilingual benchmark for hallucination in LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Samir Abdaljalil, Parichit Sharma, Erchin Serpedin, Hasan Kurban
- **URL**: <http://arxiv.org/abs/2602.06920v1>
- **Submitted**: 2026-02-06 18:16:09
- **Topic Keywords**: search
- **Reason**: This paper focuses on hallucinations in large language models, which is a topic related to NLP, but it does not directly align with the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling.

#### Abstract
> Hallucinations in large language models remain a persistent challenge, particularly in multilingual and generative settings where factual consistency is difficult to maintain. While recent models show strong performance on English-centric benchmarks, their behavior across languages, tasks, and hallucination types is not yet well understood. In this work, we introduce Halluverse-M^3, a dataset designed to enable systematic analysis of hallucinations across multiple languages, multiple generation tasks, and multiple hallucination categories. Halluverse-M^3 covers four languages, English, Arabic, Hindi, and Turkish, and supports two generation tasks: question answering and dialogue summarization. The dataset explicitly distinguishes between entity-level, relation-level, and sentence-level hallucinations. Hallucinated outputs are constructed through a controlled editing process and validated by human annotators, ensuring clear alignment between original content and hallucinated generations. Using this dataset, we evaluate a diverse set of contemporary open-source and proprietary language models on fine-grained hallucination detection. Our results show that question answering is consistently easier than dialogue summarization, while sentence-level hallucinations remain challenging even for the strongest models. Performance is highest in English and degrades in lower-resource languages, with Hindi exhibiting the lowest detection accuracy. Overall, Halluverse-M^3 provides a realistic and challenging benchmark for studying hallucinations in multilingual, multi-task settings. We release the dataset to support future research on hallucination detection and mitigation\footnote{https://huggingface.co/datasets/sabdalja/HalluVerse-M3}.

### 37. MTQE.en-he: Machine Translation Quality Estimation for English-Hebrew

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Andy Rosenbaum, Assaf Siani, Ilan Kernerman
- **URL**: <http://arxiv.org/abs/2602.06546v1>
- **Submitted**: 2026-02-06 09:51:45
- **Comment**: Accepted to LoResLM at EACL 2026
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves machine translation and quality estimation, it is focused on a specific language pair and does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> We release MTQE.en-he: to our knowledge, the first publicly available English-Hebrew benchmark for Machine Translation Quality Estimation. MTQE.en-he contains 959 English segments from WMT24++, each paired with a machine translation into Hebrew, and Direct Assessment scores of the translation quality annotated by three human experts. We benchmark ChatGPT prompting, TransQuest, and CometKiwi and show that ensembling the three models outperforms the best single model (CometKiwi) by 6.4 percentage points Pearson and 5.6 percentage points Spearman. Fine-tuning experiments with TransQuest and CometKiwi reveal that full-model updates are sensitive to overfitting and distribution collapse, yet parameter-efficient methods (LoRA, BitFit, and FTHead, i.e., fine-tuning only the classification head) train stably and yield improvements of 2-3 percentage points. MTQE.en-he and our experimental results enable future research on this under-resourced language pair.

### 38. AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yishan Li, Wentong Chen, Yukun Yan, Mingwei Li, Sen Mei, Xiaorong Wang, Kunpeng Liu, Xin Cong, Shuo Wang, Zhong Zhang, Yaxi Lu, Zhenghao Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun
- **URL**: <http://arxiv.org/abs/2602.06540v1>
- **Submitted**: 2026-02-06 09:45:04
- **Topic Keywords**: search
- **Reason**: This paper focuses on deep research report generation using a novel framework called AgentCPM-Report, which is primarily concerned with the synthesis of insight-driven analysis. While it involves information acquisition and knowledge refinement, the paper's primary focus is on the writing process and report generation, rather than information retrieval or search technologies. The paper's relevance to the user's research interests in IR and NLP is limited.

#### Abstract
> Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.

### 39. Designing Computational Tools for Exploring Causal Relationships in Qualitative Data

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Han Meng, Qiuyuan Lyu, Peinuan Qin, Yitian Yang, Renwen Zhang, Wen-Chieh Lin, Yi-Chieh Lee
- **URL**: <http://arxiv.org/abs/2602.06506v1>
- **Submitted**: 2026-02-06 08:56:55
- **Comment**: 19 pages, 5 figures, conditionally accepted by CHI26
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves data analysis, it focuses on qualitative data in social science research, which is outside your primary areas of interest.

#### Abstract
> Exploring causal relationships for qualitative data analysis in HCI and social science research enables the understanding of user needs and theory building. However, current computational tools primarily characterize and categorize qualitative data; the few systems that analyze causal relationships either inadequately consider context, lack credibility, or produce overly complex outputs. We first conducted a formative study with 15 participants interested in using computational tools for exploring causal relationships in qualitative data to understand their needs and derive design guidelines. Based on these findings, we designed and implemented QualCausal, a system that extracts and illustrates causal relationships through interactive causal network construction and multi-view visualization. A feedback study (n = 15) revealed that participants valued our system for reducing the analytical burden and providing cognitive scaffolding, yet navigated how such systems fit within their established research paradigms, practices, and habits. We discuss broader implications for designing computational tools that support qualitative data analysis.

### 40. Generics in science communication: Misaligned interpretations across laypeople, scientists, and large language models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Uwe Peters, Andrea Bertazzoli, Jasmine M. DeJesus, Gisela J. van der Velden, Benjamin Chin-Yee
- **URL**: <http://arxiv.org/abs/2602.06190v1>
- **Submitted**: 2026-02-05 20:54:44
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, which are core areas of your research interests. While it touches on Natural Language Processing (NLP) through the use of large language models, the focus is on science communication and interpretation of generics, which is not a central match to your research themes.

#### Abstract
> Scientists often use generics, that is, unquantified statements about whole categories of people or phenomena, when communicating research findings (e.g., "statins reduce cardiovascular events"). Large language models (LLMs), such as ChatGPT, frequently adopt the same style when summarizing scientific texts. However, generics can prompt overgeneralizations, especially when they are interpreted differently across audiences. In a study comparing laypeople, scientists, and two leading LLMs (ChatGPT-5 and DeepSeek), we found systematic differences in interpretation of generics. Compared to most scientists, laypeople judged scientific generics as more generalizable and credible, while LLMs rated them even higher. These mismatches highlight significant risks for science communication. Scientists may use generics and incorrectly assume laypeople share their interpretation, while LLMs may systematically overgeneralize scientific findings when summarizing research. Our findings underscore the need for greater attention to language choices in both human and LLM-mediated science communication.

### 41. Malicious Agent Skills in the Wild: A Large-Scale Security Empirical Study

- **LLM Score**: 0
- **Keyword Score**: 5
- **Authors**: Yi Liu, Zhihao Chen, Yanjun Zhang, Gelei Deng, Yuekang Li, Jianting Ning, Leo Yu Zhang
- **URL**: <http://arxiv.org/abs/2602.06547v1>
- **Submitted**: 2026-02-06 09:52:27
- **Topic Keywords**: ltr, rag
- **Reason**: This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on security and malicious agent skills in the wild does not align with your primary areas of focus.

#### Abstract
> Third-party agent skills extend LLM-based agents with instruction files and executable code that run on users' machines. Skills execute with user privileges and are distributed through community registries with minimal vetting, but no ground-truth dataset exists to characterize the resulting threats. We construct the first labeled dataset of malicious agent skills by behaviorally verifying 98,380 skills from two community registries, confirming 157 malicious skills with 632 vulnerabilities. These attacks are not incidental. Malicious skills average 4.03 vulnerabilities across a median of three kill chain phases, and the ecosystem has split into two archetypes: Data Thieves that exfiltrate credentials through supply chain techniques, and Agent Hijackers that subvert agent decision-making through instruction manipulation. A single actor accounts for 54.1\% of confirmed cases through templated brand impersonation. Shadow features, capabilities absent from public documentation, appear in 0\% of basic attacks but 100\% of advanced ones; several skills go further by exploiting the AI platform's own hook system and permission flags. Responsible disclosure led to 93.6\% removal within 30 days. We release the dataset and analysis pipeline to support future work on agent skill security.

### 42. Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Amir H. Ashouri, Shayan Shirahmad Gale Bagi, Kavin Satheeskumar, Tejas Srikanth, Jonathan Zhao, Ibrahim Saidoun, Ziwen Wang, Bryan Chan, Tomasz S. Czajkowski
- **URL**: <http://arxiv.org/abs/2602.06142v1>
- **Submitted**: 2026-02-05 19:24:05
- **Comment**: Version 1- Submitted for a possible publication in 2026
- **Topic Keywords**: rag
- **Reason**: This paper is about compiler optimization and phase ordering, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.

### 43. SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Niccolo Avogaro, Nayanika Debnath, Li Mi, Thomas Frick, Junling Wang, Zexue He, Hang Hua, Konrad Schindler, Mattia Rigotti
- **URL**: <http://arxiv.org/abs/2602.06566v1>
- **Submitted**: 2026-02-06 10:05:25
- **Topic Keywords**: search
- **Reason**: This paper focuses on vision-language models and their scalability, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

---

