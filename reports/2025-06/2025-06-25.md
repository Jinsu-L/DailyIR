# Daily Papers Report - 2025-06-25

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and Ranking

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Shenbin Qian, Diptesh Kanojia, Samarth Agrawal, Hadeel Saadany, Swapnil Bhosale, Constantin Orasan, Zhe Wu
- **URL**: <http://arxiv.org/abs/2506.19743v1>
- **Submitted**: 2025-06-24 16:02:02
- **Comment**: This paper is accepted to the 2025 SIGIR Workshop on eCommerce
- **Topic Keywords**: information retrieval, queries, ranking, retrieval, commerce, e-commerce, rank, search
- **Reason**: The paper focuses on e-commerce information retrieval, specifically product retrieval and ranking, which aligns with your interest in IR and search technologies. The proposed approach, NEAR$^2$, addresses the challenge of efficient processing of vast product catalogs, which is relevant to your experience in the e-commerce domain. While the paper does not explicitly mention query understanding, ranking models, or user behavior modeling, the problem it tackles is closely related to these topics.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: E-commerce Information Retrieval (IR) Systems
- **Aim**: To balance accuracy with efficiency in e-commerce IR systems by reducing the embedding size of encoder-based Transformer models while maintaining performance
- **Rationale**: NEAR2 is based on Matryoshka Representation Learning (MRL), which develops representations with diverse capacities within the same higher-dimensional vector by explicitly optimizing sets of lower-dimensional vectors in a nested manner
- **Ground**: The authors validate NEAR2 using different loss functions for the retrieval and ranking task on four different test sets with various IR challenges
- **Experiment**: Ablative experiments on different encoder-based models fine-tuned using different IR loss functions demonstrate that NEAR2 is robust to different IR losses or loss combinations for continued fine-tuning
- **Takeaway**: NEAR2 achieves improved performance using a much smaller embedding dimension compared to existing models, with potential applications in real-world deployment

#### Abstract
> E-commerce information retrieval (IR) systems struggle to simultaneously
achieve high accuracy in interpreting complex user queries and maintain
efficient processing of vast product catalogs. The dual challenge lies in
precisely matching user intent with relevant products while managing the
computational demands of real-time search across massive inventories. In this
paper, we propose a Nested Embedding Approach to product Retrieval and Ranking,
called NEAR$^2$, which can achieve up to $12$ times efficiency in embedding
size at inference time while introducing no extra cost in training and
improving performance in accuracy for various encoder-based Transformer models.
We validate our approach using different loss functions for the retrieval and
ranking task, including multiple negative ranking loss and online contrastive
loss, on four different test sets with various IR challenges such as short and
implicit queries. Our approach achieves an improved performance over a smaller
embedding dimension, compared to any existing models.

---

### 2. heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Ashish Chouhan, Michael Gertz
- **URL**: <http://arxiv.org/abs/2506.19512v1>
- **Submitted**: 2025-06-24 11:03:01
- **Comment**: 12 pages, 2 figures, 6 tables, Workshop on BioNLP and Shared Tasks at
  ACL 2025
- **Topic Keywords**: query, rag, retrieval augmented generation, ctr, retrieval, rank
- **Reason**: The paper is somewhat related to information retrieval, specifically in the context of retrieval augmented generation and ranked list truncation. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The focus on clinical evidence and patient-specific questions is also outside the e-commerce domain, although it does involve natural language processing and data mining.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Clinical Response Generation System using Retrieval-Augmented Generation (RAG) Framework
- **Aim**: To produce accurate, neutral, concise, and relevant responses to patient-specific questions based on electronic health records (EHRs)
- **Rationale**: The proposed pipeline utilizes a RAG framework to solve the ArchEHR-QA task, which involves answering health-related questions from patients and providing attributions based on the patients' clinical notes
- **Ground**: The system is grounded in clinical note excerpts and uses a query-dependent-k retrieval strategy to retrieve semantically similar sentences from the excerpts
- **Experiment**: The authors experimented with different methods, including using different large language models (LLMs) for answer generation with attributions, a maximum number of 200 tokens generated by the LLM, and a one-shot prompting approach instead of zero-shot prompting
- **Takeaway**: The proposed pipeline achieves a strict F1-score of 0.37 and overall relevance of 0.35, and highlights the importance of finding the optimal settings for the pipeline's performance

#### Abstract
> This paper presents the approach of our team called heiDS for the ArchEHR-QA
2025 shared task. A pipeline using a retrieval augmented generation (RAG)
framework is designed to generate answers that are attributed to clinical
evidence from the electronic health records (EHRs) of patients in response to
patient-specific questions. We explored various components of a RAG framework,
focusing on ranked list truncation (RLT) retrieval strategies and attribution
approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a
query-dependent-k retrieval strategy, including the existing surprise and
autocut methods and two new methods proposed in this work, autocut* and elbow.
The experimental results show the benefits of our strategy in producing factual
and relevant answers when compared to a fixed-$k$.

---

### 3. NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Mike Zhang, Rob van der Goot
- **URL**: <http://arxiv.org/abs/2506.19058v1>
- **Submitted**: 2025-06-23 19:18:25
- **Comment**: TalentCLEF 2025
- **Topic Keywords**: ranking, relevance, rag, rank
- **Reason**: The paper focuses on job title and skill matching, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While it touches on NLP and data mining, the specific application and tasks are not closely aligned with the user's background in e-commerce and focus on deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multilingual Job Title Matching and Job Title-Based Skill Prediction
- **Aim**: To compare and evaluate the performance of different approaches for multilingual job title matching and job title-based skill prediction tasks
- **Rationale**: Standardizing diverse job titles across organizations and regions is a challenging task, and exploring various techniques can help improve the performance of job title matching and skill prediction
- **Ground**: ESCO job titles and skills, machine learning-assisted mapping of multilingual occupational data to ESCO, entity linking in the job market domain
- **Experiment**: Comparison of fine-tuned classification-based, fine-tuned contrastive-based, and prompting methods for Task A and Task B, evaluation of performance using mean average precision (MAP) metric
- **Takeaway**: Larger models perform better, model choice should be task-specific, and prompting approach performs best for Task A, while fine-tuned classification-based approach achieves better results for Task B

#### Abstract
> Matching job titles is a highly relevant task in the computational job market
domain, as it improves e.g., automatic candidate matching, career path
prediction, and job market analysis. Furthermore, aligning job titles to job
skills can be considered an extension to this task, with similar relevance for
the same downstream tasks. In this report, we outline NLPnorth's submission to
TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title
Matching, and Job Title-Based Skill Prediction. For both tasks we compare
(fine-tuned) classification-based, (fine-tuned) contrastive-based, and
prompting methods. We observe that for Task A, our prompting approach performs
best with an average of 0.492 mean average precision (MAP) on test data,
averaged over English, Spanish, and German. For Task B, we obtain an MAP of
0.290 on test data with our fine-tuned classification-based approach.
Additionally, we made use of extra data by pulling all the language-specific
titles and corresponding \emph{descriptions} from ESCO for each job and skill.
Overall, we find that the largest multilingual language models perform best for
both tasks. Per the provisional results and only counting the unique teams, the
ranking on Task A is 5$^{\text{th}}$/20 and for Task B 3$^{\text{rd}}$/14.

---

### 4. Alleviating User-Sensitive bias with Fair Generative Sequential Recommendation Model

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yang Liu, Feng Wu, Xuefang Zhu
- **URL**: <http://arxiv.org/abs/2506.19777v1>
- **Submitted**: 2025-06-24 16:42:46
- **Topic Keywords**: user behavior, recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it primarily deals with fairness and bias issues, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's abstract does not mention information retrieval, search technologies, or deep semantic understanding, which are key aspects of the user's research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Fairness in Sequential Recommendation Systems
- **Aim**: To alleviate user-sensitive bias in sequential recommendation systems
- **Rationale**: Traditional recommendation models can capture strong correlations between sensitive features and user behavior, leading to unfairness
- **Ground**: FairGENRec model, which uses a diffusion model to model fairness and enhance diversity
- **Experiment**: Evaluation on three real-world datasets (Amazon review, Movielens, and Yelp) using top-K Hit Radio (HR@K) and top-K Normalized Discounted Cumulative Gain (NDCG@K) as evaluation metrics
- **Takeaway**: FairGENRec achieves a dual enhancement effect on accuracy and fairness, outperforming seven baseline models, and is effective in promoting recommendation fairness and accuracy

#### Abstract
> Recommendation fairness has recently attracted much attention. In the real
world, recommendation systems are driven by user behavior, and since users with
the same sensitive feature (e.g., gender and age) tend to have the same
patterns, recommendation models can easily capture the strong correlation
preference of sensitive features and thus cause recommendation unfairness.
Diffusion model (DM) as a new generative model paradigm has achieved great
success in recommendation systems. DM's ability to model uncertainty and
represent diversity, and its modeling mechanism has a high degree of
adaptability with the real-world recommendation process with bias. Therefore,
we use DM to effectively model the fairness of recommendation and enhance the
diversity. This paper proposes a FairGENerative sequential Recommendation model
based on DM, FairGENRec. In the training phase, we inject random noise into the
original distribution under the guidance of the sensitive feature recognition
model, and a sequential denoise model is designed for the reverse
reconstruction of items. Simultaneously, recommendation fairness modeling is
completed by injecting multi-interests representational information that
eliminates the bias of sensitive user features into the generated results. In
the inference phase, the model obtains the noise in the form of noise addition
by using the history interactions which is followed by reverse iteration to
reconstruct the target item representation. Finally, our extensive experiments
on three datasets demonstrate the dual enhancement effect of FairGENRec on
accuracy and fairness, while the statistical analysis of the cases visualizes
the degree of improvement on the fairness of the recommendation.

---

### 5. Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Juraj Vladika, Ihsan Soydemir, Florian Matthes
- **URL**: <http://arxiv.org/abs/2506.19607v1>
- **Submitted**: 2025-06-24 13:20:31
- **Comment**: Accepted to FEVER @ ACL 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the issue of hallucinations in language models, specifically in the context of news summarization. While it touches on the topic of search engines and external knowledge, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Correcting Hallucinations in News Summaries Generated by Large Language Models
- **Aim**: To investigate self-correcting methods for correcting hallucinations in news summaries generated by Large Language Models
- **Rationale**: Hallucinations in news summaries generated by Large Language Models can lead to factually inaccurate statements, and self-correcting methods can help refine the original response with new corrections
- **Ground**: The study explores the application of CoVE and RARR to correct hallucinated news summaries using evidence from three search engines: Google, Bing, and DuckDuckGo
- **Experiment**: The evaluation methods used include string dissimilarity, semantic similarity, Natural Language Inference (NLI) Score, G-Eval, and human evaluation, and the results show that CoVE and RARR systems are effective in refining responses to user prompts
- **Takeaway**: The study highlights the importance of using reliable evaluation tools and the need for further improvement in NLI metrics, and suggests that few-shot prompts are better for preserving faithfulness to the original draft, while zero-shot prompts are better for adding additional context and making bold edits

#### Abstract
> While large language models (LLMs) have shown remarkable capabilities to
generate coherent text, they suffer from the issue of hallucinations --
factually inaccurate statements. Among numerous approaches to tackle
hallucinations, especially promising are the self-correcting methods. They
leverage the multi-turn nature of LLMs to iteratively generate verification
questions inquiring additional evidence, answer them with internal or external
knowledge, and use that to refine the original response with the new
corrections. These methods have been explored for encyclopedic generation, but
less so for domains like news summarization. In this work, we investigate two
state-of-the-art self-correcting systems by applying them to correct
hallucinated summaries using evidence from three search engines. We analyze the
results and provide insights into systems' performance, revealing interesting
practical findings on the benefits of search engine snippets and few-shot
prompts, as well as high alignment of G-Eval and human evaluation.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. RCStat: A Statistical Framework for using Relative Contextualization in Transformers

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra
- **URL**: <http://arxiv.org/abs/2506.19549v1>
- **Submitted**: 2025-06-24 11:55:43
- **Topic Keywords**: query
- **Reason**: The paper introduces a statistical framework for using relative contextualization in transformers, which is a topic in Natural Language Processing (NLP). While it's not directly related to Information Retrieval (IR) or Search technologies, it may have implications for query understanding and ranking models. However, the focus on transformers and attention logits is not directly aligned with the user's interests in IR and search technologies.

#### Abstract
> Prior work on input-token importance in auto-regressive transformers has
relied on Softmax-normalized attention weights, which obscure the richer
structure of pre-Softmax query-key logits. We introduce RCStat, a statistical
framework that harnesses raw attention logits via Relative Contextualization
(RC), a random variable measuring contextual alignment between token segments,
and derive an efficient upper bound for RC. We demonstrate two applications:
(i) Key-Value compression, where RC-based thresholds drive adaptive key-value
eviction for substantial cache reduction with minimal quality loss; and (ii)
Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level
explanations than post-Softmax methods. Across question answering,
summarization, and attribution benchmarks, RCStat achieves significant
empirical gains, delivering state-of-the-art compression and attribution
performance without any model retraining.

### 7. Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yingji Zhang, Marco Valentino, Danilo S. Carvalho, Andr√© Freitas
- **URL**: <http://arxiv.org/abs/2506.19418v1>
- **Submitted**: 2025-06-24 08:38:03
- **Topic Keywords**: query
- **Reason**: The paper explores the idea of incorporating explicit reasoning rules within language models, which is related to query understanding and ranking models. However, the focus is more on the theoretical framework and architecture rather than practical applications in search technologies or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Incorporating explicit reasoning rules within the latent space of language
models (LMs) offers a promising pathway to enhance generalisation,
interpretability, and controllability. While current Transformer-based language
models have shown strong performance on Natural Language Inference (NLI) tasks,
they often rely on memorisation rather than rule-based inference. This work
investigates how reasoning rules can be explicitly embedded and memorised
within the LMs through Language Variational Autoencoders (VAEs). We propose a
complete pipeline for learning reasoning rules within Transformer-based
language VAEs. This pipeline encompasses three rule-based reasoning tasks, a
supporting theoretical framework, and a practical end-to-end architecture. The
experiment illustrates the following findings: Disentangled reasoning: Under
explicit signal supervision, reasoning rules - viewed as functional mappings -
can be disentangled within the encoder's parametric space. This separation
results in distinct clustering of rules in the output feature space. Prior
knowledge injection: injecting reasoning information into the Query enables the
model to more effectively retrieve the stored value Value from memory based on
Key. This approach offers a simple method for integrating prior knowledge into
decoder-only language models. Performance bottleneck: In mathematical reasoning
tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance
beyond a point. Moreover, ffn layers are better than attention layers at
preserving the separation of reasoning rules in the model's parameters.

### 8. Measuring and Guiding Monosemanticity

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ruben H√§rle, Felix Friedrich, Manuel Brack, Stephan W√§ldchen, Bj√∂rn Deiseroth, Patrick Schramowski, Kristian Kersting
- **URL**: <http://arxiv.org/abs/2506.19382v1>
- **Submitted**: 2025-06-24 07:18:20
- **Topic Keywords**: rag
- **Reason**: The paper focuses on feature extraction and representation learning in large language models, which is related to information retrieval and NLP. However, the specific problem of monosemanticity and the proposed solutions are not directly applicable to query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> There is growing interest in leveraging mechanistic interpretability and
controllability to better understand and influence the internal dynamics of
large language models (LLMs). However, current methods face fundamental
challenges in reliably localizing and manipulating feature representations.
Sparse Autoencoders (SAEs) have recently emerged as a promising direction for
feature extraction at scale, yet they, too, are limited by incomplete feature
isolation and unreliable monosemanticity. To systematically quantify these
limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric
to quantify feature monosemanticity in latent representation. Building on these
insights, we propose Guided Sparse Autoencoders (G-SAE), a method that
conditions latent representations on labeled concepts during training. We
demonstrate that reliable localization and disentanglement of target concepts
within the latent space improve interpretability, detection of behavior, and
control. Specifically, our evaluations on toxicity detection, writing style
identification, and privacy attribute recognition show that G-SAE not only
enhances monosemanticity but also enables more effective and fine-grained
steering with less quality degradation. Our findings provide actionable
guidelines for measuring and advancing mechanistic interpretability and control
of LLMs.

### 9. Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy
- **URL**: <http://arxiv.org/abs/2506.19028v2>
- **Submitted**: 2025-06-23 18:31:22
- **Comment**: 29 pages, 9 figures, 15 tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on evaluating fairness in Large Language Models (LLMs), which is a relevant topic in Natural Language Processing (NLP). However, the specific approach and metrics used (FiSCo) are not directly related to my interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. The paper's emphasis on semantic analysis and statistical testing is interesting, but it does not align with my primary research focus.

#### Abstract
> Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.

### 10. In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Puneesh Deora, Bhavya Vasudeva, Tina Behnia, Christos Thrampoulidis
- **URL**: <http://arxiv.org/abs/2506.19351v1>
- **Submitted**: 2025-06-24 06:33:00
- **Comment**: 28 pages, 19 figures
- **Topic Keywords**: search
- **Reason**: The paper explores the behavior of transformers in adapting to new tasks through in-context learning, which is related to query understanding and ranking models. However, the focus on hierarchical task structures and Occam's razor-like inductive bias is not directly aligned with the user's primary interests in information retrieval, search technologies, and user behavior modeling.

#### Abstract
> In-context learning (ICL) enables transformers to adapt to new tasks through
contextual examples without parameter updates. While existing research has
typically studied ICL in fixed-complexity environments, practical language
models encounter tasks spanning diverse complexity levels. This paper
investigates how transformers navigate hierarchical task structures where
higher-complexity categories can perfectly represent any pattern generated by
simpler ones. We design well-controlled testbeds based on Markov chains and
linear regression that reveal transformers not only identify the appropriate
complexity level for each task but also accurately infer the corresponding
parameters--even when the in-context examples are compatible with multiple
complexity hypotheses. Notably, when presented with data generated by simpler
processes, transformers consistently favor the least complex sufficient
explanation. We theoretically explain this behavior through a Bayesian
framework, demonstrating that transformers effectively implement an in-context
Bayesian Occam's razor by balancing model fit against complexity penalties. We
further ablate on the roles of model size, training mixture distribution,
inference context length, and architecture. Finally, we validate this Occam's
razor-like inductive bias on a pretrained GPT-4 model with Boolean-function
tasks as case study, suggesting it may be inherent to transformers trained on
diverse task distributions.

### 11. What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Yuchang Zhu, Huazhen Zhong, Qunshu Lin, Haotong Wei, Xiaolong Sun, Zixuan Yu, Minghao Liu, Zibin Zheng, Liang Chen
- **URL**: <http://arxiv.org/abs/2506.19262v2>
- **Submitted**: 2025-06-24 02:44:58
- **Comment**: Ongoing work
- **Topic Keywords**: search
- **Reason**: The paper explores the implications of using large language model-generated data for training downstream models, focusing on the importance of data diversity. While it touches on the topic of model performance, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper's focus on language models and data quality is somewhat relevant, but not directly aligned with the user's primary research themes.

#### Abstract
> With the remarkable generative capabilities of large language models (LLMs),
using LLM-generated data to train downstream models has emerged as a promising
approach to mitigate data scarcity in specific domains and reduce
time-consuming annotations. However, recent studies have highlighted a critical
issue: iterative training on self-generated data results in model collapse,
where model performance degrades over time. Despite extensive research on the
implications of LLM-generated data, these works often neglect the importance of
data diversity, a key factor in data quality. In this work, we aim to
understand the implications of the diversity of LLM-generated data on
downstream model performance. Specifically, we explore how varying levels of
diversity in LLM-generated data affect downstream model performance.
Additionally, we investigate the performance of models trained on data that
mixes different proportions of LLM-generated data, which we refer to as
synthetic data. Our experimental results show that, with minimal distribution
shift, moderately diverse LLM-generated data can enhance model performance in
scenarios with insufficient labeled data, whereas highly diverse generated data
has a negative impact. We hope our empirical findings will offer valuable
guidance for future studies on LLMs as data generators.

### 12. Thought Anchors: Which LLM Reasoning Steps Matter?

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Paul C. Bogdan, Uzay Macar, Neel Nanda, Arthur Conmy
- **URL**: <http://arxiv.org/abs/2506.19143v2>
- **Submitted**: 2025-06-23 21:28:45
- **Comment**: Paul C. Bogdan and Uzay Macar contributed equally to this work, and
  their listed order was determined by coinflip. Neel Nanda and Arthur Conmy
  contributed equally to this work as senior authors, and their listed order
  was determined by coinflip
- **Topic Keywords**: www
- **Reason**: The paper discusses large language models' reasoning processes, introducing three attribution methods to analyze sentence-level importance. While it touches on model interpretability, it doesn't directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Reasoning large language models have recently achieved state-of-the-art
performance in many fields. However, their long-form chain-of-thought reasoning
creates interpretability challenges as each generated token depends on all
previous ones, making the computation harder to decompose. We argue that
analyzing reasoning traces at the sentence level is a promising approach to
understanding reasoning processes. We present three complementary attribution
methods: (1) a black-box method measuring each sentence's counterfactual
importance by comparing final answers across 100 rollouts conditioned on the
model generating that sentence or one with a different meaning; (2) a white-box
method of aggregating attention patterns between pairs of sentences, which
identified "broadcasting" sentences that receive disproportionate attention
from all future sentences via "receiver" attention heads; (3) a causal
attribution method measuring logical connections between sentences by
suppressing attention toward one sentence and measuring the effect on each
future sentence's tokens. Each method provides evidence for the existence of
thought anchors, reasoning steps that have outsized importance and that
disproportionately influence the subsequent reasoning process. These thought
anchors are typically planning or backtracking sentences. We provide an
open-source tool (www.thought-anchors.com) for visualizing the outputs of our
methods, and present a case study showing converging patterns across methods
that map how a model performs multi-step reasoning. The consistency across
methods demonstrates the potential of sentence-level analysis for a deeper
understanding of reasoning models.

### 13. Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Russell Beale
- **URL**: <http://arxiv.org/abs/2506.19484v1>
- **Submitted**: 2025-06-24 10:19:09
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on Large Language Models in education, conversational AI, and pedagogy, which is a distinct area of study. While it mentions retrieval-augmented generation (RAG), it is not directly related to your core research themes.

#### Abstract
> Large Language Models (LLMs) are rapidly transforming education by enabling
rich conversational learning experiences. This article provides a comprehensive
review of how LLM-based conversational agents are being used in higher
education, with extensions to secondary and lifelong learning contexts. We
synthesize existing literature on LLMs in education and theories of
conversational and dialogic pedagogy - including Vygotsky's sociocultural
learning (scaffolding and the Zone of Proximal Development), the Socratic
method, and Laurillard's conversational framework - and examine how prompting
strategies and retrieval-augmented generation (RAG) can align LLM behaviors
with these pedagogical theories, and how it can support personalized, adaptive
learning. We map educational theories to LLM capabilities, highlighting where
LLM-driven dialogue supports established learning principles and where it
challenges or falls short of traditional pedagogical assumptions. Notable gaps
in applying prior theories to LLMs are identified, such as the models tendency
to provide direct answers instead of fostering co-construction of knowledge,
and the need to account for the constant availability and broad but non-human
expertise of LLM tutors. In response, we propose practical strategies to better
align LLM interactions with sound pedagogy - for example, designing prompts
that encourage Socratic questioning, scaffolded guidance, and student
reflection, as well as integrating retrieval mechanisms to ensure accuracy and
contextual relevance. Our aim is to bridge the gap between educational theory
and the emerging practice of AI-driven conversational learning, offering
insights and tools for making LLM-based dialogues more educationally productive
and theory-aligned.

### 14. Higher-Order Graph Databases

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Maciej Besta, Shriram Chandran, Jakub Cudak, Patrick Iff, Marcin Copik, Robert Gerstenberger, Tomasz Szydlo, J√ºrgen M√ºller, Torsten Hoefler
- **URL**: <http://arxiv.org/abs/2506.19661v1>
- **Submitted**: 2025-06-24 14:24:20
- **Topic Keywords**: query, queries
- **Reason**: The paper focuses on graph databases and higher-order interactions, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions graph neural networks, the context is different from the user's interests in IR and NLP.

#### Abstract
> Recent advances in graph databases (GDBs) have been driving interest in
large-scale analytics, yet current systems fail to support higher-order (HO)
interactions beyond first-order (one-hop) relations, which are crucial for
tasks such as subgraph counting, polyadic modeling, and HO graph learning. We
address this by introducing a new class of systems, higher-order graph
databases (HO-GDBs) that use lifting and lowering paradigms to seamlessly
extend traditional GDBs with HO. We provide a theoretical analysis of OLTP and
OLAP queries, ensuring correctness, scalability, and ACID compliance. We
implement a lightweight, modular, and parallelizable HO-GDB prototype that
offers native support for hypergraphs, node-tuples, subgraphs, and other HO
structures under a unified API. The prototype scales to large HO OLTP & OLAP
workloads and shows how HO improves analytical tasks, for example enhancing
accuracy of graph neural networks within a GDB by 44%. Our work ensures low
latency and high query throughput, and generalizes both ACID-compliant and
eventually consistent systems.

### 15. Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li
- **URL**: <http://arxiv.org/abs/2506.19433v1>
- **Submitted**: 2025-06-24 09:00:43
- **Topic Keywords**: ctr, retrieval, acl
- **Reason**: The paper focuses on Vision-and-Language Navigation in urban environments, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on spatial cognition, long-short memory systems, and multimodal embeddings is also outside the user's areas of expertise.

#### Abstract
> Vision-and-Language Navigation (VLN) in large-scale urban environments
requires embodied agents to ground linguistic instructions in complex scenes
and recall relevant experiences over extended time horizons. Prior modular
pipelines offer interpretability but lack unified memory, while end-to-end
(M)LLM agents excel at fusing vision and language yet remain constrained by
fixed context windows and implicit spatial reasoning. We introduce
\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system
that can augment any VLN backbone. Mem4Nav fuses a sparse octree for
fine-grained voxel indexing with a semantic topology graph for high-level
landmark connectivity, storing both in trainable memory tokens embedded via a
reversible Transformer. Long-term memory (LTM) compresses and retains
historical observations at both octree and graph nodes, while short-term memory
(STM) caches recent multimodal entries in relative coordinates for real-time
obstacle avoidance and local planning. At each step, STM retrieval sharply
prunes dynamic context, and, when deeper history is needed, LTM tokens are
decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and
Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based
LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13
pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW
improvement. Ablations confirm the indispensability of both the hierarchical
map and dual memory modules. Our codes are open-sourced via
https://github.com/tsinghua-fib-lab/Mem4Nav.

### 16. Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Lorenzo Proietti, Stefano Perrella, Roberto Navigli
- **URL**: <http://arxiv.org/abs/2506.19571v1>
- **Submitted**: 2025-06-24 12:35:00
- **Comment**: Accepted at ACL 2025 Main Conference. 24 pages
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on the concept of evaluation, it is focused on Machine Translation and does not explore topics relevant to your primary research interests.

#### Abstract
> In Machine Translation (MT) evaluation, metric performance is assessed based
on agreement with human judgments. In recent years, automatic metrics have
demonstrated increasingly high levels of agreement with humans. To gain a
clearer understanding of metric performance and establish an upper bound, we
incorporate human baselines in the MT meta-evaluation, that is, the assessment
of MT metrics' capabilities. Our results show that human annotators are not
consistently superior to automatic metrics, with state-of-the-art metrics often
ranking on par with or higher than human baselines. Despite these findings
suggesting human parity, we discuss several reasons for caution. Finally, we
explore the broader implications of our results for the research field, asking:
Can we still reliably measure improvements in MT evaluation? With this work, we
aim to shed light on the limits of our ability to measure progress in the
field, fostering discussion on an issue that we believe is crucial to the
entire MT evaluation community.

### 17. AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu
- **URL**: <http://arxiv.org/abs/2506.19505v1>
- **Submitted**: 2025-06-24 10:45:48
- **Topic Keywords**: ltr, rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on quantization techniques for Large Language Models, which is a topic in Natural Language Processing, but it does not align with your primary focus on information retrieval and deep semantic understanding.

#### Abstract
> Quantization has emerged as an effective and lightweight solution to reduce
the memory footprint of the KV cache in Large Language Models (LLMs).
Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV
cache quantization remains a significant challenge. We observe that quantizing
the KV cache of different tokens has varying impacts on the quality of
attention outputs. To systematically investigate this phenomenon, we perform
forward error propagation analysis on attention and propose the Anchor Score
(AnS) that quantifies the sensitivity of each token's KV cache to
quantization-induced error. Our analysis reveals significant disparities in AnS
across tokens, suggesting that preserving a small subset with full precision
(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive
quantization scenarios. Based on this insight, we introduce AnTKV, a novel
framework that leverages Anchor Token-aware Vector Quantization to compress the
KV cache. Furthermore, to support efficient deployment, we design and develop a
triton kernel that is fully compatible with FlashAttention, enabling fast
online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context
lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x
higher decoding throughput compared to the FP16 baseline. Our experiment
results demonstrate that AnTKV matches or outperforms prior works such as KIVI,
SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves
significantly lower perplexity under ultra-low-bit quantization on Mistral-7B,
with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of
4.73.

### 18. Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Craig Steven Wright
- **URL**: <http://arxiv.org/abs/2506.19191v1>
- **Submitted**: 2025-06-23 23:27:44
- **Comment**: 83 pages, 14 sections, 92 formal results, no prior conference
  publication
- **Topic Keywords**: pairwise, acl
- **Reason**: The paper's focus on Bayesian inference, population dynamics, and truth-based competition is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The concepts of query understanding, ranking models, and user behavior modeling are not addressed in this paper.

#### Abstract
> We introduce a mathematically rigorous framework for an artificial
intelligence system composed of probabilistic agents evolving through
structured competition and belief revision. The architecture, grounded in
Bayesian inference, measure theory, and population dynamics, defines agent
fitness as a function of alignment with a fixed external oracle representing
ground truth. Agents compete in a discrete-time environment, adjusting
posterior beliefs through observed outcomes, with higher-rated agents
reproducing and lower-rated agents undergoing extinction. Ratings are updated
via pairwise truth-aligned utility comparisons, and belief updates preserve
measurable consistency and stochastic convergence. We introduce hash-based
cryptographic identity commitments to ensure traceability, alongside causal
inference operators using do-calculus. Formal theorems on convergence,
robustness, and evolutionary stability are provided. The system establishes
truth as an evolutionary attractor, demonstrating that verifiable knowledge
arises from adversarial epistemic pressure within a computable, self-regulating
swarm.

### 19. Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Omer Luxembourg, Haim Permuter, Eliya Nachmani
- **URL**: <http://arxiv.org/abs/2506.19037v1>
- **Submitted**: 2025-06-23 18:49:23
- **Topic Keywords**: pairwise, rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on masked diffusion language models and text generation, which is outside your primary focus area. While it mentions some concepts related to NLP, it does not align with your specific interests in IR and related topics.

#### Abstract
> Masked diffusion language models (MDLM) have shown strong promise for
non-autoregressive text generation, yet existing samplers act as implicit
planners, selecting tokens to unmask via denoiser confidence or entropy scores.
Such heuristics falter under parallel unmasking - they ignore pairwise
interactions between tokens and cannot account for dependencies when unmasking
multiple positions at once, limiting their inference time to traditional
auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking
Strategy (DUS), an inference-only, planner-model-free method that requires no
additional training. DUS leverages a first-order Markov assumption to partition
sequence positions into dilation-based groups of non-adjacent tokens, enabling
independent, parallel unmasking steps that respect local context that minimizes
the joint entropy of each iteration step. Unlike semi-AR block approaches
(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces
the number of denoiser calls to O(log B) per generation block - yielding
substantial speedup over the O(B) run time of state-of-the-art diffusion
models, where B is the block size in the semi-AR inference process. In
experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -
domains suited to non-ordinal generation - DUS improves scores over parallel
confidence-based planner, without modifying the underlying denoiser. DUS offers
a lightweight, budget-aware approach to efficient, high-quality text
generation, paving the way to unlock the true capabilities of MDLMs.

### 20. LLM-Based Social Simulations Require a Boundary

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zengqing Wu, Run Peng, Takayuki Ito, Chuan Xiao
- **URL**: <http://arxiv.org/abs/2506.19806v1>
- **Submitted**: 2025-06-24 17:14:47
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on the topic of modeling human-like agents, it is primarily focused on the limitations and boundaries of large language model-based social simulations, which is outside the scope of the user's research interests.

#### Abstract
> This position paper argues that large language model (LLM)-based social
simulations should establish clear boundaries to meaningfully contribute to
social science research. While LLMs offer promising capabilities for modeling
human-like agents compared to traditional agent-based modeling, they face
fundamental limitations that constrain their reliability for social pattern
discovery. The core issue lies in LLMs' tendency towards an ``average persona''
that lacks sufficient behavioral heterogeneity, a critical requirement for
simulating complex social dynamics. We examine three key boundary problems:
alignment (simulated behaviors matching real-world patterns), consistency
(maintaining coherent agent behavior over time), and robustness
(reproducibility under varying conditions). We propose heuristic boundaries for
determining when LLM-based simulations can reliably advance social science
understanding. We believe that these simulations are more valuable when
focusing on (1) collective patterns rather than individual trajectories, (2)
agent behaviors aligning with real population averages despite limited
variance, and (3) proper validation methods available for testing simulation
robustness. We provide a practical checklist to guide researchers in
determining the appropriate scope and claims for LLM-based social simulations.

### 21. KnowML: Improving Generalization of ML-NIDS with Attack Knowledge Graphs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xin Fan Guo, Albert Merono Penuela, Sergio Maffeis, Fabio Pierazzi
- **URL**: <http://arxiv.org/abs/2506.19802v1>
- **Submitted**: 2025-06-24 17:08:58
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on Machine Learning-based Network Intrusion Detection Systems, which is not directly related to Information Retrieval or Search technologies. The concepts and techniques discussed, such as knowledge graphs and symbolic reasoning, are not relevant to query understanding, ranking models, or user behavior modeling.

#### Abstract
> Despite extensive research on Machine Learning-based Network Intrusion
Detection Systems (ML-NIDS), their capability to detect diverse attack variants
remains uncertain. Prior studies have largely relied on homogeneous datasets,
which artificially inflate performance scores and offer a false sense of
security. Designing systems that can effectively detect a wide range of attack
variants remains a significant challenge. The progress of ML-NIDS continues to
depend heavily on human expertise, which can embed subjective judgments of
system designers into the model, potentially hindering its ability to
generalize across diverse attack types.
  To address this gap, we propose KnowML, a framework for knowledge-guided
machine learning that integrates attack knowledge into ML-NIDS. KnowML
systematically explores the threat landscape by leveraging Large Language
Models (LLMs) to perform automated analysis of attack implementations. It
constructs a unified Knowledge Graph (KG) of attack strategies, on which it
applies symbolic reasoning to generate KG-Augmented Input, embedding domain
knowledge directly into the design process of ML-NIDS.
  We evaluate KnowML on 28 realistic attack variants, of which 10 are newly
collected for this study. Our findings reveal that baseline ML-NIDS models fail
to detect several variants entirely, achieving F1 scores as low as 0 %. In
contrast, our knowledge-guided approach achieves up to 99 % F1 score while
maintaining a False Positive Rate below 0.1 %.

### 22. NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yan Jiang, Hao Zhou, LiZhong GU, Ai Han, TianLong Li
- **URL**: <http://arxiv.org/abs/2506.19500v1>
- **Submitted**: 2025-06-24 10:39:07
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to Information Retrieval (IR) or Search technologies, and does not address query understanding, ranking models, or user behavior modeling. While it involves graph-based planning and optimization, the focus is on toolchain orchestration and function calling, which is outside the scope of the user's research interests.

#### Abstract
> LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.

### 23. ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, Jiaqi Wang, Feng Wu, Dahua Lin
- **URL**: <http://arxiv.org/abs/2506.19848v1>
- **Submitted**: 2025-06-24 17:59:55
- **Comment**: Code is available at https://github.com/Cooperx521/ScaleCap
- **Topic Keywords**: rag
- **Reason**: The paper focuses on image captioning, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions multimodal bias and linguistic bias, these concepts are not directly applicable to the user's research interests in IR and NLP.

#### Abstract
> This paper presents ScaleCap, an inference-time scalable image captioning
strategy that generates comprehensive and detailed image captions. The key
challenges of high-quality image captioning lie in the inherent biases of
LVLMs: multimodal bias resulting in imbalanced descriptive granularity,
offering detailed accounts of some elements while merely skimming over others;
linguistic bias leading to hallucinated descriptions of non-existent objects.
To address these issues, we propose a scalable debiased captioning strategy,
which continuously enriches and calibrates the caption with increased inference
budget. Specifically, we propose two novel components: heuristic question
answering and contrastive sentence rating. The former generates
content-specific questions based on the image and answers them to progressively
inject relevant information into the caption. The latter employs sentence-level
offline contrastive decoding to effectively identify and eliminate
hallucinations caused by linguistic biases. With increased inference cost, more
heuristic questions are raised by ScaleCap to progressively capture additional
visual details, generating captions that are more accurate, balanced, and
informative. Extensive modality alignment experiments demonstrate the
effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them
for LVLM pretraining leads to consistent performance gains across 11 widely
used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity
of generated captions with two additional tasks: replacing images with captions
in VQA task, and reconstructing images from captions to assess semantic
coverage. Code is available at https://github.com/Cooperx521/ScaleCap.

### 24. MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yucheng Zhou, Lingran Song, Jianbing Shen
- **URL**: <http://arxiv.org/abs/2506.19835v1>
- **Submitted**: 2025-06-24 17:52:43
- **Comment**: ACL 2025 Findings
- **Topic Keywords**: rag
- **Reason**: The paper focuses on medical diagnosis using Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions multimodal data, the primary application is in medical diagnosis, which is not a core area of interest for you.

#### Abstract
> Recent advancements in medical Large Language Models (LLMs) have showcased
their powerful reasoning and diagnostic capabilities. Despite their success,
current unified multimodal medical LLMs face limitations in knowledge update
costs, comprehensiveness, and flexibility. To address these challenges, we
introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis
(MAM). Inspired by our empirical findings highlighting the benefits of role
assignment and diagnostic discernment in LLMs, MAM decomposes the medical
diagnostic process into specialized roles: a General Practitioner, Specialist
Team, Radiologist, Medical Assistant, and Director, each embodied by an
LLM-based agent. This modular and collaborative framework enables efficient
knowledge updates and leverages existing medical LLMs and knowledge bases.
Extensive experimental evaluations conducted on a wide range of publicly
accessible multimodal medical datasets, incorporating text, image, audio, and
video modalities, demonstrate that MAM consistently surpasses the performance
of modality-specific LLMs. Notably, MAM achieves significant performance
improvements ranging from 18% to 365% compared to baseline models. Our code is
released at https://github.com/yczhou001/MAM.

### 25. Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang
- **URL**: <http://arxiv.org/abs/2506.19794v1>
- **Submitted**: 2025-06-24 17:04:23
- **Comment**: Work in progress
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large Language Models (LLMs) and data analysis, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. Although the paper mentions 'strategies to enhance the data analysis capabilities', it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> Large Language Models (LLMs) hold promise in automating data analysis tasks,
yet open-source models face significant limitations in these kinds of
reasoning-intensive scenarios. In this work, we investigate strategies to
enhance the data analysis capabilities of open-source LLMs. By curating a seed
dataset of diverse, realistic scenarios, we evaluate models across three
dimensions: data understanding, code generation, and strategic planning. Our
analysis reveals three key findings: (1) Strategic planning quality serves as
the primary determinant of model performance; (2) Interaction design and task
complexity significantly influence reasoning capabilities; (3) Data quality
demonstrates a greater impact than diversity in achieving optimal performance.
We leverage these insights to develop a data synthesis methodology,
demonstrating significant improvements in open-source LLMs' analytical
reasoning capabilities.

### 26. SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao
- **URL**: <http://arxiv.org/abs/2506.19767v1>
- **Submitted**: 2025-06-24 16:31:37
- **Topic Keywords**: rag
- **Reason**: This paper focuses on large language models and their fine-tuning methods, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on reinforcement learning, which is a related topic, the paper's primary focus is on reasoning tasks, which is not a central match for the user's research interests.

#### Abstract
> Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.

### 27. Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang
- **URL**: <http://arxiv.org/abs/2506.19697v1>
- **Submitted**: 2025-06-24 15:03:57
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on Large Language Models, quantization, and outlier mitigation, which are not directly related to your areas of interest.

#### Abstract
> Extreme activation outliers in Large Language Models (LLMs) critically
degrade quantization performance, hindering efficient on-device deployment.
While channel-wise operations and adaptive gradient scaling are recognized
causes, practical mitigation remains challenging. We introduce Outlier-Safe
Pre-Training (OSP), a practical guideline that proactively prevents outlier
formation rather than relying on post-hoc mitigation. OSP combines three key
innovations: (1) the Muon optimizer, eliminating privileged bases while
maintaining training efficiency; (2) Single-Scale RMSNorm, preventing
channel-wise amplification; and (3) a learnable embedding projection,
redistributing activation magnitudes originating from embedding matrices. We
validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is
the first production-scale LLM trained without such outliers. Under aggressive
4-bit quantization, our OSP model achieves a 35.7 average score across 10
benchmarks (compared to 26.5 for an Adam-trained model), with only a 2%
training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis
(0.04) compared to extreme values (1818.56) in standard models, fundamentally
altering LLM quantization behavior. Our work demonstrates that outliers are not
inherent to LLMs but are consequences of training strategies, paving the way
for more efficient LLM deployment. The source code and pretrained checkpoints
are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.

### 28. Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuanhe Tian, Lei Mao, Yan Song
- **URL**: <http://arxiv.org/abs/2506.19665v1>
- **Submitted**: 2025-06-24 14:29:06
- **Comment**: 7 pages, 3 figures
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on medical image report generation using computer vision and natural language processing techniques, which is a different domain and does not align with your core research themes.

#### Abstract
> Generating reports for computed tomography (CT) images is a challenging task,
while similar to existing studies for medical image report generation, yet has
its unique characteristics, such as spatial encoding of multiple images,
alignment between image volume and texts, etc. Existing solutions typically use
general 2D or 3D image processing techniques to extract features from a CT
volume, where they firstly compress the volume and then divide the compressed
CT slices into patches for visual encoding. These approaches do not explicitly
account for the transformations among CT slices, nor do they effectively
integrate multi-level image features, particularly those containing specific
organ lesions, to instruct CT report generation (CTRG). In considering the
strong correlation among consecutive slices in CT scans, in this paper, we
propose a large language model (LLM) based CTRG method with recurrent visual
feature extraction and stereo attentions for hierarchical feature modeling.
Specifically, we use a vision Transformer to recurrently process each slice in
a CT volume, and employ a set of attentions over the encoded slices from
different perspectives to selectively obtain important visual information and
align them with textual features, so as to better instruct an LLM for CTRG.
Experiment results and further analysis on the benchmark M3D-Cap dataset show
that our method outperforms strong baseline models and achieves
state-of-the-art results, demonstrating its validity and effectiveness.

### 29. Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lucie Galland, Catherine Pelachaud, Florian Pecune
- **URL**: <http://arxiv.org/abs/2506.19652v1>
- **Submitted**: 2025-06-24 14:15:26
- **Topic Keywords**: rag
- **Reason**: The paper focuses on dialogue management and reinforcement learning, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the application is in dialogue systems, which is a distinct area from search and retrieval.

#### Abstract
> In this work, we propose a novel framework that integrates large language
models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a
specific goal. By leveraging hierarchical reinforcement learning to model the
structured phases of dialogue and employ meta-learning to enhance adaptability
across diverse user profiles, our approach enhances adaptability and
efficiency, enabling the system to learn from limited data, transition fluidly
between dialogue phases, and personalize responses to heterogeneous patient
needs. We apply our framework to Motivational Interviews, aiming to foster
behavior change, and demonstrate that the proposed dialogue manager outperforms
a state-of-the-art LLM baseline in terms of reward, showing a potential benefit
of conditioning LLMs to create open-ended dialogue systems with specific goals.

### 30. MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wenhan Han, Yifan Zhang, Zhixun Chen, Binbin Liu, Haobin Lin, Bingni Zhang, Taifeng Wang, Mykola Pechenizkiy, Meng Fang, Yin Zheng
- **URL**: <http://arxiv.org/abs/2506.19468v1>
- **Submitted**: 2025-06-24 09:53:00
- **Topic Keywords**: rag
- **Reason**: The paper focuses on evaluating the multilingual capabilities of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language models, the primary focus is on language understanding and multilingual capabilities, rather than search or ranking models.

#### Abstract
> Multilingual large language models (LLMs) are advancing rapidly, with new
models frequently claiming support for an increasing number of languages.
However, existing evaluation datasets are limited and lack cross-lingual
alignment, leaving assessments of multilingual capabilities fragmented in both
language and skill coverage. To address this, we introduce MuBench, a benchmark
covering 61 languages and evaluating a broad range of capabilities. We evaluate
several state-of-the-art multilingual LLMs and find notable gaps between
claimed and actual language coverage, particularly a persistent performance
disparity between English and low-resource languages. Leveraging MuBench's
alignment, we propose Multilingual Consistency (MLC) as a complementary metric
to accuracy for analyzing performance bottlenecks and guiding model
improvement. Finally, we pretrain a suite of 1.2B-parameter models on English
and Chinese with 500B tokens, varying language ratios and parallel data
proportions to investigate cross-lingual transfer dynamics.

### 31. JCAPT: A Joint Modeling Approach for CAPT

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tzu-Hsuan Yang, Yue-Yang He, Berlin Chen
- **URL**: <http://arxiv.org/abs/2506.19315v1>
- **Submitted**: 2025-06-24 05:12:32
- **Comment**: Submitted to the ISCA SLaTE-2025 Workshop
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on computer-assisted pronunciation training (CAPT) and joint modeling of automatic pronunciation assessment and mispronunciation detection and diagnosis, which is outside the user's primary research areas.

#### Abstract
> Effective pronunciation feedback is critical in second language (L2)
learning, for which computer-assisted pronunciation training (CAPT) systems
often encompass two key tasks: automatic pronunciation assessment (APA) and
mispronunciation detection and diagnosis (MDD). Recent work has shown that
joint modeling of these two tasks can yield mutual benefits. Our unified
framework leverages Mamba, a selective state space model (SSM), while
integrating phonological features and think token strategies to jointly enhance
interpretability and fine-grained temporal reasoning in APA and MDD. To our
knowledge, this is the first study to combine phonological attribution,
SSM-based modeling, and prompting in CAPT. A series of experiments conducted on
the speechocean762 benchmark demonstrate that our model consistently
outperforms prior methods, particularly on the MDD task.

### 32. EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhiyang Qi, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba
- **URL**: <http://arxiv.org/abs/2506.19279v1>
- **Submitted**: 2025-06-24 03:18:37
- **Topic Keywords**: rag
- **Reason**: The paper focuses on empathetic response generation in AI-driven counseling systems, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the primary concern is not on ranking models or user behavior modeling, but rather on generating emotionally resonant responses.

#### Abstract
> The rising demand for mental health care has fueled interest in AI-driven
counseling systems. While large language models (LLMs) offer significant
potential, current approaches face challenges, including limited understanding
of clients' psychological states and counseling stages, reliance on
high-quality training data, and privacy concerns associated with commercial
deployment. To address these issues, we propose EmoStage, a framework that
enhances empathetic response generation by leveraging the inference
capabilities of open-source LLMs without additional training data. Our
framework introduces perspective-taking to infer clients' psychological states
and support needs, enabling the generation of emotionally resonant responses.
In addition, phase recognition is incorporated to ensure alignment with the
counseling process and to prevent contextually inappropriate or inopportune
responses. Experiments conducted in both Japanese and Chinese counseling
settings demonstrate that EmoStage improves the quality of responses generated
by base models and performs competitively with data-driven methods.

### 33. Personality Prediction from Life Stories using Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rasiq Hussain, Jerry Ma, Rithik Khandelwal, Joshua Oltmanns, Mehak Gupta
- **URL**: <http://arxiv.org/abs/2506.19258v1>
- **Submitted**: 2025-06-24 02:39:06
- **Comment**: 13 pages, 5 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on personality prediction from life stories using language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context and application are quite different from the user's primary research interests.

#### Abstract
> Natural Language Processing (NLP) offers new avenues for personality
assessment by leveraging rich, open-ended text, moving beyond traditional
questionnaires. In this study, we address the challenge of modeling long
narrative interview where each exceeds 2000 tokens so as to predict Five-Factor
Model (FFM) personality traits. We propose a two-step approach: first, we
extract contextual embeddings using sliding-window fine-tuning of pretrained
language models; then, we apply Recurrent Neural Networks (RNNs) with attention
mechanisms to integrate long-range dependencies and enhance interpretability.
This hybrid method effectively bridges the strengths of pretrained transformers
and sequence modeling to handle long-context data. Through ablation studies and
comparisons with state-of-the-art long-context models such as LLaMA and
Longformer, we demonstrate improvements in prediction accuracy, efficiency, and
interpretability. Our results highlight the potential of combining
language-based features with long-context modeling to advance personality
assessment from life narratives.

### 34. Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rylan Schaeffer, Joshua Kazdan, Yegor Denisov-Blanch, Brando Miranda, Matthias Gerstgrasser, Susan Zhang, Andreas Haupt, Isha Gupta, Elyas Obbad, Jesse Dodge, Jessica Zosa Forde, Koustuv Sinha, Francesco Orabona, Sanmi Koyejo, David Donoho
- **URL**: <http://arxiv.org/abs/2506.19882v1>
- **Submitted**: 2025-06-24 02:19:30
- **Topic Keywords**: search, iclr
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or Natural Language Processing, and does not address query understanding, ranking models, or user behavior modeling. The topic of 'Refutations and Critiques' in machine learning conferences is not a primary focus of your research interests.

#### Abstract
> Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

### 35. Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yun Tang, Eesung Kim, Vijendra Raj Apsingekar
- **URL**: <http://arxiv.org/abs/2506.19159v1>
- **Submitted**: 2025-06-23 21:51:39
- **Comment**: Accepted by Interspeech2025
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on speech recognition and text-based domain adaptation, which is outside your primary focus areas.

#### Abstract
> A joint speech and text optimization method is proposed for hybrid transducer
and attention-based encoder decoder (TAED) modeling to leverage large amounts
of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained
with both speech and text input modalities together, while it only takes speech
data as input during inference. The trained model can unify the internal
representations from different modalities, and be further extended to
text-based domain adaptation. It can effectively alleviate data scarcity for
mismatch domain tasks since no speech data is required. Our experiments show
J-TAED successfully integrates speech and linguistic information into one
model, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model
is also evaluated on two out-of-domain datasets: one is finance and another is
named entity focused. The text-based domain adaptation brings 15.3% and 17.8%
WER reduction on those two datasets respectively.

### 36. Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sahil Kale, Vijaykant Nadadur
- **URL**: <http://arxiv.org/abs/2506.18998v1>
- **Submitted**: 2025-06-23 18:01:16
- **Comment**: Accepted to the Pre-ACL Workshop 2025, Copenhagen
- **Topic Keywords**: rag
- **Reason**: The paper focuses on the limitations of Large Language Models (LLMs) in STEM domains, specifically their tendency to memorize solutions rather than genuinely learn reasoning patterns. While it touches on the concept of self-knowledge, it does not directly relate to information retrieval, search technologies, or query understanding, which are the core areas of your research interests.

#### Abstract
> When artificial intelligence mistakes memorization for intelligence, it
creates a dangerous mirage of reasoning. Existing studies treat memorization
and self-knowledge deficits in LLMs as separate issues and do not recognize an
intertwining link that degrades the trustworthiness of LLM responses. In our
study, we utilize a novel framework to ascertain if LLMs genuinely learn
reasoning patterns from training data or merely memorize them to assume
competence across problems of similar complexity focused on STEM domains. Our
analysis shows a noteworthy problem in generalization: LLMs draw confidence
from memorized solutions to infer a higher self-knowledge about their reasoning
ability, which manifests as an over 45% inconsistency in feasibility
assessments when faced with self-validated, logically coherent task
perturbations. This effect is most pronounced in science and medicine domains,
which tend to have maximal standardized jargon and problems, further confirming
our approach. Significant wavering within the self-knowledge of LLMs also shows
flaws in current architectures and training patterns, highlighting the need for
techniques that ensure a balanced, consistent stance on models' perceptions of
their own knowledge for maximum AI explainability and trustworthiness. Our code
and results are available publicly at
https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.

### 37. How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Abdullah Khondoker, Enam Ahmed Taufik, Md. Iftekhar Islam Tashik, S M Ishtiak Mahmud, Farig Sadeque
- **URL**: <http://arxiv.org/abs/2506.19831v1>
- **Submitted**: 2025-06-24 17:48:49
- **Topic Keywords**: search
- **Reason**: The paper focuses on detecting communal violent text in Bengali, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions NLP and interpretability tools, the context is specific to a particular language and domain, making it only loosely relevant to the user's research interests.

#### Abstract
> The spread of cyber hatred has led to communal violence, fueling aggression
and conflicts between various religious, ethnic, and social groups, posing a
significant threat to social harmony. Despite its critical importance, the
classification of communal violent text remains an underexplored area in
existing research. This study aims to enhance the accuracy of detecting text
that incites communal violence, focusing specifically on Bengali textual data
sourced from social media platforms. We introduce a fine-tuned BanglaBERT model
tailored for this task, achieving a macro F1 score of 0.60. To address the
issue of data imbalance, our dataset was expanded by adding 1,794 instances,
which facilitated the development and evaluation of a fine-tuned ensemble
model. This ensemble model demonstrated an improved performance, achieving a
macro F1 score of 0.63, thus highlighting its effectiveness in this domain. In
addition to quantitative performance metrics, qualitative analysis revealed
instances where the models struggled with context understanding, leading to
occasional misclassifications, even when predictions were made with high
confidence. Through analyzing the cosine similarity between words, we
identified certain limitations in the pre-trained BanglaBERT models,
particularly in their ability to distinguish between closely related communal
and non-communal terms. To further interpret the model's decisions, we applied
LIME, which helped to uncover specific areas where the model struggled in
understanding context, contributing to errors in classification. These findings
highlight the promise of NLP and interpretability tools in reducing online
communal violence. Our work contributes to the growing body of research in
communal violence detection and offers a foundation for future studies aiming
to refine these techniques for better accuracy and societal impact.

### 38. Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Johannes R√ºckert, Louise Bloch, Christoph M. Friedrich
- **URL**: <http://arxiv.org/abs/2506.19825v1>
- **Submitted**: 2025-06-24 17:42:36
- **Comment**: Accepted at ICDAR 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on data visualization and the application of large Vision Language Models to analyze diagrams, which is outside your primary research areas.

#### Abstract
> Diagrams are widely used to visualize data in publications. The research
field of data visualization deals with defining principles and guidelines for
the creation and use of these diagrams, which are often not known or adhered to
by researchers, leading to misinformation caused by providing inaccurate or
incomplete information.
  In this work, large Vision Language Models (VLMs) are used to analyze
diagrams in order to identify potential problems in regards to selected data
visualization principles and guidelines. To determine the suitability of VLMs
for these tasks, five open source VLMs and five prompting strategies are
compared using a set of questions derived from selected data visualization
guidelines.
  The results show that the employed VLMs work well to accurately analyze
diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels
(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score
96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the
image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among
the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting
strategy performs best for most of the experimental questions.
  It is shown that VLMs can be used to automatically identify a number of
potential issues in diagrams, such as missing axes labels, missing legends, and
unnecessary 3D effects. The approach laid out in this work can be extended for
further aspects of data visualization.

### 39. Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Martin Ratajczak, Jean-Philippe Robichaud, Jennifer Drexler Fox
- **URL**: <http://arxiv.org/abs/2506.19761v1>
- **Submitted**: 2025-06-24 16:21:56
- **Comment**: Accepted to Interspeech 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on speech recognition and attention mechanisms in ASR, which is outside the scope of information retrieval, search technologies, and query understanding. The paper's emphasis on efficiency and throughput also does not align with your interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Long-form speech recognition is an application area of increasing research
focus. ASR models based on multi-head attention (MHA) are ill-suited to
long-form ASR because of their quadratic complexity in sequence length. We
build on recent work that has investigated linear complexity recurrent
attention (RA) layers for ASR. We find that bidirectional RA layers can match
the accuracy of MHA for both short- and long-form applications. We present a
strong limited-context attention (LCA) baseline, and show that RA layers are
just as accurate while being more efficient. We develop a long-form training
paradigm which further improves RA performance, leading to better accuracy than
LCA with 44% higher throughput. We also present Direction Dropout, a novel
regularization method that improves accuracy, provides fine-grained control of
the accuracy/throughput trade-off of bidirectional RA, and enables a new
alternating directions decoding mode with even higher throughput.

### 40. Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, Yahui Zhou
- **URL**: <http://arxiv.org/abs/2506.19290v1>
- **Submitted**: 2025-06-24 03:53:36
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on software engineering, language models, and dataset curation, which are outside your primary areas of interest.

#### Abstract
> Software engineering (SWE) has recently emerged as a crucial testbed for
next-generation LLM agents, demanding inherent capabilities in two critical
dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)
and long-context dependency resolution (e.g., >32k tokens). However, the data
curation process in SWE remains notoriously time-consuming, as it heavily
relies on manual annotation for code file filtering and the setup of dedicated
runtime environments to execute and validate unit tests. Consequently, most
existing datasets are limited to only a few thousand GitHub-sourced instances.
To this end, we propose an incremental, automated data-curation pipeline that
systematically scales both the volume and diversity of SWE datasets. Our
dataset comprises 10,169 real-world Python task instances from 2,531 distinct
GitHub repositories, each accompanied by a task specified in natural language
and a dedicated runtime-environment image for automated unit-test validation.
We have carefully curated over 8,000 successfully runtime-validated training
trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE
model on these trajectories, we uncover a striking data scaling phenomenon: the
trained model's performance for software engineering capabilities in LLMs
continues to improve as the data size increases, showing no signs of
saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on
the SWE-bench Verified benchmark without using verifiers or multiple rollouts,
establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based
LLMs built on the OpenHands agent framework. Furthermore, with the
incorporation of test-time scaling techniques, the performance further improves
to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter
models. We release the Skywork-SWE-32B model checkpoint to accelerate future
research.

### 41. Human-Aligned Faithfulness in Toxicity Explanations of LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ramaravind K. Mothilal, Joanna Roy, Syed Ishtiaque Ahmed, Shion Guha
- **URL**: <http://arxiv.org/abs/2506.19113v1>
- **Submitted**: 2025-06-23 20:41:45
- **Comment**: 21 pages, 5 figures, 7 tables
- **Topic Keywords**: search
- **Reason**: The paper focuses on evaluating the explanations of Large Language Models (LLMs) for toxicity, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of explainability, it does not align with the user's primary research interests in IR and NLP.

#### Abstract
> The discourse around toxicity and LLMs in NLP largely revolves around
detection tasks. This work shifts the focus to evaluating LLMs' reasoning about
toxicity -- from their explanations that justify a stance -- to enhance their
trustworthiness in downstream tasks. Despite extensive research on
explainability, it is not straightforward to adopt existing methods to evaluate
free-form toxicity explanation due to their over-reliance on input text
perturbations, among other challenges. To account for these, we propose a
novel, theoretically-grounded multi-dimensional criterion, Human-Aligned
Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity
explanations align with those of a rational human under ideal conditions. We
develop six metrics, based on uncertainty quantification, to comprehensively
evaluate \haf of LLMs' toxicity explanations with no human involvement, and
highlight how "non-ideal" the explanations are. We conduct several experiments
on three Llama models (of size up to 70B) and an 8B Ministral model on five
diverse toxicity datasets. Our results show that while LLMs generate plausible
explanations to simple prompts, their reasoning about toxicity breaks down when
prompted about the nuanced relations between the complete set of reasons, the
individual reasons, and their toxicity stances, resulting in inconsistent and
nonsensical responses. We open-source our code and LLM-generated explanations
at https://github.com/uofthcdslab/HAF.

### 42. HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yimu Wang, Mozhgan Nasr Azadani, Sean Sedwards, Krzysztof Czarnecki
- **URL**: <http://arxiv.org/abs/2506.19072v1>
- **Submitted**: 2025-06-23 19:43:25
- **Comment**: Work in progress
- **Topic Keywords**: rank
- **Reason**: The paper focuses on vision-language models and knowledge transfer, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of distillation, which is a common technique in machine learning, the context is different from the user's primary research interests.

#### Abstract
> Improving the visual understanding ability of vision-language models (VLMs)
is crucial for enhancing their performance across various tasks. While using
multiple pretrained visual experts has shown great promise, it often incurs
significant computational costs during training and inference. To address this
challenge, we propose HAWAII, a novel framework that distills knowledge from
multiple visual experts into a single vision encoder, enabling it to inherit
the complementary strengths of several experts with minimal computational
overhead. To mitigate conflicts among different teachers and switch between
different teacher-specific knowledge, instead of using a fixed set of adapters
for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation
(LoRA) adapters with a corresponding router. Each adapter is aligned with a
specific teacher, avoiding noisy guidance during distillation. To enable
efficient knowledge distillation, we propose fine-grained and coarse-grained
distillation. At the fine-grained level, token importance scores are employed
to emphasize the most informative tokens from each teacher adaptively. At the
coarse-grained level, we summarize the knowledge from multiple teachers and
transfer it to the student using a set of general-knowledge LoRA adapters with
a router. Extensive experiments on various vision-language tasks demonstrate
the superiority of HAWAII, compared to the popular open-source VLMs.

---

