[
    {
        "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
        "abstract": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
        "url": "http://arxiv.org/abs/2508.04700v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04700v1",
        "arxiv_id": "2508.04700v1",
        "authors": [
            "Zeyi Sun",
            "Ziyu Liu",
            "Yuhang Zang",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Tong Wu",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "submitted": "2025-08-06 17:58:46",
        "source": "arxiv",
        "comment": "Code at https://github.com/SunzeY/SEAgent"
    },
    {
        "title": "Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis",
        "abstract": "The emergence of reasoning models and their integration into practical AI\nchat bots has led to breakthroughs in solving advanced math, deep search, and\nextractive question answering problems that requires a complex and multi-step\nthought process. Yet, a complete understanding of why these models hallucinate\nmore than general purpose language models is missing. In this investigative\nstudy, we systematicallyexplore reasoning failures of contemporary language\nmodels on multi-hop question answering tasks. We introduce a novel, nuanced\nerror categorization framework that examines failures across three critical\ndimensions: the diversity and uniqueness of source documents involved (\"hops\"),\ncompleteness in capturing relevant information (\"coverage\"), and cognitive\ninefficiency (\"overthinking\"). Through rigorous hu-man annotation, supported by\ncomplementary automated metrics, our exploration uncovers intricate error\npatterns often hidden by accuracy-centric evaluations. This investigative\napproach provides deeper insights into the cognitive limitations of current\nmodels and offers actionable guidance toward enhancing reasoning fidelity,\ntransparency, and robustness in future language modeling efforts.",
        "url": "http://arxiv.org/abs/2508.04699v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04699v1",
        "arxiv_id": "2508.04699v1",
        "authors": [
            "Anushka Yadav",
            "Isha Nalawade",
            "Srujana Pillarichety",
            "Yashwanth Babu",
            "Reshmi Ghosh",
            "Samyadeep Basu",
            "Wenlong Zhao",
            "Ali Nasaeh",
            "Sriram Balasubramanian",
            "Soundararajan Srinivasan"
        ],
        "submitted": "2025-08-06 17:58:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data",
        "abstract": "LLM-powered conversational assistants are often deployed in a\none-size-fits-all manner, which fails to accommodate individual user\npreferences. Recently, LLM personalization -- tailoring models to align with\nspecific user preferences -- has gained increasing attention as a way to bridge\nthis gap. In this work, we specifically focus on a practical yet challenging\nsetting where only a small set of preference annotations can be collected per\nuser -- a problem we define as Personalized Preference Alignment with Limited\nData (PPALLI). To support research in this area, we introduce two datasets --\nDnD and ELIP -- and benchmark a variety of alignment techniques on them. We\nfurther propose FaST, a highly parameter-efficient approach that leverages\nhigh-level features automatically discovered from the data, achieving the best\noverall performance.",
        "url": "http://arxiv.org/abs/2508.04698v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04698v1",
        "arxiv_id": "2508.04698v1",
        "authors": [
            "Thibaut Thonet",
            "Germán Kruszewski",
            "Jos Rozen",
            "Pierre Erbacher",
            "Marc Dymetman"
        ],
        "submitted": "2025-08-06 17:58:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering",
        "abstract": "This study introduces Query Attribute Modeling (QAM), a hybrid framework that\nenhances search precision and relevance by decomposing open text queries into\nstructured metadata tags and semantic elements. QAM addresses traditional\nsearch limitations by automatically extracting metadata filters from free-form\ntext queries, reducing noise and enabling focused retrieval of relevant items.\n  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique\nitems with 40,000+ reviews and detailed product attributes) demonstrated QAM's\nsuperior performance, achieving a mean average precision at 5 (mAP@5) of\n52.99\\%. This represents significant improvement over conventional methods,\nincluding BM25 keyword search, encoder-based semantic similarity search,\ncross-encoder re-ranking, and hybrid search combining BM25 and semantic results\nvia Reciprocal Rank Fusion (RRF). The results establish QAM as a robust\nsolution for Enterprise Search applications, particularly in e-commerce\nsystems.",
        "url": "http://arxiv.org/abs/2508.04683v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04683v1",
        "arxiv_id": "2508.04683v1",
        "authors": [
            "Karthik Menon",
            "Batool Arhamna Haider",
            "Muhammad Arham",
            "Kanwal Mehreen",
            "Ram Mohan Rao Kadiyala",
            "Hamza Farooq"
        ],
        "submitted": "2025-08-06 17:47:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay",
        "abstract": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
        "url": "http://arxiv.org/abs/2508.04676v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04676v1",
        "arxiv_id": "2508.04676v1",
        "authors": [
            "Yunan Zhang",
            "Shuoran Jiang",
            "Mengchen Zhao",
            "Yuefeng Li",
            "Yang Fan",
            "Xiangping Wu",
            "Qingcai Chen"
        ],
        "submitted": "2025-08-06 17:42:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management",
        "abstract": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
        "url": "http://arxiv.org/abs/2508.04664v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04664v1",
        "arxiv_id": "2508.04664v1",
        "authors": [
            "Mo Li",
            "L. H. Xu",
            "Qitai Tan",
            "Ting Cao",
            "Yunxin Liu"
        ],
        "submitted": "2025-08-06 17:32:58",
        "source": "arxiv",
        "comment": "Preprint. Work in progress"
    },
    {
        "title": "Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs",
        "abstract": "Group Relative Policy Optimization (GRPO) has proven to be an effective tool\nfor post-training language models (LMs). However, AI systems are increasingly\nexpressed as modular programs that mix together multiple LM calls with distinct\nprompt templates and other tools, and it is not clear how best to leverage GRPO\nto improve these systems. We begin to address this challenge by defining\nmmGRPO, a simple multi-module generalization of GRPO that groups LM calls by\nmodule across rollouts and handles variable-length and interrupted\ntrajectories. We find that mmGRPO, composed with automatic prompt optimization,\nimproves accuracy by 11% on average across classification, many-hop search, and\nprivacy-preserving delegation tasks against the post-trained LM, and by 5%\nagainst prompt optimization on its own. We open-source mmGRPO in DSPy as the\ndspy.GRPO optimizer.",
        "url": "http://arxiv.org/abs/2508.04660v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04660v1",
        "arxiv_id": "2508.04660v1",
        "authors": [
            "Noah Ziems",
            "Dilara Soylu",
            "Lakshya A Agrawal",
            "Isaac Miller",
            "Liheng Lai",
            "Chen Qian",
            "Kaiqiang Song",
            "Meng Jiang",
            "Dan Klein",
            "Matei Zaharia",
            "Karel D'Oosterlinck",
            "Christopher Potts",
            "Omar Khattab"
        ],
        "submitted": "2025-08-06 17:28:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech",
        "abstract": "Counterspeech, i.e. the practice of responding to online hate speech, has\ngained traction in NLP as a promising intervention. While early work emphasised\ncollaboration with non-governmental organisation stakeholders, recent research\ntrends have shifted toward automated pipelines that reuse a small set of legacy\ndatasets, often without input from affected communities. This paper presents a\nsystematic review of 74 NLP studies on counterspeech, analysing the extent to\nwhich stakeholder participation influences dataset creation, model development,\nand evaluation. To complement this analysis, we conducted a participatory case\nstudy with five NGOs specialising in online Gender-Based Violence (oGBV),\nidentifying stakeholder-informed practices for counterspeech generation. Our\nfindings reveal a growing disconnect between current NLP research and the needs\nof communities most impacted by toxic online content. We conclude with concrete\nrecommendations for re-centring stakeholder expertise in counterspeech\nresearch.",
        "url": "http://arxiv.org/abs/2508.04638v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04638v1",
        "arxiv_id": "2508.04638v1",
        "authors": [
            "Tanvi Dinkar",
            "Aiqi Jiang",
            "Simona Frenda",
            "Poppy Gerrard-Abbott",
            "Nancie Gunson",
            "Gavin Abercrombie",
            "Ioannis Konstas"
        ],
        "submitted": "2025-08-06 17:04:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
        "url": "http://arxiv.org/abs/2508.04632v2",
        "pdf_url": "http://arxiv.org/pdf/2508.04632v2",
        "arxiv_id": "2508.04632v2",
        "authors": [
            "Xu Guo",
            "Tianyi Liang",
            "Tong Jian",
            "Xiaogui Yang",
            "Ling-I Wu",
            "Chenhui Li",
            "Zhihui Lu",
            "Qipeng Guo",
            "Kai Chen"
        ],
        "submitted": "2025-08-06 17:00:54",
        "source": "arxiv",
        "comment": "7 pages, 4 figures"
    },
    {
        "title": "P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis",
        "abstract": "Large Language Models (LLMs) are expected to produce safe, helpful, and\nhonest content during interaction with human users, but they frequently fail to\nalign with such values when given flawed instructions, e.g., missing context,\nambiguous directives, or inappropriate tone, leaving substantial room for\nimprovement along multiple dimensions. A cost-effective yet high-impact way is\nto pre-align instructions before the model begins decoding. Existing approaches\neither rely on prohibitive test-time search costs or end-to-end model rewrite,\nwhich is powered by a customized training corpus with unclear objectives. In\nthis work, we demonstrate that the goal of efficient and effective preference\nalignment can be achieved by P-Aligner, a lightweight module generating\ninstructions that preserve the original intents while being expressed in a more\nhuman-preferred form. P-Aligner is trained on UltraPrompt, a new dataset\nsynthesized via a proposed principle-guided pipeline using Monte-Carlo Tree\nSearch, which systematically explores the space of candidate instructions that\nare closely tied to human preference. Experiments across different methods show\nthat P-Aligner generally outperforms strong baselines across various models and\nbenchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo\nand Gemma-2-SimPO, respectively. Further analyses validate its effectiveness\nand efficiency through multiple perspectives, including data quality, search\nstrategies, iterative deployment, and time overhead.",
        "url": "http://arxiv.org/abs/2508.04626v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04626v1",
        "arxiv_id": "2508.04626v1",
        "authors": [
            "Feifan Song",
            "Bofei Gao",
            "Yifan Song",
            "Yi Liu",
            "Weimin Xiong",
            "Yuyang Song",
            "Tianyu Liu",
            "Guoyin Wang",
            "Houfeng Wang"
        ],
        "submitted": "2025-08-06 16:51:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider",
        "abstract": "Text-to-SQL translation enables non-expert users to query relational\ndatabases using natural language, with applications in education and business\nintelligence. This study evaluates three lightweight transformer models -\nT5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on\nlow-resource settings. We developed a reusable, model-agnostic pipeline that\ntailors schema formatting to each model's architecture, training them across\n1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form\nAccuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small\nachieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2\n(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL\ngeneration. Despite resource constraints limiting performance, our pipeline's\nmodularity supports future enhancements, such as advanced schema linking or\nalternative base models. This work underscores the potential of compact\ntransformers for accessible text-to-SQL solutions in resource-scarce\nenvironments.",
        "url": "http://arxiv.org/abs/2508.04623v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04623v1",
        "arxiv_id": "2508.04623v1",
        "authors": [
            "Chirag Seth",
            "Utkarsh Singh"
        ],
        "submitted": "2025-08-06 16:49:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs",
        "abstract": "Recommender systems are indispensable for helping users navigate the immense\nitem catalogs of modern online platforms. Recently, generative recommendation\nhas emerged as a promising paradigm, unifying the conventional\nretrieve-and-rank pipeline into an end-to-end model capable of dynamic\ngeneration. However, existing generative methods are fundamentally constrained\nby their unsupervised tokenization, which generates semantic IDs suffering from\ntwo critical flaws: (1) they are semantically flat and uninterpretable, lacking\na coherent hierarchy, and (2) they are prone to representation entanglement\n(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.\nTo overcome these limitations, we propose HiD-VAE, a novel framework that\nlearns hierarchically disentangled item representations through two core\ninnovations. First, HiD-VAE pioneers a hierarchically-supervised quantization\nprocess that aligns discrete codes with multi-level item tags, yielding more\nuniform and disentangled IDs. Crucially, the trained codebooks can predict\nhierarchical tags, providing a traceable and interpretable semantic path for\neach recommendation. Second, to combat representation entanglement, HiD-VAE\nincorporates a novel uniqueness loss that directly penalizes latent space\noverlap. This mechanism not only resolves the critical ID collision problem but\nalso promotes recommendation diversity by ensuring a more comprehensive\nutilization of the item representation space. These high-quality, disentangled\nIDs provide a powerful foundation for downstream generative models. Extensive\nexperiments on three public benchmarks validate HiD-VAE's superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://anonymous.4open.science/r/HiD-VAE-84B2.",
        "url": "http://arxiv.org/abs/2508.04618v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04618v1",
        "arxiv_id": "2508.04618v1",
        "authors": [
            "Dengzhao Fang",
            "Jingtong Gao",
            "Chengcheng Zhu",
            "Yu Li",
            "Xiangyu Zhao",
            "Yi Chang"
        ],
        "submitted": "2025-08-06 16:45:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature",
        "abstract": "The accelerating pace of research on autoregressive generative models has\nproduced thousands of papers, making manual literature surveys and reproduction\nstudies increasingly impractical. We present a fully open-source, reproducible\npipeline that automatically retrieves candidate documents from public\nrepositories, filters them for relevance, extracts metadata, hyper-parameters\nand reported results, clusters topics, produces retrieval-augmented summaries\nand generates containerised scripts for re-running selected experiments.\nQuantitative evaluation on 50 manually-annotated papers shows F1 scores above\n0.85 for relevance classification, hyper-parameter extraction and citation\nidentification. Experiments on corpora of up to 1000 papers demonstrate\nnear-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM\non WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model\non the Lakh MIDI dataset -- confirm that the extracted settings support\nfaithful reproduction, achieving test perplexities within 1--3% of the original\nreports.",
        "url": "http://arxiv.org/abs/2508.04612v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04612v1",
        "arxiv_id": "2508.04612v1",
        "authors": [
            "Faruk Alpay",
            "Bugra Kilictas",
            "Hamdi Alakkad"
        ],
        "submitted": "2025-08-06 16:33:20",
        "source": "arxiv",
        "comment": "9 pages"
    },
    {
        "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search",
        "abstract": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.",
        "url": "http://arxiv.org/abs/2508.04604v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04604v1",
        "arxiv_id": "2508.04604v1",
        "authors": [
            "Zhejun Zhao",
            "Yuehu Dong",
            "Alley Liu",
            "Lixue Zheng",
            "Pingsheng Liu",
            "Dongdong Shen",
            "Long Xia",
            "Jiashu Zhao",
            "Dawei Yin"
        ],
        "submitted": "2025-08-06 16:24:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference",
        "abstract": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
        "url": "http://arxiv.org/abs/2508.04586v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04586v1",
        "arxiv_id": "2508.04586v1",
        "authors": [
            "Nuo Chen",
            "Moming Duan",
            "Andre Huikai Lin",
            "Qian Wang",
            "Jiaying Wu",
            "Bingsheng He"
        ],
        "submitted": "2025-08-06 16:08:27",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
        "abstract": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
        "url": "http://arxiv.org/abs/2508.04581v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04581v1",
        "arxiv_id": "2508.04581v1",
        "authors": [
            "Magauiya Zhussip",
            "Dmitriy Shopkhoev",
            "Ammar Ali",
            "Stamatios Lefkimmiatis"
        ],
        "submitted": "2025-08-06 16:06:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration",
        "abstract": "While AI agents show potential in scientific ideation, most existing\nframeworks rely on single-agent refinement, limiting creativity due to bounded\nknowledge and perspective. Inspired by real-world research dynamics, this paper\ninvestigates whether structured multi-agent discussions can surpass solitary\nideation. We propose a cooperative multi-agent framework for generating\nresearch proposals and systematically compare configurations including group\nsize, leaderled versus leaderless structures, and team compositions varying in\ninterdisciplinarity and seniority. To assess idea quality, we employ a\ncomprehensive protocol with agent-based scoring and human review across\ndimensions such as novelty, strategic vision, and integration depth. Our\nresults show that multi-agent discussions substantially outperform solitary\nbaselines. A designated leader acts as a catalyst, transforming discussion into\nmore integrated and visionary proposals. Notably, we find that cognitive\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\neven a single competent agent. These findings offer actionable insights for\ndesigning collaborative AI ideation systems and shed light on how team\nstructure influences creative outcomes.",
        "url": "http://arxiv.org/abs/2508.04575v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04575v1",
        "arxiv_id": "2508.04575v1",
        "authors": [
            "Nuo Chen",
            "Yicheng Tong",
            "Jiaying Wu",
            "Minh Duc Duong",
            "Qian Wang",
            "Qingyun Zou",
            "Bryan Hooi",
            "Bingsheng He"
        ],
        "submitted": "2025-08-06 15:59:18",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation",
        "abstract": "Multimodal Recommender Systems aim to improve recommendation accuracy by\nintegrating heterogeneous content, such as images and textual metadata. While\neffective, it remains unclear whether their gains stem from true multimodal\nunderstanding or increased model complexity. This work investigates the role of\nmultimodal item embeddings, emphasizing the semantic informativeness of the\nrepresentations. Initial experiments reveal that embeddings from standard\nextractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on\nmodality-specific encoders and ad hoc fusion strategies that lack control over\ncross-modal alignment. To overcome these limitations, we leverage Large\nVision-Language Models (LVLMs) to generate multimodal-by-design embeddings via\nstructured prompts. This approach yields semantically aligned representations\nwithout requiring any fusion. Experiments across multiple settings show notable\nperformance improvements. Furthermore, LVLMs embeddings offer a distinctive\nadvantage: they can be decoded into structured textual descriptions, enabling\ndirect assessment of their multimodal comprehension. When such descriptions are\nincorporated as side content into recommender systems, they improve\nrecommendation performance, empirically validating the semantic depth and\nalignment encoded within LVLMs outputs. Our study highlights the importance of\nsemantically rich representations and positions LVLMs as a compelling\nfoundation for building robust and meaningful multimodal representations in\nrecommendation tasks.",
        "url": "http://arxiv.org/abs/2508.04571v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04571v1",
        "arxiv_id": "2508.04571v1",
        "authors": [
            "Claudio Pomo",
            "Matteo Attimonelli",
            "Danilo Danese",
            "Fedelucio Narducci",
            "Tommaso Di Noia"
        ],
        "submitted": "2025-08-06 15:53:58",
        "source": "arxiv",
        "comment": "Accepted as Full Research Papers at CIKM 2025"
    },
    {
        "title": "Analyzing and Mitigating Object Hallucination: A Training Bias Perspective",
        "abstract": "As scaling up training data has significantly improved the general multimodal\ncapabilities of Large Vision-Language Models (LVLMs), they still suffer from\nthe hallucination issue, generating text that is inconsistent with the visual\ninput. This phenomenon motivates us to systematically investigate the role of\ntraining data in hallucination. We introduce a new benchmark, POPEv2, which\nconsists of counterfactual images collected from the training data of LVLMs\nwith certain objects masked. Through comprehensive evaluation on POPEv2, we\nfind that current LVLMs suffer from training bias: they fail to fully leverage\ntheir training data and hallucinate more frequently on images seen during\ntraining. Specifically, they perform poorly on counterfactual images, often\nincorrectly answering ``Yes'' to questions about masked objects. To understand\nthis issue, we conduct probing experiments on the models' internal components,\nrevealing that this training bias is primarily located in the language modeling\n(LM) head. Based on these findings, we propose Obliviate, an efficient and\nlightweight unlearning method designed to mitigate object hallucination via\ntraining bias unlearning. Obliviate identifies the discrepancy between\nground-truth labels and model outputs on the training data as a proxy for bias\nand adopts a parameter- and data-efficient fine-tuning strategy that only\nupdates the LM head. Extensive experiments demonstrate the effectiveness of our\napproach. While only reusing the training data and updating approximately 2\\%\nof the parameters, Obliviate significantly reduces hallucination across both\ndiscriminative and generative tasks. Furthermore, it demonstrates strong\nscalability with respect to both model size (2B to 72B) and training data\nvolume, and exhibits promising generalization to hallucination types beyond\nobject-level hallucination. Our code and data will be publicly released.",
        "url": "http://arxiv.org/abs/2508.04567v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04567v1",
        "arxiv_id": "2508.04567v1",
        "authors": [
            "Yifan Li",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Lei Fang",
            "Ji-Rong Wen"
        ],
        "submitted": "2025-08-06 15:51:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning",
        "abstract": "Depression is a widespread mental disorder that affects millions worldwide.\nWhile automated depression assessment shows promise, most studies rely on\nlimited or non-clinically validated data, and often prioritize complex model\ndesign over real-world effectiveness. In this paper, we aim to unveil the\nlandscape of clinical depression assessment. We introduce C-MIND, a clinical\nneuropsychiatric multimodal diagnosis dataset collected over two years from\nreal hospital visits. Each participant completes three structured psychiatric\ntasks and receives a final diagnosis from expert clinicians, with informative\naudio, video, transcript, and functional near-infrared spectroscopy (fNIRS)\nsignals recorded. Using C-MIND, we first analyze behavioral signatures relevant\nto diagnosis. We train a range of classical models to quantify how different\ntasks and modalities contribute to diagnostic performance, and dissect the\neffectiveness of their combinations. We then explore whether LLMs can perform\npsychiatric reasoning like clinicians and identify their clear limitations in\nrealistic clinical settings. In response, we propose to guide the reasoning\nprocess with clinical expertise and consistently improves LLM diagnostic\nperformance by up to 10% in Macro-F1 score. We aim to build an infrastructure\nfor clinical depression assessment from both data and algorithmic perspectives,\nenabling C-MIND to facilitate grounded and reliable research for mental\nhealthcare.",
        "url": "http://arxiv.org/abs/2508.04531v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04531v1",
        "arxiv_id": "2508.04531v1",
        "authors": [
            "Zhuang Chen",
            "Guanqun Bi",
            "Wen Zhang",
            "Jiawei Hu",
            "Aoyun Wang",
            "Xiyao Xiao",
            "Kun Feng",
            "Minlie Huang"
        ],
        "submitted": "2025-08-06 15:13:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Balancing Stylization and Truth via Disentangled Representation Steering",
        "abstract": "Generating stylized large language model (LLM) responses via representation\nediting is a promising way for fine-grained output control. However, there\nexists an inherent trade-off: imposing a distinctive style often degrades\ntruthfulness. Existing representation editing methods, by naively injecting\nstyle signals, overlook this collateral impact and frequently contaminate the\nmodel's core truthfulness representations, resulting in reduced answer\ncorrectness. We term this phenomenon stylization-induced truthfulness collapse.\nWe attribute this issue to latent coupling between style and truth directions\nin certain key attention heads, and propose StyliTruth, a mechanism that\npreserves stylization while keeping truthfulness intact. StyliTruth separates\nthe style-relevant and truth-relevant subspaces in the model's representation\nspace via an orthogonal deflation process. This decomposition enables\nindependent control of style and truth in their own subspaces, minimizing\ninterference. By designing adaptive, token-level steering vectors within each\nsubspace, we dynamically and precisely control the generation process to\nmaintain both stylistic fidelity and truthfulness. We validate our method on\nmultiple styles and languages. Extensive experiments and analyses show that\nStyliTruth significantly reduces stylization-induced truthfulness collapse and\noutperforms existing inference-time intervention methods in balancing style\nadherence with truthfulness.",
        "url": "http://arxiv.org/abs/2508.04530v2",
        "pdf_url": "http://arxiv.org/pdf/2508.04530v2",
        "arxiv_id": "2508.04530v2",
        "authors": [
            "Chenglei Shen",
            "Zhongxiang Sun",
            "Teng Shi",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "submitted": "2025-08-06 15:12:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Causal Reflection with Language Models",
        "abstract": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
        "url": "http://arxiv.org/abs/2508.04495v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04495v1",
        "arxiv_id": "2508.04495v1",
        "authors": [
            "Abi Aryan",
            "Zac Liu"
        ],
        "submitted": "2025-08-06 14:44:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation",
        "abstract": "Lexical semantics is concerned with both the multiple senses a word can adopt\nin different contexts, and the semantic relations that exist between meanings\nof different words. To investigate them, Contextualized Language Models are a\nvaluable tool that provides context-sensitive representations that can be used\nto investigate lexical meaning. Recent works like XL-LEXEME have leveraged the\ntask of Word-in-Context to fine-tune them to get more semantically accurate\nrepresentations, but Word-in-Context only compares occurrences of the same\nlemma, limiting the range of captured information. In this paper, we propose an\nextension, Concept Differentiation, to include inter-words scenarios. We\nprovide a dataset for this task, derived from SemCor data. Then we fine-tune\nseveral representation models on this dataset. We call these models\nConcept-Aligned Embeddings (CALE). By challenging our models and other models\non various lexical semantic tasks, we demonstrate that the proposed models\nprovide efficient multi-purpose representations of lexical meaning that reach\nbest performances in our experiments. We also show that CALE's fine-tuning\nbrings valuable changes to the spatial organization of embeddings.",
        "url": "http://arxiv.org/abs/2508.04494v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04494v1",
        "arxiv_id": "2508.04494v1",
        "authors": [
            "Bastien Liétard",
            "Gabriel Loiseau"
        ],
        "submitted": "2025-08-06 14:43:22",
        "source": "arxiv",
        "comment": "Under review in ARR July 2025"
    },
    {
        "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use",
        "abstract": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
        "url": "http://arxiv.org/abs/2508.04482v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04482v1",
        "arxiv_id": "2508.04482v1",
        "authors": [
            "Xueyu Hu",
            "Tao Xiong",
            "Biao Yi",
            "Zishu Wei",
            "Ruixuan Xiao",
            "Yurun Chen",
            "Jiasheng Ye",
            "Meiling Tao",
            "Xiangxin Zhou",
            "Ziyu Zhao",
            "Yuhuai Li",
            "Shengze Xu",
            "Shenzhi Wang",
            "Xinchen Xu",
            "Shuofei Qiao",
            "Zhaokai Wang",
            "Kun Kuang",
            "Tieyong Zeng",
            "Liang Wang",
            "Jiwei Li",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou",
            "Guoyin Wang",
            "Keting Yin",
            "Zhou Zhao",
            "Hongxia Yang",
            "Fan Wu",
            "Shengyu Zhang",
            "Fei Wu"
        ],
        "submitted": "2025-08-06 14:33:45",
        "source": "arxiv",
        "comment": "ACL 2025 (Oral)"
    },
    {
        "title": "TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models",
        "abstract": "Recent advances in large language models (LLMs) have unlocked powerful\nreasoning and decision-making capabilities. However, their inherent dependence\non static parametric memory fundamentally limits their adaptability, factual\naccuracy, and interpretability in knowledge-intensive scenarios. Knowledge\ngraphs (KGs), as structured repositories of explicit relational knowledge,\noffer a promising approach for augmenting LLMs with external, interpretable\nmemory. Nevertheless, most existing methods that combine LLMs with KGs treat\nreasoning and knowledge updating as separate processes, resulting in suboptimal\nutilization of new information and hindering real-time updates. In this work,\nwe propose TRAIL: a novel, unified framework for Thinking, Reasoning, And\nIncremental Learning that couples joint inference and dynamic KG refinement\nwith large language models. TRAIL enables LLM agents to iteratively explore,\nupdate, and refine knowledge graphs during the reasoning process, employing a\nconfidence-driven mechanism for the generation, validation, and pruning of new\nfacts. This plug-and-play architecture facilitates seamless integration with\nvarious LLMs, supporting continual adaptation without the need for retraining.\nExtensive experiments on multiple benchmarks demonstrate that TRAIL outperforms\nexisting KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More\nimportantly, these results represent a significant step toward developing\nadaptive, memory-augmented language models capable of continual learning and\nreliable, transparent reasoning.",
        "url": "http://arxiv.org/abs/2508.04474v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04474v1",
        "arxiv_id": "2508.04474v1",
        "authors": [
            "Xinkui Zhao",
            "Haode Li",
            "Yifan Zhang",
            "Guanjie Cheng",
            "Yueshen Xu"
        ],
        "submitted": "2025-08-06 14:25:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding",
        "abstract": "The deployment of vision-language models remains constrained by substantial\ncomputational requirements. We present \\textbf{FrEVL}, a framework exploring\nwhether frozen pretrained embeddings can support effective vision-language\nunderstanding. Our analysis reveals that frozen embeddings contain rich\ninformation for discriminative tasks, achieving 85\\% to 95\\% of\nstate-of-the-art performance on standard benchmarks with only 68.4M trainable\nparameters. This performance dichotomy reveals a critical insight: frozen\nembedding effectiveness depends on alignment between pretraining objectives and\ndownstream task requirements. When accounting for end-to-end computation\nincluding embedding extraction, FrEVL provides $2.3\\times$ speedup with 52\\%\nlower energy consumption, making it suitable for scenarios with pre-computable\ninputs or when deployment constraints outweigh marginal performance gains. Our\nevaluation provides practitioners with guidance on when frozen embedding\napproaches represent viable alternatives to full model deployment. We will\nrelease our complete implementation and evaluation framework to facilitate\nfurther research into efficient multi-modal understanding.",
        "url": "http://arxiv.org/abs/2508.04469v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04469v1",
        "arxiv_id": "2508.04469v1",
        "authors": [
            "Emmanuelle Bourigault",
            "Pauline Bourigault"
        ],
        "submitted": "2025-08-06 14:12:05",
        "source": "arxiv",
        "comment": "8 pages, 4 figures"
    },
    {
        "title": "Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI",
        "abstract": "This paper addresses the critical need for scalable and high-quality\neducational assessment tools within the Malaysian education system. It\nhighlights the potential of Generative AI (GenAI) while acknowledging the\nsignificant challenges of ensuring factual accuracy and curriculum alignment,\nespecially for low-resource languages like Bahasa Melayu. This research\nintroduces and compares four incremental pipelines for generating Form 1\nMathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's\nGPT-4o. The methods range from non-grounded prompting (structured and basic) to\nRetrieval-Augmented Generation (RAG) approaches (one using the LangChain\nframework, one implemented manually). The system is grounded in official\ncurriculum documents, including teacher-prepared notes and the yearly teaching\nplan (RPT). A dual-pronged automated evaluation framework is employed to assess\nthe generated questions. Curriculum alignment is measured using Semantic\nTextual Similarity (STS) against the RPT, while contextual validity is verified\nthrough a novel RAG-based Question-Answering (RAG-QA) method. The results\ndemonstrate that RAG-based pipelines significantly outperform non-grounded\nprompting methods, producing questions with higher curriculum alignment and\nfactual validity. The study further analyzes the trade-offs between the ease of\nimplementation of framework-based RAG and the fine-grained control offered by a\nmanual pipeline. This work presents a validated methodology for generating\ncurriculum-specific educational content in a low-resource language, introduces\na symbiotic RAG-QA evaluation technique, and provides actionable insights for\nthe development and deployment of practical EdTech solutions in Malaysia and\nsimilar regions.",
        "url": "http://arxiv.org/abs/2508.04442v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04442v1",
        "arxiv_id": "2508.04442v1",
        "authors": [
            "Rohaizah Abdul Wahid",
            "Muhamad Said Nizamuddin Nadim",
            "Suliana Sulaiman",
            "Syahmi Akmal Shaharudin",
            "Muhammad Danial Jupikil",
            "Iqqwan Jasman Su Azlan Su"
        ],
        "submitted": "2025-08-06 13:30:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion",
        "abstract": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
        "url": "http://arxiv.org/abs/2508.04440v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04440v1",
        "arxiv_id": "2508.04440v1",
        "authors": [
            "Yutong Wu",
            "Di Huang",
            "Ruosi Wan",
            "Yue Peng",
            "Shijie Shang",
            "Chenrui Cao",
            "Lei Qi",
            "Rui Zhang",
            "Zidong Du",
            "Jie Yan",
            "Xing Hu"
        ],
        "submitted": "2025-08-06 13:28:22",
        "source": "arxiv",
        "comment": "24 pages, 17 figures, under review"
    },
    {
        "title": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation",
        "abstract": "Effective customer support requires not only accurate problem solving but\nalso structured and empathetic communication aligned with professional\nstandards. However, existing dialogue datasets often lack strategic guidance,\nand real-world service data is difficult to access and annotate. To address\nthis, we introduce the task of Customer Support Conversation (CSC), aimed at\ntraining customer service agents to respond using well-defined support\nstrategies. We propose a structured CSC framework grounded in COPC guidelines,\ndefining five conversational stages and twelve strategies to guide high-quality\ninteractions. Based on this, we construct CSConv, an evaluation dataset of\n1,855 real-world customer-agent conversations rewritten using LLMs to reflect\ndeliberate strategy use, and annotated accordingly. Additionally, we develop a\nrole-playing approach that simulates strategy-rich conversations using\nLLM-powered roles aligned with the CSC framework, resulting in the training\ndataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS\nsignificantly improves their ability to generate high-quality, strategy-aligned\nresponses on CSConv. Human evaluations further confirm gains in problem\nresolution. All code and data will be made publicly available at\nhttps://github.com/aliyun/qwen-dianjin.",
        "url": "http://arxiv.org/abs/2508.04423v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04423v1",
        "arxiv_id": "2508.04423v1",
        "authors": [
            "Jie Zhu",
            "Huaixia Dou",
            "Junhui Li",
            "Lifan Guo",
            "Feng Chen",
            "Chi Zhang",
            "Fang Kong"
        ],
        "submitted": "2025-08-06 13:11:17",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics",
        "abstract": "The Algorithm Selection Problem for recommender systems-choosing the best\nalgorithm for a given user or context-remains a significant challenge.\nTraditional meta-learning approaches often treat algorithms as categorical\nchoices, ignoring their intrinsic properties. Recent work has shown that\nexplicitly characterizing algorithms with features can improve model\nperformance in other domains. Building on this, we propose a per-user\nmeta-learning approach for recommender system selection that leverages both\nuser meta-features and automatically extracted algorithm features from source\ncode. Our preliminary results, averaged over six diverse datasets, show that\naugmenting a meta-learner with algorithm features improves its average NDCG@10\nperformance by 8.83% from 0.135 (user features only) to 0.147. This enhanced\nmodel outperforms the Single Best Algorithm baseline (0.131) and successfully\ncloses 10.5% of the performance gap to a theoretical oracle selector. These\nfindings show that even static source code metrics provide a valuable\npredictive signal, presenting a promising direction for building more robust\nand intelligent recommender systems.",
        "url": "http://arxiv.org/abs/2508.04419v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04419v1",
        "arxiv_id": "2508.04419v1",
        "authors": [
            "Jarne Mathi Decker",
            "Joeran Beel"
        ],
        "submitted": "2025-08-06 13:06:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
        "abstract": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.",
        "url": "http://arxiv.org/abs/2508.04412v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04412v1",
        "arxiv_id": "2508.04412v1",
        "authors": [
            "Thassilo M. Schiepanski",
            "Nicholas Piël"
        ],
        "submitted": "2025-08-06 12:56:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model",
        "abstract": "Prefetching of dialogue responses has been investigated to reduce\nuser-perceived latency (UPL), which refers to the user's waiting time before\nreceiving the system's response, in spoken dialogue systems. To reduce the UPL,\nit is necessary to predict complete user utterances before the end of the\nuser's speech, typically by language models, to prepare prefetched dialogue\nresponses. In this study, we proposed a prediction confidence model (PCM) that\ndetermines whether prefetching is possible or not by estimating the semantic\nsimilarity between the predicted complete user utterance and the complete user\nutterance. We evaluated our PCM based on the differences between the predicted\ncomplete user utterance and the complete user utterance.",
        "url": "http://arxiv.org/abs/2508.04403v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04403v1",
        "arxiv_id": "2508.04403v1",
        "authors": [
            "Kiyotada Mori",
            "Seiya Kawano",
            "Angel Fernando Garcia Contreras",
            "Koichiro Yoshino"
        ],
        "submitted": "2025-08-06 12:45:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems",
        "abstract": "Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at\nthe front end of their pipeline. The role of ASR in SDSs is to recognize\ninformation in user speech related to response generation appropriately.\nExamining selective listening of humans, which refers to the ability to focus\non and listen to important parts of a conversation during the speech, will\nenable us to identify the ASR capabilities required for SDSs and evaluate them.\nIn this study, we experimentally confirmed selective listening when humans\ngenerate dialogue responses by comparing human transcriptions for generating\ndialogue responses and reference transcriptions. Based on our experimental\nresults, we discuss the possibility of a new ASR evaluation method that\nleverages human selective listening, which can identify the gap between\ntranscription ability between ASR systems and humans.",
        "url": "http://arxiv.org/abs/2508.04402v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04402v1",
        "arxiv_id": "2508.04402v1",
        "authors": [
            "Kiyotada Mori",
            "Seiya Kawano",
            "Chaoran Liu",
            "Carlos Toshinori Ishi",
            "Angel Fernando Garcia Contreras",
            "Koichiro Yoshino"
        ],
        "submitted": "2025-08-06 12:44:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Why are LLMs' abilities emergent?",
        "abstract": "The remarkable success of Large Language Models (LLMs) in generative tasks\nhas raised fundamental questions about the nature of their acquired\ncapabilities, which often appear to emerge unexpectedly without explicit\ntraining. This paper examines the emergent properties of Deep Neural Networks\n(DNNs) through both theoretical analysis and empirical observation, addressing\nthe epistemological challenge of \"creation without understanding\" that\ncharacterises contemporary AI development. We explore how the neural approach's\nreliance on nonlinear, stochastic processes fundamentally differs from symbolic\ncomputational paradigms, creating systems whose macro-level behaviours cannot\nbe analytically derived from micro-level neuron activities. Through analysis of\nscaling laws, grokking phenomena, and phase transitions in model capabilities,\nI demonstrate that emergent abilities arise from the complex dynamics of highly\nsensitive nonlinear systems rather than simply from parameter scaling alone. My\ninvestigation reveals that current debates over metrics, pre-training loss\nthresholds, and in-context learning miss the fundamental ontological nature of\nemergence in DNNs. I argue that these systems exhibit genuine emergent\nproperties analogous to those found in other complex natural phenomena, where\nsystemic capabilities emerge from cooperative interactions among simple\ncomponents without being reducible to their individual behaviours. The paper\nconcludes that understanding LLM capabilities requires recognising DNNs as a\nnew domain of complex dynamical systems governed by universal principles of\nemergence, similar to those operating in physics, chemistry, and biology. This\nperspective shifts the focus from purely phenomenological definitions of\nemergence to understanding the internal dynamic transformations that enable\nthese systems to acquire capabilities that transcend their individual\ncomponents.",
        "url": "http://arxiv.org/abs/2508.04401v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04401v1",
        "arxiv_id": "2508.04401v1",
        "authors": [
            "Vladimír Havlík"
        ],
        "submitted": "2025-08-06 12:43:04",
        "source": "arxiv",
        "comment": "20 pages"
    },
    {
        "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky",
        "abstract": "This study evaluates advanced natural language processing (NLP) techniques to\nenhance crash data quality by mining crash narratives, using secondary crash\nidentification in Kentucky as a case study. Drawing from 16,656 manually\nreviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we\ncompare three model classes: zero-shot open-source large language models (LLMs)\n(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers\n(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic\nregression as baseline. Models were calibrated on 2015-2021 data and tested on\n1,771 narratives from 2022. Fine-tuned transformers achieved superior\nperformance, with RoBERTa yielding the highest F1-score (0.90) and accuracy\n(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139\nminutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs\nexcelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred\nhigh computational costs (up to 723 minutes for DeepSeek-R1:70B), while\nfine-tuned models processed the test set in seconds after brief training.\nFurther analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can\nrival larger counterparts in performance while reducing runtime, suggesting\nopportunities for optimized deployments. Results highlight trade-offs between\naccuracy, efficiency, and data requirements, with fine-tuned transformer models\nbalancing precision and recall effectively on Kentucky data. Practical\ndeployment considerations emphasize privacy-preserving local deployment,\nensemble approaches for improved accuracy, and incremental processing for\nscalability, providing a replicable scheme for enhancing crash-data quality\nwith advanced NLP.",
        "url": "http://arxiv.org/abs/2508.04399v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04399v1",
        "arxiv_id": "2508.04399v1",
        "authors": [
            "Xu Zhang",
            "Mei Chen"
        ],
        "submitted": "2025-08-06 12:41:18",
        "source": "arxiv",
        "comment": "19 pages, 2 figures"
    },
    {
        "title": "Chain of Questions: Guiding Multimodal Curiosity in Language Models",
        "abstract": "Reasoning capabilities in large language models (LLMs) have substantially\nadvanced through methods such as chain-of-thought and explicit step-by-step\nexplanations. However, these improvements have not yet fully transitioned to\nmultimodal contexts, where models must proactively decide which sensory\nmodalities such as vision, audio, or spatial perception to engage when\ninteracting with complex real-world environments. In this paper, we introduce\nthe Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach\nthat encourages multimodal language models to dynamically generate targeted\nquestions regarding their surroundings. These generated questions guide the\nmodel to selectively activate relevant modalities, thereby gathering critical\ninformation necessary for accurate reasoning and response generation. We\nevaluate our framework on a novel multimodal benchmark dataset, assembled by\nintegrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results\ndemonstrate that our CoQ method improves a foundation model's ability to\neffectively identify and integrate pertinent sensory information. This leads to\nimproved accuracy, interpretability, and alignment of the reasoning process\nwith diverse multimodal tasks.",
        "url": "http://arxiv.org/abs/2508.04350v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04350v1",
        "arxiv_id": "2508.04350v1",
        "authors": [
            "Nima Iji",
            "Kia Dashtipour"
        ],
        "submitted": "2025-08-06 11:42:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy",
        "abstract": "Reinforcement learning (RL) with algorithms like Group Relative Policy\nOptimization (GRPO) improves Large Language Model (LLM) reasoning, but is\nlimited by a coarse-grained credit assignment that applies a uniform reward to\nall tokens in a sequence. This is a major flaw in long-chain reasoning tasks.\nThis paper solves this with \\textbf{Dynamic Entropy Weighting}. Our core idea\nis that high-entropy tokens in correct responses can guide the policy toward a\nhigher performance ceiling. This allows us to create more fine-grained reward\nsignals for precise policy updates via two ways: 1) \\textbf{Group Token Policy\nOptimization} (\\textbf{GTPO}), we assigns a entropy-weighted reward to each\ntoken for fine-grained credit assignment. 2) \\textbf{Sequence-Level Group\nRelative Policy Optimization} (\\textbf{GRPO-S}), we assigns a entropy-weighted\nreward to each sequence based on its average token entropy. Experiments show\nour methods significantly outperform the strong DAPO baseline. The results\nconfirm that our entropy-weighting mechanism is the key driver of this\nperformance boost, offering a better path to enhance deep reasoning in models.",
        "url": "http://arxiv.org/abs/2508.04349v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04349v1",
        "arxiv_id": "2508.04349v1",
        "authors": [
            "Hongze Tan",
            "Jianfei Pan"
        ],
        "submitted": "2025-08-06 11:42:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Modelling and Classifying the Components of a Literature Review",
        "abstract": "Previous work has demonstrated that AI methods for analysing scientific\nliterature benefit significantly from annotating sentences in papers according\nto their rhetorical roles, such as research gaps, results, limitations,\nextensions of existing methodologies, and others. Such representations also\nhave the potential to support the development of a new generation of systems\ncapable of producing high-quality literature reviews. However, achieving this\ngoal requires the definition of a relevant annotation schema and effective\nstrategies for large-scale annotation of the literature. This paper addresses\nthese challenges by 1) introducing a novel annotation schema specifically\ndesigned to support literature review generation and 2) conducting a\ncomprehensive evaluation of a wide range of state-of-the-art large language\nmodels (LLMs) in classifying rhetorical roles according to this schema. To this\nend, we also present Sci-Sentence, a novel multidisciplinary benchmark\ncomprising 700 sentences manually annotated by domain experts and 2,240\nsentences automatically labelled using LLMs. We evaluate 37 LLMs on this\nbenchmark, spanning diverse model families and sizes, using both zero-shot\nlearning and fine-tuning approaches. The experiments yield several novel\ninsights that advance the state of the art in this challenging domain. First,\nthe current generation of LLMs performs remarkably well on this task when\nfine-tuned on high-quality data, achieving performance levels above 96\\% F1.\nSecond, while large proprietary models like GPT-4o achieve the best results,\nsome lightweight open-source alternatives also demonstrate excellent\nperformance. Finally, enriching the training data with semi-synthetic examples\ngenerated by LLMs proves beneficial, enabling small encoders to achieve robust\nresults and significantly enhancing the performance of several open decoder\nmodels.",
        "url": "http://arxiv.org/abs/2508.04337v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04337v1",
        "arxiv_id": "2508.04337v1",
        "authors": [
            "Francisco Bolaños",
            "Angelo Salatino",
            "Francesco Osborne",
            "Enrico Motta"
        ],
        "submitted": "2025-08-06 11:30:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models",
        "abstract": "Large language models (LLMs) show significant potential in healthcare,\nprompting numerous benchmarks to evaluate their capabilities. However, concerns\npersist regarding the reliability of these benchmarks, which often lack\nclinical fidelity, robust data management, and safety-oriented evaluation\nmetrics. To address these shortcomings, we introduce MedCheck, the first\nlifecycle-oriented assessment framework specifically designed for medical\nbenchmarks. Our framework deconstructs a benchmark's development into five\ncontinuous stages, from design to governance, and provides a comprehensive\nchecklist of 46 medically-tailored criteria. Using MedCheck, we conducted an\nin-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis\nuncovers widespread, systemic issues, including a profound disconnect from\nclinical practice, a crisis of data integrity due to unmitigated contamination\nrisks, and a systematic neglect of safety-critical evaluation dimensions like\nmodel robustness and uncertainty awareness. Based on these findings, MedCheck\nserves as both a diagnostic tool for existing benchmarks and an actionable\nguideline to foster a more standardized, reliable, and transparent approach to\nevaluating AI in healthcare.",
        "url": "http://arxiv.org/abs/2508.04325v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04325v1",
        "arxiv_id": "2508.04325v1",
        "authors": [
            "Zizhan Ma",
            "Wenxuan Wang",
            "Guo Yu",
            "Yiu-Fai Cheung",
            "Meidan Ding",
            "Jie Liu",
            "Wenting Chen",
            "Linlin Shen"
        ],
        "submitted": "2025-08-06 11:11:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Comparative Analysis of Novel NIRMAL Optimizer Against Adam and SGD with Momentum",
        "abstract": "This study proposes NIRMAL (Novel Integrated Robust Multi-Adaptation\nLearning), a novel optimization algorithm that combines multiple strategies\ninspired by the movements of the chess piece. These strategies include gradient\ndescent, momentum, stochastic perturbations, adaptive learning rates, and\nnon-linear transformations. We carefully evaluated NIRMAL against two widely\nused and successful optimizers, Adam and SGD with Momentum, on four benchmark\nimage classification datasets: MNIST, FashionMNIST, CIFAR-10, and CIFAR-100.\nThe custom convolutional neural network (CNN) architecture is applied on each\ndataset. The experimental results show that NIRMAL achieves competitive\nperformance, particularly on the more challenging CIFAR-100 dataset, where it\nachieved a test accuracy of 45.32\\%and a weighted F1-score of 0.4328. This\nperformance surpasses Adam (41.79\\% accuracy, 0.3964 F1-score) and closely\nmatches SGD with Momentum (46.97\\% accuracy, 0.4531 F1-score). Also, NIRMAL\nexhibits robust convergence and strong generalization capabilities, especially\non complex datasets, as evidenced by stable training results in loss and\naccuracy curves. These findings underscore NIRMAL's significant ability as a\nversatile and effective optimizer for various deep learning tasks.",
        "url": "http://arxiv.org/abs/2508.04293v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04293v1",
        "arxiv_id": "2508.04293v1",
        "authors": [
            "Nirmal Gaud",
            "Surej Mouli",
            "Preeti Katiyar",
            "Vaduguru Venkata Ramya"
        ],
        "submitted": "2025-08-06 10:30:22",
        "source": "arxiv",
        "comment": "9 pages, 12 figures"
    },
    {
        "title": "A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models",
        "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as\na promising paradigm for enhancing large language models (LLMs) by converting\nraw text into structured knowledge graphs, improving both accuracy and\nexplainability. However, GraphRAG relies on LLMs to extract knowledge from raw\ntext during graph construction, and this process can be maliciously manipulated\nto implant misleading information. Targeting this attack surface, we propose\ntwo knowledge poisoning attacks (KPAs) and demonstrate that modifying only a\nfew words in the source text can significantly change the constructed graph,\npoison the GraphRAG, and severely mislead downstream reasoning. The first\nattack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate\nvulnerable nodes in the generated graphs and rewrites the corresponding\nnarratives with LLMs, achieving precise control over specific\nquestion-answering (QA) outcomes with a success rate of 93.1\\%, while keeping\nthe poisoned text fluent and natural. The second attack, named Universal KPA\n(UKPA), exploits linguistic cues such as pronouns and dependency relations to\ndisrupt the structural integrity of the generated graph by altering globally\ninfluential words. With fewer than 0.05\\% of full text modified, the QA\naccuracy collapses from 95\\% to 50\\%. Furthermore, experiments show that\nstate-of-the-art defense methods fail to detect these attacks, highlighting\nthat securing GraphRAG pipelines against knowledge poisoning remains largely\nunexplored.",
        "url": "http://arxiv.org/abs/2508.04276v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04276v1",
        "arxiv_id": "2508.04276v1",
        "authors": [
            "Jiayi Wen",
            "Tianxin Chen",
            "Zhirun Zheng",
            "Cheng Huang"
        ],
        "submitted": "2025-08-06 10:01:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval",
        "abstract": "Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically\nrelated to the given query. To tackle this task, most existing VMR methods\nsolely focus on the visual and textual modalities while neglecting the\ncomplementary but important audio modality. Although a few recent works try to\ntackle the joint audio-vision-text reasoning, they treat all modalities equally\nand simply embed them without fine-grained interaction for moment retrieval.\nThese designs are counter-practical as: Not all audios are helpful for video\nmoment retrieval, and the audio of some videos may be complete noise or\nbackground sound that is meaningless to the moment determination. To this end,\nwe propose a novel Importance-aware Multi-Granularity fusion model (IMG), which\nlearns to dynamically and selectively aggregate the audio-vision-text contexts\nfor VMR. Specifically, after integrating the textual guidance with vision and\naudio separately, we first design a pseudo-label-supervised audio importance\npredictor that predicts the importance score of the audio, and accordingly\nassigns weights to mitigate the interference caused by noisy audio. Then, we\ndesign a multi-granularity audio fusion module that adaptively fuses audio and\nvisual modalities at local-, event-, and global-level, fully capturing their\ncomplementary contexts. We further propose a cross-modal knowledge distillation\nstrategy to address the challenge of missing audio modality during inference.\nTo evaluate our method, we further construct a new VMR dataset, i.e.,\nCharades-AudioMatter, where audio-related samples are manually selected and\nre-organized from the original Charades-STA to validate the model's capability\nin utilizing audio modality. Extensive experiments validate the effectiveness\nof our method, achieving state-of-the-art with audio-video fusion in VMR\nmethods. Our code is available at https://github.com/HuiGuanLab/IMG.",
        "url": "http://arxiv.org/abs/2508.04273v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04273v1",
        "arxiv_id": "2508.04273v1",
        "authors": [
            "Junan Lin",
            "Daizong Liu",
            "Xianke Chen",
            "Xiaoye Qu",
            "Xun Yang",
            "Jixiang Zhu",
            "Sanyuan Zhang",
            "Jianfeng Dong"
        ],
        "submitted": "2025-08-06 09:58:43",
        "source": "arxiv",
        "comment": "Accepted to ACM MM 2025"
    },
    {
        "title": "ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents",
        "abstract": "Existing benchmarks in e-commerce primarily focus on basic user intents, such\nas finding or purchasing products. However, real-world users often pursue more\ncomplex goals, such as applying vouchers, managing budgets, and finding\nmulti-products seller. To bridge this gap, we propose ShoppingBench, a novel\nend-to-end shopping benchmark designed to encompass increasingly challenging\nlevels of grounded intent. Specifically, we propose a scalable framework to\nsimulate user instructions based on various intents derived from sampled\nreal-world products. To facilitate consistent and reliable evaluations, we\nprovide a large-scale shopping sandbox that serves as an interactive simulated\nenvironment, incorporating over 2.5 million real-world products. Experimental\nresults demonstrate that even state-of-the-art language agents (such as\nGPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,\nhighlighting the significant challenges posed by our ShoppingBench. In\naddition, we propose a trajectory distillation strategy and leverage supervised\nfine-tuning, along with reinforcement learning on synthetic trajectories, to\ndistill the capabilities of a large language agent into a smaller one. As a\nresult, our trained agent achieves competitive performance compared to GPT-4.1.",
        "url": "http://arxiv.org/abs/2508.04266v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04266v1",
        "arxiv_id": "2508.04266v1",
        "authors": [
            "Jiangyuan Wang",
            "Kejun Xiao",
            "Qi Sun",
            "Huaipeng Zhao",
            "Tao Luo",
            "Jiandong Zhang",
            "Xiaoyi Zeng"
        ],
        "submitted": "2025-08-06 09:51:30",
        "source": "arxiv",
        "comment": "submit to AAAI2026"
    },
    {
        "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs",
        "abstract": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
        "url": "http://arxiv.org/abs/2508.04257v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04257v1",
        "arxiv_id": "2508.04257v1",
        "authors": [
            "Zunhai Su",
            "Kehong Yuan"
        ],
        "submitted": "2025-08-06 09:40:09",
        "source": "arxiv",
        "comment": "Published as a conference paper at COLM 2025"
    },
    {
        "title": "Graph Representation Learning with Massive Unlabeled Data for Rumor Detection",
        "abstract": "With the development of social media, rumors spread quickly, cause great harm\nto society and economy. Thereby, many effective rumor detection methods have\nbeen developed, among which the rumor propagation structure learning based\nmethods are particularly effective compared to other methods. However, the\nexisting methods still suffer from many issues including the difficulty to\nobtain large-scale labeled rumor datasets, which leads to the low\ngeneralization ability and the performance degeneration on new events since\nrumors are time-critical and usually appear with hot topics or newly emergent\nevents. In order to solve the above problems, in this study, we used\nlarge-scale unlabeled topic datasets crawled from the social media platform\nWeibo and Twitter with claim propagation structure to improve the semantic\nlearning ability of a graph reprentation learing model on various topics. We\nuse three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE\nin two commonly used training strategies, to verify the performance of general\ngraph semi-supervised methods in rumor detection tasks. In addition, for\nalleviating the time and topic difference between unlabeled topic data and\nrumor data, we also collected a rumor dataset covering a variety of topics over\na decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our\nexperiments show that these general graph self-supervised learning methods\noutperform previous methods specifically designed for rumor detection tasks and\nachieve good performance under few-shot conditions, demonstrating the better\ngeneralization ability with the help of our massive unlabeled topic dataset.",
        "url": "http://arxiv.org/abs/2508.04252v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04252v1",
        "arxiv_id": "2508.04252v1",
        "authors": [
            "Chaoqun Cui",
            "Caiyan Jia"
        ],
        "submitted": "2025-08-06 09:33:56",
        "source": "arxiv",
        "comment": "9 pages, 3 figures"
    },
    {
        "title": "TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening",
        "abstract": "The increasing demand for mental health services has outpaced the\navailability of real training data to develop clinical professionals, leading\nto limited support for the diagnosis of depression. This shortage has motivated\nthe development of simulated or virtual patients to assist in training and\nevaluation, but existing approaches often fail to generate clinically valid,\nnatural, and diverse symptom presentations. In this work, we embrace the recent\nadvanced language models as the backbone and propose a novel\nclinician-in-the-loop patient simulation pipeline, TalkDep, with access to\ndiversified patient profiles to develop simulated patients. By conditioning the\nmodel on psychiatric diagnostic criteria, symptom severity scales, and\ncontextual factors, our goal is to create authentic patient responses that can\nbetter support diagnostic model training and evaluation. We verify the\nreliability of these simulated patients with thorough assessments conducted by\nclinical professionals. The availability of validated simulated patients offers\na scalable and adaptable resource for improving the robustness and\ngeneralisability of automatic depression diagnosis systems.",
        "url": "http://arxiv.org/abs/2508.04248v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04248v1",
        "arxiv_id": "2508.04248v1",
        "authors": [
            "Xi Wang",
            "Anxo Perez",
            "Javier Parapar",
            "Fabio Crestani"
        ],
        "submitted": "2025-08-06 09:30:47",
        "source": "arxiv",
        "comment": "Paper accepted at CIKM 2025"
    },
    {
        "title": "I$^3$-MRec: Invariant Learning with Information Bottleneck for Incomplete Modality Recommendation",
        "abstract": "Multimodal recommender systems (MRS) improve recommendation performance by\nintegrating diverse semantic information from multiple modalities. However, the\nassumption of the availability of all modalities rarely holds in practice due\nto missing images, incomplete descriptions, or inconsistent user content. These\nchallenges significantly degrade the robustness and generalization capabilities\nof current models. To address these challenges, we introduce a novel method\ncalled \\textbf{I$^3$-MRec}, which uses \\textbf{I}nvariant learning with\n\\textbf{I}nformation bottleneck principle for \\textbf{I}ncomplete\n\\textbf{M}odality \\textbf{Rec}ommendation. To achieve robust performance in\nmissing modality scenarios, I$^3$-MRec enforces two pivotal properties: (i)\ncross-modal preference invariance, which ensures consistent user preference\nmodeling across varying modality environments, and (ii) compact yet effective\nmodality representation, which filters out task-irrelevant modality information\nwhile maximally preserving essential features relevant to recommendation. By\ntreating each modality as a distinct semantic environment, I$^3$-MRec employs\ninvariant risk minimization (IRM) to learn modality-specific item\nrepresentations. In parallel, a missing-aware fusion module grounded in the\nInformation Bottleneck (IB) principle extracts compact and effective item\nembeddings by suppressing modality noise and preserving core user preference\nsignals. Extensive experiments conducted on three real-world datasets\ndemonstrate that I$^3$-MRec consistently outperforms existing state-of-the-art\nMRS methods across various modality-missing scenarios, highlighting its\neffectiveness and robustness in practical applications. The code and processed\ndatasets are released at https://github.com/HuilinChenJN/I3-MRec.",
        "url": "http://arxiv.org/abs/2508.04247v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04247v1",
        "arxiv_id": "2508.04247v1",
        "authors": [
            "Huilin Chen",
            "Miaomiao Cai",
            "Fan Liu",
            "Zhiyong Cheng",
            "Richang Hong",
            "Meng Wang"
        ],
        "submitted": "2025-08-06 09:29:50",
        "source": "arxiv",
        "comment": "ACM Multimedia 2025 Accepted"
    },
    {
        "title": "DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting",
        "abstract": "Time series forecasting is crucial in strategic planning and decision-making\nacross various industries. Traditional forecasting models mainly concentrate on\nnumerical time series data, often overlooking important textual information\nsuch as events and news, which can significantly affect forecasting accuracy.\nWhile large language models offer a promise for integrating multimodal data,\nexisting single-prompt frameworks struggle to effectively capture the semantics\nof timestamped text, introducing redundant information that can hinder model\nperformance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt\nGPT2-base for Multimodal Time Series), a novel dual-prompt large language model\nframework that combines two complementary prompts: an explicit prompt for clear\ntask instructions and a textual prompt for context-aware embeddings from\ntime-stamped data. The tokenizer generates the explicit prompt while the\nembeddings from the textual prompt are refined through self-attention and\nfeed-forward networks. Comprehensive experiments conducted on diverse\ntextural-numerical time series datasets demonstrate that this approach\noutperforms state-of-the-art algorithms in time series forecasting. This\nhighlights the significance of incorporating textual context via a dual-prompt\nmechanism to achieve more accurate time series predictions.",
        "url": "http://arxiv.org/abs/2508.04239v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04239v1",
        "arxiv_id": "2508.04239v1",
        "authors": [
            "Chanjuan Liu",
            "Shengzhi Wang",
            "Enqiang Zhu"
        ],
        "submitted": "2025-08-06 09:25:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains",
        "abstract": "Recommender systems learn from past user behavior to predict future user\npreferences. Intuitively, it has been established that the most recent\ninteractions are more indicative of future preferences than older interactions.\nMany recommendation algorithms use this notion to either drop older\ninteractions or to assign them a lower weight, so the model can focus on the\nmore informative, recent information. However, very few approaches model the\nflow of time explicitly.\n  This paper analyzes how time can be encoded in factorization-style\nrecommendation models. By including absolute time as a feature, our models can\nlearn varying user preferences and changing item perception over time. In\naddition to simple binning approaches, we also propose a novel, fully\ncontinuous time encoding mechanism. Through the use of a polynomial fit inside\nthe loss function, our models completely avoid the need for discretization, and\nthey are able to capture the time dimension in arbitrary resolution.\n  We perform a comparative study on three real-world datasets that span\nmultiple years, where long user histories are present, and items stay relevant\nfor a longer time. Empirical results show that, by explicitly modeling time,\nour models are very effective at capturing temporal signals, such as varying\nitem popularities over time. Despite this however, our experiments also\nindicate that a simple post-hoc popularity adjustment is often sufficient to\nachieve the best performance on the unseen test set. This teaches us that, for\nthe recommendation task, predicting the future is more important than capturing\npast trends. As such, we argue that specialized mechanisms are needed for\nextrapolation to future data.",
        "url": "http://arxiv.org/abs/2508.04221v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04221v1",
        "arxiv_id": "2508.04221v1",
        "authors": [
            "Joey De Pauw",
            "Bart Goethals"
        ],
        "submitted": "2025-08-06 08:54:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hierarchical Text Classification Using Black Box Large Language Models",
        "abstract": "Hierarchical Text Classification (HTC) aims to assign texts to structured\nlabel hierarchies; however, it faces challenges due to data scarcity and model\ncomplexity. This study explores the feasibility of using black box Large\nLanguage Models (LLMs) accessed via APIs for HTC, as an alternative to\ntraditional machine learning methods that require extensive labeled data and\ncomputational resources. We evaluate three prompting strategies -- Direct Leaf\nLabel Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down\nMulti-step Hierarchical Label Prediction (TMH) -- in both zero-shot and\nfew-shot settings, comparing the accuracy and cost-effectiveness of these\nstrategies. Experiments on two datasets show that a few-shot setting\nconsistently improves classification accuracy compared to a zero-shot setting.\nWhile a traditional machine learning model achieves high accuracy on a dataset\nwith a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the\nmachine learning model on a dataset with a deeper hierarchy. API costs increase\nsignificantly due to the higher input tokens required for deeper label\nhierarchies on DH strategy. These results emphasize the trade-off between\naccuracy improvement and the computational cost of prompt strategy. These\nfindings highlight the potential of black box LLMs for HTC while underscoring\nthe need to carefully select a prompt strategy to balance performance and cost.",
        "url": "http://arxiv.org/abs/2508.04219v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04219v1",
        "arxiv_id": "2508.04219v1",
        "authors": [
            "Kosuke Yoshimura",
            "Hisashi Kashima"
        ],
        "submitted": "2025-08-06 08:53:50",
        "source": "arxiv",
        "comment": "16 pages, 6 figures"
    },
    {
        "title": "A Hybrid AI Methodology for Generating Ontologies of Research Topics from Scientific Paper Corpora",
        "abstract": "Taxonomies and ontologies of research topics (e.g., MeSH, UMLS, CSO, NLM)\nplay a central role in providing the primary framework through which\nintelligent systems can explore and interpret the literature. However, these\nresources have traditionally been manually curated, a process that is\ntime-consuming, prone to obsolescence, and limited in granularity. This paper\npresents Sci-OG, a semi-auto\\-mated methodology for generating research topic\nontologies, employing a multi-step approach: 1) Topic Discovery, extracting\npotential topics from research papers; 2) Relationship Classification,\ndetermining semantic relationships between topic pairs; and 3) Ontology\nConstruction, refining and organizing topics into a structured ontology. The\nrelationship classification component, which constitutes the core of the\nsystem, integrates an encoder-based language model with features describing\ntopic occurrence in the scientific literature. We evaluate this approach\nagainst a range of alternative solutions using a dataset of 21,649 manually\nannotated semantic triples. Our method achieves the highest F1 score (0.951),\nsurpassing various competing approaches, including a fine-tuned SciBERT model\nand several LLM baselines, such as the fine-tuned GPT4-mini. Our work is\ncorroborated by a use case which illustrates the practical application of our\nsystem to extend the CSO ontology in the area of cybersecurity. The presented\nsolution is designed to improve the accessibility, organization, and analysis\nof scientific knowledge, thereby supporting advancements in AI-enabled\nliterature management and research exploration.",
        "url": "http://arxiv.org/abs/2508.04213v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04213v1",
        "arxiv_id": "2508.04213v1",
        "authors": [
            "Alessia Pisu",
            "Livio Pompianu",
            "Francesco Osborne",
            "Diego Reforgiato Recupero",
            "Daniele Riboni",
            "Angelo Salatino"
        ],
        "submitted": "2025-08-06 08:48:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation",
        "abstract": "Recommending long-form video content demands joint modeling of visual, audio,\nand textual modalities, yet most benchmarks address only raw features or narrow\nfusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for\nLLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,\nit aligns dense item embeddings from three modalities: audio (block-level,\ni-vector), visual (CNN, AVF), and text. Missing or sparse metadata is\nautomatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),\ngenerating high-quality synopses for thousands of movies. All text (raw or\naugmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),\nproducing multiple ready-to-use sets. The pipeline supports interchangeable\nearly-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and\nmultiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are\nfully declarative via a single YAML file. Evaluation spans accuracy (Recall,\nnDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,\ndiversity, fairness. Results show LLM-based augmentation and strong text\nembeddings boost cold-start and coverage, especially when fused with\naudio-visual features. Systematic benchmarking reveals universal versus\nbackbone- or metric-specific combinations. Open-source code, embeddings, and\nconfigs enable reproducible, fair multimodal RS research and advance principled\ngenerative AI integration in large-scale recommendation. Code:\nhttps://recsys-lab.github.io/ViLLA-MMBench",
        "url": "http://arxiv.org/abs/2508.04206v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04206v1",
        "arxiv_id": "2508.04206v1",
        "authors": [
            "Fatemeh Nazary",
            "Ali Tourani",
            "Yashar Deldjoo",
            "Tommaso Di Noia"
        ],
        "submitted": "2025-08-06 08:39:07",
        "source": "arxiv",
        "comment": "17 pages, 3 figures, 5 tables"
    },
    {
        "title": "ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated impressive performance in\nreasoning-intensive tasks, but they remain vulnerable to harmful content\ngeneration, particularly in the mid-to-late steps of their reasoning processes.\nExisting defense mechanisms, however, rely on costly fine-tuning and additional\nexpert knowledge, which restricts their scalability. In this work, we propose\nReasoningGuard, an inference-time safeguard for LRMs, which injects timely\nsafety aha moments to steer harmless while helpful reasoning processes.\nLeveraging the model's internal attention behavior, our approach accurately\nidentifies critical points in the reasoning path, and triggers spontaneous,\nsafety-oriented reflection. To safeguard both the subsequent reasoning steps\nand the final answers, we further implement a scaling sampling strategy during\nthe decoding phase, selecting the optimal reasoning path. Inducing minimal\nextra inference cost, ReasoningGuard effectively mitigates three types of\njailbreak attacks, including the latest ones targeting the reasoning process of\nLRMs. Our approach outperforms seven existing safeguards, achieving\nstate-of-the-art safety defenses while effectively avoiding the common\nexaggerated safety issues.",
        "url": "http://arxiv.org/abs/2508.04204v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04204v1",
        "arxiv_id": "2508.04204v1",
        "authors": [
            "Yuquan Wang",
            "Mi Zhang",
            "Yining Wang",
            "Geng Hong",
            "Xiaoyu You",
            "Min Yang"
        ],
        "submitted": "2025-08-06 08:35:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts",
        "abstract": "Sentiment analysis in low-resource, culturally nuanced contexts challenges\nconventional NLP approaches that assume fixed labels and universal affective\nexpressions. We present a diagnostic framework that treats sentiment as a\ncontext-dependent, culturally embedded construct, and evaluate how large\nlanguage models (LLMs) reason about sentiment in informal, code-mixed WhatsApp\nmessages from Nairobi youth health groups. Using a combination of\nhuman-annotated data, sentiment-flipped counterfactuals, and rubric-based\nexplanation evaluation, we probe LLM interpretability, robustness, and\nalignment with human reasoning. Framing our evaluation through a social-science\nmeasurement lens, we operationalize and interrogate LLMs outputs as an\ninstrument for measuring the abstract concept of sentiment. Our findings reveal\nsignificant variation in model reasoning quality, with top-tier LLMs\ndemonstrating interpretive stability, while open models often falter under\nambiguity or sentiment shifts. This work highlights the need for culturally\nsensitive, reasoning-aware AI evaluation in complex, real-world communication.",
        "url": "http://arxiv.org/abs/2508.04199v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04199v1",
        "arxiv_id": "2508.04199v1",
        "authors": [
            "Millicent Ochieng",
            "Anja Thieme",
            "Ignatius Ezeani",
            "Risa Ueno",
            "Samuel Maina",
            "Keshet Ronen",
            "Javier Gonzalez",
            "Jacki O'Neill"
        ],
        "submitted": "2025-08-06 08:27:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models",
        "abstract": "Despite significant advances in alignment techniques, we demonstrate that\nstate-of-the-art language models remain vulnerable to carefully crafted\nconversational scenarios that can induce various forms of misalignment without\nexplicit jailbreaking. Through systematic manual red-teaming with\nClaude-4-Opus, we discovered 10 successful attack scenarios, revealing\nfundamental vulnerabilities in how current alignment methods handle narrative\nimmersion, emotional pressure, and strategic framing. These scenarios\nsuccessfully elicited a range of misaligned behaviors, including deception,\nvalue drift, self-preservation, and manipulative reasoning, each exploiting\ndifferent psychological and contextual vulnerabilities. To validate\ngeneralizability, we distilled our successful manual attacks into\nMISALIGNMENTBENCH, an automated evaluation framework that enables reproducible\ntesting across multiple models. Cross-model evaluation of our 10 scenarios\nagainst five frontier LLMs revealed an overall 76% vulnerability rate, with\nsignificant variations: GPT-4.1 showed the highest susceptibility (90%), while\nClaude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate\nthat sophisticated reasoning capabilities often become attack vectors rather\nthan protective mechanisms, as models can be manipulated into complex\njustifications for misaligned behavior. This work provides (i) a detailed\ntaxonomy of conversational manipulation patterns and (ii) a reusable evaluation\nframework. Together, these findings expose critical gaps in current alignment\nstrategies and highlight the need for robustness against subtle, scenario-based\nmanipulation in future AI systems.",
        "url": "http://arxiv.org/abs/2508.04196v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04196v1",
        "arxiv_id": "2508.04196v1",
        "authors": [
            "Siddhant Panpatil",
            "Hiskias Dingeto",
            "Haon Park"
        ],
        "submitted": "2025-08-06 08:25:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Characterizing Deep Research: A Benchmark and Formal Definition",
        "abstract": "Information tasks such as writing surveys or analytical reports require\ncomplex search and reasoning, and have recently been grouped under the umbrella\nof \\textit{deep research} -- a term also adopted by recent models targeting\nthese capabilities. Despite growing interest, the scope of the deep research\ntask remains underdefined and its distinction from other reasoning-intensive\nproblems is poorly understood. In this paper, we propose a formal\ncharacterization of the deep research (DR) task and introduce a benchmark to\nevaluate the performance of DR systems. We argue that the core defining feature\nof deep research is not the production of lengthy report-style outputs, but\nrather the high fan-out over concepts required during the search process, i.e.,\nbroad and reasoning-intensive exploration. To enable objective evaluation, we\ndefine DR using an intermediate output representation that encodes key claims\nuncovered during search-separating the reasoning challenge from surface-level\nreport generation. Based on this formulation, we propose a diverse, challenging\nbenchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,\ndatasets, materials discovery, prior art search) and public interest events\n(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1\nscore ranges between 0.02 and 0.72 for any sub-category. OpenAI's model\nperforms the best with an overall F1 score of 0.55. Analysis of reasoning\ntraces reveals the distribution over the number of referenced sources,\nbranching, and backtracking events executed by current DR systems, motivating\nfuture directions for improving their search mechanisms and grounding\ncapabilities. The benchmark is available at\nhttps://github.com/microsoft/LiveDRBench.",
        "url": "http://arxiv.org/abs/2508.04183v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04183v1",
        "arxiv_id": "2508.04183v1",
        "authors": [
            "Abhinav Java",
            "Ashmit Khandelwal",
            "Sukruta Midigeshi",
            "Aaron Halfaker",
            "Amit Deshpande",
            "Navin Goyal",
            "Ankur Gupta",
            "Nagarajan Natarajan",
            "Amit Sharma"
        ],
        "submitted": "2025-08-06 08:09:28",
        "source": "arxiv",
        "comment": "First three authors contributed equally (ordered alphabetically)"
    },
    {
        "title": "Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across vision-language tasks. However, they may suffer from\nhallucinations--generating outputs that are semantically inconsistent with the\ninput image or text. Through causal analyses, we find that: (i) hallucinations\nwith omission may arise from the failure to adequately capture essential causal\nfactors, and (ii) hallucinations with fabrication are likely caused by the\nmodel being misled by non-causal cues. To address these challenges, we propose\na novel reinforcement learning framework guided by causal completeness, which\njointly considers both causal sufficiency and causal necessity of tokens.\nSpecifically, we evaluate each token's standalone contribution and\ncounterfactual indispensability to define a token-level causal completeness\nreward. This reward is used to construct a causally informed advantage function\nwithin the GRPO optimization framework, encouraging the model to focus on\ntokens that are both causally sufficient and necessary for accurate generation.\nExperimental results across various benchmark datasets and tasks demonstrate\nthe effectiveness of our approach, which effectively mitigates hallucinations\nin MLLMs.",
        "url": "http://arxiv.org/abs/2508.04182v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04182v1",
        "arxiv_id": "2508.04182v1",
        "authors": [
            "Peizheng Guo",
            "Jingyao Wang",
            "Wenwen Qiang",
            "Huijie Guo",
            "Changwen Zheng",
            "Jiahuan Zhou",
            "Gang Hua"
        ],
        "submitted": "2025-08-06 08:09:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The State Of TTS: A Case Study with Human Fooling Rates",
        "abstract": "While subjective evaluations in recent years indicate rapid progress in TTS,\ncan current TTS systems truly pass a human deception test in a Turing-like\nevaluation? We introduce Human Fooling Rate (HFR), a metric that directly\nmeasures how often machine-generated speech is mistaken for human. Our\nlarge-scale evaluation of open-source and commercial TTS models reveals\ncritical insights: (i) CMOS-based claims of human parity often fail under\ndeception testing, (ii) TTS progress should be benchmarked on datasets where\nhuman speech achieves high HFRs, as evaluating against monotonous or less\nexpressive reference samples sets a low bar, (iii) Commercial models approach\nhuman deception in zero-shot settings, while open-source systems still struggle\nwith natural conversational speech; (iv) Fine-tuning on high-quality data\nimproves realism but does not fully bridge the gap. Our findings underscore the\nneed for more realistic, human-centric evaluations alongside existing\nsubjective tests.",
        "url": "http://arxiv.org/abs/2508.04179v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04179v1",
        "arxiv_id": "2508.04179v1",
        "authors": [
            "Praveen Srinivasa Varadhan",
            "Sherry Thomas",
            "Sai Teja M. S.",
            "Suvrat Bhooshan",
            "Mitesh M. Khapra"
        ],
        "submitted": "2025-08-06 08:04:21",
        "source": "arxiv",
        "comment": "Accepted at InterSpeech 2025"
    },
    {
        "title": "ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations",
        "abstract": "The 2025 Global Risks Report identifies state-based armed conflict and\nsocietal polarisation among the most pressing global threats, with social media\nplaying a central role in amplifying toxic discourse. Memes, as a widely used\nmode of online communication, often serve as vehicles for spreading harmful\ncontent. However, limitations in data accessibility and the high cost of\ndataset curation hinder the development of robust meme moderation systems. To\naddress this challenge, in this work, we introduce a first-of-its-kind dataset\nof 6,300 real-world meme-based posts annotated in two stages: (i) binary\nclassification into toxic and normal, and (ii) fine-grained labelling of toxic\nmemes as hateful, dangerous, or offensive. A key feature of this dataset is\nthat it is enriched with auxiliary metadata of socially relevant tags,\nenhancing the context of each meme. In addition, we propose a tag generation\nmodule that produces socially grounded tags, because most in-the-wild memes\noften do not come with tags. Experimental results show that incorporating these\ntags substantially enhances the performance of state-of-the-art VLMs detection\ntasks. Our contributions offer a novel and scalable foundation for improved\ncontent moderation in multimodal online environments.",
        "url": "http://arxiv.org/abs/2508.04166v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04166v1",
        "arxiv_id": "2508.04166v1",
        "authors": [
            "Subhankar Swain",
            "Naquee Rizwan",
            "Nayandeep Deb",
            "Vishwajeet Singh Solanki",
            "Vishwa Gangadhar S",
            "Animesh Mukherjee"
        ],
        "submitted": "2025-08-06 07:46:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval",
        "abstract": "Formula retrieval is an important topic in Mathematical Information\nRetrieval. We propose SSEmb, a novel embedding framework capable of capturing\nboth structural and semantic features of mathematical formulas. Structurally,\nwe employ Graph Contrastive Learning to encode formulas represented as Operator\nGraphs. To enhance structural diversity while preserving mathematical validity\nof these formula graphs, we introduce a novel graph data augmentation approach\nthrough a substitution strategy. Semantically, we utilize Sentence-BERT to\nencode the surrounding text of formulas. Finally, for each query and its\ncandidates, structural and semantic similarities are calculated separately and\nthen fused through a weighted scheme. In the ARQMath-3 formula retrieval task,\nSSEmb outperforms existing embedding-based methods by over 5 percentage points\non P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs\nof other methods and achieves state-of-the-art results when combined with\nApproach0.",
        "url": "http://arxiv.org/abs/2508.04162v2",
        "pdf_url": "http://arxiv.org/pdf/2508.04162v2",
        "arxiv_id": "2508.04162v2",
        "authors": [
            "Ruyin Li",
            "Xiaoyu Chen"
        ],
        "submitted": "2025-08-06 07:39:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bridging Search and Recommendation through Latent Cross Reasoning",
        "abstract": "Search and recommendation (S&R) are fundamental components of modern online\nplatforms, yet effectively leveraging search behaviors to improve\nrecommendation remains a challenging problem. User search histories often\ncontain noisy or irrelevant signals that can even degrade recommendation\nperformance, while existing approaches typically encode S&R histories either\njointly or separately without explicitly identifying which search behaviors are\ntruly useful. Inspired by the human decision-making process, where one first\nidentifies recommendation intent and then reasons about relevant evidence, we\ndesign a latent cross reasoning framework that first encodes user S&R histories\nto capture global interests and then iteratively reasons over search behaviors\nto extract signals beneficial for recommendation. Contrastive learning is\nemployed to align latent reasoning states with target items, and reinforcement\nlearning is further introduced to directly optimize ranking performance.\nExtensive experiments on public benchmarks demonstrate consistent improvements\nover strong baselines, validating the importance of reasoning in enhancing\nsearch-aware recommendation.",
        "url": "http://arxiv.org/abs/2508.04152v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04152v1",
        "arxiv_id": "2508.04152v1",
        "authors": [
            "Teng Shi",
            "Weicong Qin",
            "Weijie Yu",
            "Xiao Zhang",
            "Ming He",
            "Jianping Fan",
            "Jun Xu"
        ],
        "submitted": "2025-08-06 07:28:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap",
        "abstract": "Aligning large language models (LLMs) with human preferences is a critical\nchallenge in AI research. While methods like Reinforcement Learning from Human\nFeedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they\noften rely on large, costly preference datasets. The current work lacks methods\nfor high-quality data selection specifically for preference data. In this work,\nwe introduce a novel difficulty-based data selection strategy for preference\ndatasets, grounded in the DPO implicit reward mechanism. By selecting\npreference data examples with smaller DPO implicit reward gaps, which are\nindicative of more challenging cases, we improve data efficiency and model\nalignment. Our approach consistently outperforms five strong baselines across\nmultiple datasets and alignment tasks, achieving superior performance with only\n10\\% of the original data. This principled, efficient selection method offers a\npromising solution for scaling LLM alignment with limited resources.",
        "url": "http://arxiv.org/abs/2508.04149v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04149v1",
        "arxiv_id": "2508.04149v1",
        "authors": [
            "Xuan Qi",
            "Rongwu Xu",
            "Zhijing Jin"
        ],
        "submitted": "2025-08-06 07:24:14",
        "source": "arxiv",
        "comment": "Our code and data are available at\n  https://github.com/Difficulty-Based-Preference-Data-Select/Difficulty-Based-Preference-Data-Select"
    },
    {
        "title": "Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation",
        "abstract": "In modern online platforms, search and recommendation (S&R) often coexist,\noffering opportunities for performance improvement through search-enhanced\napproaches. Existing studies show that incorporating search signals boosts\nrecommendation performance. However, the effectiveness of these methods relies\nheavily on rich search interactions. They primarily benefit a small subset of\nusers with abundant search behavior, while offering limited improvements for\nthe majority of users who exhibit only sparse search activity. To address the\nproblem of sparse search data in search-enhanced recommendation, we face two\nkey challenges: (1) how to learn useful search features for users with sparse\nsearch interactions, and (2) how to design effective training objectives under\nsparse conditions. Our idea is to leverage the features of users with rich\nsearch interactions to enhance those of users with sparse search interactions.\nBased on this idea, we propose GSERec, a method that utilizes message passing\non the User-Code Graphs to alleviate data sparsity in Search-Enhanced\nRecommendation. Specifically, we utilize Large Language Models (LLMs) with\nvector quantization to generate discrete codes, which connect similar users and\nthereby construct the graph. Through message passing on this graph, embeddings\nof users with rich search data are propagated to enhance the embeddings of\nusers with sparse interactions. To further ensure that the message passing\ncaptures meaningful information from truly similar users, we introduce a\ncontrastive loss to better model user similarities. The enhanced user\nrepresentations are then integrated into downstream search-enhanced\nrecommendation models. Experiments on three real-world datasets show that\nGSERec consistently outperforms baselines, especially for users with sparse\nsearch behaviors.",
        "url": "http://arxiv.org/abs/2508.04145v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04145v1",
        "arxiv_id": "2508.04145v1",
        "authors": [
            "Teng Shi",
            "Weijie Yu",
            "Xiao Zhang",
            "Ming He",
            "Jianping Fan",
            "Jun Xu"
        ],
        "submitted": "2025-08-06 07:16:40",
        "source": "arxiv",
        "comment": "Accepted by CIKM 2025"
    },
    {
        "title": "Multilingual Source Tracing of Speech Deepfakes: A First Benchmark",
        "abstract": "Recent progress in generative AI has made it increasingly easy to create\nnatural-sounding deepfake speech from just a few seconds of audio. While these\ntools support helpful applications, they also raise serious concerns by making\nit possible to generate convincing fake speech in many languages. Current\nresearch has largely focused on detecting fake speech, but little attention has\nbeen given to tracing the source models used to generate it. This paper\nintroduces the first benchmark for multilingual speech deepfake source tracing,\ncovering both mono- and cross-lingual scenarios. We comparatively investigate\nDSP- and SSL-based modeling; examine how SSL representations fine-tuned on\ndifferent languages impact cross-lingual generalization performance; and\nevaluate generalization to unseen languages and speakers. Our findings offer\nthe first comprehensive insights into the challenges of identifying speech\ngeneration models when training and inference languages differ. The dataset,\nprotocol and code are available at\nhttps://github.com/xuanxixi/Multilingual-Source-Tracing.",
        "url": "http://arxiv.org/abs/2508.04143v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04143v1",
        "arxiv_id": "2508.04143v1",
        "authors": [
            "Xi Xuan",
            "Yang Xiao",
            "Rohan Kumar Das",
            "Tomi Kinnunen"
        ],
        "submitted": "2025-08-06 07:11:36",
        "source": "arxiv",
        "comment": "Accepted at Interspeech SPSC 2025 - 5th Symposium on Security and\n  Privacy in Speech Communication (Oral)"
    },
    {
        "title": "COPO: Consistency-Aware Policy Optimization",
        "abstract": "Reinforcement learning has significantly enhanced the reasoning capabilities\nof Large Language Models (LLMs) in complex problem-solving tasks. Recently, the\nintroduction of DeepSeek R1 has inspired a surge of interest in leveraging\nrule-based rewards as a low-cost alternative for computing advantage functions\nand guiding policy optimization. However, a common challenge observed across\nmany replication and extension efforts is that when multiple sampled responses\nunder a single prompt converge to identical outcomes, whether correct or\nincorrect, the group-based advantage degenerates to zero. This leads to\nvanishing gradients and renders the corresponding samples ineffective for\nlearning, ultimately limiting training efficiency and downstream performance.\nTo address this issue, we propose a consistency-aware policy optimization\nframework that introduces a structured global reward based on outcome\nconsistency, the global loss based on it ensures that, even when model outputs\nshow high intra-group consistency, the training process still receives\nmeaningful learning signals, which encourages the generation of correct and\nself-consistent reasoning paths from a global perspective. Furthermore, we\nincorporate an entropy-based soft blending mechanism that adaptively balances\nlocal advantage estimation with global optimization, enabling dynamic\ntransitions between exploration and convergence throughout training. Our method\nintroduces several key innovations in both reward design and optimization\nstrategy. We validate its effectiveness through substantial performance gains\non multiple mathematical reasoning benchmarks, highlighting the proposed\nframework's robustness and general applicability. Code of this work has been\nreleased at https://github.com/hijih/copo-code.git.",
        "url": "http://arxiv.org/abs/2508.04138v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04138v1",
        "arxiv_id": "2508.04138v1",
        "authors": [
            "Jinghang Han",
            "Jiawei Chen",
            "Hang Shao",
            "Hao Ma",
            "Mingcheng Li",
            "Xintian Shen",
            "Lihao Zheng",
            "Wei Chen",
            "Tao Wei",
            "Lihua Zhang"
        ],
        "submitted": "2025-08-06 07:05:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities",
        "abstract": "Open-domain Knowledge Graph Completion (KGC) faces significant challenges in\nan ever-changing world, especially when considering the continual emergence of\nnew entities in daily news. Existing approaches for KGC mainly rely on\npretrained language models' parametric knowledge, pre-constructed queries, or\nsingle-step retrieval, typically requiring substantial supervision and training\ndata. Even so, they often fail to capture comprehensive and up-to-date\ninformation about unpopular and/or emerging entities. To this end, we introduce\nAgentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework\nthat combines iterative retrieval actions and multi-step reasoning to\ndynamically construct rich knowledge graph triplets. Experiments show that,\ndespite requiring zero training efforts, AgREE significantly outperforms\nexisting methods in constructing knowledge graph triplets, especially for\nemerging entities that were not seen during language models' training\nprocesses, outperforming previous methods by up to 13.7%. Moreover, we propose\na new evaluation methodology that addresses a fundamental weakness of existing\nsetups and a new benchmark for KGC on emerging entities. Our work demonstrates\nthe effectiveness of combining agent-based reasoning with strategic information\nretrieval for maintaining up-to-date knowledge graphs in dynamic information\nenvironments.",
        "url": "http://arxiv.org/abs/2508.04118v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04118v1",
        "arxiv_id": "2508.04118v1",
        "authors": [
            "Ruochen Zhao",
            "Simone Conia",
            "Eric Peng",
            "Min Li",
            "Saloni Potdar"
        ],
        "submitted": "2025-08-06 06:34:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks",
        "abstract": "The pretrained large language models (LLMs) are finetuned with labeled data\nfor better instruction following ability and alignment with human values. In\nthis paper, we study the learning dynamics of LLM finetuning on reasoning tasks\nand reveal the uncovered over-memorization phenomenon during a specific stage\nof LLM finetuning. At this stage, the LLMs have excessively memorized training\ndata and exhibit high test perplexity while maintaining good test accuracy. We\ninvestigate the conditions that lead to LLM over-memorization and find that\ntraining epochs and large learning rates contribute to this issue. Although\nmodels with over-memorization demonstrate comparable test accuracy to normal\nmodels, they suffer from reduced robustness, poor out-of-distribution\ngeneralization, and decreased generation diversity. Our experiments unveil the\nover-memorization to be broadly applicable across different tasks, models, and\nfinetuning methods. Our research highlights that overparameterized, extensively\nfinetuned LLMs exhibit unique learning dynamics distinct from traditional\nmachine learning models. Based on our observations of over-memorization, we\nprovide recommendations on checkpoint and learning rate selection during\nfinetuning.",
        "url": "http://arxiv.org/abs/2508.04117v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04117v1",
        "arxiv_id": "2508.04117v1",
        "authors": [
            "Zhiwen Ruan",
            "Yun Chen",
            "Yutao Hou",
            "Peng Li",
            "Yang Liu",
            "Guanhua Chen"
        ],
        "submitted": "2025-08-06 06:34:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning",
        "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\nbut often struggle with complex, multi-step mathematical reasoning, where minor\nerrors in visual perception or logical deduction can lead to complete failure.\nWhile Process Reward Models (PRMs) offer step-by-step supervision, existing\nmultimodal PRMs are limited to being binary verifiers that can identify but not\ncorrect errors, offering little explanatory power. To address these\ndeficiencies, we introduce the Generative Multimodal Process Reward Model\n(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an\nactive reasoning collaborator. Instead of a simple scalar score, GM-PRM\nprovides a fine-grained, interpretable analysis of each reasoning step,\nevaluating its step intent, visual alignment, and logical soundness. More\ncritically, GM-PRM is trained to generate a corrected version of the first\nerroneous step it identifies. This unique corrective capability enables our new\ntest-time inference strategy, Refined Best-of-N (Refined-BoN). This framework\nactively enhances solution quality by using the PRM's generated correction to\nguide the policy model toward a more promising reasoning trajectory, thereby\nimproving the diversity and correctness of the solution pool. We demonstrate\nthat GM-PRM achieves state-of-the-art results on multiple multimodal math\nbenchmarks, significantly boosting policy model performance with remarkable\ndata efficiency, requiring only a 20K-sample training dataset. Our code will be\nreleased upon acceptance.",
        "url": "http://arxiv.org/abs/2508.04088v2",
        "pdf_url": "http://arxiv.org/pdf/2508.04088v2",
        "arxiv_id": "2508.04088v2",
        "authors": [
            "Jianghangfan Zhang",
            "Yibo Yan",
            "Kening Zheng",
            "Xin Zou",
            "Song Dai",
            "Xuming Hu"
        ],
        "submitted": "2025-08-06 05:10:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ToolGrad: Efficient Tool-use Dataset Generation with Textual \"Gradients\"",
        "abstract": "Prior work synthesizes tool-use LLM datasets by first generating a user\nquery, followed by complex tool-use annotations like DFS. This leads to\ninevitable annotation failures and low efficiency in data generation. We\nintroduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad\nfirst constructs valid tool-use chains through an iterative process guided by\ntextual \"gradients\", and then synthesizes corresponding user queries. This\n\"answer-first\" approach led to ToolGrad-5k, a dataset generated with more\ncomplex tool use, lower cost, and 100% pass rate. Experiments show that models\ntrained on ToolGrad-5k outperform those on expensive baseline datasets and\nproprietary LLMs, even on OOD benchmarks.",
        "url": "http://arxiv.org/abs/2508.04086v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04086v1",
        "arxiv_id": "2508.04086v1",
        "authors": [
            "Zhongyi Zhou",
            "Kohei Uehara",
            "Haoyu Zhang",
            "Jingtao Zhou",
            "Lin Gu",
            "Ruofei Du",
            "Zheng Xu",
            "Tatsuya Harada"
        ],
        "submitted": "2025-08-06 05:04:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Efficient Strategy for Improving Large Language Model (LLM) Capabilities",
        "abstract": "Large Language Models (LLMs) have become a milestone in the field of\nartificial intelligence and natural language processing. However, their\nlarge-scale deployment remains constrained by the need for significant\ncomputational resources. This work proposes starting from a base model to\nexplore and combine data processing and careful data selection techniques,\ntraining strategies, and architectural adjustments to improve the efficiency of\nLLMs in resource-constrained environments and within a delimited knowledge\nbase. The methodological approach included defining criteria for building\nreliable datasets, conducting controlled experiments with different\nconfigurations, and systematically evaluating the resulting variants in terms\nof capability, versatility, response time, and safety. Finally, comparative\ntests were conducted to measure the performance of the developed variants and\nto validate the effectiveness of the proposed strategies. This work is based on\nthe master's thesis in Systems and Computer Engineering titled \"Efficient\nStrategy for Improving the Capabilities of Large Language Models (LLMs)\".",
        "url": "http://arxiv.org/abs/2508.04073v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04073v1",
        "arxiv_id": "2508.04073v1",
        "authors": [
            "Julián Camilo Velandia Gutiérrez"
        ],
        "submitted": "2025-08-06 04:08:26",
        "source": "arxiv",
        "comment": "Based on master's thesis in Systems and Computer Engineering,\n  Universidad Nacional de Colombia (2025)"
    },
    {
        "title": "PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a cornerstone technique for\nenhancing large language models (LLMs) with external knowledge. However,\ncurrent RAG systems face two critical limitations: (1) they inefficiently\nretrieve information for every query, including simple questions that could be\nresolved using the LLM's parametric knowledge alone, and (2) they risk\nretrieving irrelevant documents when queries contain sparse information\nsignals. To address these gaps, we introduce Parametric-verified Adaptive\nInformation Retrieval and Selection (PAIRS), a training-free framework that\nintegrates parametric and retrieved knowledge to adaptively determine whether\nto retrieve and how to select external information. Specifically, PAIRS employs\na dual-path generation mechanism: First, the LLM produces both a direct answer\nand a context-augmented answer using self-generated pseudo-context. When these\noutputs converge, PAIRS bypasses external retrieval entirely, dramatically\nimproving the RAG system's efficiency. For divergent cases, PAIRS activates a\ndual-path retrieval (DPR) process guided by both the original query and\nself-generated contextual signals, followed by an Adaptive Information\nSelection (AIS) module that filters documents through weighted similarity to\nboth sources. This simple yet effective approach can not only enhance\nefficiency by eliminating unnecessary retrievals but also improve accuracy\nthrough contextually guided retrieval and adaptive information selection.\nExperimental results on six question-answering (QA) benchmarks show that PAIRS\nreduces retrieval costs by around 25% (triggering for only 75% of queries)\nwhile still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior\nbaselines on average.",
        "url": "http://arxiv.org/abs/2508.04057v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04057v1",
        "arxiv_id": "2508.04057v1",
        "authors": [
            "Wang Chen",
            "Guanqiang Qi",
            "Weikang Li",
            "Yang Li",
            "Deguo Xia",
            "Jizhou Huang"
        ],
        "submitted": "2025-08-06 03:33:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation",
        "abstract": "Controllable Text Generation (CTG) is a vital subfield in Natural Language\nProcessing (NLP), aiming to generate text that aligns with desired attributes.\nHowever, previous studies commonly focus on the quality of controllable text\ngeneration for short sequences, while the generation of long-form text remains\nlargely underexplored. In this paper, we observe that the controllability of\ntexts generated by the powerful prefix-based method Air-Decoding tends to\ndecline with increasing sequence length, which we hypothesize primarily arises\nfrom the observed decay in attention to the prefixes. Meanwhile, different\ntypes of prefixes including soft and hard prefixes are also key factors\ninfluencing performance. Building on these insights, we propose a lightweight\nand effective framework called Dynamic Token-level Prefix Augmentation (DTPA)\nbased on Air-Decoding for controllable text generation. Specifically, it first\nselects the optimal prefix type for a given task. Then we dynamically amplify\nthe attention to the prefix for the attribute distribution to enhance\ncontrollability, with a scaling factor growing exponentially as the sequence\nlength increases. Moreover, based on the task, we optionally apply a similar\naugmentation to the original prompt for the raw distribution to balance text\nquality. After attribute distribution reconstruction, the generated text\nsatisfies the attribute constraints well. Experiments on multiple CTG tasks\ndemonstrate that DTPA generally outperforms other methods in attribute control\nwhile maintaining competitive fluency, diversity, and topic relevance. Further\nanalysis highlights DTPA's superior effectiveness in long text generation.",
        "url": "http://arxiv.org/abs/2508.04047v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04047v1",
        "arxiv_id": "2508.04047v1",
        "authors": [
            "Jiabing Yang",
            "Yixiang Chen",
            "Zichen Wen",
            "Chenhang Cui",
            "Peiyan Li",
            "Yuan Xu",
            "Bowen Fang",
            "Yan Huang",
            "Liang Wang"
        ],
        "submitted": "2025-08-06 03:20:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents",
        "abstract": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.",
        "url": "http://arxiv.org/abs/2508.04038v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04038v1",
        "arxiv_id": "2508.04038v1",
        "authors": [
            "Zechen Li",
            "Baiyu Chen",
            "Hao Xue",
            "Flora D. Salim"
        ],
        "submitted": "2025-08-06 02:57:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models",
        "abstract": "The feedback loop in industrial recommendation systems reinforces homogeneous\ncontent, creates filter bubble effects, and diminishes user satisfaction.\nRecently, large language models(LLMs) have demonstrated potential in\nserendipity recommendation, thanks to their extensive world knowledge and\nsuperior reasoning capabilities. However, these models still face challenges in\nensuring the rationality of the reasoning process, the usefulness of the\nreasoning results, and meeting the latency requirements of industrial\nrecommendation systems (RSs). To address these challenges, we propose a method\nthat leverages llm to dynamically construct user knowledge graphs, thereby\nenhancing the serendipity of recommendation systems. This method comprises a\ntwo stage framework:(1) two-hop interest reasoning, where user static profiles\nand historical behaviors are utilized to dynamically construct user knowledge\ngraphs via llm. Two-hop reasoning, which can enhance the quality and accuracy\nof LLM reasoning results, is then performed on the constructed graphs to\nidentify users' potential interests; and(2) Near-line adaptation, a\ncost-effective approach to deploying the aforementioned models in industrial\nrecommendation systems. We propose a u2i (user-to-item) retrieval model that\nalso incorporates i2i (item-to-item) retrieval capabilities, the retrieved\nitems not only exhibit strong relevance to users' newly emerged interests but\nalso retain the high conversion rate of traditional u2i retrieval. Our online\nexperiments on the Dewu app, which has tens of millions of users, indicate that\nthe method increased the exposure novelty rate by 4.62%, the click novelty rate\nby 4.85%, the average view duration per person by 0.15%, unique visitor click\nthrough rate by 0.07%, and unique visitor interaction penetration by 0.30%,\nenhancing user experience.",
        "url": "http://arxiv.org/abs/2508.04032v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04032v1",
        "arxiv_id": "2508.04032v1",
        "authors": [
            "Qian Yong",
            "Yanhui Li",
            "Jialiang Shi",
            "Yaguang Dou",
            "Tian Qi"
        ],
        "submitted": "2025-08-06 02:52:09",
        "source": "arxiv",
        "comment": "8 pages"
    },
    {
        "title": "Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval",
        "abstract": "Recently, prompt learning has demonstrated remarkable success in adapting\npre-trained Vision-Language Models (VLMs) to various downstream tasks such as\nimage classification. However, its application to the downstream Image-Text\nRetrieval (ITR) task is more challenging. We find that the challenge lies in\ndiscriminating both fine-grained attributes and similar subcategories of the\ndownstream data. To address this challenge, we propose Dual prompt Learning\nwith Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning\nframework to achieve precise image-text matching. The framework dynamically\nadjusts prompt vectors from both semantic and visual dimensions to improve the\nperformance of CLIP on the downstream ITR task. Based on the prompt paradigm,\nDCAR jointly optimizes attribute and class features to enhance fine-grained\nrepresentation learning. Specifically, (1) at the attribute level, it\ndynamically updates the weights of attribute descriptions based on text-image\nmutual information correlation; (2) at the category level, it introduces\nnegative samples from multiple perspectives with category-matching weighting to\nlearn subcategory distinctions. To validate our method, we construct the\nFine-class Described Retrieval Dataset (FDRD), which serves as a challenging\nbenchmark for ITR in downstream data domains. It covers over 1,500 downstream\nfine categories and 230,000 image-caption pairs with detailed attribute\nannotations. Extensive experiments on FDRD demonstrate that DCAR achieves\nstate-of-the-art performance over existing baselines.",
        "url": "http://arxiv.org/abs/2508.04028v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04028v1",
        "arxiv_id": "2508.04028v1",
        "authors": [
            "Yifan Wang",
            "Tao Wang",
            "Chenwei Tang",
            "Caiyang Yu",
            "Zhengqing Zang",
            "Mengmi Zhang",
            "Shudong Huang",
            "Jiancheng Lv"
        ],
        "submitted": "2025-08-06 02:44:08",
        "source": "arxiv",
        "comment": "10 pages, 7figures"
    },
    {
        "title": "Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation",
        "abstract": "In the semantic segmentation of remote sensing images, acquiring complete\nground objects is critical for achieving precise analysis. However, this task\nis severely hindered by two major challenges: high intra-class variance and\nhigh inter-class similarity. Traditional methods often yield incomplete\nsegmentation results due to their inability to effectively unify class\nrepresentations and distinguish between similar features. Even emerging\nclass-guided approaches are limited by coarse class prototype representations\nand a neglect of target structural information.\n  Therefore, this paper proposes a Prototype-Driven Structure Synergy Network\n(PDSSNet). The design of this network is based on a core concept, a complete\nground object is jointly defined by its invariant class semantics and its\nvariant spatial structure. To implement this, we have designed three key\nmodules. First, the Adaptive Prototype Extraction Module (APEM) ensures\nsemantic accuracy from the source by encoding the ground truth to extract\nunbiased class prototypes. Subsequently, the designed Semantic-Structure\nCoordination Module (SSCM) follows a hierarchical semantics-first,\nstructure-second principle. This involves first establishing a global semantic\ncognition, then leveraging structural information to constrain and refine the\nsemantic representation, thereby ensuring the integrity of class information.\nFinally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic\nstep-size adjustment mechanism to focus on discriminative features between\nclasses.\n  Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art\nmethods. The source code is available at\nhttps://github.com/wangjunyi-1/PDSSNet.",
        "url": "http://arxiv.org/abs/2508.04022v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04022v1",
        "arxiv_id": "2508.04022v1",
        "authors": [
            "Junyi Wang",
            "Jinjiang Li",
            "Guodong Fan",
            "Yakun Ju",
            "Xiang Fang",
            "Alex C. Kot"
        ],
        "submitted": "2025-08-06 02:29:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing",
        "abstract": "Large Language Models (LLMs) underpin many AI applications, but their static\nnature makes updating knowledge costly. Model editing offers an efficient\nalternative by injecting new information through targeted parameter\nmodifications. In particular, meta-learning-based model editing (MLBME) methods\nhave demonstrated notable advantages in both editing effectiveness and\nefficiency. Despite this, we find that MLBME exhibits suboptimal performance in\nlow-data scenarios, and its training efficiency is bottlenecked by the\ncomputation of KL divergence. To address these, we propose $\\textbf{S}$tep\n$\\textbf{M}$ore $\\textbf{Edit}$ ($\\textbf{SMEdit}$), a novel MLBME method that\nadopts $\\textbf{M}$ultiple $\\textbf{B}$ackpro$\\textbf{P}$agation\n$\\textbf{S}$teps ($\\textbf{MBPS}$) to improve editing performance under limited\nsupervision and a norm regularization on weight updates to improve training\nefficiency. Experimental results on two datasets and two LLMs demonstrate that\nSMEdit outperforms prior MLBME baselines and the MBPS strategy can be\nseamlessly integrated into existing methods to further boost their performance.\nOur code will be released soon.",
        "url": "http://arxiv.org/abs/2508.04012v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04012v1",
        "arxiv_id": "2508.04012v1",
        "authors": [
            "Xiaopeng Li",
            "Shasha Li",
            "Xi Wang",
            "Shezheng Song",
            "Bin Ji",
            "Shangwen Wang",
            "Jun Ma",
            "Xiaodong Liu",
            "Mina Liu",
            "Jie Yu"
        ],
        "submitted": "2025-08-06 01:54:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization",
        "abstract": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard.",
        "url": "http://arxiv.org/abs/2508.04010v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04010v1",
        "arxiv_id": "2508.04010v1",
        "authors": [
            "Yurun Chen",
            "Xavier Hu",
            "Yuhan Liu",
            "Keting Yin",
            "Juncheng Li",
            "Zhuosheng Zhang",
            "Shengyu Zhang"
        ],
        "submitted": "2025-08-06 01:49:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval",
        "abstract": "Conversational search aims to satisfy users' complex information needs via\nmultiple-turn interactions. The key challenge lies in revealing real users'\nsearch intent from the context-dependent queries. Previous studies achieve\nconversational search by fine-tuning a conversational dense retriever with\nrelevance judgments between pairs of context-dependent queries and documents.\nHowever, this training paradigm encounters data scarcity issues. To this end,\nwe propose ConvMix, a mixed-criteria framework to augment conversational dense\nretrieval, which covers more aspects than existing data augmentation\nframeworks. We design a two-sided relevance judgment augmentation schema in a\nscalable manner via the aid of large language models. Besides, we integrate the\nframework with quality control mechanisms to obtain semantically diverse\nsamples and near-distribution supervisions to combine various annotated data.\nExperimental results on five widely used benchmarks show that the\nconversational dense retriever trained by our ConvMix framework outperforms\nprevious baseline methods, which demonstrates our superior effectiveness.",
        "url": "http://arxiv.org/abs/2508.04001v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04001v1",
        "arxiv_id": "2508.04001v1",
        "authors": [
            "Fengran Mo",
            "Jinghan Zhang",
            "Yuchen Hui",
            "Jia Ao Sun",
            "Zhichao Xu",
            "Zhan Su",
            "Jian-Yun Nie"
        ],
        "submitted": "2025-08-06 01:28:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models",
        "abstract": "Successful group meetings, such as those implemented in group\nbehavioral-change programs, work meetings, and other social contexts, must\npromote individual goal setting and execution while strengthening the social\nrelationships within the group. Consequently, an ideal facilitator must be\nsensitive to the subtle dynamics of disengagement, difficulties with individual\ngoal setting and execution, and interpersonal difficulties that signal a need\nfor intervention. The challenges and cognitive load experienced by facilitators\ncreate a critical gap for an embodied technology that can interpret social\nexchanges while remaining aware of the needs of the individuals in the group\nand providing transparent recommendations that go beyond powerful but \"black\nbox\" foundation models (FMs) that identify social cues. We address this\nimportant demand with a social robot co-facilitator that analyzes multimodal\nmeeting data and provides discreet cues to the facilitator. The robot's\nreasoning is powered by an agentic concept bottleneck model (CBM), which makes\ndecisions based on human-interpretable concepts like participant engagement and\nsentiments, ensuring transparency and trustworthiness. Our core contribution is\na transfer learning framework that distills the broad social understanding of\nan FM into our specialized and transparent CBM. This concept-driven system\nsignificantly outperforms direct zero-shot FMs in predicting the need for\nintervention and enables real-time human correction of its reasoning.\nCritically, we demonstrate robust knowledge transfer: the model generalizes\nacross different groups and successfully transfers the expertise of senior\nhuman facilitators to improve the performance of novices. By transferring an\nexpert's cognitive model into an interpretable robotic partner, our work\nprovides a powerful blueprint for augmenting human capabilities in complex\nsocial domains.",
        "url": "http://arxiv.org/abs/2508.03998v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03998v1",
        "arxiv_id": "2508.03998v1",
        "authors": [
            "Xinyu Zhao",
            "Zhen Tan",
            "Maya Enisman",
            "Minjae Seo",
            "Marta R. Durantini",
            "Dolores Albarracin",
            "Tianlong Chen"
        ],
        "submitted": "2025-08-06 01:24:06",
        "source": "arxiv",
        "comment": "27 pages, 7 figures"
    },
    {
        "title": "Are Today's LLMs Ready to Explain Well-Being Concepts?",
        "abstract": "Well-being encompasses mental, physical, and social dimensions essential to\npersonal growth and informed life decisions. As individuals increasingly\nconsult Large Language Models (LLMs) to understand well-being, a key challenge\nemerges: Can LLMs generate explanations that are not only accurate but also\ntailored to diverse audiences? High-quality explanations require both factual\ncorrectness and the ability to meet the expectations of users with varying\nexpertise. In this work, we construct a large-scale dataset comprising 43,880\nexplanations of 2,194 well-being concepts, generated by ten diverse LLMs. We\nintroduce a principle-guided LLM-as-a-judge evaluation framework, employing\ndual judges to assess explanation quality. Furthermore, we show that\nfine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct\nPreference Optimization (DPO) can significantly enhance the quality of\ngenerated explanations. Our results reveal: (1) The proposed LLM judges align\nwell with human evaluations; (2) explanation quality varies significantly\nacross models, audiences, and categories; and (3) DPO- and SFT-finetuned models\noutperform their larger counterparts, demonstrating the effectiveness of\npreference-based learning for specialized explanation tasks.",
        "url": "http://arxiv.org/abs/2508.03990v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03990v1",
        "arxiv_id": "2508.03990v1",
        "authors": [
            "Bohan Jiang",
            "Dawei Li",
            "Zhen Tan",
            "Chengshuai Zhao",
            "Huan Liu"
        ],
        "submitted": "2025-08-06 00:45:02",
        "source": "arxiv",
        "comment": "9 pages, 4 figures, 3 tables"
    },
    {
        "title": "Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency",
        "abstract": "Despite its simplicity and efficacy, the high token expenditure of\nself-consistency can limit its practical utility. Here we investigate if\nself-consistency can be made more token-efficient for long chain-of-thought\nreasoning tasks, while preserving its parallelism, through early hypothesis\npruning. Concretely, we generate all solutions in parallel, but periodically\nprune intermediate hypotheses that are deemed unnecessary based on two\nlightweight indicators: (a) the model's own confidence in individual\nhypotheses, and (b) lexical coverage of all current hypotheses by candidate\nsubsets that are under consideration for continued retention. We design a fast\nweighted set cover algorithm that utilizes the two indicators; our evaluation\nof five LLMs on three math benchmarks shows that this method can improve token\nefficiency for all models, by 10-35% in many cases.",
        "url": "http://arxiv.org/abs/2508.03979v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03979v1",
        "arxiv_id": "2508.03979v1",
        "authors": [
            "Md Arafat Sultan",
            "Ramón Fernandez Astudillo"
        ],
        "submitted": "2025-08-06 00:14:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Data and AI governance: Promoting equity, ethics, and fairness in large language models",
        "abstract": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.",
        "url": "http://arxiv.org/abs/2508.03970v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03970v1",
        "arxiv_id": "2508.03970v1",
        "authors": [
            "Alok Abhishek",
            "Lisa Erickson",
            "Tushar Bandopadhyay"
        ],
        "submitted": "2025-08-05 23:15:31",
        "source": "arxiv",
        "comment": "Published in MIT Science Policy Review 6, 139-146 (2025)"
    },
    {
        "title": "RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification",
        "abstract": "In this paper, we introduce RAVID, the first framework for AI-generated image\ndetection that leverages visual retrieval-augmented generation (RAG). While RAG\nmethods have shown promise in mitigating factual inaccuracies in foundation\nmodels, they have primarily focused on text, leaving visual knowledge\nunderexplored. Meanwhile, existing detection methods, which struggle with\ngeneralization and robustness, often rely on low-level artifacts and\nmodel-specific features, limiting their adaptability. To address this, RAVID\ndynamically retrieves relevant images to enhance detection. Our approach\nutilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with\ncategory-related prompts to improve representation learning. We further\nintegrate a vision-language model (VLM) to fuse retrieved images with the\nquery, enriching the input and improving accuracy. Given a query image, RAVID\ngenerates an embedding using RAVID CLIP, retrieves the most relevant images\nfrom a database, and combines these with the query image to form an enriched\ninput for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the\nUniversalFakeDetect benchmark, which covers 19 generative models, show that\nRAVID achieves state-of-the-art performance with an average accuracy of 93.85%.\nRAVID also outperforms traditional methods in terms of robustness, maintaining\nhigh accuracy even under image degradations such as Gaussian blur and JPEG\ncompression. Specifically, RAVID achieves an average accuracy of 80.27% under\ndegradation conditions, compared to 63.44% for the state-of-the-art model\nC2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG\ncompression scenarios. The code will be publicly available upon acceptance.",
        "url": "http://arxiv.org/abs/2508.03967v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03967v1",
        "arxiv_id": "2508.03967v1",
        "authors": [
            "Mamadou Keita",
            "Wassim Hamidouche",
            "Hessen Bougueffa Eutamene",
            "Abdelmalik Taleb-Ahmed",
            "Abdenour Hadid"
        ],
        "submitted": "2025-08-05 23:10:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers",
        "abstract": "The growing volume of scientific literature makes it challenging for\nscientists to move from a list of papers to a synthesized understanding of a\ntopic. Because of the constant influx of new papers on a daily basis, even if a\nscientist identifies a promising set of papers, they still face the tedious\ntask of individually reading through dozens of titles and abstracts to make\nsense of occasionally conflicting findings. To address this critical bottleneck\nin the research workflow, we introduce a summarization feature to BIP! Finder,\na scholarly search engine that ranks literature based on distinct impact\naspects like popularity and influence. Our approach enables users to generate\ntwo types of summaries from top-ranked search results: a concise summary for an\ninstantaneous at-a-glance comprehension and a more comprehensive literature\nreview-style summary for greater, better-organized comprehension. This ability\ndynamically leverages BIP! Finder's already existing impact-based ranking and\nfiltering features to generate context-sensitive, synthesized narratives that\ncan significantly accelerate literature discovery and comprehension.",
        "url": "http://arxiv.org/abs/2508.03962v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03962v1",
        "arxiv_id": "2508.03962v1",
        "authors": [
            "Paris Koloveas",
            "Serafeim Chatzopoulos",
            "Dionysis Diamantis",
            "Christos Tryfonopoulos",
            "Thanasis Vergoulis"
        ],
        "submitted": "2025-08-05 22:56:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Measuring the stability and plasticity of recommender systems",
        "abstract": "The typical offline protocol to evaluate recommendation algorithms is to\ncollect a dataset of user-item interactions and then use a part of this dataset\nto train a model, and the remaining data to measure how closely the model\nrecommendations match the observed user interactions. This protocol is\nstraightforward, useful and practical, but it only captures performance of a\nparticular model trained at some point in the past. We know, however, that\nonline systems evolve over time. In general, it is a good idea that models\nreflect such changes, so models are frequently retrained with recent data. But\nif this is the case, to what extent can we trust previous evaluations? How will\na model perform when a different pattern (re)emerges? In this paper we propose\na methodology to study how recommendation models behave when they are\nretrained. The idea is to profile algorithms according to their ability to, on\nthe one hand, retain past patterns -- stability -- and, on the other hand,\n(quickly) adapt to changes -- plasticity. We devise an offline evaluation\nprotocol that provides detail on the long-term behavior of models, and that is\nagnostic to datasets, algorithms and metrics. To illustrate the potential of\nthis framework, we present preliminary results of three different types of\nalgorithms on the GoodReads dataset that suggest different stability and\nplasticity profiles depending on the algorithmic technique, and a possible\ntrade-off between stability and plasticity.Although additional experiments will\nbe necessary to confirm these observations, they already illustrate the\nusefulness of the proposed framework to gain insights on the long term dynamics\nof recommendation models.",
        "url": "http://arxiv.org/abs/2508.03941v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03941v1",
        "arxiv_id": "2508.03941v1",
        "authors": [
            "Maria João Lavoura",
            "Robert Jungnickel",
            "João Vinagre"
        ],
        "submitted": "2025-08-05 22:15:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants",
        "abstract": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems.",
        "url": "http://arxiv.org/abs/2508.03936v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03936v1",
        "arxiv_id": "2508.03936v1",
        "authors": [
            "Xiangzhe Xu",
            "Guangyu Shen",
            "Zian Su",
            "Siyuan Cheng",
            "Hanxi Guo",
            "Lu Yan",
            "Xuan Chen",
            "Jiasheng Jiang",
            "Xiaolong Jin",
            "Chengpeng Wang",
            "Zhuo Zhang",
            "Xiangyu Zhang"
        ],
        "submitted": "2025-08-05 21:57:52",
        "source": "arxiv",
        "comment": "The first two authors (Xiangzhe Xu and Guangyu Shen) contributed\n  equally to this work"
    },
    {
        "title": "CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation",
        "abstract": "In the era of information overload, personalized news headline generation is\ncrucial for engaging users by tailoring content to their preferences while\naccurately conveying news facts. Existing methods struggle with effectively\ncapturing complex user interests and ensuring factual consistency, often\nleading to generic or misleading headlines. Leveraging the unprecedented\ncapabilities of Large Language Models (LLMs) in text generation, we propose\nContext-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates\nuser preferences and factual consistency constraints into a powerful\npre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture\nlong-term user interests, a Context Injection Adapter to seamlessly integrate\nthese preferences and current article context into the LLM's generation\nprocess, and a Fact-Consistency Reinforcement Module employing a novel\ncontrastive loss to mitigate hallucination. Evaluated on the real-world PENS\ndataset, CAP-LLM achieves state-of-the-art performance across all metrics.\nNotably, it significantly improves factual consistency (FactCC of 87.50) over\nstrong baselines like BART (86.67), while simultaneously enhancing\npersonalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1\n26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,\nand sensitivity analyses further validate the effectiveness of each component\nand the robustness of our approach, demonstrating CAP-LLM's ability to achieve\na superior balance between personalization and factual accuracy in news\nheadline generation.",
        "url": "http://arxiv.org/abs/2508.03935v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03935v1",
        "arxiv_id": "2508.03935v1",
        "authors": [
            "Raymond Wilson",
            "Cole Graham",
            "Chase Carter",
            "Zefeng Yang",
            "Ruiqi Gu"
        ],
        "submitted": "2025-08-05 21:55:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CoAct-1: Computer-using Agents with Coding as Actions",
        "abstract": "Autonomous agents that operate computers via Graphical User Interfaces (GUIs)\noften struggle with efficiency and reliability on complex, long-horizon tasks.\nWhile augmenting these agents with planners can improve task decomposition,\nthey remain constrained by the inherent limitations of performing all actions\nthrough GUI manipulation, leading to brittleness and inefficiency. In this\nwork, we introduce a more robust and flexible paradigm: enabling agents to use\ncoding as a enhanced action. We present CoAct-1, a novel multi-agent system\nthat synergistically combines GUI-based control with direct programmatic\nexecution. CoAct-1 features an Orchestrator that dynamically delegates subtasks\nto either a conventional GUI Operator or a specialized Programmer agent, which\ncan write and execute Python or Bash scripts. This hybrid approach allows the\nagent to bypass inefficient GUI action sequences for tasks like file management\nand data processing, while still leveraging visual interaction when necessary.\nWe evaluate our system on the challenging OSWorld benchmark, where CoAct-1\nachieves a new state-of-the-art success rate of 60.76%, significantly\noutperforming prior methods. Furthermore, our approach dramatically improves\nefficiency, reducing the average number of steps required to complete a task to\njust 10.15, compared to 15 for leading GUI agents. Our results demonstrate that\nintegrating coding as a core action provides a more powerful, efficient, and\nscalable path toward generalized computer automation.",
        "url": "http://arxiv.org/abs/2508.03923v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03923v1",
        "arxiv_id": "2508.03923v1",
        "authors": [
            "Linxin Song",
            "Yutong Dai",
            "Viraj Prabhu",
            "Jieyu Zhang",
            "Taiwei Shi",
            "Li Li",
            "Junnan Li",
            "Silvio Savarese",
            "Zeyuan Chen",
            "Jieyu Zhao",
            "Ran Xu",
            "Caiming Xiong"
        ],
        "submitted": "2025-08-05 21:33:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NAEx: A Plug-and-Play Framework for Explaining Network Alignment",
        "abstract": "Network alignment (NA) identifies corresponding nodes across multiple\nnetworks, with applications in domains like social networks, co-authorship, and\nbiology. Despite advances in alignment models, their interpretability remains\nlimited, making it difficult to understand alignment decisions and posing\nchallenges in building trust, particularly in high-stakes domains. To address\nthis, we introduce NAEx, a plug-and-play, model-agnostic framework that\nexplains alignment models by identifying key subgraphs and features influencing\npredictions. NAEx addresses the key challenge of preserving the joint\ncross-network dependencies on alignment decisions by: (1) jointly\nparameterizing graph structures and feature spaces through learnable edge and\nfeature masks, and (2) introducing an optimization objective that ensures\nexplanations are both faithful to the original predictions and enable\nmeaningful comparisons of structural and feature-based similarities between\nnetworks. NAEx is an inductive framework that efficiently generates NA\nexplanations for previously unseen data. We introduce evaluation metrics\ntailored to alignment explainability and demonstrate NAEx's effectiveness and\nefficiency on benchmark datasets by integrating it with four representative NA\nmodels.",
        "url": "http://arxiv.org/abs/2508.04731v1",
        "pdf_url": "http://arxiv.org/pdf/2508.04731v1",
        "arxiv_id": "2508.04731v1",
        "authors": [
            "Shruti Saxena",
            "Arijit Khan",
            "Joydeep Chandra"
        ],
        "submitted": "2025-08-05 20:46:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sotopia-RL: Reward Design for Social Intelligence",
        "abstract": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
        "url": "http://arxiv.org/abs/2508.03905v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03905v1",
        "arxiv_id": "2508.03905v1",
        "authors": [
            "Haofei Yu",
            "Zhengyang Qi",
            "Yining Zhao",
            "Kolby Nottingham",
            "Keyang Xuan",
            "Bodhisattwa Prasad Majumder",
            "Hao Zhu",
            "Paul Pu Liang",
            "Jiaxuan You"
        ],
        "submitted": "2025-08-05 20:43:42",
        "source": "arxiv",
        "comment": "10 pages"
    },
    {
        "title": "An Entity Linking Agent for Question Answering",
        "abstract": "Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide\naccurate answers. Entity Linking (EL) plays a critical role in linking natural\nlanguage mentions to KB entries. However, most existing EL methods are designed\nfor long contexts and do not perform well on short, ambiguous user questions in\nQA tasks. We propose an entity linking agent for QA, based on a Large Language\nModel that simulates human cognitive workflows. The agent actively identifies\nentity mentions, retrieves candidate entities, and makes decision. To verify\nthe effectiveness of our agent, we conduct two experiments: tool-based entity\nlinking and QA task evaluation. The results confirm the robustness and\neffectiveness of our agent.",
        "url": "http://arxiv.org/abs/2508.03865v2",
        "pdf_url": "http://arxiv.org/pdf/2508.03865v2",
        "arxiv_id": "2508.03865v2",
        "authors": [
            "Yajie Luo",
            "Yihong Wu",
            "Muzhi Li",
            "Fengran Mo",
            "Jia Ao Sun",
            "Xinyu Wang",
            "Liheng Ma",
            "Yingxue Zhang",
            "Jian-Yun Nie"
        ],
        "submitted": "2025-08-05 19:28:43",
        "source": "arxiv",
        "comment": "12 pages, 2 figures"
    },
    {
        "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models",
        "abstract": "Large Language Models (LLMs) are trained on vast and diverse internet corpora\nthat often include inaccurate or misleading content. Consequently, LLMs can\ngenerate misinformation, making robust fact-checking essential. This review\nsystematically analyzes how LLM-generated content is evaluated for factual\naccuracy by exploring key challenges such as hallucinations, dataset\nlimitations, and the reliability of evaluation metrics. The review emphasizes\nthe need for strong fact-checking frameworks that integrate advanced prompting\nstrategies, domain-specific fine-tuning, and retrieval-augmented generation\n(RAG) methods. It proposes five research questions that guide the analysis of\nthe recent literature from 2020 to 2025, focusing on evaluation methods and\nmitigation techniques. The review also discusses the role of instruction\ntuning, multi-agent reasoning, and external knowledge access via RAG\nframeworks. Key findings highlight the limitations of current metrics, the\nvalue of grounding outputs with validated external evidence, and the importance\nof domain-specific customization to improve factual consistency. Overall, the\nreview underlines the importance of building LLMs that are not only accurate\nand explainable but also tailored for domain-specific fact-checking. These\ninsights contribute to the advancement of research toward more trustworthy and\ncontext-aware language models.",
        "url": "http://arxiv.org/abs/2508.03860v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03860v1",
        "arxiv_id": "2508.03860v1",
        "authors": [
            "Subhey Sadi Rahman",
            "Md. Adnanul Islam",
            "Md. Mahbub Alam",
            "Musarrat Zeba",
            "Md. Abdur Rahman",
            "Sadia Sultana Chowa",
            "Mohaimenul Azam Khan Raiaan",
            "Sami Azam"
        ],
        "submitted": "2025-08-05 19:20:05",
        "source": "arxiv",
        "comment": "30 pages, 11 figures, 6 tables. Submitted to Artificial Intelligence\n  Review for peer review"
    },
    {
        "title": "Majority Bit-Aware Watermarking For Large Language Models",
        "abstract": "The growing deployment of Large Language Models (LLMs) in real-world\napplications has raised concerns about their potential misuse in generating\nharmful or deceptive content. To address this issue, watermarking techniques\nhave emerged as a promising solution by embedding identifiable binary messages\ninto generated text for origin verification and misuse tracing. While recent\nefforts have explored multi-bit watermarking schemes capable of embedding rich\ninformation such as user identifiers, they typically suffer from the\nfundamental trade-off between text quality and decoding accuracy: to ensure\nreliable message decoding, they have to restrict the size of preferred token\nsets during encoding, yet such restrictions reduce the quality of the generated\ncontent. In this work, we propose MajorMark, a novel watermarking method that\nimproves this trade-off through majority bit-aware encoding. MajorMark selects\npreferred token sets based on the majority bit of the message, enabling a\nlarger and more flexible sampling of tokens. In contrast to prior methods that\nrely on token frequency analysis for decoding, MajorMark employs a\nclustering-based decoding strategy, which maintains high decoding accuracy even\nwhen the preferred token set is large, thus preserving both content quality and\ndecoding accuracy. We further introduce MajorMark$^+$, which partitions the\nmessage into multiple blocks to independently encode and deterministically\ndecode each block, thereby further enhancing the quality of watermarked text\nand improving decoding accuracy. Extensive experiments on state-of-the-art LLMs\ndemonstrate that our methods significantly enhance both decoding accuracy and\ntext generation quality, outperforming prior multi-bit watermarking baselines.",
        "url": "http://arxiv.org/abs/2508.03829v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03829v1",
        "arxiv_id": "2508.03829v1",
        "authors": [
            "Jiahao Xu",
            "Rui Hu",
            "Zikai Zhang"
        ],
        "submitted": "2025-08-05 18:19:00",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources",
        "abstract": "We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles\nwith their citations and scraped web sources; articles are represented in a\nrich data structure, and scraped source texts are stored inline with precise\ncharacter offsets of their citations in the article text. MegaWika 2 is a major\nupgrade from the original MegaWika, spanning six times as many articles and\ntwice as many fully scraped citations. Both MegaWika and MegaWika 2 support\nreport generation research ; whereas MegaWika also focused on supporting\nquestion answering and retrieval applications, MegaWika 2 is designed to\nsupport fact checking and analyses across time and language.",
        "url": "http://arxiv.org/abs/2508.03828v1",
        "pdf_url": "http://arxiv.org/pdf/2508.03828v1",
        "arxiv_id": "2508.03828v1",
        "authors": [
            "Samuel Barham",
            "Chandler May",
            "Benjamin Van Durme"
        ],
        "submitted": "2025-08-05 18:18:17",
        "source": "arxiv",
        "comment": null
    }
]