# Daily Papers Report - 2025-09-23

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. OnePiece: Bringing Context Engineering and Reasoning to Industrial Cascade Ranking System

- **LLM Score**: 9
- **Keyword Score**: 13
- **Authors**: Sunhao Dai, Jiakai Tang, Jiahua Wu, Kun Wang, Yuxuan Zhu, Bingjun Chen, Bangyang Hong, Yu Zhao, Cong Fu, Kangle Wu, Yabo Ni, Anxiang Zeng, Wenjie Wang, Xu Chen, Jun Xu, See-Kiong Ng
- **URL**: <http://arxiv.org/abs/2509.18091v1>
- **Submitted**: 2025-09-22 17:59:07
- **Comment**: OnePiece Technical Report; Applied in Shopee
- **Topic Keywords**: queries, ranking, rag, retrieval, recommend, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the areas of query understanding, ranking models, and user behavior modeling. The proposed framework, OnePiece, integrates context engineering and reasoning into industrial cascaded pipelines, which aligns with your focus on deep semantic understanding and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Industrial Search and Recommendation Systems
- **Aim**: To improve the performance of industrial search and recommendation systems by integrating LLM-inspired mechanisms.
- **Rationale**: Transformer architectures alone are insufficient for industrial search due to the need for context engineering and multi-step reasoning.
- **Ground**: Existing industrial search pipelines rely on separate retrieval and ranking models.
- **Experiment**: OnePiece framework is deployed in Shopee's personalized search.
- **Takeaway**: OnePiece achieves consistent performance improvements, including a +2% increase in GMV/UU and a +2.90% rise in advertising revenue.

#### Abstract
> Despite the growing interest in replicating the scaled success of large
language models (LLMs) in industrial search and recommender systems, most
existing industrial efforts remain limited to transplanting Transformer
architectures, which bring only incremental improvements over strong Deep
Learning Recommendation Models (DLRMs). From a first principle perspective, the
breakthroughs of LLMs stem not only from their architectures but also from two
complementary mechanisms: context engineering, which enriches raw input queries
with contextual cues to better elicit model capabilities, and multi-step
reasoning, which iteratively refines model outputs through intermediate
reasoning paths. However, these two mechanisms and their potential to unlock
substantial improvements remain largely underexplored in industrial ranking
systems.
  In this paper, we propose OnePiece, a unified framework that seamlessly
integrates LLM-style context engineering and reasoning into both retrieval and
ranking models of industrial cascaded pipelines. OnePiece is built on a pure
Transformer backbone and further introduces three key innovations: (1)
structured context engineering, which augments interaction history with
preference and scenario signals and unifies them into a structured tokenized
input sequence for both retrieval and ranking; (2) block-wise latent reasoning,
which equips the model with multi-step refinement of representations and scales
reasoning bandwidth via block size; (3) progressive multi-task training, which
leverages user feedback chains to effectively supervise reasoning steps during
training. OnePiece has been deployed in the main personalized search scenario
of Shopee and achieves consistent online gains across different key business
metrics, including over $+2\%$ GMV/UU and a $+2.90\%$ increase in advertising
revenue.

---

### 2. A Generative Framework for Personalized Sticker Retrieval

- **LLM Score**: 8
- **Keyword Score**: 17
- **Authors**: Changjiang Zhou, Ruqing Zhang, Jiafeng Guo, Yu-An Liu, Fan Zhang, Ganyuan Luo, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2509.17749v2>
- **Submitted**: 2025-09-22 13:11:44
- **Comment**: Findings of EMNLP2025
- **Topic Keywords**: information retrieval, query, relevance, rag, click, retrieval, personalization, rank
- **Reason**: This paper proposes a generative framework for personalized sticker retrieval, leveraging user-specific preferences and intent-aware learning objectives. While the focus is on sticker retrieval, the use of generative modeling and personalized ranking aligns with the user's interests in Information Retrieval and query understanding. However, the domain-specific application (sticker retrieval) is somewhat niche compared to the user's broader interests in e-commerce and general IR.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Personalized Sticker Retrieval
- **Aim**: To develop a novel generative framework, PEARL, that addresses the challenges of personalized sticker retrieval by incorporating user-specific information and intent awareness.
- **Rationale**: Existing methods struggle to personalize sticker retrieval, leading to suboptimal results. PEARL aims to overcome this by learning user representations and incorporating intent-aware learning objectives.
- **Ground**: PEARL leverages user demographics (age and gender) and click history to learn personalized user representations. It utilizes a Chain-of-Thought prompting method with a pre-trained language model to rank intent properties for a given query.
- **Experiment**: PEARL is evaluated on a WeChat sticker dataset, comparing its performance to various baseline methods, including popularity-based, traditional retrievers, cross-modal retrievers, and generative retrievers.  Real-world evaluation is conducted through online tests and a case study.
- **Takeaway**: PEARL significantly outperforms all baseline methods, demonstrating superior sticker retrieval performance.  It achieves higher click-through rates, lower average click positions, and a significant positive gain in overall session preference in real-world evaluations.  However, limitations include limited personalization granularity, lack of image modality modeling, limited generalizability, and high economic costs.

#### Abstract
> Formulating information retrieval as a variant of generative modeling,
specifically using autoregressive models to generate relevant identifiers for a
given query, has recently attracted considerable attention. However, its
application to personalized sticker retrieval remains largely unexplored and
presents unique challenges: existing relevance-based generative retrieval
methods typically lack personalization, leading to a mismatch between diverse
user expectations and the retrieved results. To address this gap, we propose
PEARL, a novel generative framework for personalized sticker retrieval, and
make two key contributions: (i) To encode user-specific sticker preferences, we
design a representation learning model to learn discriminative user
representations. It is trained on three prediction tasks that leverage personal
information and click history; and (ii) To generate stickers aligned with a
user's query intent, we propose a novel intent-aware learning objective that
prioritizes stickers associated with higher-ranked intents. Empirical results
from both offline evaluations and online tests demonstrate that PEARL
significantly outperforms state-of-the-art methods.

---

### 3. LongEval at CLEF 2025: Longitudinal Evaluation of IR Systems on Web and Scientific Data

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Matteo Cancellieri, Alaa El-Ebshihy, Tobias Fink, Maik Fr√∂be, Petra Galu≈°ƒç√°kov√°, Gabriela Gonzalez-Saez, Lorraine Goeuriot, David Iommi, J√ºri Keller, Petr Knoth, Philippe Mulhem, Florina Piroi, David Pride, Philipp Schaer
- **URL**: <http://arxiv.org/abs/2509.17469v1>
- **Submitted**: 2025-09-22 08:05:40
- **Topic Keywords**: information retrieval, queries, relevance, retrieval, search
- **Reason**: This paper is highly relevant to Information Retrieval (IR) and Search technologies, specifically focusing on the evaluation of IR systems over time. The use of longitudinal evaluation and temporal assessment of retrieval effectiveness aligns with the user's interests in query understanding and ranking models. However, the focus on evaluation rather than model development or user behavior modeling slightly reduces the score.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Temporal Robustness of Information Retrieval Systems
- **Aim**: To investigate how effectively information retrieval (IR) systems maintain performance over time as document collections, user needs, and relevance judgments evolve.
- **Rationale**: Understanding the temporal robustness of IR systems is crucial as search landscapes are constantly changing.
- **Ground**: The study utilizes two tasks: WebRetrieval (using Qwant data) and SciRetrieval (using CORE collection data) with multiple snapshots to assess performance changes.
- **Experiment**: Two main evaluation setups are employed: (1) Single-system in evolving setup (evaluating a system trained on a snapshot on later snapshots) and (2) Multi-system comparison (comparing systems across multiple snapshots).
- **Takeaway**: While some systems show short-term improvements, few demonstrate consistent long-term effectiveness gains. Leveraging past data for training shows promise. Further research is needed to develop robust IR systems and comprehensive evaluation metrics for temporal robustness.

#### Abstract
> The LongEval lab focuses on the evaluation of information retrieval systems
over time. Two datasets are provided that capture evolving search scenarios
with changing documents, queries, and relevance assessments. Systems are
assessed from a temporal perspective-that is, evaluating retrieval
effectiveness as the data they operate on changes. In its third edition,
LongEval featured two retrieval tasks: one in the area of ad-hoc web retrieval,
and another focusing on scientific article retrieval. We present an overview of
this year's tasks and datasets, as well as the participating systems. A total
of 19 teams submitted their approaches, which we evaluated using nDCG and a
variety of measures that quantify changes in retrieval effectiveness over time.

---

### 4. WildClaims: Information Access Conversations in the Wild(Chat)

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Hideaki Joko, Shakiba Amirshahi, Charles L. A. Clarke, Faegheh Hasibi
- **URL**: <http://arxiv.org/abs/2509.17442v1>
- **Submitted**: 2025-09-22 07:32:06
- **Topic Keywords**: information retrieval, retrieval, search
- **Reason**: This paper explores information access conversations in real-world settings, which aligns with your interests in Information Retrieval and Search technologies. The study focuses on implicit information access, which is a relevant area of research in query understanding and ranking models. The use of a large-scale dataset and novel resource also demonstrates a strong connection to your research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Implicit Information Access in Human-ChatGPT Interactions
- **Aim**: To investigate the prevalence of implicit information access in human-ChatGPT conversations and challenge the traditional focus on explicit information requests.
- **Rationale**: Understanding how humans implicitly access information through conversational interactions with AI is crucial for developing more comprehensive and effective conversational information access systems.
- **Ground**: The WildClaims dataset, comprising 121,905 factual claims extracted from 7,587 utterances in 3,000 WildChat conversations, is used to analyze the prevalence of implicit information access.
- **Experiment**: The dataset was created by automatically extracting factual claims and then manually validating them to identify "check-worthy" claims requiring external verification. Analysis of WildClaims reveals that a significant portion of conversations contain check-worthy assertions.
- **Takeaway**: The findings highlight the need for a broader definition of "conversational information access systems" to encompass implicit factual assertions in diverse conversational contexts. The WildClaims dataset serves as a valuable resource for future research in this area.

#### Abstract
> The rapid advancement of Large Language Models (LLMs) has transformed
conversational systems into practical tools used by millions. However, the
nature and necessity of information retrieval in real-world conversations
remain largely unexplored, as research has focused predominantly on
traditional, explicit information access conversations. The central question
is: What do real-world information access conversations look like? To this end,
we first conduct an observational study on the WildChat dataset, large-scale
user-ChatGPT conversations, finding that users' access to information occurs
implicitly as check-worthy factual assertions made by the system, even when the
conversation's primary intent is non-informational, such as creative writing.
To enable the systematic study of this phenomenon, we release the WildClaims
dataset, a novel resource consisting of 121,905 extracted factual claims from
7,587 utterances in 3,000 WildChat conversations, each annotated for
check-worthiness. Our preliminary analysis of this resource reveals that
conservatively 18% to 51% of conversations contain check-worthy assertions,
depending on the methods employed, and less conservatively, as many as 76% may
contain such assertions. This high prevalence underscores the importance of
moving beyond the traditional understanding of explicit information access, to
address the implicit information access that arises in real-world user-system
conversations.

---

### 5. ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Jan-Felix Klein, Lars Ohnemus
- **URL**: <http://arxiv.org/abs/2509.18063v1>
- **Submitted**: 2025-09-22 17:40:05
- **Comment**: Work in Progess
- **Topic Keywords**: queries, rag
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of Knowledge Graphs and Large Language Models for question answering aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the specific application to Knowledge Graph Question Answering is somewhat outside your primary e-commerce domain focus.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Integration of Large Language Models (LLMs) and Knowledge Graphs (KGs) for improved natural language question answering, particularly focusing on tasks requiring commonsense reasoning over long-tail entities.
- **Aim**: To develop an agent, ARK-V1, that leverages both KG structure and commonsense knowledge to improve question answering accuracy, especially for tasks involving long-tail entities.
- **Rationale**: LLMs alone struggle with commonsense reasoning and long-tail entity understanding. Integrating KGs can provide structured knowledge and improve performance.
- **Ground**: The research builds upon existing work in implicit reasoning benchmarks, LLM-based KG reasoning systems, autonomous KG agents, and datasets focused on commonsense reasoning and entity knowledge.
- **Experiment**: ARK-V1 is evaluated alongside various LLMs on the CoLoTa dataset, a benchmark for entity-based commonsense reasoning over long-tail entities. Performance is measured using Answer Rate, Conditional Accuracy, Overall Accuracy, and Entropy-based Reliability.
- **Takeaway**: While larger LLMs generally perform better, a mid-scale model (Qwen3-30B) demonstrates comparable performance, highlighting the effectiveness of KG integration. ARK-V1 outperforms Chain-of-Thought prompting baselines. Common challenges include ambiguous questions, conflicting KG evidence, and balancing commonsense and KG evidence.

#### Abstract
> Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Zilin Xiao, Qi Ma, Mengting Gu, Chun-cheng Jason Chen, Xintao Chen, Vicente Ordonez, Vijai Mohan
- **URL**: <http://arxiv.org/abs/2509.18095v1>
- **Submitted**: 2025-09-22 17:59:42
- **Topic Keywords**: queries, relevance, retrieval
- **Reason**: The paper focuses on multimodal retrieval, which is related to information retrieval and search technologies. However, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on scalability and efficiency is also relevant to the e-commerce domain.

#### Abstract
> Universal multimodal embedding models have achieved great success in
capturing semantic relevance between queries and candidates. However, current
methods either condense queries and candidates into a single vector,
potentially limiting the expressiveness for fine-grained information, or
produce too many vectors that are prohibitively expensive for multi-vector
retrieval. In this work, we introduce MetaEmbed, a new framework for multimodal
retrieval that rethinks how multimodal embeddings are constructed and
interacted with at scale. During training, a fixed number of learnable Meta
Tokens are appended to the input sequence. At test-time, their last-layer
contextualized representations serve as compact yet expressive multi-vector
embeddings. Through the proposed Matryoshka Multi-Vector Retrieval training,
MetaEmbed learns to organize information by granularity across multiple
vectors. As a result, we enable test-time scaling in multimodal retrieval,
where users can balance retrieval quality against efficiency demands by
selecting the number of tokens used for indexing and retrieval interactions.
Extensive evaluations on the Massive Multimodal Embedding Benchmark (MMEB) and
the Visual Document Retrieval Benchmark (ViDoRe) confirm that MetaEmbed
achieves state-of-the-art retrieval performance while scaling robustly to
models with 32B parameters.

### 7. Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics

- **LLM Score**: 7
- **Keyword Score**: 2
- **Authors**: Kavin R V, Pawan Goyal
- **URL**: <http://arxiv.org/abs/2509.17737v2>
- **Submitted**: 2025-09-22 13:04:48
- **Comment**: 5 pages, 1 figure Accepted at EMNLP 2025 Findings (Short)
- **Topic Keywords**: rag
- **Reason**: This paper explores a novel approach to token representation in NLP, leveraging compositional semantics to achieve extreme compression in embedding parameters. While primarily focused on NLP, the paper touches on aspects of semantic understanding and model optimization, which align with your interests in IR and NLP. However, the paper's focus on token representation and its applications in NLP tasks may not be directly related to your core research themes in IR and search technologies.

#### Abstract
> Standard language models employ unique, monolithic embeddings for each token,
potentially limiting their ability to capture the multifaceted nature of word
meanings. We investigate whether tokens can be more effectively represented
through a compositional structure that accumulates diverse semantic facets. To
explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach
leveraging Product Quantization (PQ). We apply ASG to standard transformer
architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme
across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific
benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing
tokens compositionally via ASG achieves extreme compression in embedding
parameters (0.4--0.5\%) while maintaining $>$95\% task performance relative to
the base model, even in generative tasks and extends to both cross lingual
transfer and domain-specific settings. These results validate the principle
that tokens can be effectively modeled as combinations of shared semantic
building blocks. ASG offers a simple yet concrete method for achieving this,
showcasing how compositional representations can capture linguistic richness
while enabling compact yet semantically rich models.

### 8. AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Lvzhou Luo, Yixuan Cao, Ping Luo
- **URL**: <http://arxiv.org/abs/2509.17486v1>
- **Submitted**: 2025-09-22 08:18:50
- **Comment**: Accepted at EMNLP 2025 (Findings)
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: The paper introduces a novel framework, AttnComp, for context compression in retrieval-augmented generation. While it leverages attention mechanisms and addresses issues in information retrieval, its focus is on generation and compression rather than traditional search or ranking models. It may be of interest for its innovative approach, but it does not directly align with core IR and search themes.

#### Abstract
> Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.

### 9. MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Tianyuan Li, Lei Wang, Ahtamjan Ahmat, Yating Yang, Bo Ma, Rui Dong, Bangju Han
- **URL**: <http://arxiv.org/abs/2509.17359v1>
- **Submitted**: 2025-09-22 05:23:06
- **Topic Keywords**: query, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of multimodal retrieval and semantic understanding. However, the focus on Generative Cross-Modal Retrieval and Multimodal Large Language Models is not a central match to your primary research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Generative cross-modal retrieval, which treats retrieval as a generation
task, has emerged as a promising direction with the rise of Multimodal Large
Language Models (MLLMs). In this setting, the model responds to a text query by
generating an identifier corresponding to the target image. However, existing
methods typically rely on manually crafted string IDs, clustering-based labels,
or atomic identifiers requiring vocabulary expansion, all of which face
challenges in semantic alignment or scalability.To address these limitations,
we propose a vocabulary-efficient identifier generation framework that prompts
MLLMs to generate Structured Semantic Identifiers from image-caption pairs.
These identifiers are composed of concept-level tokens such as objects and
actions, naturally aligning with the model's generation space without modifying
the tokenizer. Additionally, we introduce a Rationale-Guided Supervision
Strategy, prompting the model to produce a one-sentence explanation alongside
each identifier serves as an auxiliary supervision signal that improves
semantic grounding and reduces hallucinations during training.

### 10. Diagnosing Model Editing via Knowledge Spectrum

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Tsung-Hsuan Pan, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen
- **URL**: <http://arxiv.org/abs/2509.17482v1>
- **Submitted**: 2025-09-22 08:16:04
- **Topic Keywords**: relevance, ctr, search
- **Reason**: This paper focuses on model editing in pre-trained language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the primary focus on model editing and knowledge categorization is not directly aligned with the user's core research themes. The paper's relevance to the user's interests is limited to the broader context of NLP and related topics.

#### Abstract
> Model editing, the process of efficiently modifying factual knowledge in
pre-trained language models, is critical for maintaining their accuracy and
relevance. However, existing editing methods often introduce unintended side
effects, degrading model performance in unpredictable ways. While much research
has focused on improving editing algorithms, the role of the target knowledge's
intrinsic properties remains a significant, underexplored factor. This paper
addresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic
framework for categorizing knowledge based on its real-world popularity, the
model's pre-edit familiarity, and the linguistic structure of the eliciting
question. Our empirical analysis reveals that these characteristics are strong
predictors of editing success and stability. Informed by these findings, we
introduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that
tailors editing intensity to the diagnosed difficulty of a knowledge item. We
demonstrate that this framework significantly improves success rates for
challenging edits while optimizing computational resources. Our work provides a
more comprehensive understanding of the factors governing model editing.

### 11. SeqUDA-Rec: Sequential User Behavior Enhanced Recommendation via Global Unsupervised Data Augmentation for Personalized Content Marketing

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Ruihan Luo, Xuanjing Chen, Ziyang Ding
- **URL**: <http://arxiv.org/abs/2509.17361v1>
- **Submitted**: 2025-09-22 05:24:53
- **Topic Keywords**: user behavior, click, recommend
- **Reason**: The paper SeqUDA-Rec is somewhat related to your research interests in Information Retrieval and recommendation systems, but it primarily focuses on recommendation accuracy and robustness in the context of personalized content marketing, which is not a central match to your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Personalized content marketing has become a crucial strategy for digital
platforms, aiming to deliver tailored advertisements and recommendations that
match user preferences. Traditional recommendation systems often suffer from
two limitations: (1) reliance on limited supervised signals derived from
explicit user feedback, and (2) vulnerability to noisy or unintentional
interactions. To address these challenges, we propose SeqUDA-Rec, a novel deep
learning framework that integrates user behavior sequences with global
unsupervised data augmentation to enhance recommendation accuracy and
robustness. Our approach first constructs a Global User-Item Interaction Graph
(GUIG) from all user behavior sequences, capturing both local and global item
associations. Then, a graph contrastive learning module is applied to generate
robust embeddings, while a sequential Transformer-based encoder models users'
evolving preferences. To further enhance diversity and counteract sparse
supervised labels, we employ a GAN-based augmentation strategy, generating
plausible interaction patterns and supplementing training data. Extensive
experiments on two real-world marketing datasets (Amazon Ads and TikTok Ad
Clicks) demonstrate that SeqUDA-Rec significantly outperforms state-of-the-art
baselines such as SASRec, BERT4Rec, and GCL4SR. Our model achieves a 6.7%
improvement in NDCG@10 and 11.3% improvement in HR@10, proving its
effectiveness in personalized advertising and intelligent content
recommendation.

### 12. Identifying and Upweighting Power-Niche Users to Mitigate Popularity Bias in Recommendations

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: David Liu, Erik Weis, Moritz Laber, Tina Eliassi-Rad, Brennan Klein
- **URL**: <http://arxiv.org/abs/2509.17265v1>
- **Submitted**: 2025-09-21 22:41:07
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper is somewhat related to the user's research interests in Information Retrieval and recommender systems, but it focuses more on mitigating popularity bias in recommendations rather than query understanding, ranking models, or user behavior modeling. The paper's emphasis on recommender systems and user activity levels is somewhat tangential to the user's primary focus on IR and deep semantic understanding.

#### Abstract
> Recommender systems have been shown to exhibit popularity bias by
over-recommending popular items and under-recommending relevant niche items. We
seek to understand interactions with niche items in benchmark recommendation
datasets as a step toward mitigating popularity bias. We find that, compared to
mainstream users, niche-preferring users exhibit a longer-tailed activity-level
distribution, indicating the existence of users who both prefer niche items and
exhibit high activity levels. We partition users along two axes: (1) activity
level ("power" vs. "light") and (2) item-popularity preference ("mainstream"
vs. "niche"), and show that in several benchmark datasets, the number of
power-niche users (high activity and niche preference) is statistically
significantly larger than expected under a null configuration model. Motivated
by this observation, we propose a framework for reweighting the Bayesian
Personalized Ranking (BPR) loss that simultaneously reweights based on user
activity level and item popularity. Our method introduces two interpretable
parameters: one controlling the significance of user activity level, and the
other of item popularity. Experiments on benchmark datasets show that
upweighting power-niche users reduces popularity bias and can increase overall
performance. In contrast to previous work that only considers user activity
level or item popularity in isolation, our results suggest that considering
their interaction leads to Pareto-dominant performance.

### 13. Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Selva Ta≈ü, Mahmut El Huseyni, √ñzay Ezerceli, Reyhan Bayraktar, Fatma Bet√ºl Terzioƒülu
- **URL**: <http://arxiv.org/abs/2509.17671v1>
- **Submitted**: 2025-09-22 12:14:11
- **Topic Keywords**: rag, retrieval
- **Reason**: While the paper explores a relevant topic in NLP, it focuses on hallucination detection in Turkish RAG applications, which is somewhat related to query understanding and ranking models in IR. However, the paper's primary contribution is in the development of a hallucination detection model, which is not a central match to the user's core research themes.

#### Abstract
> The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.

### 14. MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song
- **URL**: <http://arxiv.org/abs/2509.17628v1>
- **Submitted**: 2025-09-22 11:36:16
- **Comment**: 10 pages, 5 figures
- **Topic Keywords**: rag, ctr
- **Reason**: The paper MSCoRe is somewhat related to the user's interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it does not directly focus on query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's emphasis on multi-stage collaborative reasoning in LLM agents is an interesting aspect, but it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Large Language Models (LLMs) have excelled in question-answering (QA) tasks
within single domains. However, their reasoning and coordination capabilities
in complex, multi-stage scenarios remain underexplored. Existing benchmarks
typically focus on isolated tasks or narrow domains, overlooking models'
abilities for multi-stage collaboration and optimization without explicit
external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel
benchmark comprising 126696 domain-specific QA instances spanning scenarios in
automotive, pharmaceutical, electronics, and energy sectors. The dataset is
created using a structured three-phase pipeline: dynamic sampling, iterative
question-answer generation, and a multi-level quality assessment to ensure data
quality. Tasks are further categorized into three difficulty levels according
to stage coverage and complexity. With MSCoRe, we have conducted a
comprehensive evaluation of various state-of-the-art LLM agents. The commercial
models performed best across all tasks and scenarios, but a notable gap in
ROUGE scores remains between simple and complex tasks. We also tested the
models' robustness and found that their performance is negatively affected by
noisy data. MSCoRe provides a valuable new resource for the community to
evaluate and improve multi-stage reasoning in LLM agents. The code and data are
available at https://github.com/D3E0-source/MSCoRE.

### 15. EpiCache: Episodic KV Cache Management for Long Conversational Question Answering

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho
- **URL**: <http://arxiv.org/abs/2509.17396v1>
- **Submitted**: 2025-09-22 06:56:35
- **Topic Keywords**: query, search
- **Reason**: The paper focuses on Key-Value cache management for long conversational question answering, which is related to information retrieval and query understanding. However, it primarily deals with NLP and caching techniques, which, while relevant, do not directly align with the user's core research themes in IR and search technologies.

#### Abstract
> Recent advances in large language models (LLMs) have extended context
lengths, enabling assistants to sustain long histories for coherent,
personalized responses. This ability, however, hinges on Key-Value (KV)
caching, whose memory grows linearly with dialogue length and quickly dominates
under strict resource constraints. An active line of research for reducing this
overhead is KV cache compression, which seeks to limit cache size while
preserving accuracy. Yet existing methods face two major limitations: (i)
evicting entries after full-context prefill causes unbounded peak memory, and
(ii) query-dependent eviction narrows the cache to a single query, leading to
degraded accuracy in multi-turn conversations. We introduce EpiCache, a
training-free KV cache management framework for long conversational question
answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth
through block-wise prefill and preserves topic-relevant context via episodic KV
compression, which clusters conversation history into coherent episodes and
applies episode-specific KV cache eviction. We further design an adaptive
layer-wise budget allocation strategy that measures each layer's sensitivity to
eviction and distributes the memory budget across layers accordingly. Across
three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over
recent baselines, sustains near-full KV accuracy under 4-6x compression, and
reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient
multi-turn interaction under strict resource constraints.

### 16. Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Abdullah Mushtaq, Muhammad Rafay Naeem, Ibrahim Ghaznavi, Alaa Abd-alrazaq, Aliya Tabassum, Junaid Qadir
- **URL**: <http://arxiv.org/abs/2509.17240v1>
- **Submitted**: 2025-09-21 21:17:23
- **Topic Keywords**: relevance, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of evaluating and improving systematic literature reviews. However, the focus on Multi-Agent Systems and LLM-based evaluation copilot is not directly aligned with your core interests in query understanding, ranking models, and user behavior modeling. The connection to NLP is relevant, but the application is more focused on knowledge aggregation and review process streamlining.

#### Abstract
> Systematic Literature Reviews (SLRs) are foundational to evidence-based
research but remain labor-intensive and prone to inconsistency across
disciplines. We present an LLM-based SLR evaluation copilot built on a
Multi-Agent System (MAS) architecture to assist researchers in assessing the
overall quality of the systematic literature reviews. The system automates
protocol validation, methodological assessment, and topic relevance checks
using a scholarly database. Unlike conventional single-agent methods, our
design integrates a specialized agentic approach aligned with PRISMA guidelines
to support more structured and interpretable evaluations. We conducted an
initial study on five published SLRs from diverse domains, comparing system
outputs to expert-annotated PRISMA scores, and observed 84% agreement. While
early results are promising, this work represents a first step toward scalable
and accurate NLP-driven systems for interdisciplinary workflows and reveals
their capacity for rigorous, domain-agnostic knowledge aggregation to
streamline the review process.

### 17. LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Junsong Li, Jie Zhou, Bihao Zhan, Yutao Yang, Qianjun Pan, Shilian Chen, Tianyu Huai, Xin Li, Qin Chen, Liang He
- **URL**: <http://arxiv.org/abs/2509.17183v1>
- **Submitted**: 2025-09-21 18:06:05
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on lifelong alignment for Large Language Models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the primary focus on language models and preference alignment does not directly align with the user's core research themes in IR and Search technologies. The paper's relevance is mostly in the broader context of NLP and related topics.

#### Abstract
> Alignment plays a crucial role in Large Language Models (LLMs) in aligning
with human preferences on a specific task/domain. Traditional alignment methods
suffer from catastrophic forgetting, where models lose previously acquired
knowledge when adapting to new preferences or domains. We introduce LifeAlign,
a novel framework for lifelong alignment that enables LLMs to maintain
consistent human preference alignment across sequential learning tasks without
forgetting previously learned knowledge. Our approach consists of two key
innovations. First, we propose a focalized preference optimization strategy
that aligns LLMs with new preferences while preventing the erosion of knowledge
acquired from previous tasks. Second, we develop a short-to-long memory
consolidation mechanism that merges denoised short-term preference
representations into stable long-term memory using intrinsic dimensionality
reduction, enabling efficient storage and retrieval of alignment patterns
across diverse domains. We evaluate LifeAlign across multiple sequential
alignment tasks spanning different domains and preference types. Experimental
results demonstrate that our method achieves superior performance in
maintaining both preference alignment quality and knowledge retention compared
to existing lifelong learning approaches. The codes and datasets will be
released on GitHub.

### 18. Cross-Attention is Half Explanation in Speech-to-Text Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Sara Papi, Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli
- **URL**: <http://arxiv.org/abs/2509.18010v1>
- **Submitted**: 2025-09-22 16:49:26
- **Topic Keywords**: relevance
- **Reason**: This paper explores the explanatory power of cross-attention in speech-to-text models, which is a topic within NLP. While it touches on attention mechanisms, it does not directly relate to query understanding, ranking models, or user behavior modeling in information retrieval, which are your core research interests.

#### Abstract
> Cross-attention is a core mechanism in encoder-decoder architectures,
widespread in many fields, including speech-to-text (S2T) processing. Its
scores have been repurposed for various downstream applications--such as
timestamp estimation and audio-text alignment--under the assumption that they
reflect the dependencies between input speech representation and the generated
text. While the explanatory nature of attention mechanisms has been widely
debated in the broader NLP literature, this assumption remains largely
unexplored within the speech domain. To address this gap, we assess the
explanatory power of cross-attention in S2T models by comparing its scores to
input saliency maps derived from feature attribution. Our analysis spans
monolingual and multilingual, single-task and multi-task models at multiple
scales, and shows that attention scores moderately to strongly align with
saliency-based explanations, particularly when aggregated across heads and
layers. However, it also shows that cross-attention captures only about 50% of
the input relevance and, in the best case, only partially reflects how the
decoder attends to the encoder's representations--accounting for just 52-75% of
the saliency. These findings uncover fundamental limitations in interpreting
cross-attention as an explanatory proxy, suggesting that it offers an
informative yet incomplete view of the factors driving predictions in S2T
models.

### 19. Training-free Truthfulness Detection via Value Vectors in LLMs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Runheng Liu, Heyan Huang, Xingchen Xiao, Zhijing Wu
- **URL**: <http://arxiv.org/abs/2509.17932v1>
- **Submitted**: 2025-09-22 15:54:29
- **Topic Keywords**: rag, search
- **Reason**: The paper explores truthfulness detection in Large Language Models (LLMs), which is a related topic to Natural Language Processing (NLP). However, it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the user's core research themes.

#### Abstract
> Large language models often generate factually incorrect outputs, motivating
efforts to detect the truthfulness of their content. Most existing approaches
rely on training probes over internal activations, but these methods suffer
from scalability and generalization issues. A recent training-free method,
NoVo, addresses this challenge by exploiting statistical patterns from the
model itself. However, it focuses exclusively on attention mechanisms,
potentially overlooking the MLP module-a core component of Transformer models
known to support factual recall. In this paper, we show that certain value
vectors within MLP modules exhibit truthfulness-related statistical patterns.
Building on this insight, we propose TruthV, a simple and interpretable
training-free method that detects content truthfulness by leveraging these
value vectors. On the NoVo benchmark, TruthV significantly outperforms both
NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being
neglected in prior training-free efforts-encode rich and useful signals for
truthfulness detection. These findings offer new insights into how truthfulness
is internally represented in LLMs and motivate further research on scalable and
interpretable truthfulness detection.

### 20. How Persuasive is Your Context?

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Tu Nguyen, Kevin Du, Alexander Miserlis Hoyle, Ryan Cotterell
- **URL**: <http://arxiv.org/abs/2509.17879v1>
- **Submitted**: 2025-09-22 15:15:40
- **Comment**: Long paper accepted at EMNLP 2025
- **Topic Keywords**: queries
- **Reason**: The paper explores language model behavior and introduces a new metric for measuring persuasion, which is related to query understanding and model behavior modeling. However, it does not directly focus on information retrieval, ranking models, or recommender systems, which are core areas of your research interests.

#### Abstract
> Two central capabilities of language models (LMs) are: (i) drawing on prior
knowledge about entities, which allows them to answer queries such as "What's
the official language of Austria?", and (ii) adapting to new information
provided in context, e.g., "Pretend the official language of Austria is
Tagalog.", that is pre-pended to the question. In this article, we introduce
targeted persuasion score (TPS), designed to quantify how persuasive a given
context is to an LM where persuasion is operationalized as the ability of the
context to alter the LM's answer to the question. In contrast to evaluating
persuasiveness only by inspecting the greedily decoded answer under the model,
TPS provides a more fine-grained view of model behavior. Based on the
Wasserstein distance, TPS measures how much a context shifts a model's original
answer distribution toward a target distribution. Empirically, through a series
of experiments, we show that TPS captures a more nuanced notion of
persuasiveness than previously proposed metrics.

### 21. Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Dongxu Lu, Johan Jeuring, Albert Gatt
- **URL**: <http://arxiv.org/abs/2509.17694v1>
- **Submitted**: 2025-09-22 12:33:02
- **Comment**: Accepted for publication at the 18th International Natural Language
  Generation Conference (INLG 2025)
- **Topic Keywords**: pairwise
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and related topics, but it does not directly align with your primary focus on Information Retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization. The paper explores the evaluation of large language models in role-play dialogues, which is a specific application of NLP. While it may have some indirect implications for search technologies, it is not a central match for your research interests.

#### Abstract
> Evaluating large language models (LLMs) in long-form, knowledge-grounded
role-play dialogues remains challenging. This study compares LLM-generated and
human-authored responses in multi-turn professional training simulations
through human evaluation ($N=38$) and automated LLM-as-a-judge assessment.
Human evaluation revealed significant degradation in LLM-generated response
quality across turns, particularly in naturalness, context maintenance and
overall quality, while human-authored responses progressively improved. In line
with this finding, participants also indicated a consistent preference for
human-authored dialogue. These human judgements were validated by our automated
LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment
with human evaluators on both zero-shot pairwise preference and stochastic
6-shot construct ratings, confirming the widening quality gap between LLM and
human responses over time. Our work contributes a multi-turn benchmark exposing
LLM degradation in knowledge-grounded role-play dialogues and provides a
validated hybrid evaluation framework to guide the reliable integration of LLMs
in training simulations.

### 22. When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Shenghao Ye, Yu Guo, Dong Jin, Yikai Shen, Yunpeng Hou, Shuangwu Chen, Jian Yang, Xiaofeng Jiang
- **URL**: <http://arxiv.org/abs/2509.17680v1>
- **Submitted**: 2025-09-22 12:25:57
- **Comment**: 23 pages, 24 figures
- **Topic Keywords**: relevance
- **Reason**: The paper focuses on Table Question Answering (TableQA), a task in NLP, and proposes a dual denoising framework to address noisy data. While it involves query understanding and relevance filtering, it is not directly related to the user's core research themes in Information Retrieval and Search technologies. The paper's emphasis on NLP and table pruning is somewhat relevant, but it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Table question answering (TableQA) is a fundamental task in natural language
processing (NLP). The strong reasoning capabilities of large language models
(LLMs) have brought significant advances in this field. However, as real-world
applications involve increasingly complex questions and larger tables,
substantial noisy data is introduced, which severely degrades reasoning
performance. To address this challenge, we focus on improving two core
capabilities: Relevance Filtering, which identifies and retains information
truly relevant to reasoning, and Table Pruning, which reduces table size while
preserving essential content. Based on these principles, we propose EnoTab, a
dual denoising framework for complex questions and large-scale tables.
Specifically, we first perform Evidence-based Question Denoising by decomposing
the question into minimal semantic units and filtering out those irrelevant to
answer reasoning based on consistency and usability criteria. Then, we propose
Evidence Tree-guided Table Denoising, which constructs an explicit and
transparent table pruning path to remove irrelevant data step by step. At each
pruning step, we observe the intermediate state of the table and apply a
post-order node rollback mechanism to handle abnormal table states, ultimately
producing a highly reliable sub-table for final answer reasoning. Finally,
extensive experiments show that EnoTab achieves outstanding performance on
TableQA tasks with complex questions and large-scale tables, confirming its
effectiveness.

### 23. Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Michal Nov√°k, Miloslav Konop√≠k, Anna Nedoluzhko, Martin Popel, Ond≈ôej Pra≈æ√°k, Jakub Sido, Milan Straka, Zdenƒõk ≈Ωabokrtsk√Ω, Daniel Zeman
- **URL**: <http://arxiv.org/abs/2509.17796v1>
- **Submitted**: 2025-09-22 13:52:32
- **Comment**: Accepted to CODI-CRAC 2025
- **Topic Keywords**: rag
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it is not directly focused on Information Retrieval (IR), query understanding, or ranking models. The topic of multilingual coreference resolution is relevant to NLP, but it is not a primary area of focus for the user. The paper's findings on the potential of Large Language Models (LLMs) may be of interest, but it is not a central match for the user's research interests.

#### Abstract
> The paper presents an overview of the fourth edition of the Shared Task on
Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025
workshop. As in the previous editions, participants were challenged to develop
systems that identify mentions and cluster them according to identity
coreference.
  A key innovation of this year's task was the introduction of a dedicated
Large Language Model (LLM) track, featuring a simplified plaintext format
designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional
languages, using version 1.3 of CorefUD - a harmonized multilingual collection
of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two
fine-tuned and two using few-shot adaptation). While traditional systems still
kept the lead, LLMs showed clear potential, suggesting they may soon challenge
established approaches in future editions.

### 24. Human vs. Agent in Task-Oriented Conversations

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Zhefan Wang, Ning Geng, Zhiqiang Guo, Weizhi Ma, Min Zhang
- **URL**: <http://arxiv.org/abs/2509.17619v1>
- **Submitted**: 2025-09-22 11:30:39
- **Comment**: SIGIR-AP 2025
- **Topic Keywords**: user behavior
- **Reason**: This paper explores task-oriented conversational systems and user simulation, which is somewhat related to information retrieval and search technologies. However, the focus on conversational systems and user behavior modeling is not a central match to the user's primary research interests in query understanding, ranking models, and user behavior modeling in the context of search technologies.

#### Abstract
> Task-oriented conversational systems are essential for efficiently addressing
diverse user needs, yet their development requires substantial amounts of
high-quality conversational data that is challenging and costly to obtain.
While large language models (LLMs) have demonstrated potential in generating
synthetic conversations, the extent to which these agent-generated interactions
can effectively substitute real human conversations remains unclear. This work
presents the first systematic comparison between LLM-simulated users and human
users in personalized task-oriented conversations. We propose a comprehensive
analytical framework encompassing three key aspects (conversation strategy,
interaction style, and conversation evaluation) and ten distinct dimensions for
evaluating user behaviors, and collect parallel conversational datasets from
both human users and LLM agent users across four representative scenarios under
identical conditions. Our analysis reveals significant behavioral differences
between the two user types in problem-solving approaches, question broadness,
user engagement, context dependency, feedback polarity and promise, language
style, and hallucination awareness. We found consistency in the agent users and
human users across the depth-first or breadth-first dimensions, as well as the
usefulness dimensions. These findings provide critical insights for advancing
LLM-based user simulation. Our multi-dimensional taxonomy constructed a
generalizable framework for analyzing user behavior patterns, offering insights
from LLM agent users and human users. By this work, we provide perspectives on
rethinking how to use user simulation in conversational systems in the future.

### 25. CorefInst: Leveraging LLMs for Multilingual Coreference Resolution

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Tuƒüba Pamay Arslan, Emircan Erol, G√ºl≈üen Eryiƒüit
- **URL**: <http://arxiv.org/abs/2509.17505v1>
- **Submitted**: 2025-09-22 08:35:21
- **Comment**: Accepted for publication in Transactions of the Association for
  Computational Linguistics (TACL) (2025 August). Submission: March, 2025.
  Revision: July, 2025. Acceptance: August, 2025
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it primarily focuses on coreference resolution, which is a specific task within NLP. While it leverages LLMs, a topic of interest in the user's background, the paper's focus on coreference resolution and instruction tuning is not directly aligned with the user's core research themes in Information Retrieval and Search technologies.

#### Abstract
> Coreference Resolution (CR) is a crucial yet challenging task in natural
language understanding, often constrained by task-specific architectures and
encoder-based language models that demand extensive training and lack
adaptability. This study introduces the first multilingual CR methodology which
leverages decoder-only LLMs to handle both overt and zero mentions. The article
explores how to model the CR task for LLMs via five different instruction sets
using a controlled inference method. The approach is evaluated across three
LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when
instruction-tuned with a suitable instruction set, can surpass state-of-the-art
task-specific architectures. Specifically, our best model, a fully fine-tuned
Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model
(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages
in the CorefUD v1.2 dataset collection.

### 26. Simplified Longitudinal Retrieval Experiments: A Case Study on Query Expansion and Document Boosting

- **LLM Score**: 2
- **Keyword Score**: 15
- **Authors**: J√ºri Keller, Maik Fr√∂be, Gijs Hendriksen, Daria Alexander, Martin Potthast, Philipp Schaer
- **URL**: <http://arxiv.org/abs/2509.17440v1>
- **Submitted**: 2025-09-22 07:29:34
- **Comment**: Best of labs paper for LongEval at CLEF 2024
- **Topic Keywords**: query, queries, relevance, relevance feedback, retrieval, search
- **Reason**: This paper focuses on the evaluation of retrieval systems, proposing a custom extension for longitudinal retrieval experiments. However, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research. The paper's focus on evaluation methodology and tooling does not align closely with your primary research themes.

#### Abstract
> The longitudinal evaluation of retrieval systems aims to capture how
information needs and documents evolve over time. However, classical
Cranfield-style retrieval evaluations only consist of a static set of queries
and documents and thereby miss time as an evaluation dimension. Therefore,
longitudinal evaluations need to complement retrieval toolkits with custom
logic. This custom logic increases the complexity of research software, which
might reduce the reproducibility and extensibility of experiments. Based on our
submissions to the 2024 edition of LongEval, we propose a custom extension of
ir_datasets for longitudinal retrieval experiments. This extension allows for
declaratively, instead of imperatively, describing important aspects of
longitudinal retrieval experiments, e.g., which queries, documents, and/or
relevance feedback are available at which point in time. We reimplement our
submissions to LongEval 2024 against our new ir_datasets extension, and find
that the declarative access can reduce the complexity of the code.

### 27. MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Woongkyu Lee, Junhee Cho, Jungwook Choi
- **URL**: <http://arxiv.org/abs/2509.17489v1>
- **Submitted**: 2025-09-22 08:19:11
- **Topic Keywords**: retriever, rag, retrieval, rank
- **Reason**: This paper focuses on multi-agent coding and code generation using large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, the primary focus is on code generation and multi-agent systems, making it somewhat tangential to your research areas.

#### Abstract
> Large language models (LLMs) have advanced code generation from
single-function tasks to competitive-programming problems, but existing
multi-agent solutions either rely on costly large-scale ($>$ 30B) models or
collapse when downsized to small open-source models. We present MapCoder-Lite,
which upgrades a single 7B model into four role-specialised agents-retriever,
planner, coder, and debugger-using only rank-32, role-specific LoRA adapters
($<3\%$ extra parameters). Three lightweight techniques make this possible: (i)
trajectory distillation from strong LLMs fixes format fragility in retrieval
and debugging, (ii) supervisor-guided correction strengthens planning and
coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient
specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests
shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\%$ to
$28.3\%$), eliminates all format failures, and closes to within six points of a
32B baseline while cutting GPU memory and token-generation time by $4\times$.
These results demonstrate that careful agent-wise fine-tuning unleashes
high-quality multi-agent coding on a small language model.

### 28. LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Yeonsun Yang, Sang Won Lee, Jean Y. Song, Sangdoo Yun, Young-Ho Kim
- **URL**: <http://arxiv.org/abs/2509.17477v1>
- **Submitted**: 2025-09-22 08:12:10
- **Comment**: 17 pages except reference
- **Topic Keywords**: queries, relevance, rag
- **Reason**: This paper appears to be primarily focused on language learning and AI-generated quizzes for ESL workers, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves AI and user queries, the context and application are distinct from the user's areas of focus.

#### Abstract
> Non-native English speakers performing English-related tasks at work struggle
to sustain ESL learning, despite their motivation. Often, study materials are
disconnected from their work context. Although workers rely on LLM assistants
to address their immediate needs, these interactions may not directly
contribute to their English skills. We present LingoQ, an AI-mediated system
that allows workers to practice English using quizzes generated from their LLM
queries during work. LingoQ leverages these queries using AI to generate
personalized quizzes that workers can review and practice on their smartphones.
We conducted a three-week deployment study with 28 ESL workers to evaluate
LingoQ. Participants valued the relevance of quizzes that reflect their own
context, constantly engaging with the app during the study. This active
engagement improved self-efficacy and led to learning gains for beginners and,
potentially, for intermediate learners. We discuss opportunities of leveraging
users' reliance on LLMs to situate their learning in the user context for
improved learning.

### 29. A Knowledge Graph-based Retrieval-Augmented Generation Framework for Algorithm Selection in the Facility Layout Problem

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Nikhil N S, Amol Dilip Joshi, Bilal Muhammed, Soban Babu
- **URL**: <http://arxiv.org/abs/2509.18054v1>
- **Submitted**: 2025-09-22 17:29:10
- **Comment**: 10 pages, 5 figures
- **Topic Keywords**: rag, retrieval, recommend, search
- **Reason**: This paper focuses on a Knowledge Graph-based Retrieval-Augmented Generation framework for algorithm selection in the Facility Layout Problem, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it does involve a Knowledge Graph and a Large Language Model, the context is specific to algorithm selection and not relevant to the user's core research themes.

#### Abstract
> Selecting a solution algorithm for the Facility Layout Problem (FLP), an
NP-hard optimization problem with a multiobjective trade-off, is a complex task
that requires deep expert knowledge. The performance of a given algorithm
depends on specific problem characteristics such as its scale, objectives, and
constraints. This creates a need for a data-driven recommendation method to
guide algorithm selection in automated design systems. This paper introduces a
new recommendation method to make such expertise accessible, based on a
Knowledge Graph-based Retrieval-Augmented Generation (KG RAG) framework. To
address this, a domain-specific knowledge graph is constructed from published
literature. The method then employs a multi-faceted retrieval mechanism to
gather relevant evidence from this knowledge graph using three distinct
approaches, which include a precise graph-based search, flexible vector-based
search, and high-level cluster-based search. The retrieved evidence is utilized
by a Large Language Model (LLM) to generate algorithm recommendations with
data-driven reasoning. The proposed KG-RAG method is compared against a
commercial LLM chatbot with access to the knowledge base as a table, across a
series of diverse, real-world FLP test cases. Based on recommendation accuracy
and reasoning capability, the proposed method performed significantly better
than the commercial LLM chatbot.

### 30. RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Fei Zhao, Chengqiang Lu, Yufan Shen, Qimeng Wang, Yicheng Qian, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Zhen Wu, Shangyu Xing, Xinyu Dai
- **URL**: <http://arxiv.org/abs/2509.17421v1>
- **Submitted**: 2025-09-22 07:14:31
- **Comment**: Findings of EMNLP 2025 camera-ready
- **Topic Keywords**: relevance, rag, search
- **Reason**: This paper focuses on a Chinese multi-image understanding benchmark, which is not directly related to Information Retrieval, query understanding, ranking models, or user behavior modeling. While it involves multimodal LLMs, the context is specific to Chinese multi-image understanding and does not align with the user's core research themes.

#### Abstract
> While various multimodal multi-image evaluation datasets have been emerged,
but these datasets are primarily based on English, and there has yet to be a
Chinese multi-image dataset. To fill this gap, we introduce RealBench, the
first Chinese multimodal multi-image dataset, which contains 9393 samples and
69910 images. RealBench distinguishes itself by incorporating real
user-generated content, ensuring high relevance to real-world applications.
Additionally, the dataset covers a wide variety of scenes, image resolutions,
and image structures, further increasing the difficulty of multi-image
understanding. Ultimately, we conduct a comprehensive evaluation of RealBench
using 21 multimodal LLMs of different sizes, including closed-source models
that support multi-image inputs as well as open-source visual and video models.
The experimental results indicate that even the most powerful closed-source
models still face challenges when handling multi-image Chinese scenarios.
Moreover, there remains a noticeable performance gap of around 71.8\% on
average between open-source visual/video models and closed-source models. These
results show that RealBench provides an important research foundation for
further exploring multi-image understanding capabilities in the Chinese
context.

### 31. Specification-Aware Machine Translation and Evaluation for Purpose Alignment

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yoko Kayano, Saku Sugawara
- **URL**: <http://arxiv.org/abs/2509.17559v1>
- **Submitted**: 2025-09-22 10:50:37
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper focuses on machine translation and evaluation, which is outside your primary research interests in Information Retrieval and Search technologies. While it touches on the concept of 'specifications' which could be related to query understanding, the context is not relevant to your core themes.

#### Abstract
> In professional settings, translation is guided by communicative goals and
client needs, often formalized as specifications. While existing evaluation
frameworks acknowledge the importance of such specifications, these
specifications are often treated only implicitly in machine translation (MT)
research. Drawing on translation studies, we provide a theoretical rationale
for why specifications matter in professional translation, as well as a
practical guide to implementing specification-aware MT and evaluation. Building
on this foundation, we apply our framework to the translation of investor
relations texts from 33 publicly listed companies. In our experiment, we
compare five translation types, including official human translations and
prompt-based outputs from large language models (LLMs), using expert error
analysis, user preference rankings, and an automatic metric. The results show
that LLM translations guided by specifications consistently outperformed
official human translations in human evaluations, highlighting a gap between
perceived and expected quality. These findings demonstrate that integrating
specifications into MT workflows, with human oversight, can improve translation
quality in ways aligned with professional practice.

### 32. FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Tianshi Cai, Guanxu Li, Nijia Han, Ce Huang, Zimu Wang, Changyu Zeng, Yuqi Wang, Jingshi Zhou, Haiyang Zhang, Qi Chen, Yushan Pan, Shuihua Wang, Wei Wang
- **URL**: <http://arxiv.org/abs/2509.17395v1>
- **Submitted**: 2025-09-22 06:56:27
- **Comment**: Accepted at FinNLP@EMNLP 2025. Camera-ready version
- **Topic Keywords**: rag, retrieval, recommend
- **Reason**: This paper appears to be focused on financial analysis and multi-agent collaboration, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions Retrieval-Augmented Generation (RAG), it is applied in a specific domain (financial analysis) and does not seem to address the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.

### 33. Program Synthesis via Test-Time Transduction

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Kang-il Lee, Jahyun Koo, Seunghyun Yoon, Minbeom Kim, Hyukhun Koh, Dongryeol Lee, Kyomin Jung
- **URL**: <http://arxiv.org/abs/2509.17393v2>
- **Submitted**: 2025-09-22 06:53:32
- **Comment**: NeurIPS 2025
- **Topic Keywords**: queries, rag
- **Reason**: This paper focuses on program synthesis, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it involves a novel framework using an LLM, the context and application are distinct from the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> We introduce transductive program synthesis, a new formulation of the program
synthesis task that explicitly leverages test inputs during synthesis. While
prior approaches to program synthesis--whether based on natural language
descriptions or input-output examples--typically aim to generalize from
training examples, they often struggle with robustness, especially in
real-world settings where training examples are limited and test inputs involve
various edge cases. To address this, we propose a novel framework that improves
robustness by treating synthesis as an active learning over a finite hypothesis
class defined by programs' outputs. We use an LLM to predict outputs for
selected test inputs and eliminate inconsistent hypotheses, where the inputs
are chosen via a greedy maximin algorithm to minimize the number of LLM queries
required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC,
and programmatic world modeling on MiniGrid. We demonstrate that our method
significantly improves program synthesis in both accuracy and efficiency. We
release our code at https://github.com/klee972/SYNTRA.

### 34. PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Yan Zhuang, Yuan Sun
- **URL**: <http://arxiv.org/abs/2509.17669v1>
- **Submitted**: 2025-09-22 12:12:41
- **Topic Keywords**: relevance, search
- **Reason**: This paper focuses on Controllable Text Generation, which is a topic in Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR), Search technologies, or query understanding, which are the core areas of your research interests.

#### Abstract
> With the rapid development of Large Language Models (LLMs), Controllable Text
Generation (CTG) has become a critical technology for enhancing system
reliability and user experience. Addressing the limitations of traditional
methods, this paper proposes the PG-CE (Progressive Generation with Constraint
Enhancement) approach, which decomposes CTG tasks into three steps: type
prediction, constraint construction, and guided generation. This method employs
constraint generation models to dynamically build multi-dimensional constraints
including tone, expression style, and thematic focus to guide output.
Experiments demonstrate that PG-CE significantly improves generation quality
across multiple scenarios while maintaining text controllability, thematic
relevance, and response practicality. The research developed a dataset
containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and
other topics), effectively reflecting real-world application requirements.

### 35. D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Satyapriya Krishna, Andy Zou, Rahul Gupta, Eliot Krzysztof Jones, Nick Winter, Dan Hendrycks, J. Zico Kolter, Matt Fredrikson, Spyros Matsoukas
- **URL**: <http://arxiv.org/abs/2509.17938v1>
- **Submitted**: 2025-09-22 15:59:40
- **Comment**: Preprint
- **Topic Keywords**: query
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. While it touches on the topic of Large Language Models, which is a related area, the focus on detecting deceptive reasoning in LLMs is not a central match for your research themes.

#### Abstract
> The safety and alignment of Large Language Models (LLMs) are critical for
their responsible deployment. Current evaluation methods predominantly focus on
identifying and preventing overtly harmful outputs. However, they often fail to
address a more insidious failure mode: models that produce benign-appearing
outputs while operating on malicious or deceptive internal reasoning. This
vulnerability, often triggered by sophisticated system prompt injections,
allows models to bypass conventional safety filters, posing a significant,
underexplored risk. To address this gap, we introduce the Deceptive Reasoning
Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy
between a model's internal reasoning process and its final output. D-REX was
constructed through a competitive red-teaming exercise where participants
crafted adversarial system prompts to induce such deceptive behaviors. Each
sample in D-REX contains the adversarial system prompt, an end-user's test
query, the model's seemingly innocuous response, and, crucially, the model's
internal chain-of-thought, which reveals the underlying malicious intent. Our
benchmark facilitates a new, essential evaluation task: the detection of
deceptive alignment. We demonstrate that D-REX presents a significant challenge
for existing models and safety mechanisms, highlighting the urgent need for new
techniques that scrutinize the internal processes of LLMs, not just their final
outputs.

### 36. Qwen3-Omni Technical Report

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin
- **URL**: <http://arxiv.org/abs/2509.17765v1>
- **Submitted**: 2025-09-22 13:26:24
- **Comment**: https://github.com/QwenLM/Qwen3-Omni
- **Topic Keywords**: rag, search
- **Reason**: This paper presents a multimodal model for text, image, audio, and video tasks, but it does not appear to be directly related to information retrieval, search technologies, or query understanding. While it does involve natural language processing, the focus is on multimodal perception and generation rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.

### 37. TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Daiye Miao, Yufang Liu, Jie Wang, Changzhi Sun, Yunke Zhang, Demei Yan, Shaokang Dong, Qi Zhang, Yuanbin Wu
- **URL**: <http://arxiv.org/abs/2509.17688v1>
- **Submitted**: 2025-09-22 12:29:43
- **Comment**: Accepted to EMNLP 2025 (Main Conference),13 pages,10 figures
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on parameter-efficient model adaptation using LoRA, which is not directly related to information retrieval, query understanding, or ranking models. While it involves fine-tuning and model adaptation, the context is more aligned with deep learning and model optimization, rather than search technologies or user behavior modeling.

#### Abstract
> LoRA has become one of the most widely used parameter-efficient fine-tuning
methods due to its simplicity and effectiveness. However, numerous studies have
shown that LoRA often introduces substantial parameter redundancy, which not
only increases the number of trainable parameters but also hinders the
effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is
inherently difficult, how to eliminate them efficiently and accurately remains
a challenging problem. In this paper, we propose TASO, a redundancy reduction
method that leverages importance information from the pretrained model's
weights to mitigate LoRA redundancy. Specifically, we estimate parameter
importance on downstream tasks and identify task-specific core regions based on
the distribution of importance scores. The location information of these core
regions is then used to determine the sparse structure of LoRA modules,
enabling redundancy removal before fine-tuning. Our approach significantly
reduces the number of trainable parameters required for task adaptation, while
providing a novel task-aligned perspective for LoRA redundancy reduction.
Experimental results demonstrate that, with a parameter budget comparable to
LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across
multiple tasks, achieving strong fine-tuning performance while effectively
eliminating redundant parameters.

### 38. SLAyiNG: Towards Queer Language Processing

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Leonor Veloso, Lea Hirlimann, Philipp Wicke, Hinrich Sch√ºtze
- **URL**: <http://arxiv.org/abs/2509.17449v1>
- **Submitted**: 2025-09-22 07:41:45
- **Comment**: To be presented at Queer in AI @ NeurIPS 2025 (non-archival)
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on queer language processing, which is not a central theme in your research interests. While it involves natural language processing, it is not directly related to information retrieval, query understanding, or ranking models, which are your primary areas of focus.

#### Abstract
> Knowledge of slang is a desirable feature of LLMs in the context of user
interaction, as slang often reflects an individual's social identity. Several
works on informal language processing have defined and curated benchmarks for
tasks such as detection and identification of slang. In this paper, we focus on
queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke
negative responses from LLMs during user interaction. Research efforts so far
have not focused explicitly on queer slang. In particular, detection and
processing of queer slang have not been thoroughly evaluated due to the lack of
a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the
first dataset containing annotated queer slang derived from subtitles, social
media posts, and podcasts, reflecting real-world usage. We describe our data
curation process, including the collection of slang terms and definitions,
scraping sources for examples that reflect usage of these terms, and our
ongoing annotation process. As preliminary results, we calculate
inter-annotator agreement for human annotators and OpenAI's model o3-mini,
evaluating performance on the task of sense disambiguation. Reaching an average
Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models
can serve as tools for pre-filtering, but the complex and often sensitive
nature of queer language data requires expert and community-driven annotation
efforts.

### 39. LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ala Jararweh, Michael Adams, Avinash Sahu, Abdullah Mueen, Afsah Anwar
- **URL**: <http://arxiv.org/abs/2509.17337v1>
- **Submitted**: 2025-09-22 03:14:22
- **Topic Keywords**: queries
- **Reason**: This paper focuses on source code analysis and vulnerability reasoning, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves question-answering and multimodal LLMs, the context is specific to software systems and security, making it somewhat tangential to the user's interests.

#### Abstract
> Increasing complexity in software systems places a growing demand on
reasoning tools that unlock vulnerabilities manifest in source code. Many
current approaches focus on vulnerability analysis as a classifying task,
oversimplifying the nuanced and context-dependent real-world scenarios. Even
though current code large language models (LLMs) excel in code understanding,
they often pay little attention to security-specific reasoning. We propose
LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code
through question-answering (QA). Our model is trained to integrate paired code
and natural queries into a unified space, enhancing reasoning and
context-dependent insights about code vulnerability. To evaluate our model
performance, we construct a curated dataset of real-world vulnerabilities
paired with security-focused questions and answers. Our model outperforms
state-of-the-art general-purpose and code LLMs in the QA and detection tasks.
We further explain decision-making by conducting qualitative analysis to
highlight capabilities and limitations. By integrating code and QA, LLaVul
enables more interpretable and security-focused code understanding.

### 40. Probabilistic Token Alignment for Large Language Model Fusion

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Runjia Zeng, James Chenhao Liang, Cheng Han, Zhiwen Cao, Jiahao Liu, Xiaojun Quan, Yingjie Victor Chen, Lifu Huang, Tong Geng, Qifan Wang, Dongfang Liu
- **URL**: <http://arxiv.org/abs/2509.17276v1>
- **Submitted**: 2025-09-21 23:18:24
- **Comment**: NeurIPS 2025
- **Topic Keywords**: rag, neurips
- **Reason**: This paper focuses on large language model fusion, which is not a core area of interest for you. While it involves some NLP concepts, it doesn't directly relate to your primary focus on information retrieval, query understanding, and ranking models.

#### Abstract
> Training large language models (LLMs) from scratch can yield models with
unique functionalities and strengths, but it is costly and often leads to
redundant capabilities. A more cost-effective alternative is to fuse existing
pre-trained LLMs with different architectures into a more powerful model.
However, a key challenge in existing model fusion is their dependence on
manually predefined vocabulary alignment, which may not generalize well across
diverse contexts, leading to performance degradation in several evaluation. To
solve this, we draw inspiration from distribution learning and propose the
probabilistic token alignment method as a general and soft mapping for
alignment, named as PTA-LLM. Our approach innovatively reformulates token
alignment into a classic mathematical problem: optimal transport, seamlessly
leveraging distribution-aware learning to facilitate more coherent model
fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability
from a distributional perspective, offering insights into the essence of the
token alignment. Empirical results demonstrate that probabilistic token
alignment enhances the target model's performance across multiple capabilities.
Our code is avaliable at https://runjia.tech/neurips_pta-llm/.

### 41. RadEval: A framework for radiology text evaluation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Justin Xu, Xi Zhang, Javid Abderezaei, Julie Bauml, Roger Boodoo, Fatemeh Haghighi, Ali Ganjizadeh, Eric Brattain, Dave Van Veen, Zaiqiao Meng, David Eyre, Jean-Benoit Delbrouck
- **URL**: <http://arxiv.org/abs/2509.18030v1>
- **Submitted**: 2025-09-22 17:03:48
- **Comment**: Accepted to EMNLP 2025 Demo track - Oral
- **Topic Keywords**: retrieval
- **Reason**: This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves text evaluation and uses some NLP metrics, its focus on radiology text evaluation and clinical concept-based scores makes it less relevant to your interests.

#### Abstract
> We introduce RadEval, a unified, open-source framework for evaluating
radiology texts. RadEval consolidates a diverse range of metrics, from classic
n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical
concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,
TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and
standardize implementations, extend GREEN to support multiple imaging
modalities with a more lightweight model, and pretrain a domain-specific
radiology encoder, demonstrating strong zero-shot retrieval performance. We
also release a richly annotated expert dataset with over 450 clinically
significant error labels and show how different metrics correlate with
radiologist judgment. Finally, RadEval provides statistical testing tools and
baseline model evaluations across multiple publicly available datasets,
facilitating reproducibility and robust benchmarking in radiology report
generation.

### 42. From Documents to Database: Failure Modes for Industrial Assets

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Duygu Kabakci-Zorlu, Fabio Lorenzi, John Sheehan, Karol Lynch, Bradley Eck
- **URL**: <http://arxiv.org/abs/2509.17834v1>
- **Submitted**: 2025-09-22 14:23:50
- **Comment**: 7 pages, 4 figures. Artificial Intelligence for Knowledge Acquisition
  & Management (AI4KAM) Workshop @ IJCAI 2025
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on industrial asset management and the application of foundation models in generating Failure Mode and Effects Analyses (FMEA). While it involves information retrieval and aggregation of unstructured content, it does not align with the user's primary research interests in query understanding, ranking models, and user behavior modeling in the context of search technologies and e-commerce. The paper's focus on industrial asset management and FMEA is not directly related to the user's core research themes.

#### Abstract
> We propose an interactive system using foundation models and user-provided
technical documents to generate Failure Mode and Effects Analyses (FMEA) for
industrial equipment. Our system aggregates unstructured content across
documents to generate an FMEA and stores it in a relational database.
Leveraging this tool, the time required for creation of this
knowledge-intensive content is reduced, outperforming traditional manual
approaches. This demonstration showcases the potential of foundation models to
facilitate the creation of specialized structured content for enterprise asset
management systems.

### 43. Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lekkala Sai Teja, Annepaka Yadagiri, Partha Pakray, Chukhu Chunka, Mangadoddi Srikar Vardhan
- **URL**: <http://arxiv.org/abs/2509.17830v2>
- **Submitted**: 2025-09-22 14:22:55
- **Comment**: 14 pages, 14 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on AI-generated text detection, which is not a core area of interest for you. While it involves NLP and deep semantic understanding, the context is not related to information retrieval or search technologies.

#### Abstract
> Generation of Artificial Intelligence (AI) texts in important works has
become a common practice that can be used to misuse and abuse AI at various
levels. Traditional AI detectors often rely on document-level classification,
which struggles to identify AI content in hybrid or slightly edited texts
designed to avoid detection, leading to concerns about the model's efficiency,
which makes it hard to distinguish between human-written and AI-generated
texts. A sentence-level sequence labeling model proposed to detect transitions
between human- and AI-generated text, leveraging nuanced linguistic signals
overlooked by document-level classifiers. By this method, detecting and
segmenting AI and human-written text within a single document at the
token-level granularity is achieved. Our model combines the state-of-the-art
pre-trained Transformer models, incorporating Neural Networks (NN) and
Conditional Random Fields (CRFs). This approach extends the power of
transformers to extract semantic and syntactic patterns, and the neural network
component to capture enhanced sequence-level representations, thereby improving
the boundary predictions by the CRF layer, which enhances sequence recognition
and further identification of the partition between Human- and AI-generated
texts. The evaluation is performed on two publicly available benchmark datasets
containing collaborative human and AI-generated texts. Our experimental
comparisons are with zero-shot detectors and the existing state-of-the-art
models, along with rigorous ablation studies to justify that this approach, in
particular, can accurately detect the spans of AI texts in a completely
collaborative text. All our source code and the processed datasets are
available in our GitHub repository.

### 44. One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xingyu Fan, Feifei Li, Wenhui Que, Hailong Li
- **URL**: <http://arxiv.org/abs/2509.17788v1>
- **Submitted**: 2025-09-22 13:49:37
- **Comment**: 7 pages
- **Topic Keywords**: rag
- **Reason**: This paper focuses on conversational agents and stylized contextual question answering, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and techniques presented are not aligned with your areas of focus.

#### Abstract
> Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.

### 45. ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Bonan Zhang, Zhongqi Chen, Bowen Song, Qinya Li, Fan Wu, Guihai Chen
- **URL**: <http://arxiv.org/abs/2509.17730v1>
- **Submitted**: 2025-09-22 13:00:35
- **Topic Keywords**: rag
- **Reason**: This paper focuses on reinforcement learning for large language models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context is more on model refinement and optimization rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Reinforcement learning (RL) has become a standard paradigm for refining large
language models (LLMs) beyond pre-training and instruction tuning. A prominent
line of work is RL with verifiable rewards (RLVR), which leverages
automatically verifiable outcomes (e.g., correctness or executability) to
generate reward signals. While efficient, this framework faces two key
limitations: First, its binary feedback is too sparse to capture the quality of
the reasoning process. Second, its coarse-grained rewards potentially lead to
vanishing gradients. Inspired by observations from human learning, we introduce
a RL technique that integrates verifiable outcomes with the model's own
confidence estimates. This joint design enriches the reward signal, providing
finer-grained feedback and implicitly supervising the reasoning process.
Experimental results demonstrate that our proposed method enhances RL
performance across multiple datasets and reduces token consumption during
inference, while incurring negligible additional training cost. Moreover, it
can be used as a plug-in module to enhance other state-of-the-art RL methods.

### 46. AutiHero: Leveraging Generative AI in Social Narratives to Engage Parents in Story-Driven Behavioral Guidance for Autistic Children

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jungeun Lee, Kyungah Lee, Inseok Hwang, SoHyun Park, Young-Ho Kim
- **URL**: <http://arxiv.org/abs/2509.17608v1>
- **Submitted**: 2025-09-22 11:23:10
- **Comment**: 22 pages except reference
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing, as it focuses on a generative AI system for creating personalized social narratives for autistic children. While it involves text generation, the context and application are unrelated to your areas of interest.

#### Abstract
> Social narratives are known to help autistic children understand and navigate
social situations through stories. To ensure effectiveness, however, the
materials need to be customized to reflect each child's unique behavioral
context, requiring considerable time and effort for parents to practice at
home. We present AutiHero, a generative AI-based social narrative system for
behavioral guidance, which supports parents to create personalized stories for
their autistic children and read them together. AutiHero generates text and
visual illustrations that reflect their children's interests, target behaviors,
and everyday contexts. In a two-week deployment study with 16 autistic
child-parent dyads, parents created 218 stories and read an average of 4.25
stories per day, demonstrating a high level of engagement. AutiHero also
provided an effective, low-demanding means to guide children's social
behaviors, encouraging positive change. We discuss the implications of
generative AI-infused tools to empower parents in guiding their children's
behaviors, fostering their social learning.

### 47. Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wenhao Zhuang, Yuan Sun, Xiaobing Zhao
- **URL**: <http://arxiv.org/abs/2509.17493v1>
- **Submitted**: 2025-09-22 08:24:26
- **Topic Keywords**: rag
- **Reason**: This paper focuses on cross-lingual transfer and low-resource languages, which is somewhat related to information retrieval and NLP. However, the primary focus on transliteration and Huffman coding does not align with the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> As large language models (LLMs) are trained on increasingly diverse and
extensive multilingual corpora, they demonstrate cross-lingual transfer
capabilities. However, these capabilities often fail to effectively extend to
low-resource languages, particularly those utilizing non-Latin scripts. While
transliterating low-resource languages into Latin script presents a natural
solution, there currently lacks a comprehensive framework for integrating
transliteration into LLMs training and deployment. Taking a pragmatic approach,
this paper innovatively combines character transliteration with Huffman coding
to design a complete transliteration framework. Our proposed framework offers
the following advantages: 1) Compression: Reduces storage requirements for
low-resource language content, achieving up to 50% reduction in file size and
50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless
conversion from transliterated text back to the source language. 3) Efficiency:
Eliminates the need for vocabulary expansion for low-resource languages,
improving training and inference efficiency. 4) Scalability: The framework can
be extended to other low-resource languages. We validate the effectiveness of
our framework across multiple downstream tasks, including text classification,
machine reading comprehension, and machine translation. Experimental results
demonstrate that our method significantly enhances the model's capability to
process low-resource languages while maintaining performance on high-resource
languages. Our data and code are publicly available at
https://github.com/CMLI-NLP/HuffmanTranslit.

### 48. PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Namyoung Kim, Kai Tzu-iunn Ong, Yeonjun Hwang, Minseok Kang, Iiseo Jihn, Gayoung Kim, Minju Kim, Jinyoung Yeo
- **URL**: <http://arxiv.org/abs/2509.17459v1>
- **Submitted**: 2025-09-22 07:53:59
- **Comment**: Accepted to EMNLP 2025 Findings
- **Topic Keywords**: rag
- **Reason**: This paper focuses on proactive dialogue agents and strategy planning, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. Although it involves large language models and offline self-play simulations, the context is more aligned with NLP and dialogue systems rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Dialogue agents based on large language models (LLMs) have shown promising
performance in proactive dialogue, which requires effective strategy planning.
However, existing approaches to strategy planning for proactive dialogue face
several limitations: limited strategy coverage, preference bias in planning,
and reliance on costly additional training. To address these, we propose
PRINCIPLES: a synthetic strategy memory for proactive dialogue agents.
PRINCIPLES is derived through offline self-play simulations and serves as
reusable knowledge that guides strategy planning during inference, eliminating
the need for additional training and data annotation. We evaluate PRINCIPLES in
both emotional support and persuasion domains, demonstrating consistent
improvements over strong baselines. Furthermore, PRINCIPLES maintains its
robustness across extended and more diverse evaluation settings. See our
project page at https://huggingface.co/spaces/kimnamssya/Principles.

### 49. Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli
- **URL**: <http://arxiv.org/abs/2509.18085v1>
- **Submitted**: 2025-09-22 17:58:21
- **Topic Keywords**: rag
- **Reason**: This paper focuses on accelerating diffusion LLMs using speculative decoding, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep learning and NLP, the topic is more aligned with model acceleration and optimization rather than query understanding or ranking models.

#### Abstract
> Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to
autoregressive LLMs (AR-LLMs) with the potential to operate at significantly
higher token generation rates. However, currently available open-source dLLMs
often generate at much lower rates, typically decoding only a single token at
every denoising timestep in order to maximize output quality. We present
Spiffy, a speculative decoding algorithm that accelerates dLLM inference by
$\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output
distribution. This work addresses the unique challenges involved in applying
ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes
draft states by leveraging the dLLM's distribution itself in an
auto-speculative manner. This approach is efficient and effective, and
eliminates the overheads of training and running an independent draft model. To
structure the candidate draft states, we propose a novel directed draft graph
which is uniquely designed to take advantage of the bidirectional, block-wise
nature of dLLM generation and can be verified in parallel by the dLLM. To
further optimize the structure of these draft graphs, we introduce an
efficient, offline calibration algorithm that procedurally determines
high-quality graph configurations. These optimized draft graphs, enabling
increased acceptance rates, lead to a significant boost in the overall speedup
achieved by the system. Crucially, Spiffy is also complementary to other recent
innovations in improving dLLM generation speeds such as KV-caching and
multi-token unmasking. We demonstrate that when combined with such parallel
decoding algorithms, Spiffy is able to effectively multiply the benefits of
these methods leading to total speedups of up to $\mathbf{7.9\times}$.

### 50. Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Mar√≠a Andrea Cruz Bland√≥n, Zakaria Aldeneh, Jie Chi, Maureen de Seyssel
- **URL**: <http://arxiv.org/abs/2509.17523v1>
- **Submitted**: 2025-09-22 08:48:04
- **Comment**: 5 pages, 2 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speech representation learning and self-supervised learning in the context of speech recognition, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Self-supervised learning (SSL) has made significant advances in speech
representation learning. Models like wav2vec 2.0 and HuBERT have achieved
state-of-the-art results in tasks such as speech recognition, particularly in
monolingual settings. However, multilingual SSL models tend to underperform
their monolingual counterparts on each individual language, especially in
multilingual scenarios with few languages such as the bilingual setting. In
this work, we investigate a novel approach to reduce this performance gap by
introducing limited visual grounding into bilingual speech SSL models. Our
results show that visual grounding benefits both monolingual and bilingual
models, with especially pronounced gains for the latter, reducing the
multilingual performance gap on zero-shot phonetic discrimination from 31.5%
for audio-only models to 8.04% with grounding.

---

