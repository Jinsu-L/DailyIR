# Daily Papers Report - 2025-10-26

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. A Data-Centric Approach to Multilingual E-Commerce Product Search: Case Study on Query-Category and Query-Item Relevance

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Yabo Yin, Yang Xi, Jialong Wang, Shanqi Wang, Jiateng Hu
- **URL**: <http://arxiv.org/abs/2510.21671v1>
- **Submitted**: 2025-10-24 17:27:35
- **Topic Keywords**: query, queries, relevance, commerce, e-commerce, search, cikm
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of multilingual e-commerce search. The focus on query understanding, relevance models, and data-centric approaches aligns with your expertise. However, the specific application to e-commerce and multilingual search is a niche area, which prevents it from being a perfect match.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Improving multilingual e-commerce search through a data-centric framework
- **Aim**: Enhance cross-lingual generalization and robustness in Query-Category (QC) and Query-Item (QI) relevance tasks by addressing data imbalance, label noise, and limited supervision for low-resource languages.
- **Rationale**: Multilingual e-commerce search faces challenges like language imbalance, label noise, and limited supervision. Traditional model-centric approaches are insufficient; data-centric strategies offer scalable solutions for real-world deployment.
- **Ground**: The framework combines translation-based augmentation (for low-resource languages), semantic negative sampling (to address class imbalance), and self-validation filtering (to reduce label noise). It leverages multilingual translation models (e.g., Seed-X-PPO-7B) and efficient training techniques (LoRA, vLLM).
- **Experiment**: Evaluated on the CIKM AnalytiCup 2025 dataset with 10+ languages. Achieved significant F1 score improvements over LLM baselines (e.g., Qwen3) for QC and QI tasks, particularly for low-resource languages. Demonstrated effectiveness of synthetic data and noise filtering.
- **Takeaway**: A model-agnostic data-centric framework outperforms model-centric approaches in multilingual e-commerce search. Key contributions include actionable strategies (translation augmentation, semantic sampling, noise filtering) and technical innovations (LoRA, vLLM) for efficient deployment.

#### Abstract
> Multilingual e-commerce search suffers from severe data imbalance across
languages, label noise, and limited supervision for low-resource
languages--challenges that impede the cross-lingual generalization of relevance
models despite the strong capabilities of large language models (LLMs). In this
work, we present a practical, architecture-agnostic, data-centric framework to
enhance performance on two core tasks: Query-Category (QC) relevance (matching
queries to product categories) and Query-Item (QI) relevance (matching queries
to product titles). Rather than altering the model, we redesign the training
data through three complementary strategies: (1) translation-based augmentation
to synthesize examples for languages absent in training, (2) semantic negative
sampling to generate hard negatives and mitigate class imbalance, and (3)
self-validation filtering to detect and remove likely mislabeled instances.
Evaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields
substantial F1 score improvements over strong LLM baselines, achieving
competitive results in the official competition. Our findings demonstrate that
systematic data engineering can be as impactful as--and often more deployable
than--complex model modifications, offering actionable guidance for building
robust multilingual search systems in the real-world e-commerce settings.

---

### 2. Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Kuicai Dong, Shurui Huang, Fangda Ye, Wei Han, Zhi Zhang, Dexun Li, Wenjun Li, Qu Yang, Gang Wang, Yichao Wang, Chen Zhang, Yong Liu
- **URL**: <http://arxiv.org/abs/2510.21603v1>
- **Submitted**: 2025-10-24 16:07:54
- **Comment**: preprint
- **Topic Keywords**: queries, retrieval, search
- **Reason**: This paper aligns well with your research interests in Information Retrieval, particularly in the context of multimodal document processing and deep semantic understanding. The proposed system, Doc-Researcher, addresses challenges in parsing and retrieval that are relevant to your work on query understanding and ranking models. However, the focus on multimodal documents and deep research may not be a central match with your primary interests in e-commerce and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Development and evaluation of Doc-Researcher, a framework for multimodal document parsing and retrieval, with a focus on hybrid text-vision retrieval schemes for document analysis tasks.
- **Aim**: To address document structure analysis and retrieval/question-answering (VQA) challenges by integrating layout-aware chunking, multimodal transcription, and adaptive retrieval schemes.
- **Rationale**: Existing methods lack robust handling of both textual and visual elements in documents, necessitating a framework that combines layout-aware parsing with hybrid retrieval strategies to improve accuracy and recall in document-centric tasks.
- **Ground**: The framework introduces a layout-aware chunking process for document element identification, multimodal-to-text transcription for structured data extraction, and distinct input-output formats for ranking and retrieval tasks. Evaluation is conducted on M4DocBench (multi-document retrieval) and MMDocIR (single-document VQA).
- **Experiment**: Three retrieval schemes are tested: text-based (BM25, E5, BGE-M3), vision-based (Vision:ColPali, Vision:Jina:dense), and hybrid (Hybrid:Jina:dense, Hybrid:Qwen3+Jina). Performance is measured via Recall@10, Recall@15, and Recall@20 metrics across datasets and domains (education, finance, insurance).
- **Takeaway**: Hybrid retrieval schemes outperform text- and vision-based methods overall, with vision-based excelling in single-document VQA and text-based in multi-document retrieval. Domain-specific performance variations highlight the importance of adaptive strategies for document analysis tasks.

#### Abstract
> Deep Research systems have revolutionized how LLMs solve complex questions
through iterative reasoning and evidence gathering. However, current systems
remain fundamentally constrained to textual web data, overlooking the vast
knowledge embedded in multimodal documents Processing such documents demands
sophisticated parsing to preserve visual semantics (figures, tables, charts,
and equations), intelligent chunking to maintain structural coherence, and
adaptive retrieval across modalities, which are capabilities absent in existing
systems. In response, we present Doc-Researcher, a unified system that bridges
this gap through three integrated components: (i) deep multimodal parsing that
preserves layout structure and visual semantics while creating multi-granular
representations from chunk to document level, (ii) systematic retrieval
architecture supporting text-only, vision-only, and hybrid paradigms with
dynamic granularity selection, and (iii) iterative multi-agent workflows that
decompose complex queries, progressively accumulate evidence, and synthesize
comprehensive answers across documents and modalities. To enable rigorous
evaluation, we introduce M4DocBench, the first benchmark for Multi-modal,
Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158
expert-annotated questions with complete evidence chains across 304 documents,
M4DocBench tests capabilities that existing benchmarks cannot assess.
Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter
than state-of-the-art baselines, validating that effective document research
requires not just better retrieval, but fundamentally deep parsing that
preserve multimodal integrity and support iterative research. Our work
establishes a new paradigm for conducting deep research on multimodal document
collections.

---

### 3. Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Faisal Hamman, Pasan Dissanayake, Yanjun Fu, Sanghamitra Dutta
- **URL**: <http://arxiv.org/abs/2510.21631v1>
- **Submitted**: 2025-10-24 16:36:34
- **Comment**: NeurIPS 2025
- **Topic Keywords**: rag
- **Reason**: This paper focuses on knowledge distillation, a technique related to information retrieval, but its primary contribution is in the area of natural language processing (NLP) and model optimization. While it touches on the idea of decision boundaries, which is relevant to ranking models, the paper's main themes and applications are not directly aligned with the user's core research interests in IR and search technologies.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Counterfactual-Explanation Data (COD) in Few-Shot Learning
- **Aim**: To evaluate COD's effectiveness in improving few-shot learning performance across NLP tasks while analyzing computational trade-offs.
- **Rationale**: COD leverages counterfactual explanations to augment training data, enhancing model generalization in low-data scenarios. Its integration with knowledge distillation (KD) and label-wise distillation (LWD) aims to optimize accuracy and robustness.
- **Ground**: Experiments conducted on IMDB, SST-2, and Sent140 datasets with 8‚Äì512 labeled examples per class. Metrics include accuracy, variance, and energy consumption.
- **Experiment**: KD+COD outperforms baselines (e.g., 0.800‚Äì0.921 accuracy on IMDB vs. 0.678‚Äì0.926 for KD alone). LWD+COD shows higher energy costs (0.01102 kWh vs. 0.00966 kWh for 8 shots). Soft-label calibration from teacher models is critical, with performance dropping by 14.6% when removed.
- **Takeaway**: COD significantly boosts few-shot learning accuracy with low variance but requires careful balancing of computational costs. Soft-label calibration is essential for COD's success, while LWD-based variants incur higher energy trade-offs.

#### Abstract
> Knowledge distillation is a promising approach to transfer capabilities from
complex teacher models to smaller, resource-efficient student models that can
be deployed easily, particularly in task-aware scenarios. However, existing
methods of task-aware distillation typically require substantial quantities of
data which may be unavailable or expensive to obtain in many practical
scenarios. In this paper, we address this challenge by introducing a novel
strategy called Counterfactual-explanation-infused Distillation CoD for
few-shot task-aware knowledge distillation by systematically infusing
counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs
that can flip the output prediction of the teacher model with minimum
perturbation. Our strategy CoD leverages these CFEs to precisely map the
teacher's decision boundary with significantly fewer samples. We provide
theoretical guarantees for motivating the role of CFEs in distillation, from
both statistical and geometric perspectives. We mathematically show that CFEs
can improve parameter estimation by providing more informative examples near
the teacher's decision boundary. We also derive geometric insights on how CFEs
effectively act as knowledge probes, helping the students mimic the teacher's
decision boundaries more effectively than standard data. We perform experiments
across various datasets and LLMs to show that CoD outperforms standard
distillation approaches in few-shot regimes (as low as 8-512 samples). Notably,
CoD only uses half of the original samples used by the baselines, paired with
their corresponding CFEs and still improves performance.

---

### 4. AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Jonathan Bragg, Mike D'Arcy, Nishant Balepur, Dan Bareket, Bhavana Dalvi, Sergey Feldman, Dany Haddad, Jena D. Hwang, Peter Jansen, Varsha Kishore, Bodhisattwa Prasad Majumder, Aakanksha Naik, Sigal Rahamimov, Kyle Richardson, Amanpreet Singh, Harshit Surana, Aryeh Tiktinsky, Rosni Vasu, Guy Wiener, Chloe Anastasiades, Stefan Candra, Jason Dunkelberger, Dan Emery, Rob Evans, Malachi Hamada, Regan Huff, Rodney Kinney, Matt Latzke, Jaron Lochner, Ruben Lozano-Aguilera, Cecile Nguyen, Smita Rao, Amber Tanaka, Brooke Vlahos, Peter Clark, Doug Downey, Yoav Goldberg, Ashish Sabharwal, Daniel S. Weld
- **URL**: <http://arxiv.org/abs/2510.21652v1>
- **Submitted**: 2025-10-24 17:10:26
- **Topic Keywords**: search
- **Reason**: The paper AstaBench focuses on benchmarking AI agents for scientific research, which is somewhat related to information retrieval and search technologies. However, the primary focus is on evaluating AI agents rather than developing new IR or NLP techniques, making it only loosely relevant to the user's core research themes.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Rigorous evaluation of AI agents in scientific research
- **Aim**: Address limitations in current benchmarks by proposing principles and tools for systematic, reproducible, and holistic assessment of AI agents in scientific research
- **Rationale**: Current benchmarks lack real-world applicability, reproducibility, controlled comparisons, standardized interfaces, and comprehensive baselines, leading to unreliable evaluation of AI capabilities in scientific contexts
- **Ground**: AstaBench framework introduced with 2400+ cross-domain scientific problems, production-grade search environment, and standardized tooling to control confounders like model cost and tool access
- **Experiment**: Evaluation of 57 agents across 22 classes using AstaBench revealed persistent gaps in AI capabilities despite progress on individual research aspects
- **Takeaway**: AI remains far from effectively assisting scientific research, emphasizing the need for improved benchmarking frameworks and more robust agent development

#### Abstract
> AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

---

### 5. DeepAgent: A General Reasoning Agent with Scalable Toolsets

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, Zhicheng Dou
- **URL**: <http://arxiv.org/abs/2510.21618v1>
- **Submitted**: 2025-10-24 16:24:01
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on developing a general reasoning agent, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves reinforcement learning and tool use, the context is not aligned with the user's core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: DeepAgent: A General Reasoning Agent with Scalable Tool Integration
- **Aim**: To develop a general-purpose reasoning agent capable of solving complex, multi-step tasks through scalable tool integration and a brain-inspired memory architecture.
- **Rationale**: Existing frameworks like ReAct, CodeAct, and Reflexion lack robustness in multi-tool coordination and task autonomy. DeepAgent addresses these gaps by combining hierarchical memory, distributed training, and dynamic tool selection for real-world problem-solving.
- **Ground**: Evaluated across ALFWorld (embodied tasks), WebShop (shopping), GAIA (real-world reasoning), and HLE (multi-disciplinary exams). Benchmarked against ReAct, CodeAct, Plan-and-Solve, Reflexion, AgentLM, WebThinker, HiRA, and OpenAI Deep Research.
- **Experiment**: Trained on 64 NVIDIA H20-141GB GPUs using VeRL, with models including QwQ-32B and Qwen3-30B-A3B. Key parameters: ToolPO reinforcement learning, batch size 64, max sequence length 32,768 tokens. Case study on ToolBench demonstrated multi-tool task execution (e.g., Vimeo documentary search, YouTube link generation).
- **Takeaway**: DeepAgent achieves state-of-the-art performance in task autonomy, tool coordination, and memory-driven reasoning, validated through diverse benchmarks and a detailed case study (Table 6). Its modular design and scalable architecture advance general reasoning agent capabilities.

#### Abstract
> Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mojca Brglez, ≈†pela Vintar
- **URL**: <http://arxiv.org/abs/2510.21575v1>
- **Submitted**: 2025-10-24 15:43:42
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on pragmatics understanding benchmarks for Slovene, which is not directly related to your core research themes in Information Retrieval and Search technologies, nor does it involve query understanding, ranking models, or user behavior modeling.

#### Abstract
> Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

### 7. RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xueyuan Lin, Cehao Yang, Ye Ma, Ming Li, Rongjunchen Zhang, Yang Ni, Xiaojun Wu, Chengjin Xu, Jian Guo, Hui Xiong
- **URL**: <http://arxiv.org/abs/2510.21604v1>
- **Submitted**: 2025-10-24 16:08:33
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models, the focus is on stock movement prediction, which is outside your primary areas of interest.

#### Abstract
> Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

---

