# Daily Papers Report - 2025-08-26

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. DiffusionGS: Generative Search with Query Conditioned Diffusion in Kuaishou

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Qinyao Li, Xiaoyang Zheng, Qihang Zhao, Ke Xu, Zhongbo Sun, Chao Wang, Chenyi Lei, Han Li, Wenwu Ou
- **URL**: <http://arxiv.org/abs/2508.17754v1>
- **Submitted**: 2025-08-25 07:46:51
- **Topic Keywords**: query, queries, ranking, rag, commerce, e-commerce, rank, search
- **Reason**: The paper proposes a novel approach to personalized search ranking, leveraging user queries as intent anchors to extract immediate interests from historical behaviors. The use of generative models and conditional denoising tasks aligns with your interests in query understanding, ranking models, and user behavior modeling. While the focus is on e-commerce and short-video platforms, the concepts and techniques explored are relevant to your broader interests in Information Retrieval and Natural Language Processing.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: DiffusionGS: A Novel Generative Search Model for Capturing Dynamic User Interests
- **Aim**: To propose a novel generative search model that leverages user queries to extract users' immediate interests from their historical behaviors
- **Rationale**: The model is motivated by the capability of diffusion models to reconstruct structured data distributions from random Gaussian noise, and is used to decouple static semantic item features from dynamic user interests based on historical behavior sequences
- **Ground**: The DiffusionGS framework is based on the generative ranking model (GRM) paradigm, which integrates the scalability of generative models with the controllable generation capability of diffusion models
- **Experiment**: The authors demonstrate the effectiveness of DiffusionGS through extensive offline and online experiments, validating the scaling law of the diffusion-based generative paradigm in search-ranking tasks
- **Takeaway**: The DiffusionGS framework provides a powerful mechanism for capturing dynamic user interest shifts and demonstrates superiority over state-of-the-art methods through extensive offline and online experiments

#### Abstract
> Personalized search ranking systems are critical for driving engagement and
revenue in modern e-commerce and short-video platforms. While existing methods
excel at estimating users' broad interests based on the filtered historical
behaviors, they typically under-exploit explicit alignment between a user's
real-time intent (represented by the user query) and their past actions. In
this paper, we propose DiffusionGS, a novel and scalable approach powered by
generative models. Our key insight is that user queries can serve as explicit
intent anchors to facilitate the extraction of users' immediate interests from
long-term, noisy historical behaviors. Specifically, we formulate interest
extraction as a conditional denoising task, where the user's query guides a
conditional diffusion process to produce a robust, user intent-aware
representation from their behavioral sequence. We propose the User-aware
Denoising Layer (UDL) to incorporate user-specific profiles into the
optimization of attention distribution on the user's past actions. By reframing
queries as intent priors and leveraging diffusion-based denoising, our method
provides a powerful mechanism for capturing dynamic user interest shifts.
Extensive offline and online experiments demonstrate the superiority of
DiffusionGS over state-of-the-art methods.

---

### 2. Semantic Search for Information Retrieval

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Kayla Farivar
- **URL**: <http://arxiv.org/abs/2508.17694v1>
- **Submitted**: 2025-08-25 06:03:26
- **Topic Keywords**: information retrieval, retriever, semantic search, retrieval, search
- **Reason**: The paper is highly relevant to your research interests in Information Retrieval, specifically in the area of semantic search and ranking models. The discussion of modern state-of-the-art semantic retrievers, including DPR, ColBERT, and SPLADE, aligns with your focus on query understanding and ranking models. The paper's emphasis on semantic understanding and real-time relevance optimization also resonates with your interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Semantic Retrieval in Information Retrieval Systems
- **Aim**: To develop and evaluate modern semantic retrievers that surpass traditional lexical scoring functions like BM25
- **Rationale**: The introduction of semantic embeddings and bidirectional encoders like BERT has led to the creation of dense bi-encoder models, which have improved the performance of IR systems
- **Ground**: The development of various dense bi-encoder models like ORQA, ColBERT, and SPLADE, which utilize BERT or its variants to generate embeddings and improve retrieval efficiency
- **Experiment**: The evaluation of these models using the MS MARCO dataset, which highlights the need for further research to address challenges like multilingual compatibility and low-resource languages
- **Takeaway**: The collective benefits of technological advancements in semantic retrieval can be achieved through continued research and development of more efficient and effective models

#### Abstract
> Information retrieval systems have progressed notably from lexical techniques
such as BM25 and TF-IDF to modern semantic retrievers. This survey provides a
brief overview of the BM25 baseline, then discusses the architecture of modern
state-of-the-art semantic retrievers. Advancing from BERT, we introduce dense
bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse
retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We
conclude with common evaluation tactics, pressing challenges, and propositions
for future directions.

---

### 3. How Do LLM-Generated Texts Impact Term-Based Retrieval Models?

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Wei Huang, Keping Bi, Yinqiong Cai, Wei Chen, Jiafeng Guo, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2508.17715v1>
- **Submitted**: 2025-08-25 06:43:27
- **Topic Keywords**: information retrieval, retriever, queries, retrieval
- **Reason**: The paper explores the impact of LLM-generated texts on term-based retrieval models, which is highly relevant to information retrieval and query understanding. The study's focus on linguistic analysis and source bias in term-based IR systems aligns with your interests in deep semantic understanding and real-time relevance optimization. While the paper does not specifically address ranking models or user behavior modeling, its relevance to IR and NLP makes it a useful contribution to the field.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Impact of Large Language Model-generated content on Term-based Information Retrieval models
- **Aim**: Provide a foundation for understanding and addressing potential biases in term-based IR systems managing mixed-source content
- **Rationale**: Neural retrievers tend to favor LLM-generated content, while classic term-based retrievers prefer human-written documents, and LLM-generated texts have distinct linguistic characteristics
- **Ground**: Linguistic analysis findings reveal distinct characteristics of LLM-generated texts, including smoother high-frequency and steeper low-frequency Zipf slopes, higher term specificity, and greater document-level diversity
- **Experiment**: Investigate whether term-based retrieval models exhibit source bias, concluding that they prioritize documents with term distributions that closely match query distributions
- **Takeaway**: Term-based IR models do not display inherent source bias, but rather prioritize documents with matching term distributions, and understanding LLM-generated content characteristics is crucial for addressing potential biases

#### Abstract
> As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

---

### 4. LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation

- **LLM Score**: 7
- **Keyword Score**: 16
- **Authors**: Shaoxiong Zhan, Hai Lin, Hongming Tan, Xiaodong Cai, Hai-Tao Zheng, Xin Su, Zifei Shan, Ruitong Liu, Hong-Gee Kim
- **URL**: <http://arxiv.org/abs/2508.17858v1>
- **Submitted**: 2025-08-25 10:07:36
- **Topic Keywords**: passage retrieval, dense retrieval, query, queries, rag, retrieval
- **Reason**: The paper proposes a framework for enhancing dense query representations through fine-grained, input-aware vector modulation, which is relevant to query understanding and ranking models in Information Retrieval. Although it does not specifically focus on user behavior modeling or click models, the work contributes to the development of more accurate retrieval models, which can be beneficial for search technologies.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Fine-grained Retrieval Tasks
- **Aim**: To enhance dense query representations for fine-grained retrieval tasks by incorporating fine-grained lexical information
- **Rationale**: Dense retrieval models struggle with fine-grained retrieval tasks that require precise keyword alignment and span-level localization, even with high lexical overlap
- **Ground**: LexSemBridge framework, which constructs latent enhancement vectors from input tokens using three paradigms (Statistical, Learned, and Contextual) and integrates them with dense embeddings via element-wise interaction
- **Experiment**: Evaluation on four retrieval tasks: Semantic Search, Keyword Matching, Part of Passage (P-o-P), and Image Retrieval, with metrics such as nDCG, MRR, and Recall@K
- **Takeaway**: LexSemBridge provides a lightweight, token-aware augmentation mechanism that enhances dense representations to improve fine-grained retrieval capabilities, while preserving the original vector structure and similarity computation

#### Abstract
> As queries in retrieval-augmented generation (RAG) pipelines powered by large
language models (LLMs) become increasingly complex and diverse, dense retrieval
models have demonstrated strong performance in semantic matching. Nevertheless,
they often struggle with fine-grained retrieval tasks, where precise keyword
alignment and span-level localization are required, even in cases with high
lexical overlap that would intuitively suggest easier retrieval. To
systematically evaluate this limitation, we introduce two targeted tasks,
keyword retrieval and part-of-passage retrieval, designed to simulate practical
fine-grained scenarios. Motivated by these observations, we propose
LexSemBridge, a unified framework that enhances dense query representations
through fine-grained, input-aware vector modulation. LexSemBridge constructs
latent enhancement vectors from input tokens using three paradigms: Statistical
(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense
embeddings via element-wise interaction. Theoretically, we show that this
modulation preserves the semantic direction while selectively amplifying
discriminative dimensions. LexSemBridge operates as a plug-in without modifying
the backbone encoder and naturally extends to both text and vision modalities.
Extensive experiments across semantic and fine-grained retrieval tasks validate
the effectiveness and generality of our approach. All code and models are
publicly available at https://github.com/Jasaxion/LexSemBridge/

---

### 5. Demographically-Inspired Query Variants Using an LLM

- **LLM Score**: 7
- **Keyword Score**: 11
- **Authors**: Marwah Alaofi, Nicola Ferro, Paul Thomas, Falk Scholer, Mark Sanderson
- **URL**: <http://arxiv.org/abs/2508.17644v1>
- **Submitted**: 2025-08-25 04:17:56
- **Comment**: Published in the proceedings of ICTIR'25, Padua, Italy
- **Topic Keywords**: query, queries, ranking, rank, search
- **Reason**: The paper explores query diversification using a Large Language Model (LLM), which is relevant to query understanding and ranking models in Information Retrieval. Although it doesn't specifically focus on user behavior modeling or click models, it does consider user profiles and their impact on system evaluation, which aligns with the user's interests in IR and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Diversifying Queries in Information Retrieval Systems
- **Aim**: To develop a method for generating query variants that reflect the diversity of search engine users
- **Rationale**: Query formulation is influenced by user profiles, and existing test collections do not account for this diversity
- **Ground**: The study uses a research pipeline that includes test collection selection, query variant generation, query variant assessment, and system evaluation
- **Experiment**: The authors evaluate the semantic similarity and profile alignment of query variants through human assessment and examine their impact on retrieval systems
- **Takeaway**: Considering query profiles is crucial in evaluating retrieval systems, and diverse profiles can provide valuable insights into system differences

#### Abstract
> This study proposes a method to diversify queries in existing test
collections to reflect some of the diversity of search engine users, aligning
with an earlier vision of an 'ideal' test collection. A Large Language Model
(LLM) is used to create query variants: alternative queries that have the same
meaning as the original. These variants represent user profiles characterised
by different properties, such as language and domain proficiency, which are
known in the IR literature to influence query formulation.
  The LLM's ability to generate query variants that align with user profiles is
empirically validated, and the variants' utility is further explored for IR
system evaluation. Results demonstrate that the variants impact how systems are
ranked and show that user profiles experience significantly different levels of
system effectiveness. This method enables an alternative perspective on system
evaluation where we can observe both the impact of user profiles on system
rankings and how system performance varies across users.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Jiyoon Myung, Jihyeon Park, Joohyung Han
- **URL**: <http://arxiv.org/abs/2508.18048v1>
- **Submitted**: 2025-08-25 14:06:27
- **Comment**: Accepted at the 2nd EARL Workshop on Evaluating and Applying
  Recommender Systems with Large Language Models (RecSys 2025)
- **Topic Keywords**: query, queries, retrieval, recommend, search
- **Reason**: The paper focuses on hybrid retrieval over semi-structured tabular data, combining structured filtering with semantic embedding search. While it doesn't specifically mention query understanding, ranking models, or user behavior modeling, it does explore the importance of structured filtering in improving retrieval precision, which is relevant to information retrieval and search technologies. However, the paper's primary focus is on recommender systems, which is not the user's primary area of interest.

#### Abstract
> User queries in real-world recommendation systems often combine structured
constraints (e.g., category, attributes) with unstructured preferences (e.g.,
product descriptions or reviews). We introduce HyST (Hybrid retrieval over
Semi-structured Tabular data), a hybrid retrieval framework that combines
LLM-powered structured filtering with semantic embedding search to support
complex information needs over semi-structured tabular data. HyST extracts
attribute-level constraints from natural language using large language models
(LLMs) and applies them as metadata filters, while processing the remaining
unstructured query components via embedding-based retrieval. Experiments on a
semi-structured benchmark show that HyST consistently outperforms tradtional
baselines, highlighting the importance of structured filtering in improving
retrieval precision, offering a scalable and accurate solution for real-world
user queries.

### 7. ST-Raptor: LLM-Powered Semi-Structured Table Question Answering

- **LLM Score**: 7
- **Keyword Score**: 3
- **Authors**: Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu
- **URL**: <http://arxiv.org/abs/2508.18190v2>
- **Submitted**: 2025-08-25 16:48:51
- **Comment**: Extension of our SIGMOD 2026 paper. Please refer to source code
  available at: https://github.com/weAIDB/ST-Raptor
- **Topic Keywords**: queries
- **Reason**: The paper proposes a framework for semi-structured table question answering using large language models, which is relevant to information retrieval and natural language processing. The focus on query understanding, tree-based framework, and hierarchical orthogonal tree model align with your interests in query understanding and ranking models. However, the paper's primary focus is on question answering and table analysis, which is not directly related to your work on user behavior modeling and click models.

#### Abstract
> Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.

### 8. Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations

- **LLM Score**: 4
- **Keyword Score**: 21
- **Authors**: Hung-Chun Hsu, Yuan-Ching Kuo, Chao-Han Huck Yang, Szu-Wei Fu, Hanrong Ye, Hongxu Yin, Yu-Chiang Frank Wang, Ming-Feng Tsai, Chuan-Ju Wang
- **URL**: <http://arxiv.org/abs/2508.18132v1>
- **Submitted**: 2025-08-25 15:38:56
- **Topic Keywords**: retriever, queries, ranking, rerank, rag, retrieval, recommend, commerce, e-commerce, rank, search
- **Reason**: The paper explores multimodal conversational recommendations, which is a related topic to information retrieval. However, the focus on generative retrieval and test-time scaling strategies is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on large language models and multimodal large language models also falls outside the user's background in e-commerce and focus on deep semantic understanding.

#### Abstract
> The rapid evolution of e-commerce has exposed the limitations of traditional
product retrieval systems in managing complex, multi-turn user interactions.
Recent advances in multimodal generative retrieval -- particularly those
leveraging multimodal large language models (MLLMs) as retrievers -- have shown
promise. However, most existing methods are tailored to single-turn scenarios
and struggle to model the evolving intent and iterative nature of multi-turn
dialogues when applied naively. Concurrently, test-time scaling has emerged as
a powerful paradigm for improving large language model (LLM) performance
through iterative inference-time refinement. Yet, its effectiveness typically
relies on two conditions: (1) a well-defined problem space (e.g., mathematical
reasoning), and (2) the model's ability to self-correct -- conditions that are
rarely met in conversational product search. In this setting, user queries are
often ambiguous and evolving, and MLLMs alone have difficulty grounding
responses in a fixed product corpus. Motivated by these challenges, we propose
a novel framework that introduces test-time scaling into conversational
multimodal product retrieval. Our approach builds on a generative retriever,
further augmented with a test-time reranking (TTR) mechanism that improves
retrieval accuracy and better aligns results with evolving user intent
throughout the dialogue. Experiments across multiple benchmarks show consistent
improvements, with average gains of 14.5 points in MRR and 10.6 points in
nDCG@1.

### 9. MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong
- **URL**: <http://arxiv.org/abs/2508.18260v1>
- **Submitted**: 2025-08-25 17:53:22
- **Comment**: 10 pages, 8 figures (including tables), plus appendix. Submitted to
  AAAI 2026
- **Topic Keywords**: queries, rag, retrieval augmented generation, retrieval, search
- **Reason**: The paper proposes a novel framework for test-time scalable reasoning, focusing on medical question-answering tasks. While it employs retrieval-augmented generation, the primary focus is on reasoning and knowledge graph exploration, rather than query understanding, ranking models, or user behavior modeling, which are key areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Large reasoning models (LRMs) have shown significant progress in test-time
scaling through chain-of-thought prompting. Current approaches like search-o1
integrate retrieval augmented generation (RAG) into multi-step reasoning
processes but rely on a single, linear reasoning chain while incorporating
unstructured textual information in a flat, context-agnostic manner. As a
result, these approaches can lead to error accumulation throughout the
reasoning chain, which significantly limits its effectiveness in medical
question-answering (QA) tasks where both accuracy and traceability are critical
requirements. To address these challenges, we propose MIRAGE (Multi-chain
Inference with Retrieval-Augmented Graph Exploration), a novel test-time
scalable reasoning framework that performs dynamic multi-chain inference over
structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex
queries into entity-grounded sub-questions, 2) executes parallel inference
chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop
traversal, and 4) integrates answers using cross-chain verification to resolve
contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,
CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,
Tree-of-Thought variants, and other retrieval-augmented baselines in both
automatic and human evaluations. Additionally, MIRAGE improves interpretability
by generating explicit reasoning chains that trace each factual claim to
concrete chains within the knowledge graph, making it well-suited for complex
medical reasoning scenarios. The code will be available for further research.

### 10. Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Julius Gun, Timo Oksanen
- **URL**: <http://arxiv.org/abs/2508.18093v1>
- **Submitted**: 2025-08-25 14:54:46
- **Topic Keywords**: information retrieval, query, rag, retrieval
- **Reason**: The paper explores the application of large language models and Retrieval-Augmented Generation strategies for technical question answering in a cross-lingual information retrieval scenario. While it touches on information retrieval and query understanding, the focus is on a specific domain (agricultural machines) and does not directly relate to the user's interests in e-commerce, ranking models, or user behavior modeling.

#### Abstract
> We present a case study evaluating large language models (LLMs) with
128K-token context windows on a technical question answering (QA) task. Our
benchmark is built on a user manual for an agricultural machine, available in
English, French, and German. It simulates a cross-lingual information retrieval
scenario where questions are posed in English against all three language
versions of the manual. The evaluation focuses on realistic
"needle-in-a-haystack" challenges and includes unanswerable questions to test
for hallucinations. We compare nine long-context LLMs using direct prompting
against three Retrieval-Augmented Generation (RAG) strategies (keyword,
semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this
specific manual show that Hybrid RAG consistently outperforms direct
long-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5
7B achieve high accuracy (over 85%) across all languages with RAG. This paper
contributes a detailed analysis of LLM performance in a specialized industrial
domain and an open framework for similar evaluations, highlighting practical
trade-offs and challenges.

### 11. Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Leqian Li, Dianxi Shi, Jialu Zhou, Xinyu Wei, Mingyue Yang, Songchang Jin, Shaowu Yang
- **URL**: <http://arxiv.org/abs/2508.17862v1>
- **Submitted**: 2025-08-25 10:13:02
- **Topic Keywords**: queries, rag, retrieval augmented generation, retrieval
- **Reason**: The paper proposes a method for retrieval-augmented generation, which is related to information retrieval and natural language processing. However, the focus is on large language models and knowledge management, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) have shown remarkable capabilities across
diverse tasks, yet they face inherent limitations such as constrained
parametric knowledge and high retraining costs. Retrieval-Augmented Generation
(RAG) augments the generation process by retrieving externally stored knowledge
absent from the models internal parameters. However, RAG methods face
challenges such as information loss and redundant retrievals during multi-round
queries, accompanying the difficulties in precisely characterizing knowledge
gaps for complex tasks. To address these problems, we propose Retrieval
Feedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms
the stateless retrieval of previous methods into stateful continuous knowledge
management by constructing a dynamic evidence pool. Specifically, our method
generates refined queries describing the models knowledge gaps using relational
triples from questions and evidence from the dynamic evidence pool; Retrieves
critical external knowledge to iteratively update this evidence pool; Employs a
R-Feedback Model to evaluate evidence completeness until convergence. Compared
to traditional RAG methods, our approach enables persistent storage of
retrieved passages and effectively distills key information from passages to
construct clearly new queries. Experiments on three public QA benchmarks
demonstrate that RFM-RAG outperforms previous methods and improves overall
system accuracy.

### 12. Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Hongyu Cao, Yuxuan Wu, Yucheng Cai, Xianyu Zhao, Zhijian Ou
- **URL**: <http://arxiv.org/abs/2508.18168v1>
- **Submitted**: 2025-08-25 16:17:16
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper focuses on retrieval-augmented generation models, which is a topic in Natural Language Processing (NLP). While it's related to information retrieval, the primary focus is on generation rather than retrieval, and the paper doesn't explicitly address query understanding, ranking models, or user behavior modeling, which are key areas of interest in your research.

#### Abstract
> Retrieval-augmented generation (RAG) has become a widely recognized paradigm
to combine parametric memory with non-parametric memories. An RAG model
consists of two serial connecting components (retriever and generator). A major
challenge in end-to-end optimization of the RAG model is that marginalization
over relevant passages (modeled as discrete latent variables) from a knowledge
base is required. Traditional top-K marginalization and variational RAG (VRAG)
suffer from biased or high-variance gradient estimates. In this paper, we
propose and develop joint stochastic approximation (JSA) based end-to-end
training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a
stochastic extension of the EM (expectation-maximization) algorithm and is
particularly powerful in estimating discrete latent variable models. Extensive
experiments are conducted on five datasets for two tasks (open-domain question
answering, knowledge-grounded dialogs) and show that JSA-RAG significantly
outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of
JSA-RAG from the perspectives of generation, retrieval, and low-variance
gradient estimate.

### 13. PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang
- **URL**: <http://arxiv.org/abs/2508.18166v2>
- **Submitted**: 2025-08-25 16:16:06
- **Comment**: 9 pages, 4 figures, conference
- **Topic Keywords**: ctr, cvr, recommend, personalization
- **Reason**: The paper proposes a framework for app recommendation, focusing on multiple-category apps and using contrastive alignment and parallel codebook representations. While it touches on some aspects of information retrieval, such as representation learning and feature fusion, the primary focus is on recommender systems, which is a related but distinct field. The paper's emphasis on app recommendation and deployment on the Microsoft Store makes it less relevant to the user's core research interests in information retrieval and search technologies.

#### Abstract
> Modern app store recommender systems struggle with multiple-category apps, as
traditional taxonomies fail to capture overlapping semantics, leading to
suboptimal personalization. We propose PCR-CA (Parallel Codebook
Representations with Contrastive Alignment), an end-to-end framework for
improved CTR prediction. PCR-CA first extracts compact multimodal embeddings
from app text, then introduces a Parallel Codebook VQ-AE module that learns
discrete semantic representations across multiple codebooks in parallel --
unlike hierarchical residual quantization (RQ-VAE). This design enables
independent encoding of diverse aspects (e.g., gameplay, art style), better
modeling multiple-category semantics. To bridge semantic and collaborative
signals, we employ a contrastive alignment loss at both the user and item
levels, enhancing representation learning for long-tail items. Additionally, a
dual-attention fusion mechanism combines ID-based and semantic features to
capture user interests, especially for long-tail apps. Experiments on a
large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong
baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further
validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement
in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new
framework has now been fully deployed on the Microsoft Store.

### 14. Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Tianjun Wei, Huizhong Guo, Yingpeng Du, Zhu Sun, Chen Huang, Dongxia Wang, Jie Zhang
- **URL**: <http://arxiv.org/abs/2508.18142v1>
- **Submitted**: 2025-08-25 15:51:24
- **Comment**: Github: https://github.com/UserMirrorer/UserMirrorer
- **Topic Keywords**: rag, user behavior, recommend, search
- **Reason**: The paper focuses on user simulation and recommender systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and user feedback is not directly aligned with your primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of Large Language Models and data distillation is also not directly relevant to your interests in Information Retrieval and Search technologies.

#### Abstract
> User simulation is increasingly vital to develop and evaluate recommender
systems (RSs). While Large Language Models (LLMs) offer promising avenues to
simulate user behavior, they often struggle with the absence of specific domain
alignment required for RSs and the efficiency demands of large-scale
simulation. A vast yet underutilized resource for enhancing this alignment is
the extensive user feedback inherent in RSs. However, directly leveraging such
feedback presents two significant challenges. First, user feedback in RSs is
often ambiguous and noisy, which negatively impacts effective preference
alignment. Second, the massive volume of feedback largely hinders the
efficiency of preference alignment, necessitating an efficient filtering
mechanism to identify more informative samples. To overcome these hurdles, we
introduce a novel data construction framework that leverages user feedback in
RSs with advanced LLM capabilities to generate high-quality simulation data.
Our framework unfolds in two key phases: (1) employing LLMs to generate
cognitive decision-making processes on constructed simulation samples, reducing
ambiguity in raw user feedback; (2) data distillation based on uncertainty
estimation and behavior sampling to filter challenging yet denoised simulation
samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using
such high-quality dataset with corresponding decision-making processes.
Extensive experiments verify that our framework significantly boosts the
alignment with human preferences and in-domain reasoning capabilities of
fine-tuned LLMs, and provides more insightful and interpretable signals when
interacting with RSs. We believe our work will advance the RS community and
offer valuable insights for broader human-centric AI research.

### 15. Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Prathamesh Kokate, Mitali Sarnaik, Manavi Khopade, Mukta Takalikar, Raviraj Joshi
- **URL**: <http://arxiv.org/abs/2508.17490v1>
- **Submitted**: 2025-08-24 18:52:37
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper proposes a method for efficient long document classification using sentence ranking, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on natural language processing and text classification, rather than search technologies or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat limited due to its narrow scope and lack of connection to search technologies.

#### Abstract
> Transformer-based models like BERT excel at short text classification but
struggle with long document classification (LDC) due to input length
limitations and computational inefficiencies. In this work, we propose an
efficient, zero-shot approach to LDC that leverages sentence ranking to reduce
input context without altering the model architecture. Our method enables the
adaptation of models trained on short texts, such as headlines, to long-form
documents by selecting the most informative sentences using a TF-IDF-based
ranking strategy. Using the MahaNews dataset of long Marathi news articles, we
evaluate three context reduction strategies that prioritize essential content
while preserving classification accuracy. Our results show that retaining only
the top 50\% ranked sentences maintains performance comparable to full-document
inference while reducing inference time by up to 35\%. This demonstrates that
sentence ranking is a simple yet effective technique for scalable and efficient
zero-shot LDC.

### 16. Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zihao Wei, Liang Pang, Jiahao Liu, Jingcheng Deng, Shicheng Xu, Zenghao Duan, Jingang Wang, Fei Sun, Xunliang Cai, Huawei Shen, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2508.17627v1>
- **Submitted**: 2025-08-25 03:17:17
- **Topic Keywords**: query
- **Reason**: The paper focuses on Large Language Models (LLMs) and their tendency to overthink, which is not directly related to Information Retrieval or Search technologies. While it touches on query understanding and modeling, the context is different from the user's primary interests. The paper's relevance is somewhat limited to the user's background in e-commerce, but it does not align with their core research themes.

#### Abstract
> Large language models (LLMs) enhance complex reasoning tasks by scaling the
individual thinking process. However, prior work shows that overthinking can
degrade overall performance. Motivated by observed patterns in thinking length
and content length, we categorize reasoning into three stages: insufficient
exploration stage, compensatory reasoning stage, and reasoning convergence
stage. Typically, LLMs produce correct answers in the compensatory reasoning
stage, whereas reasoning convergence often triggers overthinking, causing
increased resource usage or even infinite loops. Therefore, mitigating
overthinking hinges on detecting the end of the compensatory reasoning stage,
defined as the Reasoning Completion Point (RCP). RCP typically appears at the
end of the first complete reasoning cycle and can be identified by querying the
LLM sentence by sentence or monitoring the probability of an end-of-thinking
token (e.g., \texttt{</think>}), though these methods lack an efficient and
precise balance. To improve this, we mine more sensitive and consistent RCP
patterns and develop a lightweight thresholding strategy based on heuristic
rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D)
demonstrate that the proposed method reduces token consumption while preserving
or enhancing reasoning accuracy.

### 17. Preference Trajectory Modeling via Flow Matching for Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Li Li, Mingyue Cheng, Yuyang Ye, Zhiding Liu, Enhong Chen
- **URL**: <http://arxiv.org/abs/2508.17618v1>
- **Submitted**: 2025-08-25 02:55:42
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on sequential recommendation, which is a related topic to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on flow matching and vector fields is not directly applicable to my research interests in IR and NLP.

#### Abstract
> Sequential recommendation predicts each user's next item based on their
historical interaction sequence. Recently, diffusion models have attracted
significant attention in this area due to their strong ability to model user
interest distributions. They typically generate target items by denoising
Gaussian noise conditioned on historical interactions. However, these models
face two critical limitations. First, they exhibit high sensitivity to the
condition, making it difficult to recover target items from pure Gaussian
noise. Second, the inference process is computationally expensive, limiting
practical deployment. To address these issues, we propose FlowRec, a simple yet
effective sequential recommendation framework which leverages flow matching to
explicitly model user preference trajectories from current states to future
interests. Flow matching is an emerging generative paradigm, which offers
greater flexibility in initial distributions and enables more efficient
sampling. Based on this, we construct a personalized behavior-based prior
distribution to replace Gaussian noise and learn a vector field to model user
preference trajectories. To better align flow matching with the recommendation
objective, we further design a single-step alignment loss incorporating both
positive and negative samples, improving sampling efficiency and generation
quality. Extensive experiments on four benchmark datasets verify the
superiority of FlowRec over the state-of-the-art baselines.

### 18. RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zui Chen, Han Li, Xinhao Zhang, Xiaoyu Chen, Chunyin Dong, Yifeng Wang, Xin Cai, Su Zhang, Ziqi Li, Chi Ding, Jinxu Li, Shuai Wang, Dousheng Zhao, Sanhai Gao, Guangyi Liu
- **URL**: <http://arxiv.org/abs/2508.17590v1>
- **Submitted**: 2025-08-25 01:28:37
- **Comment**: 18 pages, 3 figures, 3 tables, to be submitted to VLDB 2026 (PVLDB
  Volume 19)
- **Topic Keywords**: rag, search
- **Reason**: The paper presents a novel NL2SQL system, RubikSQL, which addresses challenges in real-world enterprise-level NL2SQL. While it touches on aspects of query understanding and knowledge base maintenance, the focus is more on the NL2SQL task rather than information retrieval or search technologies. The paper's relevance to the user's interests is somewhat limited, but it may still be of interest due to its connection to NLP and data mining.

#### Abstract
> We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

### 19. UQ: Assessing Language Models on Unsolved Questions

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Fan Nie, Ken Ziyu Liu, Zihao Wang, Rui Sun, Wei Liu, Weijia Shi, Huaxiu Yao, Linjun Zhang, Andrew Y. Ng, James Zou, Sanmi Koyejo, Yejin Choi, Percy Liang, Niklas Muennighoff
- **URL**: <http://arxiv.org/abs/2508.17580v1>
- **Submitted**: 2025-08-25 01:07:59
- **Comment**: FN, KZL, and NM are project co-leads and contributed equally. Project
  website: https://uq.stanford.edu
- **Topic Keywords**: rag, search
- **Reason**: The paper explores a new paradigm for assessing language models on unsolved questions, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus is on evaluating language models rather than search technologies, and the connection to user behavior modeling is not clear. The paper's relevance to the user's interests is limited, but it may be of interest to those exploring the intersection of NLP and IR.

#### Abstract
> Benchmarks shape progress in AI research. A useful benchmark should be both
difficult and realistic: questions should challenge frontier models while also
reflecting real-world usage. Yet, current paradigms face a difficulty-realism
tension: exam-style benchmarks are often made artificially difficult with
limited real-world value, while benchmarks based on real user interaction often
skew toward easy, high-frequency problems. In this work, we explore a radically
different paradigm: assessing models on unsolved questions. Rather than a
static benchmark scored once, we curate unsolved questions and evaluate models
asynchronously over time with validator-assisted screening and community
verification. We introduce UQ, a testbed of 500 challenging, diverse questions
sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi
and history, probing capabilities including reasoning, factuality, and
browsing. UQ is difficult and realistic by construction: unsolved questions are
often hard and naturally arise when humans seek answers, thus solving them
yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset
and its collection pipeline combining rule-based filters, LLM judges, and human
review to ensure question quality (e.g., well-defined and difficult); (2)
UQ-Validators, compound validation strategies that leverage the
generator-validator gap to provide evaluation signals and pre-screen candidate
solutions for human review; and (3) UQ-Platform, an open platform where experts
collectively verify questions and solutions. The top model passes UQ-validation
on only 15% of questions, and preliminary human verification has already
identified correct answers among those that passed. UQ charts a path for
evaluating frontier models on real-world, open-ended challenges, where success
pushes the frontier of human knowledge. We release UQ at
https://uq.stanford.edu.

### 20. A Universal Framework for Offline Serendipity Evaluation in Recommender Systems via Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yu Tokutake, Kazushi Okamoto, Kei Harada, Atsushi Shibata, Koki Karube
- **URL**: <http://arxiv.org/abs/2508.17571v1>
- **Submitted**: 2025-08-25 00:45:16
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, which is related to information retrieval, but the specific topic of serendipity evaluation and large language models is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce, but it does not explore deep semantic understanding or real-time relevance optimization.

#### Abstract
> Serendipity in recommender systems (RSs) has attracted increasing attention
as a concept that enhances user satisfaction by presenting unexpected and
useful items. However, evaluating serendipitous performance remains challenging
because its ground truth is generally unobservable. The existing offline
metrics often depend on ambiguous definitions or are tailored to specific
datasets and RSs, thereby limiting their generalizability. To address this
issue, we propose a universally applicable evaluation framework that leverages
large language models (LLMs) known for their extensive knowledge and reasoning
capabilities, as evaluators. First, to improve the evaluation performance of
the proposed framework, we assessed the serendipity prediction accuracy of LLMs
using four different prompt strategies on a dataset containing user-annotated
serendipitous ground truth and found that the chain-of-thought prompt achieved
the highest accuracy. Next, we re-evaluated the serendipitous performance of
both serendipity-oriented and general RSs using the proposed framework on three
commonly used real-world datasets, without the ground truth. The results
indicated that there was no serendipity-oriented RS that consistently
outperformed across all datasets, and even a general RS sometimes achieved
higher performance than the serendipity-oriented RS.

### 21. Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Meiling Ning, Zhongbao Zhang, Junda Ye, Jiabao Guo, Qingyuan Guan
- **URL**: <http://arxiv.org/abs/2508.18212v1>
- **Submitted**: 2025-08-25 17:11:28
- **Topic Keywords**: rag
- **Reason**: The paper explores the application of language models in judging reward modeling, which is related to query understanding and ranking models in Information Retrieval. However, the focus on natural language inference and masked language models is more aligned with Natural Language Processing, and the connection to user behavior modeling and click models is not explicitly made. The paper's relevance to the user's interests is somewhat limited.

#### Abstract
> The emergence of LM-based judging reward modeling, represented by generative
reward models, has successfully made reinforcement learning from AI feedback
(RLAIF) efficient and scalable. To further advance this paradigm, we propose a
core insight: this form of reward modeling shares fundamental formal
consistency with natural language inference (NLI), a core task in natural
language understanding. This reframed perspective points to a key path for
building superior reward models: scaling the model's comprehension boundaries.
Pursuing this path, exploratory experiments on NLI tasks demonstrate that the
slot prediction masked language models (MLMs) incorporating contextual
explanations achieve significantly better performance compared to mainstream
autoregressive models. Based on this key finding, we propose ESFP-RM, a
two-stage LM-based judging reward model that utilizes an explanation based slot
framework for prediction to fully leverage the advantages of MLMs. Extensive
experiments demonstrate that in both reinforcement learning from human feedback
(RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers
more stable and generalizable reward signals compared to generative reward
models.

### 22. Named Entity Recognition of Historical Text via Large Language Model

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shibingfeng Zhang, Giovanni Colavizza
- **URL**: <http://arxiv.org/abs/2508.18090v1>
- **Submitted**: 2025-08-25 14:52:11
- **Topic Keywords**: retrieval
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it explores Named Entity Recognition (NER) in historical texts using Large Language Models (LLMs). However, the focus on NER and historical texts is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models have demonstrated remarkable versatility across a wide
range of natural language processing tasks and domains. One such task is Named
Entity Recognition (NER), which involves identifying and classifying proper
names in text, such as people, organizations, locations, dates, and other
specific entities. NER plays a crucial role in extracting information from
unstructured textual data, enabling downstream applications such as information
retrieval from unstructured text.
  Traditionally, NER is addressed using supervised machine learning approaches,
which require large amounts of annotated training data. However, historical
texts present a unique challenge, as the annotated datasets are often scarce or
nonexistent, due to the high cost and expertise required for manual labeling.
In addition, the variability and noise inherent in historical language, such as
inconsistent spelling and archaic vocabulary, further complicate the
development of reliable NER systems for these sources.
  In this study, we explore the feasibility of applying LLMs to NER in
historical documents using zero-shot and few-shot prompting strategies, which
require little to no task-specific training data. Our experiments, conducted on
the HIPE-2022 (Identifying Historical People, Places and other Entities)
dataset, show that LLMs can achieve reasonably strong performance on NER tasks
in this setting. While their performance falls short of fully supervised models
trained on domain-specific annotations, the results are nevertheless promising.
These findings suggest that LLMs offer a viable and efficient alternative for
information extraction in low-resource or historically significant corpora,
where traditional supervised methods are infeasible.

### 23. Pandora: Leveraging Code-driven Knowledge Transfer for Unified Structured Knowledge Reasoning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yongrui Chen, Junhao He, Linbo Fu, Shenyu Zhang, Rihui Jin, Xinbang Dai, Jiaqi Li, Dehai Min, Nan Hu, Yuxin Zhang, Guilin Qi, Yi Huang, Tongtong Wu
- **URL**: <http://arxiv.org/abs/2508.17905v1>
- **Submitted**: 2025-08-25 11:22:58
- **Topic Keywords**: rag
- **Reason**: The paper's focus on Unified Structured Knowledge Reasoning (USKR) and leveraging code-driven knowledge transfer is somewhat relevant to my interests in Information Retrieval and Natural Language Processing. However, the paper's emphasis on structured knowledge sources and pre-training of LLMs is not directly aligned with my research themes, which prioritize query understanding, ranking models, and user behavior modeling.

#### Abstract
> Unified Structured Knowledge Reasoning (USKR) aims to answer natural language
questions by using structured sources such as tables, databases, and knowledge
graphs in a unified way. Existing USKR methods rely on task-specific strategies
or bespoke representations, which hinder their ability to dismantle barriers
between different SKR tasks, thereby constraining their overall performance in
cross-task scenarios. In this paper, we introduce \textsc{Pandora}, a novel
USKR framework that addresses the limitations of existing methods by leveraging
two key innovations. First, we propose a code-based unified knowledge
representation using \textsc{Python}'s \textsc{Pandas} API, which aligns
seamlessly with the pre-training of LLMs. This representation facilitates a
cohesive approach to handling different structured knowledge sources. Building
on this foundation, we employ knowledge transfer to bolster the unified
reasoning process of LLMs by automatically building cross-task memory. By
adaptively correcting reasoning using feedback from code execution,
\textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive
experiments on six widely used benchmarks across three SKR tasks demonstrate
that \textsc{Pandora} outperforms existing unified reasoning frameworks and
competes effectively with task-specific methods.

### 24. CoCoA: Confidence- and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Anant Khandelwal, Manish Gupta, Puneet Agrawal
- **URL**: <http://arxiv.org/abs/2508.17670v1>
- **Submitted**: 2025-08-25 05:06:04
- **Comment**: Accepted to EMNLP'25, Main. 21 pages, 17 tables, 3 Figures
- **Topic Keywords**: rag
- **Reason**: The paper focuses on large language models and conflict resolution in knowledge generation, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on token-level algorithms, the context is more geared towards natural language processing and language generation, which is not a primary focus of your research interests.

#### Abstract
> Faithful generation in large language models (LLMs) is challenged by
knowledge conflicts between parametric memory and external context. Existing
contrastive decoding methods tuned specifically to handle conflict often lack
adaptability and can degrade performance in low conflict settings. We introduce
CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level
algorithm for principled conflict resolution and enhanced faithfulness. CoCoA
resolves conflict by utilizing confidence-aware measures (entropy gap and
contextual peakedness) and the generalized divergence between the parametric
and contextual distributions. Crucially, CoCoA maintains strong performance
even in low conflict settings. Extensive experiments across multiple LLMs on
diverse Question Answering (QA), Summarization, and Long-Form Question
Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance
over strong baselines like AdaCAD. It yields significant gains in QA accuracy,
up to 9.2 points on average compared to the strong baseline AdaCAD, and
improves factuality in summarization and LFQA by up to 2.5 points on average
across key benchmarks. Additionally, it demonstrates superior sensitivity to
conflict variations. CoCoA enables more informed, context-aware, and ultimately
more faithful token generation.

### 25. HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Junyi Chen, Lu Chi, Siliang Xu, Shiwei Ran, Bingyue Peng, Zehuan Yuan
- **URL**: <http://arxiv.org/abs/2508.18118v1>
- **Submitted**: 2025-08-25 15:23:21
- **Topic Keywords**: search
- **Reason**: The paper proposes a hierarchical LLM framework for personalized content generation, which is somewhat related to information retrieval and search technologies. However, the focus on creative generation and user clustering is not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> AI-generated content technologies are widely used in content creation.
However, current AIGC systems rely heavily on creators' inspiration, rarely
generating truly user-personalized content. In real-world applications such as
online advertising, a single product may have multiple selling points, with
different users focusing on different features. This underscores the
significant value of personalized, user-centric creative generation. Effective
personalized content generation faces two main challenges: (1) accurately
modeling user interests and integrating them into the content generation
process while adhering to factual constraints, and (2) ensuring high efficiency
and scalability to handle the massive user base in industrial scenarios.
Additionally, the scarcity of personalized creative data in practice
complicates model training, making data construction another key hurdle. We
propose HLLM-Creator, a hierarchical LLM framework for efficient user interest
modeling and personalized content generation. During inference, a combination
of user clustering and a user-ad-matching-prediction based pruning strategy is
employed to significantly enhance generation efficiency and reduce
computational overhead, making the approach suitable for large-scale
deployment. Moreover, we design a data construction pipeline based on
chain-of-thought reasoning, which generates high-quality, user-specific
creative titles and ensures factual consistency despite limited personalized
data. This pipeline serves as a critical foundation for the effectiveness of
our model. Extensive experiments on personalized title generation for Douyin
Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a
0.476% increase on Adss, paving the way for more effective and efficient
personalized generation in industrial scenarios. Codes for academic dataset are
available at https://github.com/bytedance/HLLM.

### 26. ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li
- **URL**: <http://arxiv.org/abs/2508.17892v1>
- **Submitted**: 2025-08-25 10:59:02
- **Topic Keywords**: query, ltr, retrieval
- **Reason**: The paper focuses on improving the performance of large language models in long-context scenarios, proposing a novel context compression pipeline called Intermediate Layer Retrieval (ILRe). While it's related to natural language processing, it doesn't seem to be directly relevant to information retrieval, query understanding, ranking models, or user behavior modeling, which are the core research themes in your interests.

#### Abstract
> Large Language Models (LLMs) have demonstrated success across many
benchmarks. However, they still exhibit limitations in long-context scenarios,
primarily due to their short effective context length, quadratic computational
complexity, and high memory overhead when processing lengthy inputs. To
mitigate these issues, we introduce a novel context compression pipeline,
called Intermediate Layer Retrieval (ILRe), which determines one intermediate
decoder layer offline, encodes context by streaming chunked prefill only up to
that layer, and recalls tokens by the attention scores between the input query
and full key cache in that specified layer. In particular, we propose a
multi-pooling kernels allocating strategy in the token recalling process to
maintain the completeness of semantics. Our approach not only reduces the
prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance
comparable to or better than the full context in the long context scenarios.
Without additional post training or operator development, ILRe can process a
single $1M$ tokens request in less than half a minute (speedup $\approx
180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model
Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

### 27. ISACL: Internal State Analyzer for Copyrighted Training Data Leakage

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Guangwei Zhang, Qisheng Su, Jiateng Liu, Cheng Qian, Yanzhou Pan, Yanjie Fu, Denghui Zhang
- **URL**: <http://arxiv.org/abs/2508.17767v1>
- **Submitted**: 2025-08-25 08:04:20
- **Topic Keywords**: rag, retrieval, acl
- **Reason**: The paper focuses on a specific issue in Natural Language Processing (NLP), namely copyrighted data leakage, and proposes a solution using internal state analysis. While it touches on the topic of text generation, it does not relate to information retrieval, search technologies, or query understanding, which are the user's primary research interests.

#### Abstract
> Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,
especially when such data is used for training but not intended for
distribution. Traditional methods address these leaks only after content is
generated, which can lead to the exposure of sensitive information. This study
introduces a proactive approach: examining LLMs' internal states before text
generation to detect potential leaks. By using a curated dataset of copyrighted
materials, we trained a neural network classifier to identify risks, allowing
for early intervention by stopping the generation process or altering outputs
to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)
system, this framework ensures adherence to copyright and licensing
requirements while enhancing data privacy and ethical standards. Our results
show that analyzing internal states effectively mitigates the risk of
copyrighted data leakage, offering a scalable solution that fits smoothly into
AI workflows, ensuring compliance with copyright regulations while maintaining
high-quality text generation. The implementation is available on
GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}

### 28. SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Tong Bao, Mir Tafseer Nayeem, Davood Rafiei, Chengzhi Zhang
- **URL**: <http://arxiv.org/abs/2508.17647v1>
- **Submitted**: 2025-08-25 04:22:23
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on survey generation using large language models, which is not directly related to information retrieval, query understanding, ranking models, or user behavior modeling. While it touches on natural language processing, it is more focused on text generation rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Automatic survey generation has emerged as a key task in scientific document
processing. While large language models (LLMs) have shown promise in generating
survey texts, the lack of standardized evaluation datasets critically hampers
rigorous assessment of their performance against human-written surveys. In this
work, we present SurveyGen, a large-scale dataset comprising over 4,200
human-written surveys across diverse scientific domains, along with 242,143
cited references and extensive quality-related metadata for both the surveys
and the cited papers. Leveraging this resource, we build QUAL-SG, a novel
quality-aware framework for survey generation that enhances the standard
Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware
indicators into literature retrieval to assess and select higher-quality source
papers. Using this dataset and framework, we systematically evaluate
state-of-the-art LLMs under varying levels of human involvement - from fully
automatic generation to human-guided writing. Experimental results and human
evaluations show that while semi-automatic pipelines can achieve partially
competitive outcomes, fully automatic survey generation still suffers from low
citation quality and limited critical analysis.

### 29. German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Miriam Ansch√ºtz, Thanh Mai Pham, Eslam Nasrallah, Maximilian M√ºller, Cristian-George Craciun, Georg Groh
- **URL**: <http://arxiv.org/abs/2508.17973v1>
- **Submitted**: 2025-08-25 12:40:32
- **Comment**: Accepted to INLG 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on readability-controlled paraphrasing in German, which is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. Although it involves NLP and data mining, the specific application and techniques used are not relevant to the user's primary focus.

#### Abstract
> The ability to paraphrase texts across different complexity levels is
essential for creating accessible texts that can be tailored toward diverse
reader groups. Thus, we introduce German4All, the first large-scale German
dataset of aligned readability-controlled, paragraph-level paraphrases. It
spans five readability levels and comprises over 25,000 samples. The dataset is
automatically synthesized using GPT-4 and rigorously evaluated through both
human and LLM-based judgments. Using German4All, we train an open-source,
readability-controlled paraphrasing model that achieves state-of-the-art
performance in German text simplification, enabling more nuanced and
reader-specific adaptations. We opensource both the dataset and the model to
encourage further research on multi-level paraphrasing

### 30. Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Changsong Liu, Yizhou Peng, Eng Siong Chng
- **URL**: <http://arxiv.org/abs/2508.17796v1>
- **Submitted**: 2025-08-25 08:41:52
- **Comment**: Accepted to APSIPA ASC 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on automatic speech recognition (ASR) and contextual biasing, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions text-to-speech (TTS) systems, it does not involve query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Contextual automatic speech recognition (ASR) systems allow for recognizing
out-of-vocabulary (OOV) words, such as named entities or rare words. However,
it remains challenging due to limited training data and ambiguous or
inconsistent pronunciations. In this paper, we propose a synthesis-driven
multi-pronunciation contextual biasing method that performs zero-shot
contextual ASR on a pretrained Whisper model. Specifically, we leverage
text-to-speech (TTS) systems to synthesize diverse speech samples containing
each target rare word, and then use the pretrained Whisper model to extract
multiple predicted pronunciation variants. These variant token sequences are
compiled into a prefix-trie, which assigns rewards to beam hypotheses in a
shallow-fusion manner during beam-search decoding. After which, any recognized
variant is mapped back to the original rare word in the final transcription.
The evaluation results on the Librispeech dataset show that our method reduces
biased word error rate (WER) by 42% on test-clean and 43% on test-other while
maintaining unbiased WER essentially unchanged.

### 31. Research on Evaluation Methods for Patent Novelty Search Systems and Empirical Analysis

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shu Zhang, LiSha Zhang, Kai Duan, XinKai Sun
- **URL**: <http://arxiv.org/abs/2508.17782v1>
- **Submitted**: 2025-08-25 08:24:04
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on patent novelty search systems, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves evaluation methods and metrics, the context is specific to patent search and does not align with the user's broader interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Patent novelty search systems are critical to IP protection and innovation
assessment; their retrieval accuracy directly impacts patent quality. We
propose a comprehensive evaluation methodology that builds high-quality,
reproducible datasets from examiner citations and X-type citations extracted
from technically consistent family patents, and evaluates systems using
invention descriptions as inputs. Using Top-k Detection Rate and Recall as core
metrics, we further conduct multi-dimensional analyses by language, technical
field (IPC), and filing jurisdiction. Experiments show the method effectively
exposes performance differences across scenarios and offers actionable evidence
for system improvement. The framework is scalable and practical, providing a
useful reference for development and optimization of patent novelty search
systems

### 32. EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yinda Chen, Yangfan He, Jing Yang, Dapeng Zhang, Zhenlong Yuan, Muhammad Attique Khan, Jamel Baili, Por Lip Yee
- **URL**: <http://arxiv.org/abs/2508.17703v1>
- **Submitted**: 2025-08-25 06:23:17
- **Topic Keywords**: relevance
- **Reason**: The paper focuses on prompt optimization for Large Language Models in medical applications, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper involves NLP and optimization techniques, the domain-specific medical knowledge and safety requirements are not relevant to the user's background in e-commerce and general IR.

#### Abstract
> Prompt engineering significantly influences the reliability and clinical
utility of Large Language Models (LLMs) in medical applications. Current
optimization approaches inadequately address domain-specific medical knowledge
and safety requirements. This paper introduces EMPOWER, a novel evolutionary
framework that enhances medical prompt quality through specialized
representation learning, multi-dimensional evaluation, and structure-preserving
algorithms. Our methodology incorporates: (1) a medical terminology attention
mechanism, (2) a comprehensive assessment architecture evaluating clarity,
specificity, clinical relevance, and factual accuracy, (3) a component-level
evolutionary algorithm preserving clinical reasoning integrity, and (4) a
semantic verification module ensuring adherence to medical knowledge.
Evaluation across diagnostic, therapeutic, and educational tasks demonstrates
significant improvements: 24.7% reduction in factually incorrect content, 19.6%
enhancement in domain specificity, and 15.3% higher clinician preference in
blinded evaluations. The framework addresses critical challenges in developing
clinically appropriate prompts, facilitating more responsible integration of
LLMs into healthcare settings.

### 33. Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rishikesh Devanathan, Varun Nathan, Ayush Kumar
- **URL**: <http://arxiv.org/abs/2508.18210v1>
- **Submitted**: 2025-08-25 17:10:36
- **Topic Keywords**: rag
- **Reason**: The paper focuses on synthetic dialogue generation in contact centers, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions language-agnostic generation strategies, the context is not relevant to the user's primary research interests in IR, NLP, and data mining.

#### Abstract
> Synthetic transcript generation is critical in contact center domains, where
privacy and data scarcity limit model training and evaluation. Unlike prior
synthetic dialogue generation work on open-domain or medical dialogues, contact
center conversations are goal-oriented, role-asymmetric, and behaviorally
complex, featuring disfluencies, ASR noise, and compliance-driven agent
actions. In deployments where transcripts are unavailable, standard pipelines
still yield derived call attributes such as Intent Summaries, Topic Flow, and
QA Evaluation Forms. We leverage these as supervision signals to guide
generation. To assess the quality of such outputs, we introduce a diagnostic
framework of 18 linguistically and behaviorally grounded metrics for comparing
real and synthetic transcripts. We benchmark four language-agnostic generation
strategies, from simple prompting to characteristic-aware multi-stage
approaches, alongside reference-free baselines. Results reveal persistent
challenges: no method excels across all traits, with notable deficits in
disfluency, sentiment, and behavioral realism. Our diagnostic tool exposes
these gaps, enabling fine-grained evaluation and stress testing of synthetic
dialogue across languages.

### 34. Unraveling the cognitive patterns of Large Language Models through module communities

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kushal Raj Bhandari, Pin-Yu Chen, Jianxi Gao
- **URL**: <http://arxiv.org/abs/2508.18192v1>
- **Submitted**: 2025-08-25 16:49:38
- **Topic Keywords**: rag
- **Reason**: The paper's focus on Large Language Models and their cognitive patterns is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. The paper's abstract does not mention any relevance to user behavior modeling, ranking models, or real-time relevance optimization, making it an off-topic paper.

#### Abstract
> Large Language Models (LLMs) have reshaped our world with significant
advancements in science, engineering, and society through applications ranging
from scientific discoveries and medical diagnostics to Chatbots. Despite their
ubiquity and utility, the underlying mechanisms of LLM remain concealed within
billions of parameters and complex structures, making their inner architecture
and cognitive processes challenging to comprehend. We address this gap by
adopting approaches to understanding emerging cognition in biology and
developing a network-based framework that links cognitive skills, LLM
architectures, and datasets, ushering in a paradigm shift in foundation model
analysis. The skill distribution in the module communities demonstrates that
while LLMs do not strictly parallel the focalized specialization observed in
specific biological systems, they exhibit unique communities of modules whose
emergent skill patterns partially mirror the distributed yet interconnected
cognitive organization seen in avian and small mammalian brains. Our numerical
results highlight a key divergence from biological systems to LLMs, where skill
acquisition benefits substantially from dynamic, cross-regional interactions
and neural plasticity. By integrating cognitive science principles with machine
learning, our framework provides new insights into LLM interpretability and
suggests that effective fine-tuning strategies should leverage distributed
learning dynamics rather than rigid modular interventions.

### 35. Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Luana Bulla, Gabriele Tuccio, Misael Mongiov√¨, Aldo Gangemi
- **URL**: <http://arxiv.org/abs/2508.18183v1>
- **Submitted**: 2025-08-25 16:36:36
- **Topic Keywords**: rag
- **Reason**: The paper focuses on sign language translation using large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language processing, it is primarily concerned with a specific domain (sign language) and does not address the user's core research themes.

#### Abstract
> Translating natural languages into sign languages is a highly complex and
underexplored task. Despite growing interest in accessibility and inclusivity,
the development of robust translation systems remains hindered by the limited
availability of parallel corpora which align natural language with sign
language data. Existing methods often struggle to generalize in these
data-scarce environments, as the few datasets available are typically
domain-specific, lack standardization, or fail to capture the full linguistic
richness of sign languages. To address this limitation, we propose Advanced Use
of LLMs for Sign Language Translation (AulSign), a novel method that leverages
Large Language Models via dynamic prompting and in-context learning with sample
selection and subsequent sign association. Despite their impressive abilities
in processing text, LLMs lack intrinsic knowledge of sign languages; therefore,
they are unable to natively perform this kind of translation. To overcome this
limitation, we associate the signs with compact descriptions in natural
language and instruct the model to use them. We evaluate our method on both
English and Italian languages using SignBank+, a recognized benchmark in the
field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior
performance compared to state-of-the-art models in low-data scenario. Our
findings demonstrate the effectiveness of AulSign, with the potential to
enhance accessibility and inclusivity in communication technologies for
underrepresented linguistic communities.

### 36. SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xilai Xu, Zilin Zhao, Chengye Song, Zining Wang, Jinhe Qiang, Jiongrui Yan, Yuhuai Lin
- **URL**: <http://arxiv.org/abs/2508.18108v1>
- **Submitted**: 2025-08-25 15:17:53
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on sentiment analysis in social media, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves multimodal processing and knowledge retrieval, the topic is not aligned with the user's specific areas of interest in query understanding, ranking models, and user behavior modeling.

#### Abstract
> With the increasing prevalence of multimodal content on social media,
sentiment analysis faces significant challenges in effectively processing
heterogeneous data and recognizing multi-label emotions. Existing methods often
lack effective cross-modal fusion and external knowledge integration. We
propose SentiMM, a novel multi-agent framework designed to systematically
address these challenges. SentiMM processes text and visual inputs through
specialized agents, fuses multimodal features, enriches context via knowledge
retrieval, and aggregates results for final sentiment classification. We also
introduce SentiMMD, a large-scale multimodal dataset with seven fine-grained
sentiment categories. Extensive experiments demonstrate that SentiMM achieves
superior performance compared to state-of-the-art baselines, validating the
effectiveness of our structured approach.

### 37. Speech-Based Depressive Mood Detection in the Presence of Multiple Sclerosis: A Cross-Corpus and Cross-Lingual Study

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Monica Gonzalez-Machorro, Uwe Reichel, Pascal Hecker, Helly Hammer, Hesam Sagha, Florian Eyben, Robert Hoepner, Bj√∂rn W. Schuller
- **URL**: <http://arxiv.org/abs/2508.18092v1>
- **Submitted**: 2025-08-25 14:54:13
- **Comment**: Accepted at the 8th International Conference on Natural Language and
  Speech Processing (ICNLSP 2025). To be appeared in the corresponding
  Proceedings at ACL Anthology
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on speech-based depressive mood detection in the presence of Multiple Sclerosis, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on speech features, machine learning models, and emotional changes is not aligned with your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Depression commonly co-occurs with neurodegenerative disorders like Multiple
Sclerosis (MS), yet the potential of speech-based Artificial Intelligence for
detecting depression in such contexts remains unexplored. This study examines
the transferability of speech-based depression detection methods to people with
MS (pwMS) through cross-corpus and cross-lingual analysis using English data
from the general population and German data from pwMS. Our approach implements
supervised machine learning models using: 1) conventional speech and language
features commonly used in the field, 2) emotional dimensions derived from a
Speech Emotion Recognition (SER) model, and 3) exploratory speech feature
analysis. Despite limited data, our models detect depressive mood in pwMS with
moderate generalisability, achieving a 66% Unweighted Average Recall (UAR) on a
binary task. Feature selection further improved performance, boosting UAR to
74%. Our findings also highlight the relevant role emotional changes have as an
indicator of depressive mood in both the general population and within PwMS.
This study provides an initial exploration into generalising speech-based
depression detection, even in the presence of co-occurring conditions, such as
neurodegenerative diseases.

### 38. AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Henri Savigny, Bruno Yun
- **URL**: <http://arxiv.org/abs/2508.17926v1>
- **Submitted**: 2025-08-25 11:51:39
- **Topic Keywords**: rag
- **Reason**: The paper focuses on argument mining and language models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper explores multi-task learning and fine-tuning, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Argument mining is a subfield of argumentation that aims to automatically
extract argumentative structures and their relations from natural language
texts. This paper investigates how a single large language model can be
leveraged to perform one or several argument mining tasks. Our contributions
are two-fold. First, we construct a multi-task dataset by surveying and
converting 19 well-known argument mining datasets from the literature into a
unified format. Second, we explore various training strategies using Meta AI's
Llama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2)
fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned
separately on individual tasks. Our experiments show that task-specific
fine-tuning significantly improves individual performance across all tasks.
Moreover, multi-task fine-tuning maintains strong performance without
degradation, suggesting effective transfer learning across related tasks.
Finally, we demonstrate that model merging offers a viable compromise: it
yields competitive performance while mitigating the computational costs
associated with full multi-task fine-tuning.

### 39. Feature-Refined Unsupervised Model for Loanword Detection

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Promise Dodzi Kpoglu
- **URL**: <http://arxiv.org/abs/2508.17923v1>
- **Submitted**: 2025-08-25 11:42:57
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of loanword detection in linguistics is outside the scope of your primary focus areas, and the methods and techniques described in the paper do not align with your interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> We propose an unsupervised method for detecting loanwords i.e., words
borrowed from one language into another. While prior work has primarily relied
on language-external information to identify loanwords, such approaches can
introduce circularity and constraints into the historical linguistics workflow.
In contrast, our model relies solely on language-internal information to
process both native and borrowed words in monolingual and multilingual
wordlists. By extracting pertinent linguistic features, scoring them, and
mapping them probabilistically, we iteratively refine initial results by
identifying and generalizing from emerging patterns until convergence. This
hybrid approach leverages both linguistic and statistical cues to guide the
discovery process. We evaluate our method on the task of isolating loanwords in
datasets from six standard Indo-European languages: English, German, French,
Italian, Spanish, and Portuguese. Experimental results demonstrate that our
model outperforms baseline methods, with strong performance gains observed when
scaling to cross-linguistic data.

### 40. Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haijiang Liu, Qiyuan Li, Chao Gao, Yong Cao, Xiangyu Xu, Xun Wu, Daniel Hershcovich, Jinguang Gu
- **URL**: <http://arxiv.org/abs/2508.17855v1>
- **Submitted**: 2025-08-25 10:04:23
- **Comment**: 23 pages, 6 figures, accepted to EMNLP 2025 main
- **Topic Keywords**: personalization, search
- **Reason**: The paper focuses on cultural value survey simulation and personality-driven cognitive reasoning, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on psychological frameworks and survey simulation is not directly relevant to the user's areas of focus.

#### Abstract
> Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value
survey response simulation, designed to enhance the accuracy, steerability, and
interpretability of large language models in this task. The system is inspired
by the type dynamics theory in the MBTI psychological framework for personality
research. It effectively predicts and utilizes human demographic information
for simulation: life-situational stress analysis, group-level personality
prediction, and self-weighted cognitive imitation. Experiments on the World
Values Survey show that MARK outperforms existing baselines by 10% accuracy and
reduces the divergence between model predictions and human preferences. This
highlights the potential of our framework to improve zero-shot personalization
and help social scientists interpret model predictions.

### 41. DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kaiwen Yan, Xuanqing Shi, Hongcheng Guo, Wenxuan Wang, Zhuosheng Zhang, Chengwei Qin
- **URL**: <http://arxiv.org/abs/2508.17803v1>
- **Submitted**: 2025-08-25 08:47:36
- **Topic Keywords**: rag
- **Reason**: The paper focuses on large language models and their reasoning capabilities, but it doesn't seem to be directly related to information retrieval, search technologies, or query understanding. The concepts of ranking models and user behavior modeling are not mentioned, and the paper's emphasis on resource efficiency and concise answers doesn't align with the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1,
have recently demonstrated remarkable capabilities by performing structured and
multi-step reasoning. However, recent studies reveal that RLLMs often suffer
from overthinking, i.e., producing unnecessarily lengthy reasoning chains even
for simple questions, leading to excessive token consumption and computational
inefficiency. Interestingly, we observe that when processing multiple questions
in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically
compressing reasoning steps for easier problems, due to implicit resource
competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation
(DRQA), a novel method that transfers the benefits of resource competition from
batch processing to single-question inference. Specifically, DRQA leverages
batch-generated preference data and reinforcement learning to train the model
to allocate reasoning resources adaptively. By encouraging the model to
internalize a preference for responses that are both accurate and concise, DRQA
enables it to generate concise answers for simple questions while retaining
sufficient reasoning depth for more challenging ones. Extensive experiments on
a wide range of mathematical and scientific reasoning benchmarks demonstrate
that DRQA significantly reduces token usage while maintaining, and in many
cases improving, answer accuracy. By effectively mitigating the overthinking
problem, DRQA offers a promising direction for more efficient and scalable
deployment of RLLMs, and we hope it inspires further exploration into
fine-grained control of reasoning behaviors.

### 42. CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mingyue Yang, Dianxi Shi, Jialu Zhou, Xinyu Wei, Leqian Li, Shaowu Yang, Chunping Qiu
- **URL**: <http://arxiv.org/abs/2508.17760v1>
- **Submitted**: 2025-08-25 07:58:57
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Text-to-Image generation using diffusion models, which is not directly related to Information Retrieval or Search technologies. Although it mentions Large Language Models (LLMs), the application is not in the context of query understanding, ranking models, or user behavior modeling, which are key areas of interest in IR. The paper's relevance to NLP and data mining is also limited, as it primarily deals with image generation.

#### Abstract
> In Text-to-Image (T2I) generation, the complexity of entities and their
intricate interactions pose a significant challenge for T2I method based on
diffusion model: how to effectively control entity and their interactions to
produce high-quality images. To address this, we propose CEIDM, a image
generation method based on diffusion model with dual controls for entity and
interaction. First, we propose an entity interactive relationships mining
approach based on Large Language Models (LLMs), extracting reasonable and rich
implicit interactive relationships through chain of thought to guide diffusion
models to generate high-quality images that are closer to realistic logic and
have more reasonable interactive relationships. Furthermore, We propose an
interactive action clustering and offset method to cluster and offset the
interactive action features contained in each text prompts. By constructing
global and local bidirectional offsets, we enhance semantic understanding and
detail supplementation of original actions, making the model's understanding of
the concept of interactive "actions" more accurate and generating images with
more accurate interactive actions. Finally, we design an entity control network
which generates masks with entity semantic guidance, then leveraging
multi-scale convolutional network to enhance entity feature and dynamic network
to fuse feature. It effectively controls entities and significantly improves
image quality. Experiments show that the proposed CEIDM method is better than
the most representative existing methods in both entity control and their
interaction control.

### 43. Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xinyu Wei, Guoli Yang, Jialu Zhou, Mingyue Yang, Leqian Li, Kedi Zhang, Chunping Qiu
- **URL**: <http://arxiv.org/abs/2508.17638v1>
- **Submitted**: 2025-08-25 03:57:46
- **Topic Keywords**: rag
- **Reason**: The paper focuses on vision-language fine-tuning, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions hierarchical semantic representations, the context is specific to visual features and language models, which is not aligned with the user's research interests in IR and NLP.

#### Abstract
> Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects
visual features and then concatenates them with text tokens to form a unified
sequence input for Large Language Models (LLMs). However, this paradigm leads
to a significant increase in the length of the input sequence, resulting in
substantial computational overhead. Existing methods attempt to fuse visual
information into the intermediate layers of LLMs, which alleviate the sequence
length issue but often neglect the hierarchical semantic representations within
the model and the fine-grained visual information available in the shallower
visual encoding layers. To address this limitation, we propose DEHVF, an
efficient vision-language fine-tuning method based on dynamic embedding and
fusion of hierarchical visual features. Its core lies in leveraging the
inherent hierarchical representation characteristics of visual encoders and
language models. Through a lightweight hierarchical visual fuser, it
dynamically selects and fuses hierarchical features corresponding to semantic
granularity based on the internal representations of each layer in LLMs. The
fused layer-related visual features are then projected and aligned before being
directly embedded into the Feed-Forward Network (FFN) of the corresponding
layer in LLMs. This approach not only avoids sequence expansion but also
dynamically fuses multi-layer visual information. By fine-tuning only a small
number of parameters, DEHVF achieves precise alignment and complementarity of
cross-modal information at the same semantic granularity. We conducted
experiments across various VL benchmarks, including visual question answering
on ScienceQA and image captioning on COCO Captions. The results demonstrate
that DEHVF achieves higher accuracy than existing parameter-efficient
fine-tuning (PEFT) baselines while maintaining efficient training and
inference.

### 44. EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jingwen Liu, Kan Jen Cheng, Jiachen Lian, Akshay Anand, Rishi Jain, Faith Qiao, Robin Netzorg, Huang-Cheng Chou, Tingle Li, Guan-Ting Lin, Gopala Anumanchipalli
- **URL**: <http://arxiv.org/abs/2508.17623v2>
- **Submitted**: 2025-08-25 03:01:40
- **Comment**: Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and
  Understanding Workshop
- **Topic Keywords**: rag
- **Reason**: This paper focuses on emotional reasoning in spoken dialogue systems, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on dialogue systems, the topic is more focused on human-computer interaction and emotional intelligence, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Speech emotions play a crucial role in human-computer interaction, shaping
engagement and context-aware communication. Despite recent advances in spoken
dialogue systems, a holistic system for evaluating emotional reasoning is still
lacking. To address this, we introduce EMO-Reasoning, a benchmark for assessing
emotional coherence in dialogue systems. It leverages a curated dataset
generated via text-to-speech to simulate diverse emotional states, overcoming
the scarcity of emotional speech data. We further propose the Cross-turn
Emotion Reasoning Score to assess the emotion transitions in multi-turn
dialogues. Evaluating seven dialogue systems through continuous, categorical,
and perceptual metrics, we show that our framework effectively detects
emotional inconsistencies, providing insights for improving current dialogue
systems. By releasing a systematic evaluation benchmark, we aim to advance
emotion-aware spoken dialogue modeling toward more natural and adaptive
interactions.

### 45. Exploring the Interplay between Musical Preferences and Personality through the Lens of Language

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Eliran Shem-Tov, Ella Rabinovich
- **URL**: <http://arxiv.org/abs/2508.18208v1>
- **Submitted**: 2025-08-25 17:10:08
- **Topic Keywords**: search
- **Reason**: The paper explores the relationship between musical preferences and personality traits, using linguistic analysis. While it involves natural language processing, the focus is on music psychology and personality analysis, which is not directly related to information retrieval, search technologies, or query understanding, making it irrelevant to the user's primary research interests.

#### Abstract
> Music serves as a powerful reflection of individual identity, often aligning
with deeper psychological traits. Prior research has established correlations
between musical preferences and personality traits, while separate studies have
demonstrated that personality is detectable through linguistic analysis. Our
study bridges these two research domains by investigating whether individuals'
musical preferences are recognizable in their spontaneous language through the
lens of the Big Five personality traits (Openness, Conscientiousness,
Extroversion, Agreeableness, and Neuroticism). Using a carefully curated
dataset of over 500,000 text samples from nearly 5,000 authors with reliably
identified musical preferences, we build advanced models to assess personality
characteristics. Our results reveal significant personality differences across
fans of five musical genres. We release resources for future research at the
intersection of computational linguistics, music psychology and personality
analysis.

### 46. The AI Data Scientist

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Farkhad Akimov, Munachiso Samuel Nwadike, Zangir Iklassov, Martin Tak√°ƒç
- **URL**: <http://arxiv.org/abs/2508.18113v1>
- **Submitted**: 2025-08-25 15:21:49
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on AI Data Scientist, an autonomous Agent powered by large language models, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it mentions data analysis and insights, the context is more focused on data science and decision-making, rather than search and retrieval.

#### Abstract
> Imagine decision-makers uploading data and, within minutes, receiving clear,
actionable insights delivered straight to their fingertips. That is the promise
of the AI Data Scientist, an autonomous Agent powered by large language models
(LLMs) that closes the gap between evidence and action. Rather than simply
writing code or responding to prompts, it reasons through questions, tests
ideas, and delivers end-to-end insights at a pace far beyond traditional
workflows. Guided by the scientific tenet of the hypothesis, this Agent
uncovers explanatory patterns in data, evaluates their statistical
significance, and uses them to inform predictive modeling. It then translates
these results into recommendations that are both rigorous and accessible. At
the core of the AI Data Scientist is a team of specialized LLM Subagents, each
responsible for a distinct task such as data cleaning, statistical testing,
validation, and plain-language communication. These Subagents write their own
code, reason about causality, and identify when additional data is needed to
support sound conclusions. Together, they achieve in minutes what might
otherwise take days or weeks, enabling a new kind of interaction that makes
deep data science both accessible and actionable.

### 47. Speculating LLMs' Chinese Training Data Pollution from Their Tokens

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Qingjie Zhang, Di Wang, Haoting Qian, Liu Yan, Tianwei Zhang, Ke Xu, Qi Li, Minlie Huang, Hewu Li, Han Qiu
- **URL**: <http://arxiv.org/abs/2508.17771v1>
- **Submitted**: 2025-08-25 08:08:51
- **Topic Keywords**: search
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The topic of the paper is focused on the quality of training data for Large Language Models (LLMs) and the presence of polluted tokens, which is not related to the user's areas of interest.

#### Abstract
> Tokens are basic elements in the datasets for LLM training. It is well-known
that many tokens representing Chinese phrases in the vocabulary of GPT
(4o/4o-mini/o1/o3/4.5/4.1/o4-mini) are indicating contents like pornography or
online gambling. Based on this observation, our goal is to locate Polluted
Chinese (PoC) tokens in LLMs and study the relationship between PoC tokens'
existence and training data. (1) We give a formal definition and taxonomy of
PoC tokens based on the GPT's vocabulary. (2) We build a PoC token detector via
fine-tuning an LLM to label PoC tokens in vocabularies by considering each
token's both semantics and related contents from the search engines. (3) We
study the speculation on the training data pollution via PoC tokens'
appearances (token ID). Experiments on GPT and other 23 LLMs indicate that
tokens widely exist while GPT's vocabulary behaves the worst: more than 23%
long Chinese tokens (i.e., a token with more than two Chinese characters) are
either porn or online gambling. We validate the accuracy of our speculation
method on famous pre-training datasets like C4 and Pile. Then, considering
GPT-4o, we speculate that the ratio of "Yui Hatano" related webpages in
GPT-4o's training data is around 0.5%.

### 48. LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Bingxi Zhao, Lin Geng Foo, Ping Hu, Christian Theobalt, Hossein Rahmani, Jun Liu
- **URL**: <http://arxiv.org/abs/2508.17692v1>
- **Submitted**: 2025-08-25 06:01:16
- **Comment**: 51 pages,10 figures,8 tables. Work in progress
- **Topic Keywords**: search
- **Reason**: The paper's focus on large language models (LLMs) and agent systems is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on the use of LLMs, the context is different and the paper's scope is broader, covering various domains and applications.

#### Abstract
> Recent advances in the intrinsic reasoning capabilities of large language
models (LLMs) have given rise to LLM-based agent systems that exhibit
near-human performance on a variety of automated tasks. However, although these
systems share similarities in terms of their use of LLMs, different reasoning
frameworks of the agent system steer and organize the reasoning process in
different ways. In this survey, we propose a systematic taxonomy that
decomposes agentic reasoning frameworks and analyze how these frameworks
dominate framework-level reasoning by comparing their applications across
different scenarios. Specifically, we propose an unified formal language to
further classify agentic reasoning systems into single-agent methods,
tool-based methods, and multi-agent methods. After that, we provide a
comprehensive review of their key application scenarios in scientific
discovery, healthcare, software engineering, social simulation, and economics.
We also analyze the characteristic features of each framework and summarize
different evaluation strategies. Our survey aims to provide the research
community with a panoramic view to facilitate understanding of the strengths,
suitable scenarios, and evaluation practices of different agentic reasoning
frameworks.

### 49. Heterogeneous co-occurrence embedding for visual information exploration

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Takuro Ishida, Tetsuo Furukawa
- **URL**: <http://arxiv.org/abs/2508.17663v1>
- **Submitted**: 2025-08-25 04:51:50
- **Comment**: 36pages, 9 figures, Accepted to International Journal of Innovative
  Computing, Information and Control (IJICIC), 2025
- **Topic Keywords**: neurips
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus on visual information exploration and heterogeneous co-occurrence embedding is outside the scope of the user's primary research interests.

#### Abstract
> This paper proposes an embedding method for co-occurrence data aimed at
visual information exploration. We consider cases where co-occurrence
probabilities are measured between pairs of elements from heterogeneous
domains. The proposed method maps these heterogeneous elements into
corresponding two-dimensional latent spaces, enabling visualization of
asymmetric relationships between the domains. The key idea is to embed the
elements in a way that maximizes their mutual information, thereby preserving
the original dependency structure as much as possible. This approach can be
naturally extended to cases involving three or more domains, using a
generalization of mutual information known as total correlation. For
inter-domain analysis, we also propose a visualization method that assigns
colors to the latent spaces based on conditional probabilities, allowing users
to explore asymmetric relationships interactively. We demonstrate the utility
of the method through applications to an adjective-noun dataset, the NeurIPS
dataset, and a subject-verb-object dataset, showcasing both intra- and
inter-domain analysis.

### 50. Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yunze Xiao, Lynnette Hui Xian Ng, Jiarui Liu, Mona T. Diab
- **URL**: <http://arxiv.org/abs/2508.17573v1>
- **Submitted**: 2025-08-25 00:48:39
- **Comment**: Accepted in EMNLP main proceedings
- **Topic Keywords**: search
- **Reason**: The paper focuses on the concept of anthropomorphism in Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on human-AI interactions, the paper's scope is broader and more focused on design and user experience, rather than the technical aspects of IR and NLP.

#### Abstract
> Large Language Models (LLMs) increasingly exhibit \textbf{anthropomorphism}
characteristics -- human-like qualities portrayed across their outlook,
language, behavior, and reasoning functions. Such characteristics enable more
intuitive and engaging human-AI interactions. However, current research on
anthropomorphism remains predominantly risk-focused, emphasizing over-trust and
user deception while offering limited design guidance. We argue that
anthropomorphism should instead be treated as a \emph{concept of design} that
can be intentionally tuned to support user goals. Drawing from multiple
disciplines, we propose that the anthropomorphism of an LLM-based artifact
should reflect the interaction between artifact designers and interpreters.
This interaction is facilitated by cues embedded in the artifact by the
designers and the (cognitive) responses of the interpreters to the cues. Cues
are categorized into four dimensions: \textit{perceptive, linguistic,
behavioral}, and \textit{cognitive}. By analyzing the manifestation and
effectiveness of each cue, we provide a unified taxonomy with actionable levers
for practitioners. Consequently, we advocate for function-oriented evaluations
of anthropomorphic design.

---

