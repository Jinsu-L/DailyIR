# Daily Papers Report - 2025-09-24

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking

- **LLM Score**: 9
- **Keyword Score**: 16
- **Authors**: Yaoyao Qian, Yifan Zeng, Yuchao Jiang, Chelsi Jain, Huazheng Wang
- **URL**: <http://arxiv.org/abs/2509.18575v1>
- **Submitted**: 2025-09-23 02:56:38
- **Comment**: Accepted by EMNLP 2025
- **Topic Keywords**: information retrieval, ranking, pairwise, relevance, retrieval, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of ranking models and their vulnerabilities. The study focuses on the 'Ranking Blind Spot' in Large Language Models (LLMs) and its potential exploitation by malicious content providers. The paper's emphasis on LLM-based text ranking and its implications for real-time relevance optimization aligns closely with your research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Vulnerability of Large Language Models (LLMs) to Prompt Injection Attacks
- **Aim**: To investigate and demonstrate the "Ranking Blind Spot" vulnerability in LLM-based text ranking systems.
- **Rationale**: LLM-based ranking systems are increasingly used in critical applications, making them potential targets for malicious manipulation.
- **Ground**: Large Language Models (LLMs) are susceptible to prompt injection attacks, particularly in text ranking tasks.
- **Experiment**: Two novel attack strategies, Decision Objective Hijacking (DOH) and Decision Criteria Hijacking (DCH), were tested on various LLMs and ranking paradigms using real-world datasets.
- **Takeaway**: Larger LLMs are more vulnerable to these attacks, highlighting the need for robust defense mechanisms like instructional separation, adversarial fine-tuning, and semantic anomaly detection.

#### Abstract
> Large Language Models (LLMs) have demonstrated strong performance in
information retrieval tasks like passage ranking. Our research examines how
instruction-following capabilities in LLMs interact with multi-document
comparison tasks, identifying what we term the "Ranking Blind Spot", a
characteristic of LLM decision processes during comparative evaluation. We
analyze how this ranking blind spot affects LLM evaluation systems through two
approaches: Decision Objective Hijacking, which alters the evaluation goal in
pairwise ranking systems, and Decision Criteria Hijacking, which modifies
relevance standards across ranking schemes. These approaches demonstrate how
content providers could potentially influence LLM-based ranking systems to
affect document positioning. These attacks aim to force the LLM ranker to
prefer a specific passage and rank it at the top. Malicious content providers
can exploit this weakness, which helps them gain additional exposure by
attacking the ranker. In our experiment, We empirically show that the proposed
attacks are effective in various LLMs and can be generalized to multiple
ranking schemes. We apply these attack to realistic examples to show their
effectiveness. We also found stronger LLMs are more vulnerable to these
attacks. Our code is available at:
https://github.com/blindspotorg/RankingBlindSpot

---

### 2. BloomIntent: Automating Search Evaluation with LLM-Generated Fine-Grained User Intents

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Yoonseo Choi, Eunhye Kim, Hyunwoo Kim, Donghyun Park, Honggu Lee, Jinyoung Kim, Juho Kim
- **URL**: <http://arxiv.org/abs/2509.18641v1>
- **Submitted**: 2025-09-23 04:56:06
- **Comment**: Accepted to UIST 2025; 34 pages (including 18 pages of Appendix)
- **Topic Keywords**: query, queries, search
- **Reason**: This paper aligns well with your research interests in Information Retrieval, particularly in query understanding and user behavior modeling. The focus on fine-grained user intents and large language models is also relevant to your work in NLP and search technologies. However, the primary focus on search evaluation and user-centric AI evaluation is somewhat tangential to your core research themes.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Search Engine Evaluation
- **Aim**: Develop a user-centric search evaluation method that focuses on individual user intents rather than overall queries.
- **Rationale**: Traditional search evaluation methods based on queries lack nuance and fail to capture the diverse intents behind a single query.
- **Ground**: Existing search evaluation methods primarily rely on query-level assessments, neglecting the complexity of user information needs.
- **Experiment**: BloomIntent is evaluated through three technical evaluations: intent quality, scalability and accuracy, and practical application.
- **Takeaway**: BloomIntent provides a more comprehensive and user-centric approach to search evaluation by focusing on individual intents, leading to a deeper understanding of user needs and improved search system performance.

#### Abstract
> If 100 people issue the same search query, they may have 100 different goals.
While existing work on user-centric AI evaluation highlights the importance of
aligning systems with fine-grained user intents, current search evaluation
methods struggle to represent and assess this diversity. We introduce
BloomIntent, a user-centric search evaluation method that uses user intents as
the evaluation unit. BloomIntent first generates a set of plausible,
fine-grained search intents grounded on taxonomies of user attributes and
information-seeking intent types. Then, BloomIntent provides an automated
evaluation of search results against each intent powered by large language
models. To support practical analysis, BloomIntent clusters semantically
similar intents and summarizes evaluation outcomes in a structured interface.
With three technical evaluations, we showed that BloomIntent generated
fine-grained, evaluable, and realistic intents and produced scalable
assessments of intent-level satisfaction that achieved 72% agreement with
expert evaluators. In a case study (N=4), we showed that BloomIntent supported
search specialists in identifying intents for ambiguous queries, uncovering
underserved user needs, and discovering actionable insights for improving
search experiences. By shifting from query-level to intent-level evaluation,
BloomIntent reimagines how search systems can be assessed -- not only for
performance but for their ability to serve a multitude of user goals.

---

### 3. Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering

- **LLM Score**: 8
- **Keyword Score**: 1
- **Authors**: Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Zhuowan Li, Spurthi Amba Hombaiah, Weize Kong, Tao Chen, Hamed Zamani, Michael Bendersky
- **URL**: <http://arxiv.org/abs/2509.19094v1>
- **Submitted**: 2025-09-23 14:44:46
- **Topic Keywords**: personalization
- **Reason**: This paper on 'Pathways of Thoughts' for long-form personalized question answering aligns with your interests in Information Retrieval, particularly in query understanding and ranking models. The focus on user behavior modeling and personalized responses is also relevant to your background in e-commerce and NLP. However, the specific application to question answering and the use of large language models might not be a central match to your primary focus on search technologies and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Personalization in Question Answering (QA) Systems
- **Aim**: To develop a novel method called Pathways of Thoughts (PoT) for personalizing QA systems without task-specific fine-tuning.
- **Rationale**: LLMs can benefit from modeling their reasoning process as an iterative decision-making process, allowing for exploration of diverse reasoning paths and personalized responses.
- **Ground**: Large Language Models (LLMs) and the LaMP-QA benchmark for personalized QA.
- **Experiment**: PoT was evaluated on the LaMP-QA benchmark and compared to existing baselines.
- **Takeaway**: PoT outperforms existing baselines, achieving up to a 13.1% relative improvement and receiving preference from human annotators in 66% of cases.

#### Abstract
> Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

---

### 4. From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system

- **LLM Score**: 6
- **Keyword Score**: 1
- **Authors**: Maxime Manderlier, Fabian Lecron, Olivier Vu Thanh, Nicolas Gillis
- **URL**: <http://arxiv.org/abs/2509.18980v1>
- **Submitted**: 2025-09-23 13:30:03
- **Topic Keywords**: recommend
- **Reason**: The paper explores explainable AI in recommender systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on recommender systems and user-facing explanations is not a central match to your primary research themes, which include query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Explainability in Recommender Systems
- **Aim**: To investigate the effectiveness of Large Language Models (LLMs) in generating human-understandable explanations for recommendations made by a transparent matrix factorization model (BSSMF).
- **Rationale**: Understanding why a recommender system makes certain recommendations is crucial for building trust and user satisfaction.
- **Ground**: A user study with 326 participants evaluated four types of explanations: No Explanation, Model-Based Explanation, History-Based Explanation, and Combined Explanation.
- **Experiment**: Participants rated the recommendations and explanations across five dimensions: transparency, effectiveness, persuasion, trust, and satisfaction.
- **Takeaway**: User-history based explanations were sometimes preferred over model-based explanations, highlighting the importance of user perception in explainability. Model-based explanations lacked specificity, while history-based explanations lacked depth. Combined explanations were confusing.

#### Abstract
> We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

---

### 5. A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Olalekan K. Akindele, Bhupesh Kumar Mishra, Kenneth Y. Wertheim
- **URL**: <http://arxiv.org/abs/2509.19209v1>
- **Submitted**: 2025-09-23 16:29:22
- **Comment**: 25 Pages
- **Topic Keywords**: query, relevance, rag, retrieval augmented generation, retrieval, search
- **Reason**: This paper presents a Retrieval-Augmented Generation (RAG) chatbot that leverages a knowledge graph and vector search retrieval. While it touches on information retrieval and natural language processing, its primary focus is on conversational AI and chatbots, which is somewhat related to your interests in search technologies and query understanding, but not a central match.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) Chatbot for Domain-Specific Applications
- **Aim**: To develop a RAG chatbot that addresses factual accuracy and contextual integrity challenges in large datasets, particularly in domain-specific applications.
- **Rationale**: Existing chatbots often struggle with factual accuracy and contextual understanding in large, domain-specific datasets.
- **Ground**: A knowledge graph constructed from 100,000 project-related emails and documents stored in a Neo4j database.
- **Experiment**: Evaluation of the chatbot's performance using a novel tripartite evaluation framework (RAG-Eval) that considers user query, retrieved document, and generated response.
- **Takeaway**: The RAG chatbot demonstrates improved factual accuracy and contextual integrity compared to traditional chatbots, particularly when using few-shot prompting and chain-of-thought reasoning. The RAG-Eval framework provides a comprehensive and effective method for assessing chatbot performance in domain-specific applications.

#### Abstract
> Large Language Models (LLMs) have significantly enhanced conversational
Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the
avoidance of factual inconsistencies remain pressing challenges, particularly
for large datasets. Designing an effective chatbot with appropriate methods and
evaluating its effectiveness is among the challenges in this domain. This study
presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a
knowledge graph and vector search retrieval to deliver precise, context-rich
responses in an exemplary use case from over high-volume engineering
project-related emails, thereby minimising the need for document chunking. A
central innovation of this work is the introduction of RAG Evaluation
(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework
specifically developed to assess RAG applications. This framework operates in
parallel with the chatbot, jointly assessing the user's query, the retrieved
document, and the generated response, enabling a holistic evaluation across
multiple quality metrics like query relevance, factual accuracy, coverage,
coherence and fluency. The resulting scoring system is provided directly to
users as a confidence score (1 to 100%), enabling quick identification of
possible misaligned or incomplete answers. This proposed approach promotes
transparency and rapid verification by incorporating metadata email IDs,
timestamps into responses. Experimental comparisons against BERTScore and
G-EVAL for summarisation evaluation tasks confirm its effectiveness, and
empirical analysis also shows RAG-Eval reliably detects factual gaps and query
mismatches, thereby fostering trust in high demand, data centric environments.
These findings highlight a scalable path for developing accurate,
user-verifiable chatbots that bridge the gap between high-level conversational
fluency and factual accuracy.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Robust Denoising Neural Reranker for Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Wenyu Mao, Shuchang Liu, Hailan Yang, Xiaobei Wang, Xiaoyu Yang, Xu Gao, Xiang Li, Lantao Hu, Han Li, Kun Gai, An Zhang, Xiang Wang
- **URL**: <http://arxiv.org/abs/2509.18736v1>
- **Submitted**: 2025-09-23 07:29:52
- **Topic Keywords**: retriever, ranking, rerank, retrieval, recommend, rank
- **Reason**: The paper discusses a two-stage retrieval-ranking framework, which is somewhat related to information retrieval and search technologies. However, the focus is on recommender systems and denoising neural rerankers, which is not a central match to the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> For multi-stage recommenders in industry, a user request would first trigger
a simple and efficient retriever module that selects and ranks a list of
relevant items, then calls a slower but more sophisticated deep reranking model
that refines the item arrangement before exposure to the user. The latter model
typically reranks the item list conditioned on the user's history content and
the initial ranking from retrievers. Although this two-stage retrieval-ranking
framework demonstrates practical effectiveness, the significance of retriever
scores from the previous stage has been limitedly explored, which is
informative. In this work, we first theoretically analyze the limitations of
using retriever scores as the rerankers' input directly and argue that the
reranking task is essentially a noise reduction problem from the retriever
scores. Following this notion, we derive an adversarial framework, DNR, that
associates the denoising reranker with a carefully designed noise generation
module. We extend the conventional score error minimization term with three
augmented objectives, including: 1) a denoising objective that aims to denoise
the noisy retriever scores to align with the user feedback; 2) an adversarial
retriever score generation objective that improves the exploration in the
retriever score space; and 3) a distribution regularization term that aims to
align the distribution of generated noisy retriever scores with the real ones.
Extensive experiments are conducted on three public datasets, together with
analytical support, validating the effectiveness of the proposed DNR.

### 7. RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Olawumi Olasunkanmi, Mathew Satursky, Hong Yi, Chris Bizon, Harlin Lee, Stanley Ahalt
- **URL**: <http://arxiv.org/abs/2509.19057v1>
- **Submitted**: 2025-09-23 14:21:46
- **Topic Keywords**: ranking, rerank, retrieval, rank, search
- **Reason**: The paper focuses on relation extraction in biomedical abstracts using LLMs and ontology constraints, which is somewhat related to information retrieval and NLP. However, the specific domain (biomedical) and application (relation extraction) are not directly aligned with the user's core research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical
decision support but remain incomplete. Large language models (LLMs) excel at
extracting biomedical relations, yet their outputs lack standardization and
alignment with ontologies, limiting KG integration. We introduce RELATE, a
three-stage pipeline that maps LLM-extracted relations to standardized ontology
predicates using ChemProt and the Biolink Model. The pipeline includes: (1)
ontology preprocessing with predicate embeddings, (2) similarity-based
retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit
negation handling. This approach transforms relation extraction from free-text
outputs to structured, ontology-constrained representations. On the ChemProt
benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400
HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)
and identifies negated assertions. RELATE captures nuanced biomedical
relationships while ensuring quality for KG augmentation. By combining vector
search with contextual LLM reasoning, RELATE provides a scalable, semantically
accurate framework for converting unstructured biomedical literature into
standardized KGs.

### 8. Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Saksham Khatwani, He Cheng, Majid Afshar, Dmitriy Dligach, Yanjun Gao
- **URL**: <http://arxiv.org/abs/2509.18316v1>
- **Submitted**: 2025-09-22 18:39:09
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: This paper explores the application of knowledge graphs in diagnostic reasoning, leveraging large language models as reward models. While it touches on aspects of query understanding and ranking models, its primary focus is on knowledge graph-based reward modeling, which is somewhat related to the user's interests in information retrieval and NLP, but not directly aligned with their core research themes.

#### Abstract
> Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

### 9. AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Chen Liang, Zhaoqi Huang, Haofen Wang, Fu Chai, Chunying Yu, Huanhuan Wei, Zhengjie Liu, Yanpeng Li, Hongjun Wang, Ruifeng Luo, Xianzhong Zhao
- **URL**: <http://arxiv.org/abs/2509.18776v1>
- **Submitted**: 2025-09-23 08:09:58
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of evaluating large language models. However, the focus on the Architecture, Engineering, and Construction (AEC) field and the specific tasks defined in the benchmark are not directly aligned with your core research themes.

#### Abstract
> Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

### 10. Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Ben Finkelshtein, Silviu Cucerzan, Sujay Kumar Jauhar, Ryen White
- **URL**: <http://arxiv.org/abs/2509.18487v1>
- **Submitted**: 2025-09-23 00:46:21
- **Topic Keywords**: rag, recommend, commerce, e-commerce
- **Reason**: This paper explores the capabilities of Large Language Models (LLMs) in graph inference tasks, which is somewhat related to information retrieval, but primarily focuses on NLP and graph machine learning. While it touches on recommender systems, which is a tangential interest, it does not directly address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to your core research themes.

#### Abstract
> Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

### 11. Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Yunan Wang, Jianxin Li, Ziwei Zhang
- **URL**: <http://arxiv.org/abs/2509.18742v1>
- **Submitted**: 2025-09-23 07:35:42
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on reasoning on dynamic text-attributed graphs using Large Language Models, which is somewhat related to information retrieval and search technologies. However, the primary focus is on graph reasoning and temporal semantics, which is not a central match to the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

### 12. CompLLM: Compression for Long Context Q&A

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Gabriele Berton, Jayakrishnan Unnikrishnan, Son Tran, Mubarak Shah
- **URL**: <http://arxiv.org/abs/2509.19228v1>
- **Submitted**: 2025-09-23 16:49:43
- **Topic Keywords**: queries
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it deals with Large Language Models and context compression. However, the focus on query understanding, ranking models, and user behavior modeling is not directly addressed. The paper's emphasis on efficiency and scalability in processing long contexts may be of interest, but it does not align with your primary research themes.

#### Abstract
> Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

### 13. Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Tariq Abdul-Quddoos, Xishuang Dong, Lijun Qian
- **URL**: <http://arxiv.org/abs/2509.19224v1>
- **Submitted**: 2025-09-23 16:48:28
- **Topic Keywords**: ctr, search
- **Reason**: The paper is somewhat related to your research interests in Natural Language Processing (NLP) and data mining, but it focuses on a specific application in the medical domain, which is not a central match for your interests in Information Retrieval and Search technologies. Although it involves deep semantic understanding, the context is not directly related to your primary focus on real-time relevance optimization and query understanding.

#### Abstract
> Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

### 14. Steering Multimodal Large Language Models Decoding for Context-Aware Safety

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zheyuan Liu, Zhangchen Xu, Guangyao Dou, Xiangchi Yuan, Zhaoxuan Tan, Radha Poovendran, Meng Jiang
- **URL**: <http://arxiv.org/abs/2509.19212v1>
- **Submitted**: 2025-09-23 16:32:25
- **Comment**: A lightweight and model-agnostic decoding framework that dynamically
  adjusts token generation based on multimodal context
- **Topic Keywords**: queries
- **Reason**: The paper discusses multimodal large language models and their safety alignment, which is somewhat related to information retrieval and search technologies. However, the focus on safety and multimodal context is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The connection to natural language processing is more relevant, but still not a central match.

#### Abstract
> Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

### 15. Measuring AI "Slop" in Text

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Chantal Shaib, Tuhin Chakrabarty, Diego Garcia-Olano, Byron C. Wallace
- **URL**: <http://arxiv.org/abs/2509.19163v1>
- **Submitted**: 2025-09-23 15:41:19
- **Topic Keywords**: relevance
- **Reason**: This paper explores the concept of 'slop' in AI-generated text, which is related to NLP, but its focus on text quality and coherence doesn't directly align with your primary interests in IR, query understanding, and ranking models. However, it may offer some insights into linguistic and stylistic factors that could be relevant to your work in IR, particularly in areas requiring deep semantic understanding.

#### Abstract
> AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

### 16. Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Kun Zhu, Lizi Liao, Yuxuan Gu, Lei Huang, Xiaocheng Feng, Bing Qin
- **URL**: <http://arxiv.org/abs/2509.19125v1>
- **Submitted**: 2025-09-23 15:12:58
- **Comment**: Accepted to EMNLP 2025 Main
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves taxonomy generation and clustering. However, the focus on scientific papers and the use of large language models (LLMs) for multi-aspect encoding is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

### 17. Agentic AutoSurvey: Let LLMs Survey LLMs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yixin Liu, Yonghui Wu, Denghui Zhang, Lichao Sun
- **URL**: <http://arxiv.org/abs/2509.18661v1>
- **Submitted**: 2025-09-23 05:28:43
- **Comment**: 29 pages, 7 figures
- **Topic Keywords**: rag, search
- **Reason**: The paper presents a novel framework for automated literature survey generation, which is somewhat related to information retrieval and NLP. However, the focus is on survey generation rather than query understanding, ranking models, or user behavior modeling, making it less central to the user's core research themes.

#### Abstract
> The exponential growth of scientific literature poses unprecedented
challenges for researchers attempting to synthesize knowledge across rapidly
evolving fields. We present \textbf{Agentic AutoSurvey}, a multi-agent
framework for automated survey generation that addresses fundamental
limitations in existing approaches. Our system employs four specialized agents
(Paper Search Specialist, Topic Mining \& Clustering, Academic Survey Writer,
and Quality Evaluator) working in concert to generate comprehensive literature
surveys with superior synthesis quality. Through experiments on six
representative LLM research topics from COLM 2024 categories, we demonstrate
that our multi-agent approach achieves significant improvements over existing
baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent
architecture processes 75--443 papers per topic (847 total across six topics)
while targeting high citation coverage (often $\geq$80\% on 75--100-paper sets;
lower on very large sets such as RLHF) through specialized agent orchestration.
Our 12-dimension evaluation captures organization, synthesis integration, and
critical analysis beyond basic metrics. These findings demonstrate that
multi-agent architectures represent a meaningful advancement for automated
literature survey generation in rapidly evolving scientific domains.

### 18. Individualized non-uniform quantization for vector search

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Mariano Tepper, Ted Willke
- **URL**: <http://arxiv.org/abs/2509.18471v1>
- **Submitted**: 2025-09-22 23:20:07
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, specifically in the area of vector search techniques. However, it focuses on vector compression and quantization, which, while relevant to search, is not a central match to your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

### 19. Reinforcement Learning on Pre-Training Data

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Siheng Li, Kejiao Li, Zenan Xu, Guanhua Huang, Evander Yang, Kun Li, Haoyuan Wu, Jiajia Wu, Zihao Zheng, Chenchen Zhang, Kun Shi, Kyrierl Deng, Qi Yi, Ruibin Xiong, Tingqiang Xu, Yuhao Jiang, Jianfeng Yan, Yuyuan Zeng, Guanghui Xu, Jinbao Xue, Zhijiang Xu, Zheng Fang, Shuai Li, Qibin Liu, Xiaoxue Li, Zhuoyu Li, Yangyu Tao, Fei Gao, Cheng Jiang, Bo Chao Wang, Kai Liu, Jianchen Zhu, Wai Lam, Wayyt Wang, Bo Zhou, Di Wang
- **URL**: <http://arxiv.org/abs/2509.19249v1>
- **Submitted**: 2025-09-23 17:10:40
- **Comment**: Work in progress
- **Topic Keywords**: rag
- **Reason**: The paper discusses a novel training paradigm for large language models, which is related to my interests in Natural Language Processing (NLP) and related topics. However, the focus on reinforcement learning and pre-training data does not directly align with my primary research themes in Information Retrieval (IR), query understanding, and ranking models.

#### Abstract
> The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

### 20. MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Liting Zhang, Shiwan Zhao, Aobo Kong, Qicheng Li
- **URL**: <http://arxiv.org/abs/2509.18813v2>
- **Submitted**: 2025-09-23 09:00:43
- **Topic Keywords**: rag
- **Reason**: The paper MAPEX focuses on keyphrase extraction, a task related to Natural Language Processing (NLP), which is within your research interests. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of focus for you. While it involves deep semantic understanding, the context is not specifically related to information retrieval or search technologies.

#### Abstract
> Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44% and standard LLM baselines by
4.01% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

### 21. Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Huanxin Sheng, Xinyi Liu, Hangfeng He, Jieyu Zhao, Jian Kang
- **URL**: <http://arxiv.org/abs/2509.18658v1>
- **Submitted**: 2025-09-23 05:26:28
- **Comment**: To appear in EMNLP 2025. Our code and data are available at
  \url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge
- **Topic Keywords**: rag
- **Reason**: The paper explores the uncertainty of LLM-based evaluation, which is related to query understanding and ranking models in Information Retrieval. However, the focus on natural language generation and evaluation rather than search technologies limits its relevance to your core research themes.

#### Abstract
> LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

### 22. Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Lingwen Deng, Yifei Han, Long Zhang, Yue Du, Bin Li
- **URL**: <http://arxiv.org/abs/2509.18655v1>
- **Submitted**: 2025-09-23 05:17:39
- **Comment**: Submitted to ICASSP 2026
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on a knowledge editing framework for multi-hop question answering, which is related to information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to IR is indirect, as it involves knowledge retrieval and question answering.

#### Abstract
> Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

### 23. Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yeongbin Seo, Gayoung Kim, Jaehyung Kim, Jinyoung Yeo
- **URL**: <http://arxiv.org/abs/2509.18577v1>
- **Submitted**: 2025-09-23 02:57:29
- **Topic Keywords**: rag
- **Reason**: This paper focuses on data filtering for large language models, proposing a prior-based method that estimates token priors using corpus-level term frequency statistics. While it touches on the topic of model performance, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to information retrieval is somewhat tangential, as it primarily deals with data preprocessing.

#### Abstract
> As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

### 24. Single-Branch Network Architectures to Close the Modality Gap in Multimodal Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Christian Ganh√∂r, Marta Moscati, Anna Hausberger, Shah Nawaz, Markus Schedl
- **URL**: <http://arxiv.org/abs/2509.18807v1>
- **Submitted**: 2025-09-23 08:58:53
- **Comment**: Accepted by ACM Transactions on Recommender Systems (TORS)
- **Topic Keywords**: recommend
- **Reason**: This paper focuses on multimodal recommendation systems, which is somewhat related to information retrieval and search technologies. However, the primary focus on recommender systems and the use of neural networks for bridging the modality gap in recommendation scenarios does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Traditional recommender systems rely on collaborative filtering, using past
user-item interactions to help users discover new items in a vast collection.
In cold start, i.e., when interaction histories of users or items are not
available, content-based recommender systems use side information instead.
Hybrid recommender systems (HRSs) often employ multimodal learning to combine
collaborative and side information, which we jointly refer to as modalities.
Though HRSs can provide recommendations when some modalities are missing, their
quality degrades. In this work, we utilize single-branch neural networks
equipped with weight sharing, modality sampling, and contrastive loss to
provide accurate recommendations even in missing modality scenarios by
narrowing the modality gap. We compare these networks with multi-branch
alternatives and conduct extensive experiments on three datasets. Six
accuracy-based and four beyond-accuracy-based metrics help assess the
recommendation quality for the different training paradigms and their
hyperparameters in warm-start and missing modality scenarios. We quantitatively
and qualitatively study the effects of these different aspects on bridging the
modality gap. Our results show that single-branch networks achieve competitive
performance in warm-start scenarios and are significantly better in missing
modality settings. Moreover, our approach leads to closer proximity of an
item's modalities in the embedding space. Our full experimental setup is
available at https://github.com/hcai-mms/single-branch-networks.

### 25. Investigating Test-Time Scaling with Reranking for Machine Translation

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Shaomu Tan, Ryosuke Mitani, Ritvik Choudhary, Toshiyuki Sekiya
- **URL**: <http://arxiv.org/abs/2509.19020v1>
- **Submitted**: 2025-09-23 13:58:16
- **Topic Keywords**: ranking, rerank, rank
- **Reason**: This paper is not relevant to your research interests as it focuses on machine translation and test-time scaling, which are not directly related to information retrieval, search technologies, or natural language processing in the context of query understanding, ranking models, or user behavior modeling.

#### Abstract
> Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

### 26. DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Arijit Maji, Raghvendra Kumar, Akash Ghosh, Anushka, Nemil Shah, Abhilekh Borah, Vanshika Shah, Nishant Mishra, Sriparna Saha
- **URL**: <http://arxiv.org/abs/2509.19274v1>
- **Submitted**: 2025-09-23 17:40:43
- **Comment**: EMNLP MAINS 2025
- **Topic Keywords**: rag, search
- **Reason**: This paper appears to be focused on evaluating language models' understanding of Indian culture, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve language models, the specific application and scope are quite different from the user's areas of focus.

#### Abstract
> We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

### 27. VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hao Wang, Eiki Murata, Lingfang Zhang, Ayako Sato, So Fukuda, Ziqi Yin, Wentao Hu, Keisuke Nakao, Yusuke Nakamura, Sebastian Zwirner, Yi-Chia Chen, Hiroyuki Otomo, Hiroki Ouchi, Daisuke Kawahara
- **URL**: <http://arxiv.org/abs/2509.19002v1>
- **Submitted**: 2025-09-23 13:46:31
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on evaluating the geospatial and temporal understanding of multimodal large language models (MLLMs) using travel video itinerary reconstruction. While it involves video understanding and real-world applications, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

### 28. Evaluating the Creativity of LLMs in Persian Literary Text Generation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Armin Tourajmehr, Mohammad Reza Modarres, Yadollah Yaghoobzadeh
- **URL**: <http://arxiv.org/abs/2509.18401v1>
- **Submitted**: 2025-09-22 20:32:56
- **Topic Keywords**: search, acl
- **Reason**: This paper focuses on the creativity of Large Language Models (LLMs) in generating Persian literary text, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and scope are quite different from your areas of focus.

#### Abstract
> Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

### 29. NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Minki Hong, Jangho Choi, Jihie Kim
- **URL**: <http://arxiv.org/abs/2509.18395v1>
- **Submitted**: 2025-09-22 20:29:25
- **Comment**: 39 pages, 17 figures, EMNLP 2025 Main Conference
- **Topic Keywords**: rag, korea
- **Reason**: This paper focuses on dialogue generation and social norm modeling, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it touches on aspects of language understanding, the context is more aligned with Natural Language Processing and dialogue systems rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

### 30. Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Saeed Almheiri, Rania Hossam, Mena Attia, Chenxi Wang, Preslav Nakov, Timothy Baldwin, Fajri Koto
- **URL**: <http://arxiv.org/abs/2509.19265v1>
- **Submitted**: 2025-09-23 17:24:14
- **Comment**: EMNLP 2025 - Findings
- **Topic Keywords**: rag
- **Reason**: This paper focuses on cross-cultural transfer of commonsense reasoning in LLMs, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves LLMs and NLP, the context and application are quite different from your areas of focus.

#### Abstract
> Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

### 31. Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Junhao Su, Yuanliang Wan, Junwei Yang, Hengyu Shi, Tianyang Han, Junfeng Luo, Yurui Qiu
- **URL**: <http://arxiv.org/abs/2509.18847v1>
- **Submitted**: 2025-09-23 09:35:49
- **Comment**: 9pages
- **Topic Keywords**: rag
- **Reason**: This paper focuses on enhancing the reliability of tool interactions for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves a form of model improvement, the context and objectives are quite different from the user's core research themes.

#### Abstract
> Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

### 32. Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Damian Stachura, Joanna Konieczna, Artur Nowak
- **URL**: <http://arxiv.org/abs/2509.18843v1>
- **Submitted**: 2025-09-23 09:27:57
- **Comment**: CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain
- **Topic Keywords**: rag
- **Reason**: This paper focuses on the comparison of open-weight and proprietary language models for biomedical question answering, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context and application are specific to the biomedical domain and question answering, which does not align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

### 33. Financial Risk Relation Identification through Dual-view Adaptation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wei-Ning Chiu, Yu-Hsiang Wang, Andy Hsiao, Yu-Shiang Huang, Chuan-Ju Wang
- **URL**: <http://arxiv.org/abs/2509.18775v1>
- **Submitted**: 2025-09-23 08:09:30
- **Comment**: 11 pages, 3 figures, EMNLP 2025 Main Conference
- **Topic Keywords**: rag
- **Reason**: This paper focuses on financial risk relation identification using natural language processing, which is somewhat related to the user's interests in NLP. However, the topic is not directly related to information retrieval, search technologies, or query understanding, which are the user's core research themes.

#### Abstract
> A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

### 34. MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yizhe Huang, Yang Liu, Ruiyu Zhao, Xiaolong Zhong, Xingming Yue, Ling Jiang
- **URL**: <http://arxiv.org/abs/2509.18713v1>
- **Submitted**: 2025-09-23 06:57:07
- **Topic Keywords**: commerce, e-commerce
- **Reason**: This paper focuses on improving the reliability of Large Language Model-based agents in customer service, which is a specific application in the e-commerce domain. While it involves natural language processing, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

### 35. Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Aditya Bhattacharjee, Marco Pasini, Emmanouil Benetos
- **URL**: <http://arxiv.org/abs/2509.18620v1>
- **Submitted**: 2025-09-23 04:11:15
- **Comment**: Under review for International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), Barcelona, 2026
- **Topic Keywords**: retrieval
- **Reason**: This paper is not relevant to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. The focus on audio fingerprinting and scalability in the audio domain does not align with your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The evaluation of audio fingerprinting at a realistic scale is limited by the
scarcity of large public music databases. We present an audio-free approach
that synthesises latent fingerprints which approximate the distribution of real
fingerprints. Our method trains a Rectified Flow model on embeddings extracted
by pre-trained neural audio fingerprinting systems. The synthetic fingerprints
generated using our system act as realistic distractors and enable the
simulation of retrieval performance at a large scale without requiring
additional audio. We assess the fidelity of synthetic fingerprints by comparing
the distributions to real data. We further benchmark the retrieval performances
across multiple state-of-the-art audio fingerprinting frameworks by augmenting
real reference databases with synthetic distractors, and show that the scaling
trends obtained with synthetic distractors closely track those obtained with
real distractors. Finally, we scale the synthetic distractor database to model
retrieval performance for very large databases, providing a practical metric of
system scalability that does not depend on access to audio corpora.

### 36. OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhuoxiao Chen, Hongyang Yu, Ying Xu, Yadan Luo, Long Duong, Yuan-Fang Li
- **URL**: <http://arxiv.org/abs/2509.18600v1>
- **Submitted**: 2025-09-23 03:42:26
- **Topic Keywords**: acl
- **Reason**: This paper focuses on radiology report generation, which is not a core area of information retrieval. While it involves some NLP aspects, the primary goal is not query understanding or ranking models, making it less relevant to your research interests.

#### Abstract
> Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

### 37. UniECG: Understanding and Generating ECG in One Unified Model

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiarui Jin, Haoyu Wang, Xiang Lan, Jun Li, Gaofeng Cheng, Hongyan Li, Shenda Hong
- **URL**: <http://arxiv.org/abs/2509.18588v1>
- **Submitted**: 2025-09-23 03:15:53
- **Topic Keywords**: rag
- **Reason**: This paper focuses on a unified model for ECG interpretation and generation, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

### 38. Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Runyan Yang, Yuke Si, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang
- **URL**: <http://arxiv.org/abs/2509.18579v1>
- **Submitted**: 2025-09-23 02:58:16
- **Comment**: 5 pages; submitted to ICASSP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on audio language models and knowledge distillation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of model transfer, the context and application are quite different from your areas of focus.

#### Abstract
> While large audio language models excel at tasks like ASR and emotion
recognition, they still struggle with complex reasoning due to the modality gap
between audio and text as well as the lack of structured intermediate
supervision. To address this, we propose a unified knowledge distillation
framework to transfer reasoning capabilities from a high-capacity textual
teacher model to a student audio models while preserving its acoustic
competence. Our method introduces two key dimensions: source-wise distillation,
which leverages both textual and acoustic teachers to provide complementary
modality-specific supervision; and layer-wise distillation, which aligns
teacher signals with appropriate student layers to improve transfer efficiency.
This dual-dimensional strategy enables fine-grained control over the
distillation process, effectively bridging the gap between symbolic reasoning
and speech representations. Experimental results show significant improvements
in audio reasoning performance, demonstrating the effectiveness of our
framework as a reasoning transfer solution for audio modeling.

### 39. HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuke Si, Runyan Yang, Yingying Gao, Junlan Feng, Chao Deng, Shilei Zhang
- **URL**: <http://arxiv.org/abs/2509.18570v1>
- **Submitted**: 2025-09-23 02:53:38
- **Comment**: 5 pages; submitted to ICASSP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speech language modeling and multi-task learning, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning and model adaptation, the context and application are quite different from the user's areas of expertise.

#### Abstract
> Recent advances in large language models have facilitated the development of
unified speech language models (SLMs) capable of supporting multiple speech
tasks within a shared architecture. However, tasks such as automatic speech
recognition (ASR) and speech emotion recognition (SER) rely on distinct types
of information: ASR primarily depends on linguistic content, whereas SER
requires the integration of both linguistic and paralinguistic cues. Existing
multitask SLMs typically adopt naive parameter sharing or prompt-based
conditioning without explicitly modeling the differences in information
composition required by each task. Such designs risk task interference and
performance degradation, especially under limited data conditions. To address
these limitations, we propose HarmoniFuse, a component-selective and
prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse
is designed to harmonize heterogeneous task demands by selecting and fusing
task-relevant components of speech representations. Specifically, it integrates
a gated speech encoder to extract task-specific acoustic features and a
prompt-adaptive dynamic fusion module to aggregate transformer layers based on
task characteristics. In addition, a batch-interleaved training strategy
enables leveraging separate ASR and SER datasets without requiring joint
annotation. Experimental results demonstrate that HarmoniFuse improves both ASR
and SER performance, offering a scalable and robust solution for multitask
speech understanding under realistic data constraints.

### 40. LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zeyu Liu, Souvik Kundu, Lianghao Jiang, Anni Li, Srikanth Ronanki, Sravan Bodapati, Gourav Datta, Peter A. Beerel
- **URL**: <http://arxiv.org/abs/2509.18467v1>
- **Submitted**: 2025-09-22 22:43:44
- **Comment**: 17 pages, 8 figures
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on improving the efficiency of transformer architectures for long-context applications, which is not directly related to your core research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While it does involve attention mechanisms, the primary goal is to reduce computational complexity, which is not a central theme in your research.

#### Abstract
> Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

### 41. Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Oscar J. Ponce-Ponte, David Toro-Tobon, Luis F. Figueroa, Michael Gionfriddo, Megan Branda, Victor M. Montori, Saturnino Luz, Juan P. Brito
- **URL**: <http://arxiv.org/abs/2509.18439v1>
- **Submitted**: 2025-09-22 21:50:13
- **Comment**: 53 pages, 1 figure, 4 tables, 5 supplementary figures, 13
  supplementary tables
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The paper focuses on developing an AI framework for detecting shared decision-making in patient-doctor conversations, which is a topic in the healthcare domain and does not align with your core research themes.

#### Abstract
> Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

### 42. Memory-QA: Answering Recall Questions Based on Multimodal Memories

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hongda Jiang, Xinyuan Zhang, Siddhant Garg, Rishab Arora, Shiun-Zu Kuo, Jiayang Xu, Christopher Brossman, Yue Liu, Aaron Colak, Ahmed Aly, Anuj Kumar, Xin Luna Dong
- **URL**: <http://arxiv.org/abs/2509.18436v1>
- **Submitted**: 2025-09-22 21:41:35
- **Topic Keywords**: retrieval
- **Reason**: While the paper touches on multimodal memories and retrieval, it primarily focuses on question answering and multimodal benchmark creation, which is not directly aligned with the user's core research themes in Information Retrieval and Search technologies, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

### 43. Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu
- **URL**: <http://arxiv.org/abs/2509.18344v1>
- **Submitted**: 2025-09-22 19:08:57
- **Comment**: Accepted by NeurIPS 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on accelerating large language models through speculative decoding, which is outside your areas of expertise in Information Retrieval and Search technologies.

#### Abstract
> The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

### 44. Evaluating Large Language Models for Detecting Antisemitism

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jay Patel, Hrudayangam Mehta, Jeremy Blackburn
- **URL**: <http://arxiv.org/abs/2509.18293v1>
- **Submitted**: 2025-09-22 18:23:21
- **Comment**: Accepted to EMNLP 2025 Main Conference
- **Topic Keywords**: rag
- **Reason**: This paper focuses on evaluating large language models for detecting antisemitism, which is a topic outside of the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and context are not directly related to the user's core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

### 45. WolBanking77: Wolof Banking Speech Intent Classification Dataset

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Abdou Karim Kandji, Fr√©d√©ric Precioso, Cheikh Ba, Samba Ndiaye, Augustin Ndione
- **URL**: <http://arxiv.org/abs/2509.19271v1>
- **Submitted**: 2025-09-23 17:34:10
- **Comment**: 10 pages, 7 figures
- **Topic Keywords**: search
- **Reason**: This paper focuses on intent classification in the Wolof language, which is a low-resource language. While it involves NLP and has some relation to information retrieval, it does not directly align with the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

### 46. Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chiara Alzetta, Serena Auriemma, Alessandro Bondielli, Luca Dini, Chiara Fazzone, Alessio Miaschi, Martina Miliani, Marta Sartor
- **URL**: <http://arxiv.org/abs/2509.19033v2>
- **Submitted**: 2025-09-23 14:06:09
- **Comment**: Submitted to IJCoL
- **Topic Keywords**: search
- **Reason**: This paper appears to be focused on tracking research trends in Italian Computational Linguistics and NLP, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While it touches on NLP, it does not seem to address the user's specific areas of focus, such as ranking models or user behavior modeling.

#### Abstract
> Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

### 47. DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Abderrahmane Issam, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis
- **URL**: <http://arxiv.org/abs/2509.18987v1>
- **Submitted**: 2025-09-23 13:37:15
- **Comment**: Accepted at WMT2025
- **Topic Keywords**: search
- **Reason**: This paper focuses on speech translation and bridging the modality gap, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text and speech embeddings, the context and application are quite different from your areas of focus.

#### Abstract
> End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

### 48. Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Junyu Wang, Ziyang Ma, Zhengding Luo, Tianrui Wang, Meng Ge, Xiaobao Wang, Longbiao Wang
- **URL**: <http://arxiv.org/abs/2509.18816v1>
- **Submitted**: 2025-09-23 09:02:15
- **Comment**: Submitted to ICASSP 2026
- **Topic Keywords**: search
- **Reason**: This paper focuses on mitigating attention imbalance in large audio-language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep semantic understanding, the context is specific to audio processing and multi-modal models, making it somewhat tangential to the user's core research interests.

#### Abstract
> Large Audio-Language Models (LALMs) often suffer from audio-textual attention
imbalance, prioritizing text over acoustic information, particularly in the
multi-modal fusion layers of the Transformer architecture. This bias hinders
their ability to fully utilize acoustic cues, causing suboptimal performance on
audio reasoning tasks. To mitigate this, we propose \textbf{MATA}, a novel
training-free method that dynamically pushes LALMs to pay \textbf{M}ore
\textbf{A}ttention \textbf{T}o \textbf{A}udio tokens within the self-attention
mechanism. Specifically, MATA intervenes post raw attention scoring, targeting
only the last token in intermediate layers without introducing additional
parameters or computational overhead. Experiments on the MMAU and MMAR
benchmarks confirm MATA's effectiveness, with consistent performance gains.
Notably, on MMAR, MATA enables an open-source model to surpass the proprietary
Gemini 2.0 Flash for the first time. Our work provides an efficient solution to
mitigate attention bias and opens a new research direction for enhancing the
audio-processing capabilities of multi-modal models.

### 49. LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Pattara Tipaksorn, Sumonmas Thatphithakkul, Vataya Chunwijitra, Kwanchiva Thangthai
- **URL**: <http://arxiv.org/abs/2509.18722v1>
- **Submitted**: 2025-09-23 07:11:06
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on conversational ASR and speech recognition in the Thai language, which does not align with your primary focus on Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

### 50. SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data

- **LLM Score**: 1
- **Keyword Score**: 1
- **Authors**: Erik Bo≈æ√≠k, Marek ≈†uppa
- **URL**: <http://arxiv.org/abs/2509.19270v1>
- **Submitted**: 2025-09-23 17:33:57
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus is on Automatic Speech Recognition for low-resource languages, which is outside your primary areas of interest.

#### Abstract
> Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

---

