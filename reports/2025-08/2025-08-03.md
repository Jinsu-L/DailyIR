# Daily Papers Report - 2025-08-03

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu
- **URL**: <http://arxiv.org/abs/2508.00719v1>
- **Submitted**: 2025-08-01 15:38:21
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: The paper focuses on Knowledge Graph Question Answering (KGQA), which is a related topic to Information Retrieval. The use of Learning to Rank and Monte Carlo Tree Search (MCTS) is also relevant to my interests. However, the paper's primary focus is on KGQA, which is not directly aligned with my core research themes. The application of LLM-guided MCTS and Transformer-based scorer is interesting, but the paper's relevance to my research interests is somewhat limited.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Dynamically Adaptive MCTS-based Reasoning (DAMR) for Knowledge Graph Question Answering (KGQA)
- **Aim**: Addressing computational inefficiency and limited path evaluation accuracy in dynamic reasoning
- **Rationale**: Modularizing reasoning to reduce LLM overuse during search, accurately evaluating evolving reasoning paths, and training a reliable path evaluation model with limited supervision
- **Ground**: Integrating symbolic search with adaptive path evaluation using a Monte Carlo Tree Search (MCTS) backbone guided by a Large Language Model (LLM)-based planner
- **Experiment**: Experimental results show that DAMR consistently outperforms state-of-the-art baselines across KGQA datasets, with efficiency analysis and ablation study demonstrating its effectiveness
- **Takeaway**: DAMR is an effective and efficient model for multi-hop reasoning in KGQA, leveraging powerful Large Language Models (LLMs) for robust reasoning

#### Abstract
> Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

---

### 2. Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Shubham Kumar Nigam, Tanmay Dubey, Noel Shallum, Arnab Bhattacharya
- **URL**: <http://arxiv.org/abs/2508.00679v1>
- **Submitted**: 2025-08-01 14:49:33
- **Topic Keywords**: retriever, queries, ranking, retrieval, rank, search
- **Reason**: The paper is somewhat related to information retrieval, specifically in the context of legal precedent retrieval. However, the focus on rhetorical role-based queries and the use of specific models (e.g., BM25, Vector Database, Cross-Encoder) is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to the user's interests is limited.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Hybrid legal case retrieval system
- **Aim**: Design a system that mimics real-world legal search scenarios by operating with limited case information
- **Rationale**: Traditional retrieval methods deviate from real-world legal practice by using entire prior case documents as queries
- **Ground**: The system combines BM25, Vector Database, and Cross-Encoder models with Reciprocal Rank Fusion and a Hierarchical BiLSTM CRF classifier
- **Experiment**: The system is evaluated on two legal datasets: IL-PCR and COLIEE 2025, with results demonstrating significant variations in retrieval performance across different query formulations
- **Takeaway**: The proposed approach provides a reliable and scalable foundation for precedent retrieval in real-world legal search scenarios, addressing the limitations of existing systems

#### Abstract
> Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

---

### 3. MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Ziyu Gong, Yihua Huang, Chengcheng Mai
- **URL**: <http://arxiv.org/abs/2508.00579v1>
- **Submitted**: 2025-08-01 12:22:53
- **Topic Keywords**: ranking, rag, retrieval, rank
- **Reason**: The paper focuses on multi-modal question-answering and document retrieval, which is related to information retrieval and search technologies. However, the specific application domain and methodology (e.g., hierarchical indexing, multi-granularity semantic retrieval) are not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multi-modal Retrieval-Augmented Generation for Document Question-Answering
- **Aim**: To develop a novel method for document question-answering that addresses the challenges of hallucinations and inter-modal disconnection in existing methods
- **Rationale**: By leveraging both textual and visual information across long-range pages, the method can facilitate accurate question answering and establish multi-modal connections and long-distance reasoning
- **Ground**: The method uses a hierarchical indexing approach, multi-granularity semantic retrieval, and language models to encode text attributes and learn representations that capture their semantic meaning
- **Experiment**: Extensive experiments on two public datasets (MMLongBench-Doc and LongDocURL) demonstrate the superiority and effectiveness of the method, outperforming existing state-of-the-art methods
- **Takeaway**: MMRAG-DocQA is a novel approach that addresses the challenges of multi-modal long-context document question-answering, achieving higher accuracy and F1 scores than existing methods

#### Abstract
> The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.

---

### 4. ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Atakan Site, Emre Hakan Erdemir, G√ºl≈üen Eryiƒüit
- **URL**: <http://arxiv.org/abs/2508.00762v1>
- **Submitted**: 2025-08-01 16:38:18
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper focuses on question-answering over tabular data, leveraging Large Language Model-based code generation, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on code generation, it is not specifically focused on query understanding, ranking models, or user behavior modeling, which are key areas of interest for me.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Zero-shot system for question-answering over tabular data
- **Aim**: Develop a novel framework that leverages Large Language Model (LLM)-based code generation to generate executable Pandas code for SemEval-2025 Task 8: DataBench
- **Rationale**: Address the challenges of SemEval-2025 Task 8 by integrating schema-guided prompting, controlled execution, and an error-handling mechanism to generate accurate answers across diverse answer formats
- **Ground**: The DataBench dataset, which provides 1308 questions from 65 different domains, each containing question-answer pairs written in English
- **Experiment**: Evaluate the performance of the proposed system, ITUNLP, using state-of-the-art open-source LLMs, including DeepSeek-R1, DeepSeek-V3, Qwen2.5-Coder-32B, Instruct, and Llama-3.3-70B
- **Takeaway**: The proposed system achieved eighth place in Subtask I and sixth place in Subtask II among the 30 systems that outperformed the baseline in the open-source models category, demonstrating the effectiveness of the novel framework in generating accurate answers for question-answering over tabular data

#### Abstract
> This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

---

### 5. Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Qing Zhang, Alex Deng, Michelle Du, Huiji Gao, Liwei He, Sanjeev Katariya
- **URL**: <http://arxiv.org/abs/2508.00751v1>
- **Submitted**: 2025-08-01 16:28:18
- **Comment**: 10 pages
- **Topic Keywords**: ranking, recommend, rank, search
- **Reason**: The paper focuses on evaluation methods for ranking algorithms in search and recommender systems, which is related to information retrieval. However, the specific context of Airbnb search ranking and the emphasis on interleaving and counterfactual evaluation methods do not directly align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating ranking algorithms for search and recommender systems
- **Aim**: To propose interleaving and counterfactual evaluation methods as alternatives to traditional A/B testing and offline evaluations
- **Rationale**: To facilitate rapid online assessments for identifying promising candidates for A/B tests
- **Ground**: Limitations of traditional A/B testing and offline evaluations, including time-consuming and costly A/B testing, and inaccurate offline evaluations
- **Experiment**: Interleaving experiments involving providing users with two variants of a ranker and inferring their preferences based on user actions, and counterfactual evaluation experiments combining elements of A/B testing and interleaving
- **Takeaway**: The proposed approach can provide practical insights for organizations with similar interests and can benefit businesses that face similar challenges in A/B testing

#### Abstract
> Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Jeongwoo Kang, Markarit Vartampetian, Felix Herron, Yongxin Zhou, Diandra Fabre, Gabriela Gonzalez-Saez
- **URL**: <http://arxiv.org/abs/2508.00476v1>
- **Submitted**: 2025-08-01 09:51:05
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: The paper focuses on question-answering based on meeting transcripts, using retrieval augmented generation (RAG) and Abstract Meaning Representations (AMR). While it touches on information retrieval, the context is quite different from the user's primary focus on query understanding, ranking models, and user behavior modeling in the e-commerce domain. The connection to NLP is more apparent, but the paper's scope is narrower than the user's interests.

#### Abstract
> This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

### 7. When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Hongxiang Lin, Hao Guo, Zeshun Li, Erpeng Xue, Yongqian He, Xiangyu Hou, Zhaoyu Hu, Lei Wang, Sheng Chen
- **URL**: <http://arxiv.org/abs/2508.00450v1>
- **Submitted**: 2025-08-01 09:10:56
- **Topic Keywords**: relevance, rag, recommend
- **Reason**: The paper explores recommender systems, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the focus on exploratory recommendation and novelty detection is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.

### 8. Agentic large language models improve retrieval-based radiology question answering

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald K√∂stler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh
- **URL**: <http://arxiv.org/abs/2508.00743v1>
- **Submitted**: 2025-08-01 16:18:52
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores the application of large language models in radiology question answering, focusing on retrieval-based systems. While it touches on query understanding and ranking models, the primary focus is on radiology-specific tasks, which is not directly aligned with the user's core research themes in information retrieval and search technologies. The paper's relevance is somewhat related, but not a central match.

#### Abstract
> Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

### 9. Lucy: edgerunning agentic web search on mobile with machine generated task vectors

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Alan Dao, Dinh Bach Vu, Alex Nguyen, Norapat Buppodom
- **URL**: <http://arxiv.org/abs/2508.00360v1>
- **Submitted**: 2025-08-01 06:45:29
- **Topic Keywords**: rag, web search, search
- **Reason**: The paper explores a novel approach to task vectors in small language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on machine-generated task vectors and reasoning mechanisms is not directly aligned with my primary research interests in user behavior modeling, click models, and real-time relevance optimization.

#### Abstract
> Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

### 10. A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Mingruo Yuan, Shuyi Zhang, Ben Kao
- **URL**: <http://arxiv.org/abs/2508.00600v1>
- **Submitted**: 2025-08-01 12:58:34
- **Topic Keywords**: relevance
- **Reason**: The paper proposes a confidence estimation framework for large language models, which is relevant to the NLP aspect of the user's research interests. However, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of focus for the user. The paper's emphasis on context-awareness and consistency examination is somewhat related to the user's interests in deep semantic understanding, but the application is limited to language models rather than search technologies.

#### Abstract
> Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

### 11. Session-Based Recommendation with Validated and Enriched LLM Intents

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang
- **URL**: <http://arxiv.org/abs/2508.00570v1>
- **Submitted**: 2025-08-01 12:11:10
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on session-based recommendation, which is not directly related to information retrieval or search technologies. While it mentions large language models, the primary goal is to enhance recommendation systems rather than query understanding or ranking models. The paper's relevance to the user's interests is limited, but it may be of interest due to the overlap with natural language processing and data mining.

#### Abstract
> Session-based recommendation (SBR) aims to predict the next item for an
anonymous user in a timely manner. However, SBR suffers from data sparsity due
to the short and anonymous nature of sessions. Recently, an emerging line of
work has explored inferring the underlying user intents of a session using
large language models (LLMs), with the generated intents serving as auxiliary
training signals to enhance SBR models. Despite its promise, this approach
faces three key challenges: validating intent quality, incorporating
session-level multi-intents, and complementing inevitable LLM failure cases. In
this paper, we propose VELI4SBR, a two-stage framework that leverages Validated
and Enriched LLM-generated Intents for SBR. In the first stage, we generate
high-quality intents using a predict-and-correct loop that validates the
informativeness of LLM-generated intents with a global intent pool to constrain
the LLM's output space and reduce hallucination. In the second stage, we
enhance the SBR model using the generated intents through a lightweight
multi-intent prediction and fusion mechanism. Furthermore, we introduce a
training strategy that compensates for LLM failures by inferring intents from
inter-session behavioral similarities. Extensive experiments show that VELI4SBR
outperforms state-of-the-art baselines while improving explainability.

### 12. Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Sohaib Imran, Rob Lamb, Peter M. Atkinson
- **URL**: <http://arxiv.org/abs/2508.00741v1>
- **Submitted**: 2025-08-01 16:12:23
- **Topic Keywords**: rag
- **Reason**: The paper explores the ability of large language models to reason about information present in their training data, which is related to query understanding and ranking models in Information Retrieval. However, the focus on procedural data and declarative facts is not directly aligned with the user's interests in search technologies and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not specifically address the user's primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

### 13. SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, Ziqian Zeng
- **URL**: <http://arxiv.org/abs/2508.00574v1>
- **Submitted**: 2025-08-01 12:17:35
- **Topic Keywords**: rag
- **Reason**: The paper proposes a novel framework for efficient reasoning in large language models, which is related to query understanding and ranking models. However, it does not directly address user behavior modeling or click models, and its focus on language models and reasoning is not directly applicable to information retrieval. The paper's relevance to the user's interests is somewhat limited.

#### Abstract
> While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

### 14. Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Peixian Li, Yu Tian, Ruiqi Tu, Chengkai Wu, Jingjing Ren, Jingsong Li
- **URL**: <http://arxiv.org/abs/2508.00285v1>
- **Submitted**: 2025-08-01 03:05:43
- **Comment**: 23 pages, 8 figures
- **Topic Keywords**: rag
- **Reason**: The paper explores the integration of clinical reasoning into large language model-based diagnosis, which is related to information retrieval and search technologies. However, the focus is on medical diagnosis and does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited due to its specific domain and lack of connection to the user's primary research themes.

#### Abstract
> Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

### 15. Systematic Evaluation of Optimization Techniques for Long-Context Language Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ammar Ahmed, Sheng Di, Franck Cappello, Zirui Liu, Jingoo Han, Ali Anwar
- **URL**: <http://arxiv.org/abs/2508.00305v1>
- **Submitted**: 2025-08-01 04:17:24
- **Topic Keywords**: search
- **Reason**: The paper focuses on optimizing large language models for long-context scenarios, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on topics like efficiency and scalability, the primary focus is on natural language processing and model optimization, which is somewhat relevant to the user's interests but not a central match.

#### Abstract
> Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

### 16. Multi-Layer Attention is the Amplifier of Demonstration Effectiveness

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Dingzirui Wang, Xuangliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng
- **URL**: <http://arxiv.org/abs/2508.00385v1>
- **Submitted**: 2025-08-01 07:26:39
- **Topic Keywords**: query, relevance, rag, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on in-context learning, demonstration effectiveness, and gradient flow, which are not core areas of interest for the user.

#### Abstract
> Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

### 17. Melody-Lyrics Matching with Contrastive Alignment Loss

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Changhong Wang, Michel Olvera, Ga√´l Richard
- **URL**: <http://arxiv.org/abs/2508.00123v1>
- **Submitted**: 2025-07-31 19:23:57
- **Comment**: 10 pages, 7 figures, 3 tables. This work has been submitted to the
  IEEE for possible publication
- **Topic Keywords**: information retrieval, rag, retrieval
- **Reason**: The paper focuses on melody-lyrics matching in music information retrieval, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper uses contrastive alignment loss, which is a technique used in some ranking models, the context is very different from the user's core research themes.

#### Abstract
> The connection between music and lyrics is far beyond semantic bonds.
Conceptual pairs in the two modalities such as rhythm and rhyme, note duration
and syllabic stress, and structure correspondence, raise a compelling yet
seldom-explored direction in the field of music information retrieval. In this
paper, we present melody-lyrics matching (MLM), a new task which retrieves
potential lyrics for a given symbolic melody from text sources. Rather than
generating lyrics from scratch, MLM essentially exploits the relationships
between melody and lyrics. We propose a self-supervised representation learning
framework with contrastive alignment loss for melody and lyrics. This has the
potential to leverage the abundance of existing songs with paired melody and
lyrics. No alignment annotations are required. Additionally, we introduce
sylphone, a novel representation for lyrics at syllable-level activated by
phoneme identity and vowel stress. We demonstrate that our method can match
melody with coherent and singable lyrics with empirical results and intuitive
examples. We open source code and provide matching examples on the companion
webpage: https://github.com/changhongw/mlm.

### 18. Demo: TOSense -- What Did You Just Agree to?

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Xinzhang Chen, Hassan Ali, Arash Shaghaghi, Salil S. Kanhere, Sanjay Jha
- **URL**: <http://arxiv.org/abs/2508.00659v1>
- **Submitted**: 2025-08-01 14:26:23
- **Comment**: Accepted as a demonstration paper at IEEE LCN 2025
- **Topic Keywords**: relevance, retrieval
- **Reason**: The paper focuses on a specific problem in online services, proposing a Chrome extension to help users understand Terms of Service. While it involves natural language processing and information retrieval, the context is unrelated to query understanding, ranking models, or user behavior modeling, which are core areas of interest in your research.

#### Abstract
> Online services often require users to agree to lengthy and obscure Terms of
Service (ToS), leading to information asymmetry and legal risks. This paper
proposes TOSense-a Chrome extension that allows users to ask questions about
ToS in natural language and get concise answers in real time. The system
combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and
(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval
and BART-encoder for answer relevance verification. To avoid expensive manual
annotation, we present a novel Question Answering Evaluation Pipeline (QEP)
that generates synthetic questions and verifies the correctness of answers
using clustered topic matching. Experiments on five major platforms, Apple,
Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of
TOSense (with up to 44.5% accuracy) across varying number of topic clusters.
During the demonstration, we will showcase TOSense in action. Attendees will be
able to experience seamless extraction, interactive question answering, and
instant indexing of new sites.

### 19. Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Stefan Englmeier, Max A. B√ºttner, Katharina Winter, Fabian B. Flohr
- **URL**: <http://arxiv.org/abs/2508.00589v1>
- **Submitted**: 2025-08-01 12:41:52
- **Comment**: 9 pages, 10 figure, project page
  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on
  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for
  possible publication
- **Topic Keywords**: queries, retrieval
- **Reason**: The paper focuses on autonomous driving and motion retrieval, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions multimodal embedding and text queries, the context is specific to autonomous driving and does not align with the user's broader interests.

#### Abstract
> Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

### 20. ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Li Zhao, Rui Sun, Zuoyou Jiang, Bo Yang, Yuxiao Bai, Mengting Chen, Xinyang Wang, Jing Li, Zuo Bai
- **URL**: <http://arxiv.org/abs/2508.00554v1>
- **Submitted**: 2025-08-01 11:48:13
- **Topic Keywords**: ranking, rank, search
- **Reason**: The paper focuses on a multi-agent trading system, which is unrelated to information retrieval, search technologies, or natural language processing. While it mentions ranking mechanisms, they are not applied to query understanding or user behavior modeling, and the context is financial trading rather than e-commerce or general search.

#### Abstract
> In financial trading, large language model (LLM)-based agents demonstrate
significant potential. However, the high sensitivity to market noise undermines
the performance of LLM-based trading systems. To address this limitation, we
propose a novel multi-agent system featuring an internal competitive mechanism
inspired by modern corporate management structures. The system consists of two
specialized teams: (1) Data Team - responsible for processing and condensing
massive market data into diversified text factors, ensuring they fit the
model's constrained context. (2) Research Team - tasked with making
parallelized multipath trading decisions based on deep research methods. The
core innovation lies in implementing a real-time evaluation and ranking
mechanism within each team, driven by authentic market feedback. Each agent's
performance undergoes continuous scoring and ranking, with only outputs from
top-performing agents being adopted. The design enables the system to
adaptively adjust to dynamic environment, enhances robustness against market
noise and ultimately delivers superior trading performance. Experimental
results demonstrate that our proposed system significantly outperforms
prevailing multiagent systems and traditional quantitative investment methods
across diverse evaluation metrics.

### 21. Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Paul Albert, Frederic Z. Zhang, Hemanth Saratchandran, Anton van den Hengel, Ehsan Abbasnejad
- **URL**: <http://arxiv.org/abs/2508.00230v1>
- **Submitted**: 2025-08-01 00:29:13
- **Comment**: To appear in ICCV 2025
- **Topic Keywords**: rag, ctr, rank
- **Reason**: The paper focuses on parameter-efficient fine-tuning of pre-trained models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the context is not about search or ranking, but rather about adapting models for other tasks.

#### Abstract
> Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

### 22. NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya
- **URL**: <http://arxiv.org/abs/2508.00709v1>
- **Submitted**: 2025-08-01 15:23:20
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Legal Judgment Prediction in the Indian Common Law System, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions Retrieval-Augmented Generation (RAG) framework, it is not directly applicable to the user's areas of focus.

#### Abstract
> Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

### 23. EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jiaxin Deng, Qingcheng Zhu, Junbiao Pang, Linlin Yang, Zhongqian Fu, Baochang Zhang
- **URL**: <http://arxiv.org/abs/2508.00522v1>
- **Submitted**: 2025-08-01 10:59:49
- **Topic Keywords**: rag, rank, search
- **Reason**: The paper focuses on optimizing large language models and vision-language models, which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on the concept of generalization, it does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

### 24. Fine-grained Spatiotemporal Grounding on Egocentric Videos

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Shuo Liang, Yiwu Zhong, Zi-Yuan Hu, Yeyao Tao, Liwei Wang
- **URL**: <http://arxiv.org/abs/2508.00518v1>
- **Submitted**: 2025-08-01 10:53:27
- **Comment**: Accepted by ICCV 2025
- **Topic Keywords**: queries, search
- **Reason**: The paper focuses on a specific problem in computer vision, spatiotemporal video grounding, and does not relate to the user's primary research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper's abstract does not mention any of these topics, and the user's background in e-commerce is not relevant to this paper.

#### Abstract
> Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

### 25. ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang
- **URL**: <http://arxiv.org/abs/2508.00429v1>
- **Submitted**: 2025-08-01 08:37:54
- **Comment**: 17 pages, work in progress
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Graph Neural Networks and proposes a new framework, ReaGAN, which empowers each node with autonomous decision-making. While it mentions 'retrieval-augmented generation', the context is unclear and seems unrelated to information retrieval, query understanding, or ranking models, which are core areas of interest for you.

#### Abstract
> Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

### 26. Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu
- **URL**: <http://arxiv.org/abs/2508.00414v1>
- **Submitted**: 2025-08-01 08:11:31
- **Comment**: 16 pages
- **Topic Keywords**: queries, search
- **Reason**: The paper focuses on the development of a general AI agent framework, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions the construction of queries, the context is different from the user's research interests, and the paper does not address ranking models, user behavior modeling, or real-time relevance optimization.

#### Abstract
> General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

### 27. Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sarah Mercer, Daniel P. Martin, Phil Swatton
- **URL**: <http://arxiv.org/abs/2508.00742v1>
- **Submitted**: 2025-08-01 16:16:16
- **Comment**: 26 pages, 14 figures
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the application of psychometrics to large language model simulated populations, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the focus is on personality modeling and social science research, which is not a central match for your research interests.

#### Abstract
> Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

### 28. Activation-Guided Local Editing for Jailbreaking Attacks

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jiecong Wang, Haoran Li, Hao Peng, Ziqian Zeng, Zihao Wang, Haohua Du, Zhengtao Yu
- **URL**: <http://arxiv.org/abs/2508.00555v1>
- **Submitted**: 2025-08-01 11:52:24
- **Topic Keywords**: query
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or related topics. The paper focuses on adversarial attacks and security flaws in machine learning models, which is a distinct area of research.

#### Abstract
> Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

### 29. Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mikel Vandeloise
- **URL**: <http://arxiv.org/abs/2508.00534v1>
- **Submitted**: 2025-08-01 11:19:40
- **Comment**: Preprint submitted to the Journal of Object Technology on July 29,
  2025. Data available upon request until peer-review is completed
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of programming paradigms and their classification formalisms is outside the scope of your research areas, and the paper does not mention query understanding, ranking models, user behavior modeling, or real-time relevance optimization.

#### Abstract
> The rise of multi-paradigm languages challenges traditional classification
methods, leading to practical software engineering issues like interoperability
defects. This systematic literature review (SLR) maps the formal foundations of
programming paradigms. Our objective is twofold: (1) to assess the state of the
art of classification formalisms and their limitations, and (2) to identify the
conceptual primitives and mathematical frameworks for a more powerful,
reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies
lack conceptual granularity, a unified formal basis, and struggle with hybrid
languages. In response, our analysis reveals a strong convergence toward a
compositional reconstruction of paradigms. This approach identifies a minimal
set of orthogonal, atomic primitives and leverages mathematical frameworks,
predominantly Type theory, Category theory and Unifying Theories of Programming
(UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift
away from classification towards these promising formal, reconstructive
frameworks. This review provides a map of this evolution and proposes a
research agenda for their unification.

### 30. Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding
- **URL**: <http://arxiv.org/abs/2508.00454v1>
- **Submitted**: 2025-08-01 09:26:01
- **Comment**: 15 pages, 2 pages, under review at AAAI 2026
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on dialogue evaluation and large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of aggregating multiple judges' opinions, the context is different from the user's interests in ranking models and user behavior modeling.

#### Abstract
> Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

### 31. M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Chuan He, Yongchao Liu, Qiang Li, Wenliang Zhong, Chuntao Hong, Xinwei Yao
- **URL**: <http://arxiv.org/abs/2508.00452v1>
- **Submitted**: 2025-08-01 09:16:26
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. The paper's emphasis on multi-modal and multi-view features, while interesting, does not align with your primary focus on information retrieval and deep semantic understanding.

#### Abstract
> Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.

### 32. Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xiaofeng Wu, Alan Ritter, Wei Xu
- **URL**: <http://arxiv.org/abs/2508.00217v1>
- **Submitted**: 2025-07-31 23:41:31
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on tabular data understanding with large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the concept of table understanding tasks, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

### 33. Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin
- **URL**: <http://arxiv.org/abs/2508.00819v1>
- **Submitted**: 2025-08-01 17:56:07
- **Comment**: Code is available at https://github.com/Li-Jinsong/DAEDAL
- **Topic Keywords**: rag
- **Reason**: This paper focuses on a specific aspect of language models, namely the generation length, which is not directly related to information retrieval, search technologies, or query understanding. The techniques and concepts presented are primarily relevant to the field of natural language processing, but do not align with the user's core research themes.

#### Abstract
> Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

### 34. Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Gleb Schmidt, Johannes R√∂misch, Mariia Halchynska, Svetlana Gorovaia, Ivan P. Yamshchikov
- **URL**: <http://arxiv.org/abs/2508.00675v1>
- **Submitted**: 2025-08-01 14:48:17
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, and does not involve query understanding, ranking models, or user behavior modeling. The focus is on style change detection in computational authorship analysis, which is a distinct area of research.

#### Abstract
> Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

### 35. The Prosody of Emojis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Giulio Zhou, Tsz Kin Lam, Alexandra Birch, Barry Haddow
- **URL**: <http://arxiv.org/abs/2508.00537v1>
- **Submitted**: 2025-08-01 11:24:12
- **Topic Keywords**: rag
- **Reason**: The paper focuses on the role of emojis in spoken communication, examining how they influence prosody and how listeners interpret prosodic cues to recover emoji meanings. While it touches on the topic of communication, it does not address information retrieval, search technologies, or query understanding, which are the primary areas of interest. The paper's focus on spoken communication and emojis is not directly relevant to the user's research themes.

#### Abstract
> Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

### 36. Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rana Salama, Abdou Youssef, Mona Diab
- **URL**: <http://arxiv.org/abs/2508.00420v1>
- **Submitted**: 2025-08-01 08:17:41
- **Topic Keywords**: rag
- **Reason**: The paper focuses on applying wavelet transforms to sentence embeddings, which is a topic in Natural Language Processing (NLP). However, it does not seem to be directly related to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the user's primary research interests.

#### Abstract
> Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

### 37. Benchmarking LLMs for Unit Test Generation from Real-World Functions

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dong Huang, Jie M. Zhang, Mark Harman, Qianru Zhang, Mingzhe Du, See-Kiong Ng
- **URL**: <http://arxiv.org/abs/2508.00408v1>
- **Submitted**: 2025-08-01 08:08:26
- **Comment**: Under Review
- **Topic Keywords**: rag
- **Reason**: The paper focuses on benchmarking large language models for unit test generation, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, the context is not relevant to the user's primary research interests.

#### Abstract
> Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

### 38. Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kaiyan Zhao, Zhongtao Miao, Yoshimasa Tsuruoka
- **URL**: <http://arxiv.org/abs/2508.00332v1>
- **Submitted**: 2025-08-01 05:42:28
- **Comment**: Work in progress
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal sentence embedding models, object-phrase alignment, and contrastive learning, which are not directly related to information retrieval, search technologies, or query understanding. While it involves NLP and data mining, the topic is more specific to computer vision and multimodal representation learning, making it less relevant to the user's primary research interests.

#### Abstract
> Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

### 39. RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, Zhi Jin, Fei Huang, Yongbin Li, Ge Li
- **URL**: <http://arxiv.org/abs/2508.00222v1>
- **Submitted**: 2025-07-31 23:55:29
- **Topic Keywords**: rag
- **Reason**: The paper focuses on reinforcement learning and large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions 'reasoning' and 'problem-solving scope', the context is not relevant to the user's interests in IR and NLP.

#### Abstract
> Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

### 40. Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rana Aref Salama, Abdou Youssef, Mona Diab
- **URL**: <http://arxiv.org/abs/2508.00220v1>
- **Submitted**: 2025-07-31 23:46:40
- **Topic Keywords**: rag
- **Reason**: The paper focuses on applying Discrete Wavelet Transforms to word and sentence embeddings for semantic compression and analysis, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on NLP, the application is more focused on signal processing and data representation, rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

### 41. Semiotic Complexity and Its Epistemological Implications for Modeling Culture

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zachary K. Stine, James E. Deitrick
- **URL**: <http://arxiv.org/abs/2508.00095v1>
- **Submitted**: 2025-07-31 18:44:48
- **Comment**: Preprint. Manuscript currently under review
- **Topic Keywords**: recommend, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of semiotic complexity and its epistemological implications for modeling culture is outside the scope of your areas of focus, and the paper does not address query understanding, ranking models, user behavior modeling, or real-time relevance optimization.

#### Abstract
> Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

### 42. Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang
- **URL**: <http://arxiv.org/abs/2508.00788v1>
- **Submitted**: 2025-08-01 17:11:42
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on language models and pronoun handling, which is a topic in Natural Language Processing, but not a primary interest of yours.

#### Abstract
> Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

### 43. Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Sergio Rubio-Mart√≠n, Mar√≠a Teresa Garc√≠a-Ord√°s, Antonio Serrano-Garc√≠a, Clara Margarita Franch-Pato, Arturo Crespo-√Ålvaro, Jos√© Alberto Ben√≠tez-Andrades
- **URL**: <http://arxiv.org/abs/2508.00695v1>
- **Submitted**: 2025-08-01 15:11:39
- **Topic Keywords**: search
- **Reason**: The paper focuses on classifying clinical notes into specific diagnostic categories using various AI models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves machine learning and deep learning approaches, the context is healthcare and diagnosis, which is not aligned with the user's primary research interests.

#### Abstract
> The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

### 44. GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Farhana Haque, Md. Abdur Rahman, Sumon Ahmed
- **URL**: <http://arxiv.org/abs/2508.00605v1>
- **Submitted**: 2025-08-01 13:08:26
- **Topic Keywords**: search
- **Reason**: The paper focuses on topic modeling in the Bengali language, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. Although it involves Natural Language Processing, the specific application and techniques used are not relevant to the user's research areas.

#### Abstract
> Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

### 45. Audio Prototypical Network For Controllable Music Recommendation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Fƒ±rat √ñncel, Emiliano Penaloza, Haolun Wu, Shubham Gupta, Mirco Ravanelli, Laurent Charlin, Cem Subakan
- **URL**: <http://arxiv.org/abs/2508.00194v1>
- **Submitted**: 2025-07-31 22:27:22
- **Comment**: Accepted to MLSP2025
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on music recommendation, which is outside the user's primary area of interest in Information Retrieval and Search technologies. Although it mentions interpretable and controllable user profiles, the approach is based on audio prototypical networks, which is not directly related to query understanding, ranking models, or user behavior modeling.

#### Abstract
> Traditional recommendation systems represent user preferences in dense
representations obtained through black-box encoder models. While these models
often provide strong recommendation performance, they lack interpretability for
users, leaving users unable to understand or control the system's modeling of
their preferences. This limitation is especially challenging in music
recommendation, where user preferences are highly personal and often evolve
based on nuanced qualities like mood, genre, tempo, or instrumentation. In this
paper, we propose an audio prototypical network for controllable music
recommendation. This network expresses user preferences in terms of prototypes
representative of semantically meaningful features pertaining to musical
qualities. We show that the model obtains competitive recommendation
performance compared to popular baseline models while also providing
interpretable and controllable user profiles.

### 46. Comparison of Large Language Models for Deployment Requirements

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Alper Yaman, Jannik Schwab, Christof Nitsche, Abhirup Sinha, Marco Huber
- **URL**: <http://arxiv.org/abs/2508.00185v1>
- **Submitted**: 2025-07-31 22:03:07
- **Topic Keywords**: search
- **Reason**: The paper focuses on Large Language Models and their deployment requirements, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on topics like content creation and generation, it lacks relevance to the user's primary research interests in IR, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

### 47. A Survey on Code Generation with LLM-based Agents

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yihong Dong, Xue Jiang, Jiaru Qian, Tian Wang, Kechi Zhang, Zhi Jin, Ge Li
- **URL**: <http://arxiv.org/abs/2508.00083v1>
- **Submitted**: 2025-07-31 18:17:36
- **Comment**: Work in progress
- **Topic Keywords**: search
- **Reason**: The paper focuses on code generation with LLM-based agents, which is not directly related to information retrieval, search technologies, or natural language processing. While it mentions large language models, the context is different from query understanding, ranking models, or user behavior modeling, and the paper's scope is limited to software development and code generation.

#### Abstract
> Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

---

