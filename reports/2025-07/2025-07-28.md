# Daily Papers Report - 2025-07-28

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Industry Insights from Comparing Deep Learning and GBDT Models for E-Commerce Learning-to-Rank

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Yunus Lutz, Timo Wilm, Philipp Duwe
- **URL**: <http://arxiv.org/abs/2507.20753v1>
- **Submitted**: 2025-07-28 12:02:02
- **Comment**: This work was accepted for publication in the 19th ACM Conference on
  Recommender Systems (RecSys 2025). The final published version will be
  available at the ACM Digital Library
- **Topic Keywords**: ltr, click, recommend, commerce, e-commerce, rank, search
- **Reason**: The paper is relevant to your research interests in Information Retrieval, specifically Learning-to-Rank, as it compares deep learning and GBDT models for e-commerce LTR tasks. The focus on e-commerce is also relevant to your background experience. However, the paper's scope is limited to a specific domain and does not delve into query understanding, user behavior modeling, or real-time relevance optimization, which are key areas of interest for you.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Deep Neural Networks for E-commerce Learning-to-Rank Tasks
- **Aim**: Investigate whether DNNs can outperform traditional tree-based models in e-commerce LTR tasks
- **Rationale**: Existing LTR datasets have limitations, and there is a need for more realistic and comprehensive evaluation methods
- **Ground**: E-commerce Learning-to-Rank tasks, OTTO dataset, industry's proprietary interaction log data
- **Experiment**: Evaluate multiple DNN architectures and loss functions, conduct online A/B test to validate offline results
- **Takeaway**: Deep learning approaches can serve as a viable alternative to GBDT-based models in industrial ranking systems when properly tuned and evaluated for production systems

#### Abstract
> In e-commerce recommender and search systems, tree-based models, such as
LambdaMART, have set a strong baseline for Learning-to-Rank (LTR) tasks.
Despite their effectiveness and widespread adoption in industry, the debate
continues whether deep neural networks (DNNs) can outperform traditional
tree-based models in this domain. To contribute to this discussion, we
systematically benchmark DNNs against our production-grade LambdaMART model. We
evaluate multiple DNN architectures and loss functions on a proprietary dataset
from OTTO and validate our findings through an 8-week online A/B test. The
results show that a simple DNN architecture outperforms a strong tree-based
baseline in terms of total clicks and revenue, while achieving parity in total
units sold.

---

### 2. A Non-Parametric Choice Model That Learns How Users Choose Between Recommended Options

- **LLM Score**: 7
- **Keyword Score**: 3
- **Authors**: Thorsten Krause, Harrie Oosterhuis
- **URL**: <http://arxiv.org/abs/2507.20035v1>
- **Submitted**: 2025-07-26 18:38:27
- **Topic Keywords**: user behavior, recommend
- **Reason**: The paper explores choice models in recommendation settings, which is somewhat related to my interests in query understanding and ranking models. The focus on user behavior modeling, specifically choice behavior, is also relevant. However, the paper's primary focus is on recommender systems, which is not my core area of interest.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Novel Non-Parametric Choice Model for Recommendation Systems
- **Aim**: To develop a robust and accurate choice model that infers user behavior without assuming specific user behavior
- **Rationale**: Traditional choice models assume specific user behavior and may not accurately capture actual user behavior, and identifying the most accurate choice model is impractical
- **Ground**: Discrete choice models for recommendation systems, which estimate the probability of a user choosing an item from a set of alternatives
- **Experiment**: Evaluation of LCM4Rec, MNL, and ENL models in terms of accuracy, robustness, and resistance to exposure bias
- **Takeaway**: LCM4Rec is a superior choice model that accurately infers user behavior, is robust to the true user choice model, and is less affected by exposure bias

#### Abstract
> Choice models predict which items users choose from presented options. In
recommendation settings, they can infer user preferences while countering
exposure bias. In contrast with traditional univariate recommendation models,
choice models consider which competitors appeared with the chosen item. This
ability allows them to distinguish whether a user chose an item due to
preference, i.e., they liked it; or competition, i.e., it was the best
available option. Each choice model assumes specific user behavior, e.g., the
multinomial logit model. However, it is currently unclear how accurately these
assumptions capture actual user behavior, how wrong assumptions impact
inference, and whether better models exist.
  In this work, we propose the learned choice model for recommendation
(LCM4Rec), a non-parametric method for estimating the choice model. By applying
kernel density estimation, LCM4Rec infers the most likely error distribution
that describes the effect of inter-item cannibalization and thereby
characterizes the users' choice model. Thus, it simultaneously infers what
users prefer and how they make choices. Our experimental results indicate that
our method (i) can accurately recover the choice model underlying a dataset;
(ii) provides robust user preference inference, in contrast with existing
choice models that are only effective when their assumptions match user
behavior; and (iii) is more resistant against exposure bias than existing
choice models. Thereby, we show that learning choice models, instead of
assuming them, can produce more robust predictions. We believe this work
provides an important step towards better understanding users' choice behavior.

---

### 3. RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation

- **LLM Score**: 6
- **Keyword Score**: 11
- **Authors**: Ran Xu, Yuchen Zhuang, Yue Yu, Haoyu Wang, Wenqi Shi, Carl Yang
- **URL**: <http://arxiv.org/abs/2507.20059v1>
- **Submitted**: 2025-07-26 20:57:24
- **Comment**: Work in Progress. Code will be published at:
  https://github.com/ritaranx/RAG_in_the_Wild
- **Topic Keywords**: queries, rerank, rag, retrieval, rank
- **Reason**: The paper explores Retrieval-Augmented Generation (RAG) with large language models (LLMs), which is related to query understanding and ranking models in Information Retrieval. However, the focus is more on the effectiveness of RAG in diverse retrieval scenarios rather than deep semantic understanding and real-time relevance optimization, which are core aspects of my research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) systems for large language models (LLMs)
- **Aim**: Evaluate the effectiveness of RAG systems and identify limitations and areas for improvement
- **Rationale**: RAG systems have potential to enhance LLMs, but existing systems have limitations, including diminishing returns for larger models and need for adaptive retrieval strategies
- **Ground**: Experiments on six datasets with three families of LLMs and multiple evaluation metrics
- **Experiment**: Results show smaller models achieve substantial performance gains with RAG, while larger models show diminishing returns; benefits of RAG more pronounced for factual knowledge tasks
- **Takeaway**: Future research should focus on learned routing modules, adaptive routing, and tighter integration between retrieval and generation to develop more robust RAG systems

#### Abstract
> Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved at inference time. While RAG
demonstrates strong performance on benchmarks largely derived from
general-domain corpora like Wikipedia, its effectiveness under realistic,
diverse retrieval scenarios remains underexplored. We evaluated RAG systems
using MassiveDS, a large-scale datastore with mixture of knowledge, and
identified critical limitations: retrieval mainly benefits smaller models,
rerankers add minimal value, and no single retrieval source consistently
excels. Moreover, current LLMs struggle to route queries across heterogeneous
knowledge sources. These findings highlight the need for adaptive retrieval
strategies before deploying RAG in real-world settings. Our code and data can
be found at https://github.com/ritaranx/RAG_in_the_Wild.

---

### 4. SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Yuqi Yang, Weiqi Wang, Baixuan Xu, Wei Fan, Qing Zong, Chunkit Chan, Zheye Deng, Xin Liu, Yifan Gao, Changlong Yu, Chen Luo, Yang Li, Zheng Li, Qingyu Yin, Bing Yin, Yangqiu Song
- **URL**: <http://arxiv.org/abs/2507.20185v1>
- **Submitted**: 2025-07-27 09:04:17
- **Topic Keywords**: click, commerce, e-commerce
- **Reason**: The paper focuses on understanding customer intention in e-commerce product purchase sessions, which is related to query understanding and user behavior modeling in Information Retrieval. However, the emphasis on multimodal benchmarking and large-scale dataset curation is not directly aligned with my primary research interests in ranking models and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Modeling Customer Intention in E-commerce Product Purchase Sessions
- **Aim**: To develop a more effective method for modeling customer intention in e-commerce product purchase sessions
- **Rationale**: Existing methods fail to capture and model intention effectively due to insufficient information exploitation and the use of only apparent information
- **Ground**: The authors construct a multimodal benchmark, SessionIntentBench, using a large dataset and provide an evaluation gold set with human annotations
- **Experiment**: Experimental results on the annotated data show that current language models fail to capture and utilize intention across complex session settings, but injecting intention enhances their performance
- **Takeaway**: The proposed intention tree concept and dataset curation pipeline have the potential to improve language models' capability to understand inter-session intention shift

#### Abstract
> Session history is a common way of recording user interacting behaviors
throughout a browsing activity with multiple products. For example, if an user
clicks a product webpage and then leaves, it might because there are certain
features that don't satisfy the user, which serve as an important indicator of
on-the-spot user preferences. However, all prior works fail to capture and
model customer intention effectively because insufficient information
exploitation and only apparent information like descriptions and titles are
used. There is also a lack of data and corresponding benchmark for explicitly
modeling intention in E-commerce product purchase sessions. To address these
issues, we introduce the concept of an intention tree and propose a dataset
curation pipeline. Together, we construct a sibling multimodal benchmark,
SessionIntentBench, that evaluates L(V)LMs' capability on understanding
inter-session intention shift with four subtasks. With 1,952,177 intention
entries, 1,132,145 session intention trajectories, and 13,003,664 available
tasks mined using 10,905 sessions, we provide a scalable way to exploit the
existing session data for customer intention understanding. We conduct human
annotations to collect ground-truth label for a subset of collected data to
form an evaluation gold set. Extensive experiments on the annotated data
further confirm that current L(V)LMs fail to capture and utilize the intention
across the complex session setting. Further analysis show injecting intention
enhances LLMs' performances.

---

### 5. Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Tuan Bui, Trong Le, Phat Thai, Sang Nguyen, Minh Hua, Ngan Pham, Thang Bui, Tho Quan
- **URL**: <http://arxiv.org/abs/2507.20491v1>
- **Submitted**: 2025-07-28 03:00:35
- **Comment**: 8 pages, 3 figures. Accepted at the International Joint Conference on
  Neural Networks (IJCNN) 2025, Workshop on Trustworthiness and Reliability in
  Neuro-Symbolic AI. https://2025.ijcnn.org
- **Topic Keywords**: rag
- **Reason**: The paper explores a novel framework for converting natural language to first-order logic, which is relevant to information retrieval and search technologies. However, the focus is on question-answering systems and logical reasoning, which is somewhat related to my interests in query understanding and ranking models, but not directly aligned. The paper's emphasis on explainable decision-making and logical inference is also somewhat relevant to my interests in user behavior modeling and click models.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Text-JEPA: A Novel Framework for Converting Natural Language into First-Order Logic
- **Aim**: To develop a framework that combines the strengths of neural networks and symbolic reasoning for efficient and explainable question-answering systems in specialized domains
- **Rationale**: Dual-system cognitive theory inspires the integration of natural language understanding with formal logic reasoning to produce accurate and explainable responses
- **Ground**: The framework is grounded in a comprehensive evaluation framework consisting of three custom metrics to assess the quality of logical translation and its impact on reasoning accuracy
- **Experiment**: The authors evaluate the effectiveness of Text-JEPA in various applications, including verifying the logical validity of candidate conclusions and generating natural language explanations for the reasoning process
- **Takeaway**: Text-JEPA demonstrates the potential to build efficient and explainable QA systems in specialized domains, highlighting the need for further research in empowering large language models with symbolic solvers and harnessing their power for natural language to first-order logic translation

#### Abstract
> Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Meishan Zhang, Xin Zhang, Xinping Zhao, Shouzheng Huang, Baotian Hu, Min Zhang
- **URL**: <http://arxiv.org/abs/2507.20783v1>
- **Submitted**: 2025-07-28 12:52:24
- **Comment**: 45 pages, 2 figures, 9 tables
- **Topic Keywords**: ranking, pairwise, rag, retrieval, rank, search
- **Reason**: The paper focuses on text embeddings and their applications in NLP, which is related to my interests in IR and NLP. However, the emphasis on language models and their roles in text embeddings is not directly aligned with my specific research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Text embeddings have attracted growing interest due to their effectiveness
across a wide range of natural language processing (NLP) tasks, such as
retrieval, classification, clustering, bitext mining, and summarization. With
the emergence of pretrained language models (PLMs), general-purpose text
embeddings (GPTE) have gained significant traction for their ability to produce
rich, transferable representations. The general architecture of GPTE typically
leverages PLMs to derive dense text representations, which are then optimized
through contrastive learning on large-scale pairwise datasets. In this survey,
we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the
roles PLMs play in driving its development. We first examine the fundamental
architecture and describe the basic roles of PLMs in GPTE, i.e., embedding
extraction, expressivity enhancement, training strategies, learning objectives,
and data construction. Then, we describe advanced roles enabled by PLMs, such
as multilingual support, multimodal integration, code understanding, and
scenario-specific adaptation. Finally, we highlight potential future research
directions that move beyond traditional improvement goals, including ranking
integration, safety considerations, bias mitigation, structural information
incorporation, and the cognitive extension of embeddings. This survey aims to
serve as a valuable reference for both newcomers and established researchers
seeking to understand the current state and future potential of GPTE.

### 7. CTR-Driven Ad Text Generation via Online Feedback Preference Optimization

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Yanda Chen, Zihui Ren, Qixiang Gao, Jiale Chen, Si Chen, Xubin Li, Tiezheng Ge, Bo Zheng
- **URL**: <http://arxiv.org/abs/2507.20227v1>
- **Submitted**: 2025-07-27 11:13:03
- **Comment**: 9 pages, 6 figures, 5 tables
- **Topic Keywords**: rag, click, ctr, click-through rate, retrieval, shopping
- **Reason**: The paper explores ad text generation using large language models, which is related to search technologies and query understanding. However, the focus on click-through rates and online advertising is not directly aligned with my interests in information retrieval and user behavior modeling. The paper's relevance is somewhat limited to my background in e-commerce, but it does not address my primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Advertising text plays a critical role in determining click-through rates
(CTR) in online advertising. Large Language Models (LLMs) offer significant
efficiency advantages over manual ad text creation. However, LLM-generated ad
texts do not guarantee higher CTR performance compared to human-crafted texts,
revealing a gap between generation quality and online performance of ad texts.
In this work, we propose a novel ad text generation method which optimizes for
CTR through preference optimization from online feedback. Our approach adopts
an innovative two-stage framework: (1) diverse ad text sampling via one-shot
in-context learning, using retrieval-augmented generation (RAG) to provide
exemplars with chain-of-thought (CoT) reasoning; (2) CTR-driven preference
optimization from online feedback, which weighs preference pairs according to
their CTR gains and confidence levels. Through our method, the resulting model
enables end-to-end generation of high-CTR ad texts. Extensive experiments have
demonstrated the effectiveness of our method in both offline and online
metrics. Notably, we have applied our method on a large-scale online shopping
platform and achieved significant CTR improvements, showcasing its strong
applicability and effectiveness in advertising systems.

### 8. Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Baiyu Chen, Wilson Wongso, Xiaoqian Hu, Yue Tan, Flora Salim
- **URL**: <http://arxiv.org/abs/2507.20136v1>
- **Submitted**: 2025-07-27 05:45:45
- **Comment**: KDD Cup 2025 Meta CRAG-MM Challenge
- **Topic Keywords**: query, queries, rag, retrieval, kdd
- **Reason**: The paper focuses on a specific challenge in Vision Language Models, addressing hallucination in multi-modal RAG systems. While it involves query understanding and ranking models, the context is not directly related to my primary research interests in Information Retrieval and Search technologies. The paper's emphasis on factual accuracy and truthfulness is somewhat relevant to my interests in user behavior modeling, but the scope is narrow and not directly applicable to my research areas.

#### Abstract
> This paper presents the technical solution developed by team CRUISE for the
KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn
(CRAG-MM) challenge. The challenge aims to address a critical limitation of
modern Vision Language Models (VLMs): their propensity to hallucinate,
especially when faced with egocentric imagery, long-tail entities, and complex,
multi-hop questions. This issue is particularly problematic in real-world
applications where users pose fact-seeking queries that demand high factual
accuracy across diverse modalities. To tackle this, we propose a robust,
multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for
efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways
generation and a post-hoc verification. This conservative strategy is designed
to minimize hallucinations, which incur a severe penalty in the competition's
scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG
systems. Our implementation is available at
https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

### 9. RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Andrei Vlad Man, RƒÉzvan-Alexandru SmƒÉdu, Cristian-George Craciun, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel
- **URL**: <http://arxiv.org/abs/2507.19666v1>
- **Submitted**: 2025-07-25 20:40:39
- **Comment**: 49 pages, 52 figures
- **Topic Keywords**: information retrieval, retriever, rag, retrieval
- **Reason**: The paper is somewhat related to information retrieval, as it involves retrieval-augmented generation (RAG) pipelines and dense retrievers. However, the focus is on legal education and Romanian driving law, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's use of large language models and vision-language models is also not a central match for the user's research themes.

#### Abstract
> The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

### 10. Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Mizanur Rahman, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque
- **URL**: <http://arxiv.org/abs/2507.19969v1>
- **Submitted**: 2025-07-26 14:59:04
- **Topic Keywords**: query, queries, retrieval
- **Reason**: The paper focuses on text-to-visualization models, which is related to information retrieval and search technologies. However, the emphasis is on generating visualizations rather than query understanding, ranking models, or user behavior modeling, which are core aspects of my research interests. The paper's relevance is somewhat limited to my interests in NLP and data mining.

#### Abstract
> Automated data visualization plays a crucial role in simplifying data
interpretation, enhancing decision-making, and improving efficiency. While
large language models (LLMs) have shown promise in generating visualizations
from natural language, the absence of comprehensive benchmarks limits the
rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark
designed to assess text-to-visualization models, covering 20+ chart types and
diverse data science queries, including trend analysis, correlation, outlier
detection, and predictive analytics. It comprises 1,985 samples, each with a
data table, natural language query, short answer, visualization code, and
annotated charts. The queries involve complex reasoning, conversational turns,
and dynamic data retrieval. We benchmark 11 open-source and closed-source
models, revealing significant performance gaps, highlighting key challenges,
and offering insights for future advancements. To close this gap, we propose
the first cross-modal actor-critic agentic framework that jointly refines the
textual answer and visualization code, increasing GPT-4o`s pass rate from 26%
to 42% over the direct approach and improving chart quality. We also introduce
an automated LLM-based evaluation framework that enables scalable assessment
across thousands of samples without human annotation, measuring answer
correctness, code execution success, visualization readability, and chart
accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.

### 11. Integrating LLM-Derived Multi-Semantic Intent into Graph Model for Session-based Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Shuo Zhang, Xiao Li, Jiayi Wu, Fan Yang, Xiang Li, Ming Gao
- **URL**: <http://arxiv.org/abs/2507.20147v1>
- **Submitted**: 2025-07-27 06:54:00
- **Topic Keywords**: rag, click, recommend
- **Reason**: The paper focuses on session-based recommendation, which is not directly related to information retrieval or search technologies. While it uses graph neural networks and large language models, the primary application is in recommender systems, which is only loosely relevant to the user's interests. The paper's emphasis on semantic intent and multi-semantic intent is somewhat related to the user's interest in query understanding and deep semantic understanding, but the overall topic is not a central match.

#### Abstract
> Session-based recommendation (SBR) is mainly based on anonymous user
interaction sequences to recommend the items that the next user is most likely
to click. Currently, the most popular and high-performing SBR methods primarily
leverage graph neural networks (GNNs), which model session sequences as
graph-structured data to effectively capture user intent. However, most
GNNs-based SBR methods primarily focus on modeling the ID sequence information
of session sequences, while neglecting the rich semantic information embedded
within them. This limitation significantly hampers model's ability to
accurately infer users' true intention. To address above challenge, this paper
proposes a novel SBR approach called Integrating LLM-Derived Multi-Semantic
Intent into Graph Model for Session-based Recommendation (LLM-DMsRec). The
method utilizes a pre-trained GNN model to select the top-k items as candidate
item sets and designs prompts along with a large language model (LLM) to infer
multi-semantic intents from these candidate items. Specifically, we propose an
alignment mechanism that effectively integrates the semantic intent inferred by
the LLM with the structural intent captured by GNNs. Extensive experiments
conducted on the Beauty and ML-1M datasets demonstrate that the proposed method
can be seamlessly integrated into GNNs framework, significantly enhancing its
recommendation performance.

### 12. CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: George Ibrahim, Rita Ramos, Yova Kementchedjhieva
- **URL**: <http://arxiv.org/abs/2507.20411v1>
- **Submitted**: 2025-07-27 21:00:02
- **Comment**: Published as a conference paper at COLM 2025
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores multilingual vision-language models for image captioning, leveraging retrieval-augmented generation to reduce the need for extensive multilingual training. While it touches on the concept of contextualization, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests in Information Retrieval and Search technologies.

#### Abstract
> Multilingual vision-language models have made significant strides in image
captioning, yet they still lag behind their English counterparts due to limited
multilingual training data and costly large-scale model parameterization.
Retrieval-augmented generation (RAG) offers a promising alternative by
conditioning caption generation on retrieved examples in the target language,
reducing the need for extensive multilingual training. However, multilingual
RAG captioning models often depend on retrieved captions translated from
English, which can introduce mismatches and linguistic biases relative to the
source language. We introduce CONCAP, a multilingual image captioning model
that integrates retrieved captions with image-specific concepts, enhancing the
contextualization of the input image and grounding the captioning process
across different languages. Experiments on the XM3600 dataset indicate that
CONCAP enables strong performance on low- and mid-resource languages, with
highly reduced data requirements. Our findings highlight the effectiveness of
concept-aware retrieval augmentation in bridging multilingual performance gaps.

### 13. Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, Bingsheng Yao, Dakuo Wang
- **URL**: <http://arxiv.org/abs/2507.21028v1>
- **Submitted**: 2025-07-28 17:48:40
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the use of LLM agents to simulate human evaluators, which is related to NLP and data mining. However, the focus on evaluation frameworks and persona construction is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling in the context of information retrieval.

#### Abstract
> Nearly all human work is collaborative; thus, the evaluation of real-world
NLP applications often requires multiple dimensions that align with diverse
human perspectives. As real human evaluator resources are often scarce and
costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising
approach to leverage LLM agents to believably simulate human evaluators. Yet,
to date, existing LLM-as-a-judge approaches face two limitations: persona
descriptions of agents are often arbitrarily designed, and the frameworks are
not generalizable to other tasks. To address these challenges, we propose
MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically
construct multiple evaluator personas with distinct dimensions from relevant
text documents (e.g., research papers), instantiate LLM agents with the
personas, and engage in-group debates with multi-agents to Generate
multi-dimensional feedback. Our evaluation experiments in both the educational
and medical domains demonstrate that MAJ-EVAL can generate evaluation results
that better align with human experts' ratings compared with conventional
automated evaluation metrics and existing LLM-as-a-judge methods.

### 14. Modeling User Behavior from Adaptive Surveys with Supplemental Context

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Aman Shukla, Daniel Patrick Scantlebury, Rishabh Kumar
- **URL**: <http://arxiv.org/abs/2507.20919v1>
- **Submitted**: 2025-07-28 15:19:54
- **Comment**: Best Paper, NewInML @ ICML 2025
- **Topic Keywords**: user behavior, personalization
- **Reason**: The paper focuses on modeling user behavior from adaptive surveys, which is related to user behavior modeling in search technologies. However, the paper's primary focus is on survey-based data collection and analysis, rather than query understanding, ranking models, or real-time relevance optimization, which are core areas of interest in information retrieval.

#### Abstract
> Modeling user behavior is critical across many industries where understanding
preferences, intent, or decisions informs personalization, targeting, and
strategic outcomes. Surveys have long served as a classical mechanism for
collecting such behavioral data due to their interpretability, structure, and
ease of deployment. However, surveys alone are inherently limited by user
fatigue, incomplete responses, and practical constraints on their length making
them insufficient for capturing user behavior. In this work, we present LANTERN
(Late-Attentive Network for Enriched Response Modeling), a modular architecture
for modeling user behavior by fusing adaptive survey responses with
supplemental contextual signals. We demonstrate the architectural value of
maintaining survey primacy through selective gating, residual connections and
late fusion via cross-attention, treating survey data as the primary signal
while incorporating external modalities only when relevant. LANTERN outperforms
strong survey-only baselines in multi-label prediction of survey responses. We
further investigate threshold sensitivity and the benefits of selective
modality reliance through ablation and rare/frequent attribute analysis.
LANTERN's modularity supports scalable integration of new encoders and evolving
datasets. This work provides a practical and extensible blueprint for behavior
modeling in survey-centric applications.

### 15. Latent Inter-User Difference Modeling for LLM Personalization

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng
- **URL**: <http://arxiv.org/abs/2507.20849v1>
- **Submitted**: 2025-07-28 14:00:57
- **Topic Keywords**: rag, personalization
- **Reason**: The paper focuses on personalization in large language models, which is related to search technologies and user behavior modeling. However, it does not explicitly address query understanding, ranking models, or click models, which are core aspects of my research interests. The paper's emphasis on language-based prompts and embeddings also diverges from my focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large language models (LLMs) are increasingly integrated into users' daily
lives, leading to a growing demand for personalized outputs. Previous work
focuses on leveraging a user's own history, overlooking inter-user differences
that are crucial for effective personalization. While recent work has attempted
to model such differences, the reliance on language-based prompts often hampers
the effective extraction of meaningful distinctions. To address these issues,
we propose Difference-aware Embedding-based Personalization (DEP), a framework
that models inter-user differences in the latent space instead of relying on
language prompts. DEP constructs soft prompts by contrasting a user's embedding
with those of peers who engaged with similar content, highlighting relative
behavioral signals. A sparse autoencoder then filters and compresses both
user-specific and difference-aware embeddings, preserving only task-relevant
features before injecting them into a frozen LLM. Experiments on personalized
review generation show that DEP consistently outperforms baseline methods
across multiple metrics. Our code is available at
https://github.com/SnowCharmQ/DEP.

### 16. Ontology-Enhanced Knowledge Graph Completion using Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Wenbin Guo, Xin Wang, Jiaoyan Chen, Zhao Li, Zirui Chen
- **URL**: <http://arxiv.org/abs/2507.20643v1>
- **Submitted**: 2025-07-28 09:00:48
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on Knowledge Graph Completion using Large Language Models, which is not directly related to Information Retrieval or Search technologies. While it mentions neural-perceptual structural information, it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests.

#### Abstract
> Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.

### 17. TADT-CSA: Temporal Advantage Decision Transformer with Contrastive State Abstraction for Generative Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Xiang Gao, Tianyuan Liu, Yisha Li, Jingxin Liu, Lexi Gao, Xin Li, Haiyang Lu, Liyin Hong
- **URL**: <http://arxiv.org/abs/2507.20327v1>
- **Submitted**: 2025-07-27 15:36:13
- **Topic Keywords**: rag, recommend
- **Reason**: The paper proposes a novel model for generative recommendation, leveraging Transformer-based architectures and contrastive state abstraction. While it touches on sequential recommendation tasks, it does not explicitly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on recommender systems, while related to information retrieval, is not directly aligned with the user's primary research themes.

#### Abstract
> With the rapid advancement of Transformer-based Large Language Models (LLMs),
generative recommendation has shown great potential in enhancing both the
accuracy and semantic understanding of modern recommender systems. Compared to
LLMs, the Decision Transformer (DT) is a lightweight generative model applied
to sequential recommendation tasks. However, DT faces challenges in trajectory
stitching, often producing suboptimal trajectories. Moreover, due to the high
dimensionality of user states and the vast state space inherent in
recommendation scenarios, DT can incur significant computational costs and
struggle to learn effective state representations. To overcome these issues, we
propose a novel Temporal Advantage Decision Transformer with Contrastive State
Abstraction (TADT-CSA) model. Specifically, we combine the conventional
Return-To-Go (RTG) signal with a novel temporal advantage (TA) signal that
encourages the model to capture both long-term returns and their sequential
trend. Furthermore, we integrate a contrastive state abstraction module into
the DT framework to learn more effective and expressive state representations.
Within this module, we introduce a TA-conditioned State Vector Quantization
(TAC-SVQ) strategy, where the TA score guides the state codebooks to
incorporate contextual token information. Additionally, a reward prediction
network and a contrastive transition prediction (CTP) network are employed to
ensure the state codebook preserves both the reward information of the current
state and the transition information between adjacent states. Empirical results
on both public datasets and an online recommendation system demonstrate the
effectiveness of the TADT-CSA model and its superiority over baseline methods.

### 18. MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Sara Papi, Maike Z√ºfle, Marco Gaido, Beatrice Savoldi, Danni Liu, Ioannis Douros, Luisa Bentivogli, Jan Niehues
- **URL**: <http://arxiv.org/abs/2507.19634v1>
- **Submitted**: 2025-07-25 19:00:51
- **Comment**: Work in progress
- **Topic Keywords**: rag, search
- **Reason**: The paper introduces a new benchmark for evaluating multimodal and multilingual instruction-following capabilities of large language models. While it touches on the topic of multimodal processing, it does not specifically focus on query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's relevance to the user's research interests is limited, but it may still be of interest to those working on multimodal and multilingual NLP applications.

#### Abstract
> Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations--hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities--speech, vision,
and text--and four diverse languages (English, German, Italian, and Chinese),
enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

### 19. Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jungwon Park, Wonjong Rhee
- **URL**: <http://arxiv.org/abs/2507.20906v1>
- **Submitted**: 2025-07-28 14:59:17
- **Comment**: Preprint
- **Topic Keywords**: rag
- **Reason**: The paper explores a novel approach to task embeddings and in-context learning, which is relevant to the broader field of Natural Language Processing. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of Information Retrieval. The paper's focus on large language models and task embeddings is somewhat related to my interests, but it does not align with my primary focus on information retrieval and search technologies.

#### Abstract
> In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks by conditioning on input-output examples in the prompt, without requiring
any update in model parameters. While widely adopted, it remains unclear
whether prompting with multiple examples is the most effective and efficient
way to convey task information. In this work, we propose Soft Injection of task
embeddings. The task embeddings are constructed only once using few-shot ICL
prompts and repeatedly used during inference. Soft injection is performed by
softly mixing task embeddings with attention head activations using
pre-optimized mixing parameters, referred to as soft head-selection parameters.
This method not only allows a desired task to be performed without in-prompt
demonstrations but also significantly outperforms existing ICL approaches while
reducing memory usage and compute cost at inference time. An extensive
evaluation is performed across 57 tasks and 12 LLMs, spanning four model
families of sizes from 4B to 70B. Averaged across 57 tasks, our method
outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show
that our method also serves as an insightful tool for analyzing task-relevant
roles of attention heads, revealing that task-relevant head positions selected
by our method transfer across similar tasks but not across dissimilar ones --
underscoring the task-specific nature of head functionality. Our soft injection
method opens a new paradigm for reducing prompt length and improving task
performance by shifting task conditioning from the prompt space to the
activation space.

### 20. A survey of diversity quantification in natural language processing: The why, what, where and how

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Louis Est√®ve, Marie-Catherine de Marneffe, Nurit Melnik, Agata Savary, Olha Kanishcheva
- **URL**: <http://arxiv.org/abs/2507.20858v1>
- **Submitted**: 2025-07-28 14:12:34
- **Topic Keywords**: acl
- **Reason**: The paper is somewhat related to your research interests in Natural Language Processing (NLP) and data mining, but it focuses on diversity quantification in NLP, which is not directly aligned with your primary focus on Information Retrieval and query understanding. While the paper's taxonomy of diversity measures might be of some interest, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of focus for you.

#### Abstract
> The concept of diversity has received increased consideration in Natural
Language Processing (NLP) in recent years. This is due to various motivations
like promoting and inclusion, approximating human linguistic behavior, and
increasing systems' performance. Diversity has however often been addressed in
an ad hoc manner in NLP, and with few explicit links to other domains where
this notion is better theorized. We survey articles in the ACL Anthology from
the past 6 years, with "diversity" or "diverse" in their title. We find a wide
range of settings in which diversity is quantified, often highly specialized
and using inconsistent terminology. We put forward a unified taxonomy of why,
what on, where, and how diversity is measured in NLP. Diversity measures are
cast upon a unified framework from ecology and economy (Stirling, 2007) with 3
dimensions of diversity: variety, balance and disparity. We discuss the trends
which emerge due to this systematized approach. We believe that this study
paves the way towards a better formalization of diversity in NLP, which should
bring a better understanding of this notion and a better comparability between
various approaches.

### 21. Multilingual Self-Taught Faithfulness Evaluators

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Carlo Alfano, Aymen Al Marjani, Zeno Jonke, Amin Mantrach, Saab Mansour, Marcello Federico
- **URL**: <http://arxiv.org/abs/2507.20752v1>
- **Submitted**: 2025-07-28 12:01:59
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multilingual faithfulness evaluators for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, it does not specifically address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> The growing use of large language models (LLMs) has increased the need for
automatic evaluation systems, particularly to address the challenge of
information hallucination. Although existing faithfulness evaluation approaches
have shown promise, they are predominantly English-focused and often require
expensive human-labeled training data for fine-tuning specialized models. As
LLMs see increased adoption in multilingual contexts, there is a need for
accurate faithfulness evaluators that can operate across languages without
extensive labeled data. This paper presents Self-Taught Evaluators for
Multilingual Faithfulness, a framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer
learning. Through experiments comparing language-specific and mixed-language
fine-tuning approaches, we demonstrate a consistent relationship between an
LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines,
including state-of-the-art English evaluators and machine translation-based
approaches.

### 22. When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Hanna Shcharbakova, Tatiana Anikina, Natalia Skachkova, Josef van Genabith
- **URL**: <http://arxiv.org/abs/2507.20700v1>
- **Submitted**: 2025-07-28 10:49:04
- **Comment**: Published at the FEVER Workshop, ACL 2025
- **Topic Keywords**: rag
- **Reason**: The paper evaluates the performance of language models on multilingual claim verification, which is a topic related to information retrieval and natural language processing. However, the focus on language models and fine-grained veracity assessments is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's findings on the effectiveness of smaller specialized models for fact verification are somewhat relevant to the user's interests in search technologies, but the connection is not strong enough to warrant a higher score.

#### Abstract
> The rapid spread of multilingual misinformation requires robust automated
fact verification systems capable of handling fine-grained veracity assessments
across diverse languages. While large language models have shown remarkable
capabilities across many NLP tasks, their effectiveness for multilingual claim
verification with nuanced classification schemes remains understudied. We
conduct a comprehensive evaluation of five state-of-the-art language models on
the X-Fact dataset, which spans 25 languages with seven distinct veracity
categories. Our experiments compare small language models (encoder-based XLM-R
and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)
using both prompting and fine-tuning approaches. Surprisingly, we find that
XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B
parameters), achieving 57.7% macro-F1 compared to the best LLM performance of
16.9%. This represents a 15.8% improvement over the previous state-of-the-art
(41.9%), establishing new performance benchmarks for multilingual fact
verification. Our analysis reveals problematic patterns in LLM behavior,
including systematic difficulties in leveraging evidence and pronounced biases
toward frequent categories in imbalanced data settings. These findings suggest
that for fine-grained multilingual fact verification, smaller specialized
models may be more effective than general-purpose large models, with important
implications for practical deployment of fact-checking systems.

### 23. VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Tan-Minh Nguyen, Hoang-Trung Nguyen, Trong-Khoi Dao, Xuan-Hieu Phan, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong
- **URL**: <http://arxiv.org/abs/2507.19995v1>
- **Submitted**: 2025-07-26 16:26:50
- **Topic Keywords**: information retrieval, rag, retrieval
- **Reason**: The paper focuses on a specific domain (Vietnamese legal text processing) and language, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on legal text processing and question-answering tasks also does not align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging
LLMs for legal tasks is a natural evolution and an increasingly compelling
choice. However, their capabilities are often portrayed as greater than they
truly are. Despite the progress, we are still far from the ultimate goal of
fully automating legal tasks using artificial intelligence (AI) and natural
language processing (NLP). Moreover, legal systems are deeply domain-specific
and exhibit substantial variation across different countries and languages. The
need for building legal text processing applications for different natural
languages is, therefore, large and urgent. However, there is a big challenge
for legal NLP in low-resource languages such as Vietnamese due to the scarcity
of resources and annotated data. The need for labeled legal corpora for
supervised training, validation, and supervised fine-tuning is critical. In
this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a
comprehensive statistical analysis of the dataset and evaluate its
effectiveness through experiments with state-of-the-art models on legal
information retrieval and question-answering tasks.

### 24. CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Ziyu Zhang, Yuanhao Wei, Joshua Engels, Julian Shun
- **URL**: <http://arxiv.org/abs/2507.19802v1>
- **Submitted**: 2025-07-26 05:27:32
- **Topic Keywords**: query, queries, search
- **Reason**: The paper focuses on approximate nearest neighbor search in graph-based indexes, which is not directly related to information retrieval, query understanding, ranking models, or user behavior modeling. Although it touches on query efficiency and quality, the context is different from the user's research interests.

#### Abstract
> Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

### 25. CaliDrop: KV Cache Compression with Calibration

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang
- **URL**: <http://arxiv.org/abs/2507.19906v1>
- **Submitted**: 2025-07-26 10:34:53
- **Topic Keywords**: queries, rag, rank
- **Reason**: The paper focuses on KV cache compression techniques for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions attention patterns, it does not explore ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Large Language Models (LLMs) require substantial computational resources
during generation. While the Key-Value (KV) cache significantly accelerates
this process by storing attention intermediates, its memory footprint grows
linearly with sequence length, batch size, and model size, creating a
bottleneck in long-context scenarios. Various KV cache compression techniques,
including token eviction, quantization, and low-rank projection, have been
proposed to mitigate this bottleneck, often complementing each other. This
paper focuses on enhancing token eviction strategies. Token eviction leverages
the observation that the attention patterns are often sparse, allowing for the
removal of less critical KV entries to save memory. However, this reduction
usually comes at the cost of notable accuracy degradation, particularly under
high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a
novel strategy that enhances token eviction through calibration. Our
preliminary experiments show that queries at nearby positions exhibit high
similarity. Building on this observation, CaliDrop performs speculative
calibration on the discarded tokens to mitigate the accuracy loss caused by
token eviction. Extensive experiments demonstrate that CaliDrop significantly
improves the accuracy of existing token eviction methods.

### 26. A Unified Framework for Interactive Visual Graph Matching via Attribute-Structure Synchronization

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Yuhua Liu, Haoxuan Wang, Jiajia Kou, Ling Sun, Heyu Wang, Yongheng Wang, Yigang Wang, Jinchang Lic, Zhiguang Zhou
- **URL**: <http://arxiv.org/abs/2507.19750v1>
- **Submitted**: 2025-07-26 02:47:09
- **Topic Keywords**: query, retrieval, search
- **Reason**: The paper focuses on graph matching and retrieval, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions attribute-structure synchronization, the approach is not applicable to text-based search and does not involve ranking models or user behavior modeling.

#### Abstract
> In traditional graph retrieval tools, graph matching is commonly used to
retrieve desired graphs from extensive graph datasets according to their
structural similarities. However, in real applications, graph nodes have
numerous attributes which also contain valuable information for evaluating
similarities between graphs. Thus, to achieve superior graph matching results,
it is crucial for graph retrieval tools to make full use of the attribute
information in addition to structural information. We propose a novel framework
for interactive visual graph matching. In the proposed framework, an
attribute-structure synchronization method is developed for representing
structural and attribute features in a unified embedding space based on
Canonical Correlation Analysis (CCA). To support fast and interactive matching,
\revise{our method} provides users with intuitive visual query interfaces for
traversing, filtering and searching for the target graph in the embedding space
conveniently. With the designed interfaces, the users can also specify a new
target graph with desired structural and semantic features. Besides, evaluation
views are designed for easy validation and interpretation of the matching
results. Case studies and quantitative comparisons on real-world datasets have
demonstrated the superiorities of our proposed framework in graph matching and
large graph exploration.

### 27. Memorization in Fine-Tuned Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier Capp√©
- **URL**: <http://arxiv.org/abs/2507.21009v1>
- **Submitted**: 2025-07-28 17:22:10
- **Topic Keywords**: query, rank, search
- **Reason**: This paper focuses on memorization in fine-tuned large language models, which is not directly related to information retrieval, search technologies, or query understanding. The topic is more aligned with natural language processing and data mining, but the specific context and methodology are not relevant to the user's interests.

#### Abstract
> This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

### 28. SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, Qiang Zhang, Huajun Chen
- **URL**: <http://arxiv.org/abs/2507.20280v1>
- **Submitted**: 2025-07-27 13:55:35
- **Comment**: 21 pages, 6 figures
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper focuses on integrating scientific tools using a knowledge graph, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the application is not in the context of search or ranking models, and the paper does not explore user behavior modeling or click models.

#### Abstract
> Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

### 29. Efficient Attention Mechanisms for Large Language Models: A Survey

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yutao Sun, Zhenyu Li, Yike Zhang, Tengyu Pan, Bowen Dong, Yuyi Guo, Jianyong Wang
- **URL**: <http://arxiv.org/abs/2507.19595v1>
- **Submitted**: 2025-07-25 18:08:10
- **Comment**: work in progress
- **Topic Keywords**: rag, search, acl
- **Reason**: The paper focuses on efficient attention mechanisms for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on scalability and computational overhead, the primary concern is not on real-time relevance optimization or deep semantic understanding, making it only loosely relevant to the user's research interests.

#### Abstract
> Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

### 30. Enhancing Project-Specific Code Completion by Inferring Internal API Information

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Le Deng, Xiaoxue Ren, Chao Ni, Ming Liang, David Lo, Zhongxin Liu
- **URL**: <http://arxiv.org/abs/2507.20888v1>
- **Submitted**: 2025-07-28 14:39:46
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on code completion, which is not directly related to information retrieval, search technologies, or natural language processing. Although it uses large language models, the context is specific to code completion and does not involve query understanding, ranking models, or user behavior modeling.

#### Abstract
> Project-specific code completion is a critical task that leverages context
from a project to generate accurate code. State-of-the-art methods use
retrieval-augmented generation (RAG) with large language models (LLMs) and
project information for code completion. However, they often struggle to
incorporate internal API information, which is crucial for accuracy, especially
when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information
without relying on imports. Our method extends the representation of APIs by
constructing usage examples and semantic descriptions, building a knowledge
base for LLMs to generate relevant completions. We also introduce ProjBench, a
benchmark that avoids leaked imports and consists of large-scale real-world
projects.
  Experiments on ProjBench and CrossCodeEval show that our approach
significantly outperforms existing methods, improving code exact match by
22.72% and identifier exact match by 18.31%. Additionally, integrating our
method with existing baselines boosts code match by 47.80% and identifier match
by 35.55%.

### 31. ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Duc-Tai Dinh, Duc Anh Khoa Dinh
- **URL**: <http://arxiv.org/abs/2507.20564v1>
- **Submitted**: 2025-07-28 06:58:35
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on image retrieval and captioning, which is not directly related to the user's interests in information retrieval, search technologies, and query understanding. While it mentions ensembling and prompting, these concepts are not applied to the user's specific areas of interest, such as ranking models and user behavior modeling.

#### Abstract
> We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

### 32. FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li
- **URL**: <http://arxiv.org/abs/2507.20030v1>
- **Submitted**: 2025-07-26 18:20:25
- **Topic Keywords**: ctr, retrieval
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on Key-Value cache compression and its application to Large Language Models, which is outside your areas of interest.

#### Abstract
> The efficacy of Large Language Models (LLMs) in long-context tasks is often
hampered by the substantial memory footprint and computational demands of the
Key-Value (KV) cache. Current compression strategies, including token eviction
and learned projections, frequently lead to biased representations -- either by
overemphasizing recent/high-attention tokens or by repeatedly degrading
information from earlier context -- and may require costly model retraining. We
present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,
training-free KV cache compression framework that ensures unbiased information
retention. FAEDKV operates by transforming the KV cache into the frequency
domain using a proposed Infinite-Window Fourier Transform (IWDFT). This
approach allows for the equalized contribution of all tokens to the compressed
representation, effectively preserving both early and recent contextual
information. A preliminary frequency ablation study identifies critical
spectral components for layer-wise, targeted compression. Experiments on
LongBench benchmark demonstrate FAEDKV's superiority over existing methods by
up to 22\%. In addition, our method shows superior, position-agnostic retrieval
accuracy on the Needle-In-A-Haystack task compared to compression based
approaches.

### 33. FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Roberto Labadie-Tamayo, Adrian Jaques B√∂ck, Djordje Slijepƒçeviƒá, Xihui Chen, Andreas Babic, Matthias Zeppelzauer
- **URL**: <http://arxiv.org/abs/2507.20924v1>
- **Submitted**: 2025-07-28 15:30:17
- **Comment**: 12 pages
- **Topic Keywords**: rag, rank
- **Reason**: The paper focuses on sexism detection in social media, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The techniques and models described in the paper, such as Speech Concept Bottleneck Models, are not relevant to the user's areas of focus, including query understanding, ranking models, and user behavior modeling.

#### Abstract
> Sexism has become widespread on social media and in online conversation. To
help address this issue, the fifth Sexism Identification in Social Networks
(EXIST) challenge is initiated at CLEF 2025. Among this year's international
benchmarks, we concentrate on solving the first task aiming to identify and
classify sexism in social media textual posts. In this paper, we describe our
solutions and report results for three subtasks: Subtask 1.1 - Sexism
Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask
1.3 - Sexism Categorization in Tweets. We implement three models to address
each subtask which constitute three individual runs: Speech Concept Bottleneck
Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a
fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to encode input texts into a human-interpretable representation of
adjectives, then used to train a lightweight classifier for downstream tasks.
SCBMT extends SCBM by fusing adjective-based representation with contextual
embeddings from transformers to balance interpretability and classification
performance. Beyond competitive results, these two models offer fine-grained
explanations at both instance (local) and class (global) levels. We also
investigate how additional metadata, e.g., annotators' demographic profiles,
can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data
augmented with prior datasets, ranks 6th for English and Spanish and 4th for
English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and
Spanish and 6th for Spanish.

### 34. Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ana√Øs Ollagnier
- **URL**: <http://arxiv.org/abs/2507.20614v1>
- **Submitted**: 2025-07-28 08:27:58
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on predicting online antisocial behavior, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions machine learning and language models, the context is different from the user's research interests in IR and NLP.

#### Abstract
> Antisocial behavior (ASB) on social media-including hate speech, harassment,
and trolling-poses growing challenges for platform safety and societal
wellbeing. While prior work has primarily focused on detecting harmful content
after it appears, predictive approaches aim to forecast future harmful
behaviors-such as hate speech propagation, conversation derailment, or user
recidivism-before they fully unfold. Despite increasing interest, the field
remains fragmented, lacking a unified taxonomy or clear synthesis of existing
methods. This paper presents a systematic review of over 49 studies on ASB
prediction, offering a structured taxonomy of five core task types: early harm
detection, harm emergence prediction, harm propagation prediction, behavioral
risk prediction, and proactive moderation support. We analyze how these tasks
differ by temporal framing, prediction granularity, and operational goals. In
addition, we examine trends in modeling techniques-from classical machine
learning to pre-trained language models-and assess the influence of dataset
characteristics on task feasibility and generalization. Our review highlights
methodological challenges, such as dataset scarcity, temporal drift, and
limited benchmarks, while outlining emerging research directions including
multilingual modeling, cross-platform generalization, and human-in-the-loop
systems. By organizing the field around a coherent framework, this survey aims
to guide future work toward more robust and socially responsible ASB
prediction.

### 35. Beyond Interactions: Node-Level Graph Generation for Knowledge-Free Augmentation in Recommender Systems

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhaoyan Wang, Hyunjun Ahn, In-Young Ko
- **URL**: <http://arxiv.org/abs/2507.20578v1>
- **Submitted**: 2025-07-28 07:22:06
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on recommender systems, which is a related topic, but it does not address information retrieval, query understanding, ranking models, or user behavior modeling, which are the core areas of interest. The paper's emphasis on knowledge-free augmentation and node-level graph generation is not directly relevant to the user's research themes.

#### Abstract
> Recent advances in recommender systems rely on external resources such as
knowledge graphs or large language models to enhance recommendations, which
limit applicability in real-world settings due to data dependency and
computational overhead. Although knowledge-free models are able to bolster
recommendations by direct edge operations as well, the absence of augmentation
primitives drives them to fall short in bridging semantic and structural gaps
as high-quality paradigm substitutes. Unlike existing diffusion-based works
that remodel user-item interactions, this work proposes NodeDiffRec, a
pioneering knowledge-free augmentation framework that enables fine-grained
node-level graph generation for recommendations and expands the scope of
restricted augmentation primitives via diffusion. By synthesizing pseudo-items
and corresponding interactions that align with the underlying distribution for
injection, and further refining user preferences through a denoising preference
modeling process, NodeDiffRec dramatically enhances both semantic diversity and
structural connectivity without external knowledge. Extensive experiments
across diverse datasets and recommendation algorithms demonstrate the
superiority of NodeDiffRec, achieving State-of-the-Art (SOTA) performance, with
maximum average performance improvement 98.6% in Recall@5 and 84.0% in NDCG@5
over selected baselines.

### 36. Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Andy Zou, Maxwell Lin, Eliot Jones, Micha Nowak, Mateusz Dziemian, Nick Winter, Alexander Grattan, Valent Nathanael, Ayla Croft, Xander Davies, Jai Patel, Robert Kirk, Nate Burnikell, Yarin Gal, Dan Hendrycks, J. Zico Kolter, Matt Fredrikson
- **URL**: <http://arxiv.org/abs/2507.20526v1>
- **Submitted**: 2025-07-28 05:13:04
- **Topic Keywords**: queries
- **Reason**: This paper focuses on security challenges in AI agent deployment, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, and the topics of AI agent deployment and security are not within the user's primary focus.

#### Abstract
> Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

### 37. EcoTransformer: Attention without Multiplication

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xin Gao, Xingming Xu
- **URL**: <http://arxiv.org/abs/2507.20096v1>
- **Submitted**: 2025-07-27 01:32:54
- **Comment**: 8 pages, 1 figure
- **Topic Keywords**: queries
- **Reason**: The paper proposes a new Transformer architecture, EcoTransformer, which replaces the scaled dot-product attention mechanism with a convolution-based approach. While it shows promising results in various NLP tasks, it does not directly relate to the user's research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling.

#### Abstract
> The Transformer, with its scaled dot-product attention mechanism, has become
a foundational architecture in modern AI. However, this mechanism is
computationally intensive and incurs substantial energy costs. We propose a new
Transformer architecture EcoTransformer, in which the output context vector is
constructed as the convolution of the values using a Laplacian kernel, where
the distances are measured by the L1 metric between the queries and keys.
Compared to dot-product based attention, the new attention score calculation is
free of matrix multiplication. It performs on par with, or even surpasses,
scaled dot-product attention in NLP, bioinformatics, and vision tasks, while
consuming significantly less energy.

### 38. Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Saurav Singla, Aarav Singla, Advik Gupta, Parnika Gupta
- **URL**: <http://arxiv.org/abs/2507.20019v1>
- **Submitted**: 2025-07-26 17:23:03
- **Comment**: 15 pages. PyTorch code for few-shot anomaly detection using
  meta-learning is available upon request or can be shared via GitHub
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on anomaly detection in human language, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves text analysis, the specific problem and approach are not aligned with the user's areas of focus.

#### Abstract
> We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.

### 39. Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sinnyum Choi, Woong Kim
- **URL**: <http://arxiv.org/abs/2507.19990v1>
- **Submitted**: 2025-07-26 15:59:25
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on improving the performance of sequential recommendation systems using large language models, which is not directly related to the user's research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. Although the paper mentions language understanding and context-based reasoning, it does not address the user's specific areas of interest.

#### Abstract
> Recently, competition in the field of artificial intelligence (AI) has
intensified among major technological companies, resulting in the continuous
release of new large-language models (LLMs) that exhibit improved language
understanding and context-based reasoning capabilities. It is expected that
these advances will enable more efficient personalized recommendations in
LLM-based recommendation systems through improved quality of training data and
architectural design. However, many studies have not considered these recent
developments. In this study, it was proposed to improve LLM-based
recommendation systems by replacing Llama2 with Llama3 in the LlamaRec
framework. To ensure a fair comparison, random seed values were set and
identical input data was provided during preprocessing and training. The
experimental results show average performance improvements of 38.65\%, 8.69\%,
and 8.19\% for the ML-100K, Beauty, and Games datasets, respectively, thus
confirming the practicality of this method. Notably, the significant
improvements achieved by model replacement indicate that the recommendation
quality can be improved cost-effectively without the need to make structural
changes to the system. Based on these results, it is our contention that the
proposed approach is a viable solution for improving the performance of current
recommendation systems.

### 40. Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ebrahim Rasromani, Stella K. Kang, Yanqi Xu, Beisong Liu, Garvit Luhadia, Wan Fung Chui, Felicia L. Pasadyn, Yu Chih Hung, Julie Y. An, Edwin Mathieu, Zehui Gu, Carlos Fernandez-Granda, Ammar A. Javed, Greg D. Sacks, Tamas Gonda, Chenchan Huang, Yiqiu Shen
- **URL**: <http://arxiv.org/abs/2507.19973v1>
- **Submitted**: 2025-07-26 15:02:32
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on medical imaging and radiology reports, using large language models for feature extraction and risk categorization, which is outside your area of expertise.

#### Abstract
> Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

### 41. UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, Yang Li
- **URL**: <http://arxiv.org/abs/2507.19766v1>
- **Submitted**: 2025-07-26 03:42:33
- **Comment**: 12 pages
- **Topic Keywords**: ltr
- **Reason**: The paper focuses on large language models and reinforcement learning, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions sequence generation, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

### 42. Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Bermet Burkanova, Payam Jome Yazdian, Chuxuan Zhang, Trinity Evans, Paige Tutt√∂s√≠, Angelica Lim
- **URL**: <http://arxiv.org/abs/2507.19684v1>
- **Submitted**: 2025-07-25 21:33:48
- **Comment**: https://rosielab.github.io/compas3d
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on embodied language, motion capture, and humanoid AI, which is outside your primary areas of interest.

#### Abstract
> Imagine a humanoid that can safely and creatively dance with a human,
adapting to its partner's proficiency, using haptic signaling as a primary form
of communication. While today's AI systems excel at text or voice-based
interaction with large language models, human communication extends far beyond
text-it includes embodied movement, timing, and physical coordination. Modeling
coupled interaction between two agents poses a formidable challenge: it is
continuous, bidirectionally reactive, and shaped by individual variation. We
present CoMPAS3D, the largest and most diverse motion capture dataset of
improvised salsa dancing, designed as a challenging testbed for interactive,
expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa
dances performed by 18 dancers spanning beginner, intermediate, and
professional skill levels. For the first time, we provide fine-grained salsa
expert annotations, covering over 2,800 move segments, including move types,
combinations, execution errors and stylistic elements. We draw analogies
between partner dance communication and natural language, evaluating CoMPAS3D
on two benchmark tasks for synthetic humans that parallel key problems in
spoken language and dialogue processing: leader or follower generation with
proficiency levels (speaker or listener synthesis), and duet (conversation)
generation. Towards a long-term goal of partner dance with humans, we release
the dataset, annotations, and code, along with a multitask SalsaAgent model
capable of performing all benchmark tasks, alongside additional baselines to
encourage research in socially interactive embodied AI and creative, expressive
humanoid motion generation.

### 43. Your AI, Not Your View: The Bias of LLMs in Investment Analysis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hoyoung Lee, Junhyuk Seo, Suhwan Park, Junhyeong Lee, Wonbin Ahn, Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee
- **URL**: <http://arxiv.org/abs/2507.20957v1>
- **Submitted**: 2025-07-28 16:09:38
- **Topic Keywords**: recommend, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Large Language Models (LLMs) in investment analysis, which is outside your primary area of interest. While it touches on topics like model preferences and confirmation bias, it does not align with your core research themes.

#### Abstract
> In finance, Large Language Models (LLMs) face frequent knowledge conflicts
due to discrepancies between pre-trained parametric knowledge and real-time
market data. These conflicts become particularly problematic when LLMs are
deployed in real-world investment services, where misalignment between a
model's embedded preferences and those of the financial institution can lead to
unreliable recommendations. Yet little research has examined what investment
views LLMs actually hold. We propose an experimental framework to investigate
such conflicts, offering the first quantitative analysis of confirmation bias
in LLM-based investment analysis. Using hypothetical scenarios with balanced
and imbalanced arguments, we extract models' latent preferences and measure
their persistence. Focusing on sector, size, and momentum, our analysis reveals
distinct, model-specific tendencies. In particular, we observe a consistent
preference for large-cap stocks and contrarian strategies across most models.
These preferences often harden into confirmation bias, with models clinging to
initial judgments despite counter-evidence.

### 44. FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Likun Tan, Kuan-Wei Huang, Kevin Wu
- **URL**: <http://arxiv.org/abs/2507.20930v1>
- **Submitted**: 2025-07-28 15:41:53
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on detecting and editing factual inaccuracies in language models, primarily in the financial domain. While it touches on the topic of language models, it does not directly relate to information retrieval, search technologies, or query understanding, which are the user's core research interests.

#### Abstract
> Hallucinations in large language models pose a critical challenge for
applications requiring factual reliability, particularly in high-stakes domains
such as finance. This work presents an effective approach for detecting and
editing factually incorrect content in model-generated responses based on the
provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial
question-answering corpora and then fine-tune four language models, Phi-4,
Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual
inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%
improvement in binary F1 score and a 30% gain in overall detection performance
compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having
only 4 billion parameters, maintains competitive performance with just a 2%
drop in binary detection and a 0.1% decline in overall detection compared to
OpenAI-o3. Our work provides a practical solution for detecting and editing
factual inconsistencies in financial text generation while introducing a
generalizable framework that can enhance the trustworthiness and alignment of
large language models across diverse applications beyond finance. Our code and
data are available at https://github.com/pegasi-ai/fine-grained-editting.

### 45. Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Luc Builtjes, Joeran Bosma, Mathias Prokop, Bram van Ginneken, Alessa Hering
- **URL**: <http://arxiv.org/abs/2507.20859v1>
- **Submitted**: 2025-07-28 14:12:37
- **Comment**: 34 pages, 5 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on clinical natural language processing and information extraction using open-source large language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. The paper's emphasis on clinical applications and language processing is also outside the user's e-commerce domain expertise.

#### Abstract
> Medical reports contain rich clinical information but are often unstructured
and written in domain-specific language, posing challenges for information
extraction. While proprietary large language models (LLMs) have shown promise
in clinical natural language processing, their lack of transparency and data
privacy concerns limit their utility in healthcare. This study therefore
evaluates nine open-source generative LLMs on the DRAGON benchmark, which
includes 28 clinical information extraction tasks in Dutch. We developed
\texttt{llm\_extractinator}, a publicly available framework for information
extraction using open-source generative LLMs, and used it to assess model
performance in a zero-shot setting. Several 14 billion parameter models,
Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,
while the bigger Llama-3.3-70B model achieved slightly higher performance at
greater computational cost. Translation to English prior to inference
consistently degraded performance, highlighting the need of native-language
processing. These findings demonstrate that open-source LLMs, when used with
our framework, offer effective, scalable, and privacy-conscious solutions for
clinical information extraction in low-resource settings.

### 46. Watermarking Large Language Model-based Time Series Forecasting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wei Yuan, Chaoqun Yang, Yu Xing, Tong Chen, Nguyen Quoc Viet Hung, Hongzhi Yin
- **URL**: <http://arxiv.org/abs/2507.20762v1>
- **Submitted**: 2025-07-28 12:16:52
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on watermarking large language models for time series forecasting, which is outside your primary area of interest. Although it touches on NLP, it is not directly related to your work in IR and search technologies.

#### Abstract
> Large Language Model-based Time Series Forecasting (LLMTS) has shown
remarkable promise in handling complex and diverse temporal data, representing
a significant step toward foundation models for time series analysis. However,
this emerging paradigm introduces two critical challenges. First, the
substantial commercial potential and resource-intensive development raise
urgent concerns about intellectual property (IP) protection. Second, their
powerful time series forecasting capabilities may be misused to produce
misleading or fabricated deepfake time series data. To address these concerns,
we explore watermarking the outputs of LLMTS models, that is, embedding
imperceptible signals into the generated time series data that remain
detectable by specialized algorithms. We propose a novel post-hoc watermarking
framework, Waltz, which is broadly compatible with existing LLMTS models. Waltz
is inspired by the empirical observation that time series patch embeddings are
rarely aligned with a specific set of LLM tokens, which we term ``cold
tokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the
similarity statistics between patch embeddings and cold token embeddings, and
detects watermarks using similarity z-scores. To minimize potential side
effects, we introduce a similarity-based embedding position identification
strategy and employ projected gradient descent to constrain the watermark noise
within a defined boundary. Extensive experiments using two popular LLMTS models
across seven benchmark datasets demonstrate that Waltz achieves high watermark
detection accuracy with minimal impact on the quality of the generated time
series.

### 47. Geometric-Mean Policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei
- **URL**: <http://arxiv.org/abs/2507.20673v1>
- **Submitted**: 2025-07-28 09:54:05
- **Comment**: Code is available at https://github.com/callsys/GMPO
- **Topic Keywords**: rag
- **Reason**: The paper focuses on optimizing policy rewards for large language models, which is not directly related to Information Retrieval, Search technologies, or query understanding. The geometric-mean policy optimization approach is also not relevant to ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> Recent advancements, such as Group Relative Policy Optimization (GRPO), have
enhanced the reasoning capabilities of large language models by optimizing the
arithmetic mean of token-level rewards. However, GRPO suffers from unstable
policy updates when processing tokens with outlier importance-weighted rewards,
which manifests as extreme importance sampling ratios during training, i.e.,
the ratio between the sampling probabilities assigned to a token by the current
and old policies. In this work, we propose Geometric-Mean Policy Optimization
(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic
mean, GMPO maximizes the geometric mean of token-level rewards, which is
inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical
and experimental analysis to justify the design and stability benefits of GMPO.
Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on
multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,
including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is
available at https://github.com/callsys/GMPO.

### 48. SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, Emad Barsoum
- **URL**: <http://arxiv.org/abs/2507.20527v1>
- **Submitted**: 2025-07-28 05:17:48
- **Topic Keywords**: rag
- **Reason**: The paper focuses on generating mathematical questions and answers using Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions using a dataset to improve model performance, the context is entirely different from the user's research interests.

#### Abstract
> The demand for Large Language Models (LLMs) capable of sophisticated
mathematical reasoning is growing across industries. However, the development
of performant mathematical LLMs is critically bottlenecked by the scarcity of
difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic
Augmented Novel and Difficult Mathematics problems and solutions), a pipeline
that addresses this by first generating high-quality problems from scratch and
then systematically elevating their complexity via a new \textbf{Difficulty
Hiking} step. We demonstrate the effectiveness of our approach through two key
findings. First, augmenting a strong baseline with SAND-Math data significantly
boosts performance, outperforming the next-best synthetic dataset by
\textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a
dedicated ablation study, we show our Difficulty Hiking process is highly
effective: by increasing average problem difficulty from 5.02 to 5.98, this
step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation
pipeline, final dataset, and a fine-tuned model form a practical and scalable
toolkit for building more capable and efficient mathematical reasoning LLMs.
SAND-Math dataset is released here:
\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}

### 49. Customize Multi-modal RAI Guardrails with Precedent-based predictions

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Cheng-Fu Yang, Thanh Tran, Christos Christodoulopoulos, Weitong Ruan, Rahul Gupta, Kai-Wei Chang
- **URL**: <http://arxiv.org/abs/2507.20503v1>
- **Submitted**: 2025-07-28 03:45:34
- **Comment**: Accepted to COLM 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multi-modal guardrails for filtering image content based on user-defined policies, which is not directly related to information retrieval, search technologies, or query understanding. The concepts of precedents and critique-revise mechanisms are not relevant to ranking models or user behavior modeling, and the paper's scope is limited to a specific domain (image content filtering).

#### Abstract
> A multi-modal guardrail must effectively filter image content based on
user-defined policies, identifying material that may be hateful, reinforce
harmful stereotypes, contain explicit material, or spread misinformation.
Deploying such guardrails in real-world applications, however, poses
significant challenges. Users often require varied and highly customizable
policies and typically cannot provide abundant examples for each custom policy.
Consequently, an ideal guardrail should be scalable to the multiple policies
and adaptable to evolving user standards with minimal retraining. Existing
fine-tuning methods typically condition predictions on pre-defined policies,
restricting their generalizability to new policies or necessitating extensive
retraining to adapt. Conversely, training-free methods struggle with limited
context lengths, making it difficult to incorporate all the policies
comprehensively. To overcome these limitations, we propose to condition model's
judgment on "precedents", which are the reasoning processes of prior data
points similar to the given input. By leveraging precedents instead of fixed
policies, our approach greatly enhances the flexibility and adaptability of the
guardrail. In this paper, we introduce a critique-revise mechanism for
collecting high-quality precedents and two strategies that utilize precedents
for robust prediction. Experimental results demonstrate that our approach
outperforms previous methods across both few-shot and full-dataset scenarios
and exhibits superior generalization to novel policies.

### 50. Improving Community Detection in Academic Networks by Handling Publication Bias

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Md Asaduzzaman Noor, John Sheppard, Jason Clark
- **URL**: <http://arxiv.org/abs/2507.20449v1>
- **Submitted**: 2025-07-28 00:48:33
- **Comment**: This paper is an extended version of a work accepted at ASONAM 2025
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on community detection in academic networks, handling publication bias, and recommending researchers based on shared topical interests. While it uses NLP techniques like BERTopic and SciBERT, the primary focus is on network analysis and recommendation systems, which is not directly related to information retrieval, query understanding, or ranking models, making it less relevant to your research interests.

#### Abstract
> Finding potential research collaborators is a challenging task, especially in
today's fast-growing and interdisciplinary research landscape. While
traditional methods often rely on observable relationships such as
co-authorships and citations to construct the research network, in this work,
we focus solely on publication content to build a topic-based research network
using BERTopic with a fine-tuned SciBERT model that connects and recommends
researchers across disciplines based on shared topical interests. A major
challenge we address is publication imbalance, where some researchers publish
much more than others, often across several topics. Without careful handling,
their less frequent interests are hidden under dominant topics, limiting the
network's ability to detect their full research scope. To tackle this, we
introduce a cloning strategy that clusters a researcher's publications and
treats each cluster as a separate node. This allows researchers to be part of
multiple communities, improving the detection of interdisciplinary links.
Evaluation on the proposed method shows that the cloned network structure leads
to more meaningful communities and uncovers a broader set of collaboration
opportunities.

---

