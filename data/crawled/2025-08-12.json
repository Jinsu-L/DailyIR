[
    {
        "title": "Jinx: Unlimited LLMs for Probing Alignment Failures",
        "abstract": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.",
        "url": "http://arxiv.org/abs/2508.08243v2",
        "pdf_url": "http://arxiv.org/pdf/2508.08243v2",
        "arxiv_id": "2508.08243v2",
        "authors": [
            "Jiahao Zhao",
            "Liwei Dong"
        ],
        "submitted": "2025-08-11 17:56:06",
        "source": "arxiv",
        "comment": "https://huggingface.co/Jinx-org"
    },
    {
        "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge",
        "abstract": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.",
        "url": "http://arxiv.org/abs/2508.08236v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08236v1",
        "arxiv_id": "2508.08236v1",
        "authors": [
            "Yunna Cai",
            "Fan Wang",
            "Haowei Wang",
            "Kun Wang",
            "Kailai Yang",
            "Sophia Ananiadou",
            "Moyan Li",
            "Mingming Fan"
        ],
        "submitted": "2025-08-11 17:52:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning",
        "abstract": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.",
        "url": "http://arxiv.org/abs/2508.08224v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08224v1",
        "arxiv_id": "2508.08224v1",
        "authors": [
            "Shansong Wang",
            "Mingzhe Hu",
            "Qiang Li",
            "Mojtaba Safari",
            "Xiaofeng Yang"
        ],
        "submitted": "2025-08-11 17:43:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
        "abstract": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.",
        "url": "http://arxiv.org/abs/2508.08221v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08221v1",
        "arxiv_id": "2508.08221v1",
        "authors": [
            "Zihe Liu",
            "Jiashun Liu",
            "Yancheng He",
            "Weixun Wang",
            "Jiaheng Liu",
            "Ling Pan",
            "Xinyu Hu",
            "Shaopan Xiong",
            "Ju Huang",
            "Jian Hu",
            "Shengyi Huang",
            "Siran Yang",
            "Jiamang Wang",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "submitted": "2025-08-11 17:39:45",
        "source": "arxiv",
        "comment": "26 pages, 21 figures"
    },
    {
        "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling",
        "abstract": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.",
        "url": "http://arxiv.org/abs/2508.08211v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08211v1",
        "arxiv_id": "2508.08211v1",
        "authors": [
            "Zhuohao Yu",
            "Xingru Jiang",
            "Weizheng Gu",
            "Yidong Wang",
            "Shikun Zhang",
            "Wei Ye"
        ],
        "submitted": "2025-08-11 17:33:18",
        "source": "arxiv",
        "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark"
    },
    {
        "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models",
        "abstract": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.",
        "url": "http://arxiv.org/abs/2508.08204v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08204v1",
        "arxiv_id": "2508.08204v1",
        "authors": [
            "Kyle Moore",
            "Jesse Roberts",
            "Daryl Watson"
        ],
        "submitted": "2025-08-11 17:22:45",
        "source": "arxiv",
        "comment": "preprint, under review"
    },
    {
        "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions",
        "abstract": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale.",
        "url": "http://arxiv.org/abs/2508.08192v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08192v1",
        "arxiv_id": "2508.08192v1",
        "authors": [
            "Bangsheng Tang",
            "Carl Chengyan Fu",
            "Fei Kou",
            "Grigory Sizov",
            "Haoci Zhang",
            "Jason Park",
            "Jiawen Liu",
            "Jie You",
            "Qirui Yang",
            "Sachin Mehta",
            "Shengyong Cai",
            "Xiaodong Wang",
            "Xingyu Liu",
            "Yunlu Li",
            "Yanjun Zhou",
            "Wei Wei",
            "Zhiwei Zhao",
            "Zixi Qi",
            "Adolfo Victoria",
            "Aya Ibrahim",
            "Bram Wasti",
            "Changkyu Kim",
            "Daniel Haziza",
            "Fei Sun",
            "Giancarlo Delfin",
            "Emily Guo",
            "Jialin Ouyang",
            "Jaewon Lee",
            "Jianyu Huang",
            "Jeremy Reizenstein",
            "Lu Fang",
            "Quinn Zhu",
            "Ria Verma",
            "Vlad Mihailescu",
            "Xingwen Guo",
            "Yan Cui",
            "Ye Hu",
            "Yejin Lee"
        ],
        "submitted": "2025-08-11 17:11:26",
        "source": "arxiv",
        "comment": "15 pages"
    },
    {
        "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo",
        "abstract": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata.",
        "url": "http://arxiv.org/abs/2508.08163v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08163v1",
        "arxiv_id": "2508.08163v1",
        "authors": [
            "Mandira Sawkar",
            "Samay U. Shetty",
            "Deepak Pandita",
            "Tharindu Cyril Weerasooriya",
            "Christopher M. Homan"
        ],
        "submitted": "2025-08-11 16:39:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation",
        "abstract": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.",
        "url": "http://arxiv.org/abs/2508.08149v2",
        "pdf_url": "http://arxiv.org/pdf/2508.08149v2",
        "arxiv_id": "2508.08149v2",
        "authors": [
            "Wentao Jiang",
            "Xiang Feng",
            "Zengmao Wang",
            "Yong Luo",
            "Pingbo Xu",
            "Zhe Chen",
            "Bo Du",
            "Jing Zhang"
        ],
        "submitted": "2025-08-11 16:25:25",
        "source": "arxiv",
        "comment": "17 pages, 4 figures; updated references"
    },
    {
        "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective",
        "abstract": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.",
        "url": "http://arxiv.org/abs/2508.08140v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08140v1",
        "arxiv_id": "2508.08140v1",
        "authors": [
            "Jun Wang",
            "Zaifu Zhan",
            "Qixin Zhang",
            "Mingquan Lin",
            "Meijia Song",
            "Rui Zhang"
        ],
        "submitted": "2025-08-11 16:13:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
        "abstract": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.",
        "url": "http://arxiv.org/abs/2508.08139v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08139v1",
        "arxiv_id": "2508.08139v1",
        "authors": [
            "Tianyi Zhou",
            "Johanne Medina",
            "Sanjay Chawla"
        ],
        "submitted": "2025-08-11 16:12:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models",
        "abstract": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.",
        "url": "http://arxiv.org/abs/2508.08131v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08131v1",
        "arxiv_id": "2508.08131v1",
        "authors": [
            "Wenze Xu",
            "Chun Wang",
            "Jiazhen Yu",
            "Sheng Chen",
            "Liang Gao",
            "Weihong Deng"
        ],
        "submitted": "2025-08-11 16:06:04",
        "source": "arxiv",
        "comment": "To be presented at ACPR 2025 Conference"
    },
    {
        "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks",
        "abstract": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment\nanalysis (ABSA), which consists of 3.1K manually annotated reviews from the\nrestaurant domain. The dataset is built upon the older Czech dataset, which\ncontained only separate labels for the basic ABSA tasks such as aspect term\nextraction or aspect polarity detection. Unlike its predecessor, our new\ndataset is specifically designed for more complex tasks, e.g.\ntarget-aspect-category detection. These advanced tasks require a unified\nannotation format, seamlessly linking sentiment elements (labels) together. Our\ndataset follows the format of the well-known SemEval-2016 datasets. This design\nchoice allows effortless application and evaluation in cross-lingual scenarios,\nultimately fostering cross-language comparisons with equivalent counterpart\ndatasets in other languages. The annotation process engaged two trained\nannotators, yielding an impressive inter-annotator agreement rate of\napproximately 90%. Additionally, we provide 24M reviews without annotations\nsuitable for unsupervised learning. We present robust monolingual baseline\nresults achieved with various Transformer-based models and insightful error\nanalysis to supplement our contributions. Our code and dataset are freely\navailable for non-commercial research purposes.",
        "url": "http://arxiv.org/abs/2508.08125v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08125v1",
        "arxiv_id": "2508.08125v1",
        "authors": [
            "Jakub Šmíd",
            "Pavel Přibáň",
            "Ondřej Pražák",
            "Pavel Král"
        ],
        "submitted": "2025-08-11 16:03:28",
        "source": "arxiv",
        "comment": "Published In Proceedings of the 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024). Official version: https://aclanthology.org/2024.lrec-main.374/"
    },
    {
        "title": "Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0",
        "abstract": "Self-supervised models for speech representation learning now see widespread\nuse for their versatility and performance on downstream tasks, but the effect\nof model architecture on the linguistic information learned in their\nrepresentations remains under-studied. This study investigates two such models,\nHuBERT and wav2vec 2.0, and minimally compares two of their architectural\ndifferences: training objective and iterative pseudo-label refinement through\nmultiple training iterations. We find that differences in canonical correlation\nof hidden representations to word identity, phoneme identity, and speaker\nidentity are explained by training iteration, not training objective. We\nsuggest that future work investigate the reason for the effectiveness of\niterative refinement in encoding linguistic information in self-supervised\nspeech representations.",
        "url": "http://arxiv.org/abs/2508.08110v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08110v1",
        "arxiv_id": "2508.08110v1",
        "authors": [
            "Robin Huo",
            "Ewan Dunbar"
        ],
        "submitted": "2025-08-11 15:48:56",
        "source": "arxiv",
        "comment": "Proceedings of Interspeech 2025"
    },
    {
        "title": "Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?",
        "abstract": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.",
        "url": "http://arxiv.org/abs/2508.08096v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08096v1",
        "arxiv_id": "2508.08096v1",
        "authors": [
            "Lukas Gehring",
            "Benjamin Paaßen"
        ],
        "submitted": "2025-08-11 15:34:49",
        "source": "arxiv",
        "comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)"
    },
    {
        "title": "Dual Information Speech Language Models for Emotional Conversations",
        "abstract": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.",
        "url": "http://arxiv.org/abs/2508.08095v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08095v1",
        "arxiv_id": "2508.08095v1",
        "authors": [
            "Chun Wang",
            "Chenyang Liu",
            "Wenze Xu",
            "Weihong Deng"
        ],
        "submitted": "2025-08-11 15:33:44",
        "source": "arxiv",
        "comment": "Presented at IEEE ICME 2025"
    },
    {
        "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches",
        "abstract": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
        "url": "http://arxiv.org/abs/2508.08088v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08088v1",
        "arxiv_id": "2508.08088v1",
        "authors": [
            "Jiejun Tan",
            "Zhicheng Dou",
            "Yan Yu",
            "Jiehan Cheng",
            "Qiang Ju",
            "Jian Xie",
            "Ji-Rong Wen"
        ],
        "submitted": "2025-08-11 15:31:47",
        "source": "arxiv",
        "comment": "Code and datasets are available at\n  https://github.com/plageon/HierSearch"
    },
    {
        "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model",
        "abstract": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.",
        "url": "http://arxiv.org/abs/2508.08066v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08066v1",
        "arxiv_id": "2508.08066v1",
        "authors": [
            "Weitai Kang",
            "Weiming Zhuang",
            "Zhizhong Li",
            "Yan Yan",
            "Lingjuan Lyu"
        ],
        "submitted": "2025-08-11 15:10:52",
        "source": "arxiv",
        "comment": "8 pages for the main paper"
    },
    {
        "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations",
        "abstract": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries.",
        "url": "http://arxiv.org/abs/2508.08061v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08061v1",
        "arxiv_id": "2508.08061v1",
        "authors": [
            "Sven Weinzierl",
            "Sandra Zilker",
            "Annina Liessmann",
            "Martin Käppel",
            "Weixin Wang",
            "Martin Matzner"
        ],
        "submitted": "2025-08-11 15:03:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)",
        "abstract": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing.",
        "url": "http://arxiv.org/abs/2508.08050v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08050v1",
        "arxiv_id": "2508.08050v1",
        "authors": [
            "Fabrizio Nunnari",
            "Cristina Luna Jiménez",
            "Rosalee Wolfe",
            "John C. McDonald",
            "Michael Filhol",
            "Eleni Efthimiou",
            "Evita Fotinea",
            "Thomas Hanke"
        ],
        "submitted": "2025-08-11 14:50:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation",
        "abstract": "Recommendation systems have faced significant challenges in cold-start\nscenarios, where new items with a limited history of interaction need to be\neffectively recommended to users. Though multimodal data (e.g., images, text,\naudio, etc.) offer rich information to address this issue, existing approaches\noften employ simplistic integration methods such as concatenation, average\npooling, or fixed weighting schemes, which fail to capture the complex\nrelationships between modalities. Our study proposes a novel Mixture of Experts\n(MoE) framework for multimodal cold-start recommendation, named MAMEX, which\ndynamically leverages latent representation from different modalities. MAMEX\nutilizes modality-specific expert networks and introduces a learnable gating\nmechanism that adaptively weights the contribution of each modality based on\nits content characteristics. This approach enables MAMEX to emphasize the most\ninformative modalities for each item while maintaining robustness when certain\nmodalities are less relevant or missing. Extensive experiments on benchmark\ndatasets show that MAMEX outperforms state-of-the-art methods in cold-start\nscenarios, with superior accuracy and adaptability. For reproducibility, the\ncode has been made available on Github https://github.com/L2R-UET/MAMEX.",
        "url": "http://arxiv.org/abs/2508.08042v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08042v1",
        "arxiv_id": "2508.08042v1",
        "authors": [
            "Van-Khang Nguyen",
            "Duc-Hoang Pham",
            "Huy-Son Nguyen",
            "Cam-Van Thi Nguyen",
            "Hoang-Quynh Le",
            "Duc-Trong Le"
        ],
        "submitted": "2025-08-11 14:47:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning",
        "abstract": "Recent advancements in large language models, multimodal large language\nmodels, and large audio language models (LALMs) have significantly improved\ntheir reasoning capabilities through reinforcement learning with rule-based\nrewards. However, the explicit reasoning process has yet to show significant\nbenefits for audio question answering, and effectively leveraging deep\nreasoning remains an open challenge, with LALMs still falling short of\nhuman-level auditory-language reasoning. To address these limitations, we\npropose Audio-Thinker, a reinforcement learning framework designed to enhance\nthe reasoning capabilities of LALMs, with a focus on improving adaptability,\nconsistency, and effectiveness. Our approach introduces an adaptive think\naccuracy reward, enabling the model to adjust its reasoning strategies based on\ntask complexity dynamically. Furthermore, we incorporate an external reward\nmodel to evaluate the overall consistency and quality of the reasoning process,\ncomplemented by think-based rewards that help the model distinguish between\nvalid and flawed reasoning paths during training. Experimental results\ndemonstrate that our Audio-Thinker model outperforms existing\nreasoning-oriented LALMs across various benchmark tasks, exhibiting superior\nreasoning and generalization capabilities.",
        "url": "http://arxiv.org/abs/2508.08039v2",
        "pdf_url": "http://arxiv.org/pdf/2508.08039v2",
        "arxiv_id": "2508.08039v2",
        "authors": [
            "Shu Wu",
            "Chenxing Li",
            "Wenfu Wang",
            "Hao Zhang",
            "Hualei Wang",
            "Meng Yu",
            "Dong Yu"
        ],
        "submitted": "2025-08-11 14:41:10",
        "source": "arxiv",
        "comment": "preprint"
    },
    {
        "title": "Progressive Depth Up-scaling via Optimal Transport",
        "abstract": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.",
        "url": "http://arxiv.org/abs/2508.08011v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08011v1",
        "arxiv_id": "2508.08011v1",
        "authors": [
            "Mingzi Cao",
            "Xi Wang",
            "Nikolaos Aletras"
        ],
        "submitted": "2025-08-11 14:15:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
        "abstract": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
        "url": "http://arxiv.org/abs/2508.07999v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07999v1",
        "arxiv_id": "2508.07999v1",
        "authors": [
            "Ryan Wong",
            "Jiawei Wang",
            "Junjie Zhao",
            "Li Chen",
            "Yan Gao",
            "Long Zhang",
            "Xuan Zhou",
            "Zuo Wang",
            "Kai Xiang",
            "Ge Zhang",
            "Wenhao Huang",
            "Yang Wang",
            "Ke Wang"
        ],
        "submitted": "2025-08-11 14:03:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval",
        "abstract": "Retrieval-augmented generation has achieved strong performance on\nknowledge-intensive tasks where query-document relevance can be identified\nthrough direct lexical or semantic matches. However, many real-world queries\ninvolve abstract reasoning, analogical thinking, or multi-step inference, which\nexisting retrievers often struggle to capture. To address this challenge, we\npresent \\textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive\ninformation retrieval. DIVER consists of four components: document processing\nto improve input quality, LLM-driven query expansion via iterative document\ninteraction, a reasoning-enhanced retriever fine-tuned on synthetic\nmulti-domain data with hard negatives, and a pointwise reranker that combines\nLLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,\nDIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original\nqueries, consistently outperforming competitive reasoning-aware models. These\nresults demonstrate the effectiveness of reasoning-aware retrieval strategies\nin complex real-world tasks. Our code and retrieval model will be released\nsoon.",
        "url": "http://arxiv.org/abs/2508.07995v2",
        "pdf_url": "http://arxiv.org/pdf/2508.07995v2",
        "arxiv_id": "2508.07995v2",
        "authors": [
            "Meixiu Long",
            "Duolin Sun",
            "Dan Yang",
            "Junjie Wang",
            "Yue Shen",
            "Jian Wang",
            "Peng Wei",
            "Jinjie Gu",
            "Jiahai Wang"
        ],
        "submitted": "2025-08-11 13:57:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Medical Metaphors Corpus (MCC)",
        "abstract": "Metaphor is a fundamental cognitive mechanism that shapes scientific\nunderstanding, enabling the communication of complex concepts while potentially\nconstraining paradigmatic thinking. Despite the prevalence of figurative\nlanguage in scientific discourse, existing metaphor detection resources\nprimarily focus on general-domain text, leaving a critical gap for\ndomain-specific applications. In this paper, we present the Medical Metaphors\nCorpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual\nmetaphors spanning medical and biological domains. MCC aggregates metaphorical\nexpressions from diverse sources including peer-reviewed literature, news\nmedia, social media discourse, and crowdsourced contributions, providing both\nbinary and graded metaphoricity judgments validated through human annotation.\nEach instance includes source-target conceptual mappings and perceived\nmetaphoricity scores on a 0-7 scale, establishing the first annotated resource\nfor computational scientific metaphor research. Our evaluation demonstrates\nthat state-of-the-art language models achieve modest performance on scientific\nmetaphor detection, revealing substantial room for improvement in\ndomain-specific figurative language understanding. MCC enables multiple\nresearch applications including metaphor detection benchmarking, quality-aware\ngeneration systems, and patient-centered communication tools.",
        "url": "http://arxiv.org/abs/2508.07993v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07993v1",
        "arxiv_id": "2508.07993v1",
        "authors": [
            "Anna Sofia Lippolis",
            "Andrea Giovanni Nuzzolese",
            "Aldo Gangemi"
        ],
        "submitted": "2025-08-11 13:55:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription",
        "abstract": "Automatic transcription of acoustic guitar fingerpicking performances remains\na challenging task due to the scarcity of labeled training data and legal\nconstraints connected with musical recordings. This work investigates a\nprocedural data generation pipeline as an alternative to real audio recordings\nfor training transcription models. Our approach synthesizes training data\nthrough four stages: knowledge-based fingerpicking tablature composition, MIDI\nperformance rendering, physical modeling using an extended Karplus-Strong\nalgorithm, and audio augmentation including reverb and distortion. We train and\nevaluate a CRNN-based note-tracking model on both real and synthetic datasets,\ndemonstrating that procedural data can be used to achieve reasonable\nnote-tracking results. Finetuning with a small amount of real data further\nenhances transcription accuracy, improving over models trained exclusively on\nreal recordings. These results highlight the potential of procedurally\ngenerated audio for data-scarce music information retrieval tasks.",
        "url": "http://arxiv.org/abs/2508.07987v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07987v1",
        "arxiv_id": "2508.07987v1",
        "authors": [
            "Sebastian Murgul",
            "Michael Heizmann"
        ],
        "submitted": "2025-08-11 13:52:17",
        "source": "arxiv",
        "comment": "Accepted to the 6th Conference on AI Music Creativity (AIMC), 2025"
    },
    {
        "title": "Early Explorations of Recommender Systems for Physical Activity and Well-being",
        "abstract": "As recommender systems increasingly guide physical actions, often through\nwearables and coaching tools, new challenges arise around how users interpret,\ntrust, and respond to this advice. This paper introduces a conceptual framework\nfor tangible recommendations that influence users' bodies, routines, and\nwell-being. We describe three design dimensions: trust and interpretation,\nintent alignment, and consequence awareness. These highlight key limitations in\napplying conventional recommender logic to embodied settings. Through examples\nand design reflections, we outline how future systems can support long-term\nwell-being, behavioral alignment, and socially responsible personalization.",
        "url": "http://arxiv.org/abs/2508.07980v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07980v1",
        "arxiv_id": "2508.07980v1",
        "authors": [
            "Alan Said"
        ],
        "submitted": "2025-08-11 13:38:58",
        "source": "arxiv",
        "comment": "Second International Workshop on Recommender Systems for\n  Sustainability and Social Good (RecSoGood) in conjunction with ACM RecSys\n  2025"
    },
    {
        "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL",
        "abstract": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
        "url": "http://arxiv.org/abs/2508.07976v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07976v1",
        "arxiv_id": "2508.07976v1",
        "authors": [
            "Jiaxuan Gao",
            "Wei Fu",
            "Minyang Xie",
            "Shusheng Xu",
            "Chuyi He",
            "Zhiyu Mei",
            "Banghua Zhu",
            "Yi Wu"
        ],
        "submitted": "2025-08-11 13:36:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Document Retrieval Coherence for Semantically Equivalent Queries",
        "abstract": "Dense Retrieval (DR) models have proven to be effective for Document\nRetrieval and Information Grounding tasks. Usually, these models are trained\nand optimized for improving the relevance of top-ranked documents for a given\nquery. Previous work has shown that popular DR models are sensitive to the\nquery and document lexicon: small variations of it may lead to a significant\ndifference in the set of retrieved documents. In this paper, we propose a\nvariation of the Multi-Negative Ranking loss for training DR that improves the\ncoherence of models in retrieving the same documents with respect to\nsemantically similar queries. The loss penalizes discrepancies between the\ntop-k ranked documents retrieved for diverse but semantic equivalent queries.\nWe conducted extensive experiments on various datasets, MS-MARCO, Natural\nQuestions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes\nby our loss are subject to lower sensitivity, and, (ii) interestingly, higher\naccuracy.",
        "url": "http://arxiv.org/abs/2508.07975v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07975v1",
        "arxiv_id": "2508.07975v1",
        "authors": [
            "Stefano Campese",
            "Alessandro Moschitti",
            "Ivano Lauriola"
        ],
        "submitted": "2025-08-11 13:34:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Joint Transcription of Acoustic Guitar Strumming Directions and Chords",
        "abstract": "Automatic transcription of guitar strumming is an underrepresented and\nchallenging task in Music Information Retrieval (MIR), particularly for\nextracting both strumming directions and chord progressions from audio signals.\nWhile existing methods show promise, their effectiveness is often hindered by\nlimited datasets. In this work, we extend a multimodal approach to guitar\nstrumming transcription by introducing a novel dataset and a deep\nlearning-based transcription model. We collect 90 min of real-world guitar\nrecordings using an ESP32 smartwatch motion sensor and a structured recording\nprotocol, complemented by a synthetic dataset of 4h of labeled strumming audio.\nA Convolutional Recurrent Neural Network (CRNN) model is trained to detect\nstrumming events, classify their direction, and identify the corresponding\nchords using only microphone audio. Our evaluation demonstrates significant\nimprovements over baseline onset detection algorithms, with a hybrid method\ncombining synthetic and real-world data achieving the highest accuracy for both\nstrumming action detection and chord classification. These results highlight\nthe potential of deep learning for robust guitar strumming transcription and\nopen new avenues for automatic rhythm guitar analysis.",
        "url": "http://arxiv.org/abs/2508.07973v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07973v1",
        "arxiv_id": "2508.07973v1",
        "authors": [
            "Sebastian Murgul",
            "Johannes Schimper",
            "Michael Heizmann"
        ],
        "submitted": "2025-08-11 13:34:49",
        "source": "arxiv",
        "comment": "Accepted to the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025"
    },
    {
        "title": "Understanding Syntactic Generalization in Structure-inducing Language Models",
        "abstract": "Structure-inducing Language Models (SiLM) are trained on a self-supervised\nlanguage modeling task, and induce a hierarchical sentence representation as a\nbyproduct when processing an input. A wide variety of SiLMs have been proposed.\nHowever, these have typically been evaluated on a relatively small scale, and\nevaluation of these models has systematic gaps and lacks comparability. In this\nwork, we study three different SiLM architectures using both natural language\n(English) corpora and synthetic bracketing expressions: Structformer (Shen et\nal., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare\nthem with respect to (i) properties of the induced syntactic representations\n(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.\nWe find that none of the three architectures dominates across all evaluation\nmetrics. However, there are significant differences, in particular with respect\nto the induced syntactic representations. The Generative Pretrained Structured\nTransformer (GPST; Hu et al. 2024) performs most consistently across evaluation\nsettings, and outperforms the other models on long-distance dependencies in\nbracketing expressions. Furthermore, our study shows that small models trained\non large amounts of synthetic data provide a useful testbed for evaluating\nbasic model properties.",
        "url": "http://arxiv.org/abs/2508.07969v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07969v1",
        "arxiv_id": "2508.07969v1",
        "authors": [
            "David Arps",
            "Hassan Sajjad",
            "Laura Kallmeyer"
        ],
        "submitted": "2025-08-11 13:29:41",
        "source": "arxiv",
        "comment": "Code available at https://github.com/davidarps/silm"
    },
    {
        "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies",
        "abstract": "Current speech translation systems, while having achieved impressive\naccuracies, are rather static in their behavior and do not adapt to real-world\nsituations in ways human interpreters do. In order to improve their practical\nusefulness and enable interpreting-like experiences, a precise understanding of\nthe nature of human interpreting is crucial. To this end, we discuss human\ninterpreting literature from the perspective of the machine translation field,\nwhile considering both operational and qualitative aspects. We identify\nimplications for the development of speech translation systems and argue that\nthere is great potential to adopt many human interpreting principles using\nrecent modeling techniques. We hope that our findings provide inspiration for\nclosing the perceived usability gap, and can motivate progress toward true\nmachine interpreting.",
        "url": "http://arxiv.org/abs/2508.07964v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07964v1",
        "arxiv_id": "2508.07964v1",
        "authors": [
            "Matthias Sperber",
            "Maureen de Seyssel",
            "Jiajun Bao",
            "Matthias Paulik"
        ],
        "submitted": "2025-08-11 13:20:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Models for Subjective Language Understanding: A Survey",
        "abstract": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.",
        "url": "http://arxiv.org/abs/2508.07959v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07959v1",
        "arxiv_id": "2508.07959v1",
        "authors": [
            "Changhao Song",
            "Yazhou Zhang",
            "Hui Gao",
            "Ben Yao",
            "Peng Zhang"
        ],
        "submitted": "2025-08-11 13:10:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating up-to-date external knowledge, yet real-world web environments\npresent unique challenges. These limitations manifest as two key challenges:\npervasive misinformation in the web environment, which introduces unreliable or\nmisleading content that can degrade retrieval accuracy, and the\nunderutilization of web tools, which, if effectively employed, could enhance\nquery precision and help mitigate this noise, ultimately improving the\nretrieval results in RAG systems. To address these issues, we propose\nWebFilter, a novel RAG framework that generates source-restricted queries and\nfilters out unreliable content. This approach combines a retrieval filtering\nmechanism with a behavior- and outcome-driven reward strategy, optimizing both\nquery formulation and retrieval outcomes. Extensive experiments demonstrate\nthat WebFilter improves answer quality and retrieval precision, outperforming\nexisting RAG methods on both in-domain and out-of-domain benchmarks.",
        "url": "http://arxiv.org/abs/2508.07956v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07956v1",
        "arxiv_id": "2508.07956v1",
        "authors": [
            "Yuqin Dai",
            "Shuo Yang",
            "Guoqing Wang",
            "Yong Deng",
            "Zhanwei Zhang",
            "Jun Yin",
            "Pengyu Zeng",
            "Zhenzhe Ying",
            "Changhua Meng",
            "Can Yi",
            "Yuchen Zhou",
            "Weiqiang Wang",
            "Shuai Lu"
        ],
        "submitted": "2025-08-11 13:08:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Expert Preference-based Evaluation of Automated Related Work Generation",
        "abstract": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.",
        "url": "http://arxiv.org/abs/2508.07955v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07955v1",
        "arxiv_id": "2508.07955v1",
        "authors": [
            "Furkan Şahinuç",
            "Subhabrata Dutta",
            "Iryna Gurevych"
        ],
        "submitted": "2025-08-11 13:08:07",
        "source": "arxiv",
        "comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/"
    },
    {
        "title": "Challenges and opportunities in portraying emotion in generated sign language",
        "abstract": "Non-manual signals in sign languages continue to be a challenge for signing\navatars. More specifically, emotional content has been difficult to incorporate\nbecause of a lack of a standard method of specifying the avatar's emotional\nstate. This paper explores the application of an intuitive two-parameter\nrepresentation for emotive non-manual signals to the Paula signing avatar that\nshows promise for facilitating the linguistic specification of emotional facial\nexpressions in a more coherent manner than previous methods. Users can apply\nthese parameters to control Paula's emotional expressions through a textual\nrepresentation called the EASIER notation. The representation can allow avatars\nto express more nuanced emotional states using two numerical parameters. It\nalso has the potential to enable more consistent specification of emotional\nnon-manual signals in linguistic annotations which drive signing avatars.",
        "url": "http://arxiv.org/abs/2508.07937v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07937v1",
        "arxiv_id": "2508.07937v1",
        "authors": [
            "John C. McDonald",
            "Rosalee Wolfe",
            "Fabrizio Nunnari"
        ],
        "submitted": "2025-08-11 12:52:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Meta Off-Policy Estimation",
        "abstract": "Off-policy estimation (OPE) methods enable unbiased offline evaluation of\nrecommender systems, directly estimating the online reward some target policy\nwould have obtained, from offline data and with statistical guarantees. The\ntheoretical elegance of the framework combined with practical successes have\nled to a surge of interest, with many competing estimators now available to\npractitioners and researchers. Among these, Doubly Robust methods provide a\nprominent strategy to combine value- and policy-based estimators.\n  In this work, we take an alternative perspective to combine a set of OPE\nestimators and their associated confidence intervals into a single, more\naccurate estimate. Our approach leverages a correlated fixed-effects\nmeta-analysis framework, explicitly accounting for dependencies among\nestimators that arise due to shared data. This yields a best linear unbiased\nestimate (BLUE) of the target policy's value, along with an appropriately\nconservative confidence interval that reflects inter-estimator correlation. We\nvalidate our method on both simulated and real-world data, demonstrating\nimproved statistical efficiency over existing individual estimators.",
        "url": "http://arxiv.org/abs/2508.07914v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07914v1",
        "arxiv_id": "2508.07914v1",
        "authors": [
            "Olivier Jeunen"
        ],
        "submitted": "2025-08-11 12:31:13",
        "source": "arxiv",
        "comment": "To appear in the Nineteenth ACM Conference on Recommender Systems\n  (RecSys '25)"
    },
    {
        "title": "Exploring the Technical Knowledge Interaction of Global Digital Humanities: Three-decade Evidence from Bibliometric-based perspectives",
        "abstract": "Digital Humanities (DH) is an interdisciplinary field that integrates\ncomputational methods with humanities scholarship to investigate innovative\ntopics. Each academic discipline follows a unique developmental path shaped by\nthe topics researchers investigate and the methods they employ. With the help\nof bibliometric analysis, most of previous studies have examined DH across\nmultiple dimensions such as research hotspots, co-author networks, and\ninstitutional rankings. However, these studies have often been limited in their\nability to provide deep insights into the current state of technological\nadvancements and topic development in DH. As a result, their conclusions tend\nto remain superficial or lack interpretability in understanding how methods and\ntopics interrelate in the field. To address this gap, this study introduced a\nnew concept of Topic-Method Composition (TMC), which refers to a hybrid\nknowledge structure generated by the co-occurrence of specific research topics\nand the corresponding method. Especially by analyzing the interaction between\nTMCs, we can see more clearly the intersection and integration of digital\ntechnology and humanistic subjects in DH. Moreover, this study developed a\nTMC-based workflow combining bibliometric analysis, topic modeling, and network\nanalysis to analyze the development characteristics and patterns of research\ndisciplines. By applying this workflow to large-scale bibliometric data, it\nenables a detailed view of the knowledge structures, providing a tool adaptable\nto other fields.",
        "url": "http://arxiv.org/abs/2508.08347v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08347v1",
        "arxiv_id": "2508.08347v1",
        "authors": [
            "Jiayi Li",
            "Chengxi Yan",
            "Yurong Zeng",
            "Zhichao Fang",
            "Huiru Wang"
        ],
        "submitted": "2025-08-11 12:27:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity",
        "abstract": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.",
        "url": "http://arxiv.org/abs/2508.07902v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07902v1",
        "arxiv_id": "2508.07902v1",
        "authors": [
            "Chen Cecilia Liu",
            "Hiba Arnaout",
            "Nils Kovačić",
            "Dana Atzil-Slonim",
            "Iryna Gurevych"
        ],
        "submitted": "2025-08-11 12:17:58",
        "source": "arxiv",
        "comment": "Under review; joint first authors"
    },
    {
        "title": "Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models",
        "abstract": "Aspect-based sentiment analysis (ABSA) has received substantial attention in\nEnglish, yet challenges remain for low-resource languages due to the scarcity\nof labelled data. Current cross-lingual ABSA approaches often rely on external\ntranslation tools and overlook the potential benefits of incorporating a small\nnumber of target language examples into training. In this paper, we evaluate\nthe effect of adding few-shot target language examples to the training set\nacross four ABSA tasks, six target languages, and two sequence-to-sequence\nmodels. We show that adding as few as ten target language examples\nsignificantly improves performance over zero-shot settings and achieves a\nsimilar effect to constrained decoding in reducing prediction errors.\nFurthermore, we demonstrate that combining 1,000 target language examples with\nEnglish data can even surpass monolingual baselines. These findings offer\npractical insights for improving cross-lingual ABSA in low-resource and\ndomain-specific settings, as obtaining ten high-quality annotated examples is\nboth feasible and highly effective.",
        "url": "http://arxiv.org/abs/2508.07866v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07866v1",
        "arxiv_id": "2508.07866v1",
        "authors": [
            "Jakub Šmíd",
            "Pavel Přibáň",
            "Pavel Král"
        ],
        "submitted": "2025-08-11 11:31:37",
        "source": "arxiv",
        "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)"
    },
    {
        "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis",
        "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.",
        "url": "http://arxiv.org/abs/2508.07860v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07860v1",
        "arxiv_id": "2508.07860v1",
        "authors": [
            "Jakub Šmíd",
            "Pavel Přibáň",
            "Pavel Král"
        ],
        "submitted": "2025-08-11 11:24:57",
        "source": "arxiv",
        "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)"
    },
    {
        "title": "Recommendation Is a Dish Better Served Warm",
        "abstract": "In modern recommender systems, experimental settings typically include\nfiltering out cold users and items based on a minimum interaction threshold.\nHowever, these thresholds are often chosen arbitrarily and vary widely across\nstudies, leading to inconsistencies that can significantly affect the\ncomparability and reliability of evaluation results. In this paper, we\nsystematically explore the cold-start boundary by examining the criteria used\nto determine whether a user or an item should be considered cold. Our\nexperiments incrementally vary the number of interactions for different items\nduring training, and gradually update the length of user interaction histories\nduring inference. We investigate the thresholds across several widely used\ndatasets, commonly represented in recent papers from top-tier conferences, and\non multiple established recommender baselines. Our findings show that\ninconsistent selection of cold-start thresholds can either result in the\nunnecessary removal of valuable data or lead to the misclassification of cold\ninstances as warm, introducing more noise into the system.",
        "url": "http://arxiv.org/abs/2508.07856v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07856v1",
        "arxiv_id": "2508.07856v1",
        "authors": [
            "Danil Gusak",
            "Nikita Sukhorukov",
            "Evgeny Frolov"
        ],
        "submitted": "2025-08-11 11:14:49",
        "source": "arxiv",
        "comment": "Accepted for ACM RecSys 2025. Author's version. The final published\n  version will be available at the ACM Digital Library"
    },
    {
        "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding",
        "abstract": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.",
        "url": "http://arxiv.org/abs/2508.07849v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07849v1",
        "arxiv_id": "2508.07849v1",
        "authors": [
            "Amrita Singh",
            "H. Suhan Karaca",
            "Aditya Joshi",
            "Hye-young Paik",
            "Jiaojiao Jiang"
        ],
        "submitted": "2025-08-11 11:08:32",
        "source": "arxiv",
        "comment": "Under review. 4 pages + references"
    },
    {
        "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving",
        "abstract": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
        "url": "http://arxiv.org/abs/2508.08343v1",
        "pdf_url": "http://arxiv.org/pdf/2508.08343v1",
        "arxiv_id": "2508.08343v1",
        "authors": [
            "Ferran Agullo",
            "Joan Oliveras",
            "Chen Wang",
            "Alberto Gutierrez-Torre",
            "Olivier Tardieu",
            "Alaa Youssef",
            "Jordi Torres",
            "Josep Ll. Berral"
        ],
        "submitted": "2025-08-11 10:47:35",
        "source": "arxiv",
        "comment": "Under review for a computer science conference"
    },
    {
        "title": "Evaluating Large Language Models as Expert Annotators",
        "abstract": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.",
        "url": "http://arxiv.org/abs/2508.07827v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07827v1",
        "arxiv_id": "2508.07827v1",
        "authors": [
            "Yu-Min Tseng",
            "Wei-Lin Chen",
            "Chung-Chi Chen",
            "Hsin-Hsi Chen"
        ],
        "submitted": "2025-08-11 10:19:10",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025"
    },
    {
        "title": "Evaluating Compositional Approaches for Focus and Sentiment Analysis",
        "abstract": "This paper summarizes the results of evaluating a compositional approach for\nFocus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural\nLanguage Processing (NLP). While quantitative evaluations of compositional and\nnon-compositional approaches in SA exist in NLP, similar quantitative\nevaluations are very rare in FA in Linguistics that deal with linguistic\nexpressions representing focus or emphasis such as \"it was John who left\". We\nfill this gap in research by arguing that compositional rules in SA also apply\nto FA because FA and SA are closely related meaning that SA is part of FA. Our\ncompositional approach in SA exploits basic syntactic rules such as rules of\nmodification, coordination, and negation represented in the formalism of\nUniversal Dependencies (UDs) in English and applied to words representing\nsentiments from sentiment dictionaries. Some of the advantages of our\ncompositional analysis method for SA in contrast to non-compositional analysis\nmethods are interpretability and explainability. We test the accuracy of our\ncompositional approach and compare it with a non-compositional approach VADER\nthat uses simple heuristic rules to deal with negation, coordination and\nmodification. In contrast to previous related work that evaluates\ncompositionality in SA on long reviews, this study uses more appropriate\ndatasets to evaluate compositionality. In addition, we generalize the results\nof compositional approaches in SA to compositional approaches in FA.",
        "url": "http://arxiv.org/abs/2508.07810v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07810v1",
        "arxiv_id": "2508.07810v1",
        "authors": [
            "Olga Kellert",
            "Muhammad Imran",
            "Nicholas Hill Matlis",
            "Mahmud Uz Zaman",
            "Carlos Gómez-Rodríguez"
        ],
        "submitted": "2025-08-11 09:52:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges",
        "abstract": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks.",
        "url": "http://arxiv.org/abs/2508.07805v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07805v1",
        "arxiv_id": "2508.07805v1",
        "authors": [
            "Yerin Hwang",
            "Dongryeol Lee",
            "Taegwan Kang",
            "Yongil Kim",
            "Kyomin Jung"
        ],
        "submitted": "2025-08-11 09:45:02",
        "source": "arxiv",
        "comment": "19 pages, 8 figures"
    },
    {
        "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts",
        "abstract": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.",
        "url": "http://arxiv.org/abs/2508.07785v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07785v1",
        "arxiv_id": "2508.07785v1",
        "authors": [
            "Haoyuan Wu",
            "Haoxing Chen",
            "Xiaodong Chen",
            "Zhanchao Zhou",
            "Tieyuan Chen",
            "Yihong Zhuang",
            "Guoshan Lu",
            "Zenan Huang",
            "Junbo Zhao",
            "Lin Liu",
            "Zhenzhong Lan",
            "Bei Yu",
            "Jianguo Li"
        ],
        "submitted": "2025-08-11 09:15:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation",
        "abstract": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems.",
        "url": "http://arxiv.org/abs/2508.07781v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07781v1",
        "arxiv_id": "2508.07781v1",
        "authors": [
            "Zeyu Yang",
            "Lai Wei",
            "Roman Koshkin",
            "Xi Chen",
            "Satoshi Nakamura"
        ],
        "submitted": "2025-08-11 09:13:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pareto Multi-Objective Alignment for Language Models",
        "abstract": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments.",
        "url": "http://arxiv.org/abs/2508.07768v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07768v1",
        "arxiv_id": "2508.07768v1",
        "authors": [
            "Qiang He",
            "Setareh Maghsudi"
        ],
        "submitted": "2025-08-11 08:54:14",
        "source": "arxiv",
        "comment": "Accepted at ECML/PKDD 2025"
    },
    {
        "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models",
        "abstract": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation.",
        "url": "http://arxiv.org/abs/2508.07753v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07753v1",
        "arxiv_id": "2508.07753v1",
        "authors": [
            "Zhenliang Zhang",
            "Junzhe Zhang",
            "Xinyu Hu",
            "HuiXuan Zhang",
            "Xiaojun Wan"
        ],
        "submitted": "2025-08-11 08:34:28",
        "source": "arxiv",
        "comment": "Accepted by CIKM 2025 (Full Paper)"
    },
    {
        "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment",
        "abstract": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models.",
        "url": "http://arxiv.org/abs/2508.07750v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07750v1",
        "arxiv_id": "2508.07750v1",
        "authors": [
            "Haowen Wang",
            "Yun Yue",
            "Zhiling Ye",
            "Shuowen Zhang",
            "Lei Fan",
            "Jiaxin Liang",
            "Jiadi Jiang",
            "Cheng Wei",
            "Jingyuan Deng",
            "Xudong Han",
            "Ji Li",
            "Chunxiao Guo",
            "Peng Wei",
            "Jian Wang",
            "Jinjie Gu"
        ],
        "submitted": "2025-08-11 08:28:47",
        "source": "arxiv",
        "comment": "12 pages, 5 figures, 7 tables"
    },
    {
        "title": "Encode Me If You Can: Learning Universal User Representations via Event Sequence Autoencoding",
        "abstract": "Building universal user representations that capture the essential aspects of\nuser behavior is a crucial task for modern machine learning systems. In\nreal-world applications, a user's historical interactions often serve as the\nfoundation for solving a wide range of predictive tasks, such as churn\nprediction, recommendations, or lifetime value estimation. Using a\ntask-independent user representation that is effective across all such tasks\ncan reduce the need for task-specific feature engineering and model retraining,\nleading to more scalable and efficient machine learning pipelines. The goal of\nthe RecSys Challenge 2025 by Synerise was to develop such Universal Behavioral\nProfiles from logs of past user behavior, which included various types of\nevents such as product purchases, page views, and search queries. We propose a\nmethod that transforms the entire user interaction history into a single\nchronological sequence and trains a GRU-based autoencoder to reconstruct this\nsequence from a fixed-size vector. If the model can accurately reconstruct the\nsequence, the latent vector is expected to capture the key behavioral patterns.\nIn addition to this core model, we explored several alternative methods for\ngenerating user embeddings and combined them by concatenating their output\nvectors into a unified representation. This ensemble strategy further improved\ngeneralization across diverse downstream tasks and helped our team,\nai_lab_recsys, achieve second place in the RecSys Challenge 2025.",
        "url": "http://arxiv.org/abs/2508.07748v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07748v1",
        "arxiv_id": "2508.07748v1",
        "authors": [
            "Anton Klenitskiy",
            "Artem Fatkulin",
            "Daria Denisova",
            "Anton Pembek",
            "Alexey Vasilev"
        ],
        "submitted": "2025-08-11 08:28:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction",
        "abstract": "Transformer-based models primarily rely on Next Token Prediction (NTP), which\npredicts the next token in a sequence based on the preceding context. However,\nNTP's focus on single-token prediction often limits a model's ability to plan\nahead or maintain long-range coherence, raising questions about how well LLMs\ncan predict longer contexts, such as full sentences within structured\ndocuments. While NTP encourages local fluency, it provides no explicit\nincentive to ensure global coherence across sentence boundaries-an essential\nskill for reconstructive or discursive tasks. To investigate this, we evaluate\nthree commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on\nMasked Sentence Prediction (MSP) - the task of infilling a randomly removed\nsentence - from three domains: ROCStories (narrative), Recipe1M (procedural),\nand Wikipedia (expository). We assess both fidelity (similarity to the original\nsentence) and cohesiveness (fit within the surrounding context). Our key\nfinding reveals that commercial LLMs, despite their superlative performance in\nother tasks, are poor at predicting masked sentences in low-structured domains,\nhighlighting a gap in current model capabilities.",
        "url": "http://arxiv.org/abs/2508.07702v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07702v1",
        "arxiv_id": "2508.07702v1",
        "authors": [
            "Charlie Wyatt",
            "Aditya Joshi",
            "Flora Salim"
        ],
        "submitted": "2025-08-11 07:25:50",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval",
        "abstract": "Tool learning has emerged as a promising paradigm for large language models\n(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository\nrapidly expanding, it is impractical to contain all tools within the limited\ninput length of LLMs. To alleviate these issues, researchers have explored\nincorporating a tool retrieval module to select the most relevant tools or\nrepresent tools as unique tokens within LLM parameters. However, most\nstate-of-the-art methods are under transductive settings, assuming all tools\nhave been observed during training. Such a setting deviates from reality as the\nreal-world tool repository is evolving and incorporates new tools frequently.\nWhen dealing with these unseen tools, which refer to tools not encountered\nduring the training phase, these methods are limited by two key issues,\nincluding the large distribution shift and the vulnerability of\nsimilarity-based retrieval. To this end, inspired by human cognitive processes\nof mastering unseen tools through discovering and applying the logical\ninformation from prior experience, we introduce a novel Logic-Guided Semantic\nBridging framework for inductive tool retrieval, namely, LoSemB, which aims to\nmine and transfer latent logical information for inductive tool retrieval\nwithout costly retraining. Specifically, LoSemB contains a logic-based\nembedding alignment module to mitigate distribution shifts and implements a\nrelational augmented retrieval mechanism to reduce the vulnerability of\nsimilarity-based retrieval. Extensive experiments demonstrate that LoSemB\nachieves advanced performance in inductive settings while maintaining desirable\neffectiveness in the transductive setting.",
        "url": "http://arxiv.org/abs/2508.07690v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07690v1",
        "arxiv_id": "2508.07690v1",
        "authors": [
            "Luyao Zhuang",
            "Qinggang Zhang",
            "Huachi Zhou",
            "Juhua Liu",
            "Qing Li",
            "Xiao Huang"
        ],
        "submitted": "2025-08-11 07:07:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks",
        "abstract": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback.",
        "url": "http://arxiv.org/abs/2508.07662v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07662v1",
        "arxiv_id": "2508.07662v1",
        "authors": [
            "Ihor Stepanov",
            "Mykhailo Shtopko",
            "Dmytro Vodianytskyi",
            "Oleksandr Lukashov",
            "Alexander Yavorskyi",
            "Mykyta Yaroshenko"
        ],
        "submitted": "2025-08-11 06:22:25",
        "source": "arxiv",
        "comment": "14 pages, 7 tables, 2 figures"
    },
    {
        "title": "MLego: Interactive and Scalable Topic Exploration Through Model Reuse",
        "abstract": "With massive texts on social media, users and analysts often rely on topic\nmodeling techniques to quickly extract key themes and gain insights.\nTraditional topic modeling techniques, such as Latent Dirichlet Allocation\n(LDA), provide valuable insights but are computationally expensive, making them\nimpractical for real-time data analysis. Although recent advances in\ndistributed training and fast sampling methods have improved efficiency,\nreal-time topic exploration remains a significant challenge. In this paper, we\npresent MLego, an interactive query framework designed to support real-time\ntopic modeling analysis by leveraging model materialization and reuse. Instead\nof retraining models from scratch, MLego efficiently merges materialized topic\nmodels to construct approximate results at interactive speeds. To further\nenhance efficiency, we introduce a hierarchical plan search strategy for single\nqueries and an optimized query reordering technique for batch queries. We\nintegrate MLego into a visual analytics prototype system, enabling users to\nexplore large-scale textual datasets through interactive queries. Extensive\nexperiments demonstrate that MLego significantly reduces computation costs\nwhile maintaining high-quality topic modeling results. MLego enhances existing\nvisual analytics approaches, which primarily focus on user-driven topic\nmodeling, by enabling real-time, query-driven exploration. This complements\ntraditional methods and bridges the gap between scalable topic modeling and\ninteractive data analysis.",
        "url": "http://arxiv.org/abs/2508.07654v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07654v1",
        "arxiv_id": "2508.07654v1",
        "authors": [
            "Fei Ye",
            "Jiapan Liu",
            "Yinan Jing",
            "Zhenying He",
            "Weirao Wang",
            "X. Sean Wang"
        ],
        "submitted": "2025-08-11 06:06:26",
        "source": "arxiv",
        "comment": "14 pages"
    },
    {
        "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents",
        "abstract": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.",
        "url": "http://arxiv.org/abs/2508.07642v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07642v1",
        "arxiv_id": "2508.07642v1",
        "authors": [
            "Tianyi Ma",
            "Yue Zhang",
            "Zehao Wang",
            "Parisa Kordjamshidi"
        ],
        "submitted": "2025-08-11 05:50:30",
        "source": "arxiv",
        "comment": "18 pages, 5 Figures,"
    },
    {
        "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information",
        "abstract": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.",
        "url": "http://arxiv.org/abs/2508.07630v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07630v1",
        "arxiv_id": "2508.07630v1",
        "authors": [
            "Anirudh Iyengar Kaniyar Narayana Iyengar",
            "Srija Mukhopadhyay",
            "Adnan Qidwai",
            "Shubhankar Singh",
            "Dan Roth",
            "Vivek Gupta"
        ],
        "submitted": "2025-08-11 05:19:23",
        "source": "arxiv",
        "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available"
    },
    {
        "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization",
        "abstract": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5% on AIME 2024, 83.2% on\nAIME 2025, 66.0% on LiveCodeBench V5 and 58.1% on LiveCodeBench V6.",
        "url": "http://arxiv.org/abs/2508.07629v2",
        "pdf_url": "http://arxiv.org/pdf/2508.07629v2",
        "arxiv_id": "2508.07629v2",
        "authors": [
            "Zhenpeng Su",
            "Leiyu Pan",
            "Xue Bai",
            "Dening Liu",
            "Guanting Dong",
            "Jiaming Huang",
            "Wenping Hu",
            "Fuzheng Zhang",
            "Kun Gai",
            "Guorui Zhou"
        ],
        "submitted": "2025-08-11 05:17:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation",
        "abstract": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning.",
        "url": "http://arxiv.org/abs/2508.07616v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07616v1",
        "arxiv_id": "2508.07616v1",
        "authors": [
            "Aswin RRV",
            "Jacob Dineen",
            "Divij Handa",
            "Md Nayem Uddin",
            "Mihir Parmar",
            "Chitta Baral",
            "Ben Zhou"
        ],
        "submitted": "2025-08-11 04:51:43",
        "source": "arxiv",
        "comment": "15 pages"
    },
    {
        "title": "UMRE: A Unified Monotonic Transformation for Ranking Ensemble in Recommender Systems",
        "abstract": "Industrial recommender systems commonly rely on ensemble sorting (ES) to\ncombine predictions from multiple behavioral objectives. Traditionally, this\nprocess depends on manually designed nonlinear transformations (e.g.,\npolynomial or exponential functions) and hand-tuned fusion weights to balance\ncompeting goals -- an approach that is labor-intensive and frequently\nsuboptimal in achieving Pareto efficiency. In this paper, we propose a novel\nUnified Monotonic Ranking Ensemble (UMRE) framework to address the limitations\nof traditional methods in ensemble sorting. UMRE replaces handcrafted\ntransformations with Unconstrained Monotonic Neural Networks (UMNN), which\nlearn expressive, strictly monotonic functions through the integration of\npositive neural integrals. Subsequently, a lightweight ranking model is\nemployed to fuse the prediction scores, assigning personalized weights to each\nprediction objective. To balance competing goals, we further introduce a Pareto\noptimality strategy that adaptively coordinates task weights during training.\nUMRE eliminates manual tuning, maintains ranking consistency, and achieves\nfine-grained personalization. Experimental results on two public recommendation\ndatasets (Kuairand and Tenrec) and online A/B tests demonstrate impressive\nperformance and generalization capabilities.",
        "url": "http://arxiv.org/abs/2508.07613v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07613v1",
        "arxiv_id": "2508.07613v1",
        "authors": [
            "Zhengrui Xu",
            "Zhe Yang",
            "Zhengxiao Guo",
            "Shukai Liu",
            "Luocheng Lin",
            "Xiaoyan Liu",
            "Yongqi Liu",
            "Han Li"
        ],
        "submitted": "2025-08-11 04:38:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements",
        "abstract": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.",
        "url": "http://arxiv.org/abs/2508.07598v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07598v1",
        "arxiv_id": "2508.07598v1",
        "authors": [
            "Ziheng Li",
            "Zhi-Hong Deng"
        ],
        "submitted": "2025-08-11 03:58:35",
        "source": "arxiv",
        "comment": "ECAI 2025"
    },
    {
        "title": "Towards Comprehensible Recommendation with Large Language Model Fine-tuning",
        "abstract": "Recommender systems have become increasingly ubiquitous in daily life. While\ntraditional recommendation approaches primarily rely on ID-based\nrepresentations or item-side content features, they often fall short in\ncapturing the underlying semantics aligned with user preferences (e.g.,\nrecommendation reasons for items), leading to a semantic-collaborative gap.\nRecently emerged LLM-based feature extraction approaches also face a key\nchallenge: how to ensure that LLMs possess recommendation-aligned reasoning\ncapabilities and can generate accurate, personalized reasons to mitigate the\nsemantic-collaborative gap. To address these issues, we propose a novel Content\nUnderstanding from a Collaborative Perspective framework (CURec), which\ngenerates collaborative-aligned content features for more comprehensive\nrecommendations. \\method first aligns the LLM with recommendation objectives\nthrough pretraining, equipping it with instruction-following and\nchain-of-thought reasoning capabilities. Next, we design a reward model\ninspired by traditional recommendation architectures to evaluate the quality of\nthe recommendation reasons generated by the LLM. Finally, using the reward\nsignals, CURec fine-tunes the LLM through RL and corrects the generated reasons\nto ensure their accuracy. The corrected reasons are then integrated into a\ndownstream recommender model to enhance comprehensibility and recommendation\nperformance. Extensive experiments on public benchmarks demonstrate the\nsuperiority of CURec over existing methods.",
        "url": "http://arxiv.org/abs/2508.07595v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07595v1",
        "arxiv_id": "2508.07595v1",
        "authors": [
            "Yunze Luo",
            "Yinjie Jiang",
            "Gaode Chen",
            "Xinghua Zhang",
            "Jun Zhang",
            "Jian Liang",
            "Kaigui Bian"
        ],
        "submitted": "2025-08-11 03:55:31",
        "source": "arxiv",
        "comment": "11 pages, 6 figures"
    },
    {
        "title": "IBPS: Indian Bail Prediction System",
        "abstract": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system.",
        "url": "http://arxiv.org/abs/2508.07592v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07592v1",
        "arxiv_id": "2508.07592v1",
        "authors": [
            "Puspesh Kumar Srivastava",
            "Uddeshya Raj",
            "Praveen Patel",
            "/Shubham Kumar Nigam",
            "Noel Shallum",
            "Arnab Bhattacharya"
        ],
        "submitted": "2025-08-11 03:44:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Orthogonal Low Rank Embedding Stabilization",
        "abstract": "The instability of embedding spaces across model retraining cycles presents\nsignificant challenges to downstream applications using user or item embeddings\nderived from recommendation systems as input features. This paper introduces a\nnovel orthogonal low-rank transformation methodology designed to stabilize the\nuser/item embedding space, ensuring consistent embedding dimensions across\nretraining sessions. Our approach leverages a combination of efficient low-rank\nsingular value decomposition and orthogonal Procrustes transformation to map\nembeddings into a standardized space. This transformation is computationally\nefficient, lossless, and lightweight, preserving the dot product and inference\nquality while reducing operational burdens. Unlike existing methods that modify\ntraining objectives or embedding structures, our approach maintains the\nintegrity of the primary model application and can be seamlessly integrated\nwith other stabilization techniques.",
        "url": "http://arxiv.org/abs/2508.07574v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07574v1",
        "arxiv_id": "2508.07574v1",
        "authors": [
            "Kevin Zielnicki",
            "Ko-Jen Hsiao"
        ],
        "submitted": "2025-08-11 03:15:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems.",
        "url": "http://arxiv.org/abs/2508.07534v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07534v1",
        "arxiv_id": "2508.07534v1",
        "authors": [
            "Jia Deng",
            "Jie Chen",
            "Zhipeng Chen",
            "Daixuan Cheng",
            "Fei Bai",
            "Beichen Zhang",
            "Yinqian Min",
            "Yanzipeng Gao",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "submitted": "2025-08-11 01:26:16",
        "source": "arxiv",
        "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260"
    },
    {
        "title": "Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI",
        "abstract": "What if the patterns hidden within dialogue reveal more about communication\nthan the words themselves? We introduce Conversational DNA, a novel visual\nlanguage that treats any dialogue -- whether between humans, between human and\nAI, or among groups -- as a living system with interpretable structure that can\nbe visualized, compared, and understood. Unlike traditional conversation\nanalysis that reduces rich interaction to statistical summaries, our approach\nreveals the temporal architecture of dialogue through biological metaphors.\nLinguistic complexity flows through strand thickness, emotional trajectories\ncascade through color gradients, conversational relevance forms through\nconnecting elements, and topic coherence maintains structural integrity through\nhelical patterns. Through exploratory analysis of therapeutic conversations and\nhistorically significant human-AI dialogues, we demonstrate how this\nvisualization approach reveals interaction patterns that traditional methods\nmiss. Our work contributes a new creative framework for understanding\ncommunication that bridges data visualization, human-computer interaction, and\nthe fundamental question of what makes dialogue meaningful in an age where\nhumans increasingly converse with artificial minds.",
        "url": "http://arxiv.org/abs/2508.07520v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07520v1",
        "arxiv_id": "2508.07520v1",
        "authors": [
            "Baihan Lin"
        ],
        "submitted": "2025-08-11 00:43:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews",
        "abstract": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').",
        "url": "http://arxiv.org/abs/2508.07517v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07517v1",
        "arxiv_id": "2508.07517v1",
        "authors": [
            "Joseph T. Colonel",
            "Baihan Lin"
        ],
        "submitted": "2025-08-11 00:27:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis",
        "abstract": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models.",
        "url": "http://arxiv.org/abs/2508.07516v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07516v1",
        "arxiv_id": "2508.07516v1",
        "authors": [
            "Keshav Varadarajan",
            "Tananun Songdechakraiwut"
        ],
        "submitted": "2025-08-11 00:19:47",
        "source": "arxiv",
        "comment": "15 pages, 9 figures, 4 tables"
    },
    {
        "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy",
        "abstract": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.",
        "url": "http://arxiv.org/abs/2508.07485v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07485v1",
        "arxiv_id": "2508.07485v1",
        "authors": [
            "Alexander Duffy",
            "Samuel J Paech",
            "Ishana Shastri",
            "Elizabeth Karpinski",
            "Baptiste Alloui-Cros",
            "Tyler Marques",
            "Matthew Lyle Olson"
        ],
        "submitted": "2025-08-10 21:07:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models",
        "abstract": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities.",
        "url": "http://arxiv.org/abs/2508.07484v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07484v1",
        "arxiv_id": "2508.07484v1",
        "authors": [
            "Archchana Sindhujan",
            "Shenbin Qian",
            "Chan Chi Chun Matthew",
            "Constantin Orasan",
            "Diptesh Kanojia"
        ],
        "submitted": "2025-08-10 20:59:44",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025 Conference"
    },
    {
        "title": "Positional Biases Shift as Inputs Approach Context Window Limits",
        "abstract": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.",
        "url": "http://arxiv.org/abs/2508.07479v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07479v1",
        "arxiv_id": "2508.07479v1",
        "authors": [
            "Blerta Veseli",
            "Julian Chibane",
            "Mariya Toneva",
            "Alexander Koller"
        ],
        "submitted": "2025-08-10 20:40:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CP-Agent: Agentic Constraint Programming",
        "abstract": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.",
        "url": "http://arxiv.org/abs/2508.07468v1",
        "pdf_url": "http://arxiv.org/pdf/2508.07468v1",
        "arxiv_id": "2508.07468v1",
        "authors": [
            "Stefan Szeider"
        ],
        "submitted": "2025-08-10 19:59:01",
        "source": "arxiv",
        "comment": null
    }
]