[
    {
        "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering",
        "abstract": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.",
        "url": "http://arxiv.org/abs/2506.22900v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22900v1",
        "arxiv_id": "2506.22900v1",
        "authors": [
            "Mai A. Shaaban",
            "Tausifa Jan Saleem",
            "Vijay Ram Papineni",
            "Mohammad Yaqub"
        ],
        "submitted": "2025-06-28 14:30:37",
        "source": "arxiv",
        "comment": null,
        "score": 17,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores medical visual question answering, which is not directly related to the user's primary focus on information retrieval, search technologies, and query understanding. While it touches on retrieval-augmented generation and re-ranking, the multimodal context and medical domain are not directly applicable to the user's interests in e-commerce or general information retrieval."
    },
    {
        "title": "Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models",
        "abstract": "This paper investigates the impact of shallow versus deep relevance judgments\non the performance of BERT-based reranking models in neural Information\nRetrieval. Shallow-judged datasets, characterized by numerous queries each with\nfew relevance judgments, and deep-judged datasets, involving fewer queries with\nextensive relevance judgments, are compared. The research assesses how these\ndatasets affect the performance of BERT-based reranking models trained on them.\nThe experiments are run on the MS MARCO and LongEval collections. Results\nindicate that shallow-judged datasets generally enhance generalization and\neffectiveness of reranking models due to a broader range of available contexts.\nThe disadvantage of the deep-judged datasets might be mitigated by a larger\nnumber of negative training examples.",
        "url": "http://arxiv.org/abs/2506.23191v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23191v1",
        "arxiv_id": "2506.23191v1",
        "authors": [
            "Gabriel Iturra-Bocaz",
            "Danny Vo",
            "Petra Galuscakova"
        ],
        "submitted": "2025-06-29 11:30:50",
        "source": "arxiv",
        "comment": "Accepted at ICTIR'25",
        "score": 16,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper is highly relevant to Information Retrieval, specifically neural IR, and explores the impact of relevance judgments on BERT-based reranking models. The use of MS MARCO and LongEval collections adds to the paper's relevance, as these datasets are commonly used in IR research. While the focus is on reranking models, the paper's emphasis on deep semantic understanding and real-time relevance optimization aligns with the user's interests."
    },
    {
        "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval",
        "abstract": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.",
        "url": "http://arxiv.org/abs/2506.22864v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22864v1",
        "arxiv_id": "2506.22864v1",
        "authors": [
            "Li-Cheng Shen",
            "Jih-Kang Hsieh",
            "Wei-Hua Li",
            "Chu-Song Chen"
        ],
        "submitted": "2025-06-28 12:19:49",
        "source": "arxiv",
        "comment": "ICMR 2025",
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on text-to-image retrieval, which is related to information retrieval, but the specific task of referring expression segmentation and object localization is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. While the paper does mention multimodal large language models, which is a related topic, the primary focus is on computer vision and image processing, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Learning to Rank with Variable Result Presentation Lengths",
        "abstract": "Learning to Rank (LTR) methods generally assume that each document in a top-K\nranking is presented in an equal format. However, previous work has shown that\nusers' perceptions of relevance can be changed by varying presentations, i.e.,\nallocating more vertical space to some documents to provide additional textual\nor image information. Furthermore, presentation length can also redirect\nattention, as users are more likely to notice longer presentations when\nscrolling through results. Deciding on the document presentation lengths in a\nfixed vertical space ranking is an important problem that has not been\naddressed by existing LTR methods.\n  We address this gap by introducing the variable presentation length ranking\ntask, where simultaneously the ordering of documents and their presentation\nlength is decided. Despite being a generalization of standard ranking, we show\nthat this setting brings significant new challenges: Firstly, the probability\nranking principle no longer applies to this setting, and secondly, the problem\ncannot be divided into separate ordering and length selection tasks.\n  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient\nestimation methods for the joint optimization of document ordering and lengths.\nOur semi-synthetic experiments show that VLPL can effectively balance the\nexpected exposure and attractiveness of all documents, achieving the best\nperformance across different ranking settings. Furthermore, we observe that\neven simple length-aware methods can achieve significant performance\nimprovements over fixed-length models. Altogether, our theoretical and\nempirical results highlight the importance and difficulties of combining\ndocument presentation with LTR.",
        "url": "http://arxiv.org/abs/2506.23319v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23319v1",
        "arxiv_id": "2506.23319v1",
        "authors": [
            "Norman Knyazev",
            "Harrie Oosterhuis"
        ],
        "submitted": "2025-06-29 16:28:17",
        "source": "arxiv",
        "comment": "SIGIR 2025",
        "score": 13,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'learning to rank' (score: +3)",
            "Found 'ltr' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores Learning to Rank (LTR) with variable result presentation lengths, which is a novel and relevant topic in Information Retrieval. The focus on presentation length and its impact on user behavior modeling is also aligned with my interests. However, the paper's primary focus is on ranking and presentation, rather than query understanding, ranking models, or user behavior modeling, which are my core research themes."
    },
    {
        "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation",
        "abstract": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.",
        "url": "http://arxiv.org/abs/2506.23136v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23136v1",
        "arxiv_id": "2506.23136v1",
        "authors": [
            "Shadman Sobhan",
            "Mohammad Ariful Haque"
        ],
        "submitted": "2025-06-29 08:22:03",
        "source": "arxiv",
        "comment": "29 Pages, 11 Tables",
        "score": 11,
        "keyword_reasons": [
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a Retrieval-Augmented Generation pipeline for question-answering on technical documents with structured data, which is somewhat related to my research interests in Information Retrieval and Search technologies. However, the focus on question-answering and technical documents is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Machine Assistant with Reliable Knowledge: Enhancing Student Learning via RAG-based Retrieval",
        "abstract": "We present Machine Assistant with Reliable Knowledge (MARK), a\nretrieval-augmented question-answering system designed to support student\nlearning through accurate and contextually grounded responses. The system is\nbuilt on a retrieval-augmented generation (RAG) framework, which integrates a\ncurated knowledge base to ensure factual consistency. To enhance retrieval\neffectiveness across diverse question types, we implement a hybrid search\nstrategy that combines dense vector similarity with sparse keyword-based\nretrieval. This dual-retrieval mechanism improves robustness for both general\nand domain-specific queries. The system includes a feedback loop in which\nstudents can rate responses and instructors can review and revise them.\nInstructor corrections are incorporated into the retrieval corpus, enabling\nadaptive refinement over time. The system was deployed in a classroom setting\nas a substitute for traditional office hours, where it successfully addressed a\nbroad range of student queries. It was also used to provide technical support\nby integrating with a customer-specific knowledge base, demonstrating its\nability to handle routine, context-sensitive tasks in applied domains. MARK is\npublicly accessible at https://app.eduquery.ai.",
        "url": "http://arxiv.org/abs/2506.23026v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23026v1",
        "arxiv_id": "2506.23026v1",
        "authors": [
            "Yongsheng Lian"
        ],
        "submitted": "2025-06-28 22:17:27",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus is on a specific application of retrieval-augmented question-answering in an educational setting, which is not a core area of your research."
    },
    {
        "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge",
        "abstract": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.",
        "url": "http://arxiv.org/abs/2506.22644v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22644v1",
        "arxiv_id": "2506.22644v1",
        "authors": [
            "Chase Fensore",
            "Kaustubh Dhole",
            "Joyce C Ho",
            "Eugene Agichtein"
        ],
        "submitted": "2025-06-27 21:20:43",
        "source": "arxiv",
        "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)",
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper evaluates retrieval-augmented generation systems, which is a topic in Natural Language Processing (NLP). However, the focus is on generation and re-ranking, rather than query understanding, ranking models, or user behavior modeling, which are key areas of interest in Information Retrieval (IR). While the paper touches on retrieval methods, it does not delve into the specific areas of query understanding, ranking models, or user behavior modeling that are central to IR."
    },
    {
        "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On",
        "abstract": "The global fashion e-commerce industry has become integral to people's daily\nlives, leveraging technological advancements to offer personalized shopping\nexperiences, primarily through recommendation systems that enhance customer\nengagement through personalized suggestions. To improve customers' experience\nin online shopping, we propose a novel comprehensive KiseKloset system for\noutfit retrieval, recommendation, and try-on. We explore two approaches for\noutfit retrieval: similar item retrieval and text feedback-guided item\nretrieval. Notably, we introduce a novel transformer architecture designed to\nrecommend complementary items from diverse categories. Furthermore, we enhance\nthe overall performance of the search pipeline by integrating approximate\nalgorithms to optimize the search process. Additionally, addressing the crucial\nneeds of online shoppers, we employ a lightweight yet efficient virtual try-on\nframework capable of real-time operation, memory efficiency, and maintaining\nrealistic outputs compared to its predecessors. This virtual try-on module\nempowers users to visualize specific garments on themselves, enhancing the\ncustomers' experience and reducing costs associated with damaged items for\nretailers. We deployed our end-to-end system for online users to test and\nprovide feedback, enabling us to measure their satisfaction levels. The results\nof our user study revealed that 84% of participants found our comprehensive\nsystem highly useful, significantly improving their online shopping experience.",
        "url": "http://arxiv.org/abs/2506.23471v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23471v1",
        "arxiv_id": "2506.23471v1",
        "authors": [
            "Thanh-Tung Phan-Nguyen",
            "Khoi-Nguyen Nguyen-Ngoc",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "submitted": "2025-06-30 02:25:39",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'shopping' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a comprehensive system for outfit retrieval, recommendation, and try-on, which is related to search technologies and user behavior modeling. However, the focus is on the fashion e-commerce domain, which is not the primary area of interest. The paper does not explicitly address query understanding, ranking models, or deep semantic understanding, which are key aspects of the user's research interests."
    },
    {
        "title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance",
        "abstract": "There is an increasing demand for extending existing DBMSs with vector\nindices so that they become unified systems capable of supporting modern\npredictive applications, which require joint querying of vector embeddings\ntogether with the structured properties and connections of objects. We present\nNaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design\ngoals. First, we aim to implement a disk-based vector index that leverages the\ncore storage and query-processing capabilities of the underlying GDBMS. To this\nend, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,\nwhich itself is a graph-based structure. Second, we aim to support\npredicate-agnostic filtered vector search queries, in which the k nearest\nneighbors (kNNs) of a query vector vQ are searched only within an arbitrary\nsubset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a\nprefiltering approach that evaluates QS first and passes the full description\nof subset S to the kNN search operator. We study how to design a prefiltering\nsearch algorithm that remains robust under varying selectivities and under\ndifferent correlations between subset S and query vector vQ. We propose an\nadaptive algorithm that uses the local selectivity of each vector in the HNSW\ngraph to choose an appropriate heuristic at every iteration of the kNN search.\nFinally, We demonstrate NaviX's robustness and efficiency through extensive\nexperiments against both existing prefiltering- and postfiltering-based\nbaselines.",
        "url": "http://arxiv.org/abs/2506.23397v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23397v1",
        "arxiv_id": "2506.23397v1",
        "authors": [
            "Gaurav Sehgal",
            "Semih Salihoglu"
        ],
        "submitted": "2025-06-29 21:16:07",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on designing a native vector index for graph DBMSs, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions search queries, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams",
        "abstract": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.",
        "url": "http://arxiv.org/abs/2506.23192v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23192v1",
        "arxiv_id": "2506.23192v1",
        "authors": [
            "Gabriel Iturra-Bocaz",
            "Felipe Bravo-Marquez"
        ],
        "submitted": "2025-06-29 11:34:23",
        "source": "arxiv",
        "comment": "Accepted at SIGIR'23",
        "score": 9,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a Python library for training and evaluating incremental word embeddings from text data streams, which is related to Natural Language Processing and Information Retrieval. However, the focus is on word embeddings and their adaptation to evolving language patterns, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user."
    },
    {
        "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries",
        "abstract": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.",
        "url": "http://arxiv.org/abs/2506.23071v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23071v1",
        "arxiv_id": "2506.23071v1",
        "authors": [
            "Zhengren Wang",
            "Bozhou Li",
            "Dongwen Yao",
            "Wentao Zhang"
        ],
        "submitted": "2025-06-29 03:17:42",
        "source": "arxiv",
        "comment": "Work in progess",
        "score": 9,
        "keyword_reasons": [
            "Found 'semantic search' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the intersection of natural language processing and information retrieval, specifically in the context of text-to-SQL and vector search. While it doesn't directly address query understanding, ranking models, or user behavior modeling, it does contribute to the development of more versatile and intuitive database interfaces, which may be of interest to researchers in the broader field of information retrieval."
    },
    {
        "title": "On the Predictive Power of Representation Dispersion in Language Models",
        "abstract": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.",
        "url": "http://arxiv.org/abs/2506.24106v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24106v1",
        "arxiv_id": "2506.24106v1",
        "authors": [
            "Yanhong Li",
            "Ming Li",
            "Karen Livescu",
            "Jiawei Zhou"
        ],
        "submitted": "2025-06-30 17:53:50",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the predictive power of representation dispersion in language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of model selection and retrieval-based methods, the connection is loose and the paper's primary focus is on language models and their internal workings, rather than information retrieval or search."
    },
    {
        "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data",
        "abstract": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.",
        "url": "http://arxiv.org/abs/2506.23139v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23139v1",
        "arxiv_id": "2506.23139v1",
        "authors": [
            "Prafulla Kumar Choubey",
            "Xiangyu Peng",
            "Shilpa Bhagavath",
            "Kung-Hsiang Huang",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "submitted": "2025-06-29 08:34:59",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper presents a benchmark for evaluating Deep Search, which involves multi-hop reasoning over diverse sources. While it's not directly focused on query understanding, ranking models, or user behavior modeling, it's related to information retrieval and search technologies. The paper's emphasis on deep semantic understanding and real-time relevance optimization aligns with the user's interests, but the topic is somewhat specialized and not directly applicable to the user's primary focus."
    },
    {
        "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models",
        "abstract": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
        "url": "http://arxiv.org/abs/2506.22791v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22791v1",
        "arxiv_id": "2506.22791v1",
        "authors": [
            "Jianxin Yan",
            "Wangze Ni",
            "Lei Chen",
            "Xuemin Lin",
            "Peng Cheng",
            "Zhan Qin",
            "Kui Ren"
        ],
        "submitted": "2025-06-28 07:25:12",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores context-aware semantic caching for multi-turn queries in large language models, which is related to query understanding and ranking models in Information Retrieval. Although it's not directly focused on user behavior modeling or click models, it's still relevant to the broader area of search technologies. The paper's emphasis on real-time relevance optimization and efficiency also aligns with the user's interests in Information Retrieval."
    },
    {
        "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute",
        "abstract": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.",
        "url": "http://arxiv.org/abs/2506.22716v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22716v1",
        "arxiv_id": "2506.22716v1",
        "authors": [
            "Dujian Ding",
            "Ankur Mallick",
            "Shaokun Zhang",
            "Chi Wang",
            "Daniel Madrigal",
            "Mirian Del Carmen Hipolito Garcia",
            "Menglin Xia",
            "Laks V. S. Lakshmanan",
            "Qingyun Wu",
            "Victor RÃ¼hle"
        ],
        "submitted": "2025-06-28 01:52:50",
        "source": "arxiv",
        "comment": "Accepted to ICML 2025 (main conference)",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses query routing and large language models, which are related to information retrieval and search technologies. However, the focus on cost optimization and model selection is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Emergent musical properties of a transformer under contrastive self-supervised learning",
        "abstract": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR.",
        "url": "http://arxiv.org/abs/2506.23873v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23873v1",
        "arxiv_id": "2506.23873v1",
        "authors": [
            "Yuexuan Kong",
            "Gabriel Meseguer-Brocal",
            "Vincent Lostanlen",
            "Mathieu Lagrange",
            "Romain Hennequin"
        ],
        "submitted": "2025-06-30 14:04:59",
        "source": "arxiv",
        "comment": "Accepted at ISMIR 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on music information retrieval and transformer models for sequence modeling in music is outside the user's primary areas of interest."
    },
    {
        "title": "Synergizing Implicit and Explicit User Interests: A Multi-Embedding Retrieval Framework at Pinterest",
        "abstract": "Industrial recommendation systems are typically composed of multiple stages,\nincluding retrieval, ranking, and blending. The retrieval stage plays a\ncritical role in generating a high-recall set of candidate items that covers a\nwide range of diverse user interests. Effectively covering the diverse and\nlong-tail user interests within this stage poses a significant challenge:\ntraditional two-tower models struggle in this regard due to limited user-item\nfeature interaction and often bias towards top use cases. To address these\nissues, we propose a novel multi-embedding retrieval framework designed to\nenhance user interest representation by generating multiple user embeddings\nconditioned on both implicit and explicit user interests. Implicit interests\nare captured from user history through a Differentiable Clustering Module\n(DCM), whereas explicit interests, such as topics that the user has followed,\nare modeled via Conditional Retrieval (CR). These methodologies represent a\nform of conditioned user representation learning that involves condition\nrepresentation construction and associating the target item with the relevant\nconditions. Synergizing implicit and explicit user interests serves as a\ncomplementary approach to achieve more effective and comprehensive candidate\nretrieval as they benefit on different user segments and extract conditions\nfrom different but supplementary sources. Extensive experiments and A/B testing\nreveal significant improvements in user engagements and feed diversity metrics.\nOur proposed framework has been successfully deployed on Pinterest home feed.",
        "url": "http://arxiv.org/abs/2506.23060v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23060v1",
        "arxiv_id": "2506.23060v1",
        "authors": [
            "Zhibo Fan",
            "Hongtao Lin",
            "Haoyu Chen",
            "Bowen Deng",
            "Hedi Xia",
            "Yuke Yan",
            "James Li"
        ],
        "submitted": "2025-06-29 02:14:21",
        "source": "arxiv",
        "comment": "KDD 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper proposes a multi-embedding retrieval framework that synergizes implicit and explicit user interests, which is relevant to your interests in query understanding, ranking models, and user behavior modeling. The framework's focus on enhancing user interest representation and generating multiple user embeddings conditioned on both implicit and explicit user interests aligns with your research themes. However, the paper's primary focus is on recommender systems, which is not your primary area of interest, but still relevant to your broader interests in information retrieval and NLP."
    },
    {
        "title": "Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems",
        "abstract": "Over the past decade, recommender systems have experienced a surge in\npopularity. Despite notable progress, they grapple with challenging issues,\nsuch as high data dimensionality and sparseness. Representing users and items\nas low-dimensional embeddings learned via neural networks has become a leading\nsolution. However, while recent studies show promising results, many approaches\nrely on complex architectures or require content data, which may not always be\navailable. This paper presents Interact2Vec, a novel neural network-based model\nthat simultaneously learns distributed embeddings for users and items while\ndemanding only implicit feedback. The model employs state-of-the-art strategies\nthat natural language processing models commonly use to optimize the training\nphase and enhance the final embeddings. Two types of experiments were conducted\nregarding the extrinsic and intrinsic quality of the model. In the former, we\nbenchmarked the recommendations generated by Interact2Vec's embeddings in a\ntop-$N$ ranking problem, comparing them with six other recommender algorithms.\nThe model achieved the second or third-best results in 30\\% of the datasets,\nbeing competitive with other recommenders, and has proven to be very efficient\nwith an average training time reduction of 274\\% compared to other\nembedding-based models. Later, we analyzed the intrinsic quality of the\nembeddings through similarity tables. Our findings suggest that Interact2Vec\ncan achieve promising results, especially on the extrinsic task, and is an\nexcellent embedding-generator model for scenarios of scarce computing\nresources, enabling the learning of item and user embeddings simultaneously and\nefficiently.",
        "url": "http://arxiv.org/abs/2506.22648v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22648v1",
        "arxiv_id": "2506.22648v1",
        "authors": [
            "Pedro R. Pires",
            "Tiago A. Almeida"
        ],
        "submitted": "2025-06-27 21:30:03",
        "source": "arxiv",
        "comment": "Accepted for publication in Applied Soft Computing (ASOC), 49 pages,\n  14 figures",
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is related to the user's interests in search technologies and information retrieval. However, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on neural networks and embeddings is also more relevant to NLP and data mining than the user's primary focus on IR."
    },
    {
        "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems",
        "abstract": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.",
        "url": "http://arxiv.org/abs/2506.22852v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22852v1",
        "arxiv_id": "2506.22852v1",
        "authors": [
            "Yucheng Cai",
            "Yuxuan Wu",
            "Yi Huang",
            "Junlan Feng",
            "Zhijian Ou"
        ],
        "submitted": "2025-06-28 11:26:31",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of large language models in dialog systems, focusing on knowledge-intensive scenarios. While it touches on the topic of retrieval and generation, the primary focus is on finetuning LLMs with domain-specific data and knowledge, which is not directly related to my research interests in Information Retrieval, query understanding, and ranking models."
    },
    {
        "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning",
        "abstract": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.",
        "url": "http://arxiv.org/abs/2506.23056v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23056v1",
        "arxiv_id": "2506.23056v1",
        "authors": [
            "Xiang Zhuang",
            "Bin Wu",
            "Jiyu Cui",
            "Kehua Feng",
            "Xiaotong Li",
            "Huabin Xing",
            "Keyan Ding",
            "Qiang Zhang",
            "Huajun Chen"
        ],
        "submitted": "2025-06-29 02:00:38",
        "source": "arxiv",
        "comment": "ACL 2025 Main",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of molecular structure elucidation and chemical knowledge is outside the scope of your expertise and interests."
    },
    {
        "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages",
        "abstract": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
        "url": "http://arxiv.org/abs/2506.23930v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23930v1",
        "arxiv_id": "2506.23930v1",
        "authors": [
            "Ruhina Tabasshum Prome",
            "Tarikul Islam Tamiti",
            "Anomadarshi Barua"
        ],
        "submitted": "2025-06-30 14:59:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on hate speech detection in low-resource languages using prompt engineering and large language models, which is outside your primary focus areas."
    },
    {
        "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation",
        "abstract": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.",
        "url": "http://arxiv.org/abs/2506.23662v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23662v1",
        "arxiv_id": "2506.23662v1",
        "authors": [
            "Philip Lippmann",
            "Jie Yang"
        ],
        "submitted": "2025-06-30 09:38:50",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a method for generating synthetic context corpus for contextual embeddings, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on adapting embeddings for domain-specific distributions, which is not directly aligned with my research interests in user behavior modeling and real-time relevance optimization."
    },
    {
        "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
        "abstract": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.",
        "url": "http://arxiv.org/abs/2506.23485v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23485v1",
        "arxiv_id": "2506.23485v1",
        "authors": [
            "Haocheng Yu",
            "Yaxiong Wu",
            "Hao Wang",
            "Wei Guo",
            "Yong Liu",
            "Yawen Li",
            "Yuyang Ye",
            "Junping Du",
            "Enhong Chen"
        ],
        "submitted": "2025-06-30 03:15:50",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores interactive recommender systems, which is related to information retrieval and search technologies. The use of large language models and planning capabilities is also relevant to query understanding and ranking models. However, the focus on recommender systems and user simulation schemes is not directly aligned with the user's primary interest in information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations",
        "abstract": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.",
        "url": "http://arxiv.org/abs/2506.24016v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24016v1",
        "arxiv_id": "2506.24016v1",
        "authors": [
            "Hyunjong Kim",
            "Sangyeop Kim",
            "Jongheon Jeong",
            "Yeongjae Cho",
            "Sungzoon Cho"
        ],
        "submitted": "2025-06-30 16:20:51",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025 Findings",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval (IR) or Search technologies, and does not address query understanding, ranking models, or user behavior modeling. While it involves language models and evaluation metrics, the focus is on image captioning and explainability, which is not a primary area of interest for the user."
    },
    {
        "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning",
        "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.",
        "url": "http://arxiv.org/abs/2506.23998v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23998v1",
        "arxiv_id": "2506.23998v1",
        "authors": [
            "Seungjun Yi",
            "Joakim Nguyen",
            "Huimin Xu",
            "Terence Lim",
            "Andrew Well",
            "Mia Markey",
            "Ying Ding"
        ],
        "submitted": "2025-06-30 16:02:28",
        "source": "arxiv",
        "comment": "Presented at ACL 2025 SRW",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions large language models, the focus is on thematic analysis and clinical narratives, which is outside the scope of the user's research interests."
    },
    {
        "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.",
        "url": "http://arxiv.org/abs/2506.23940v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23940v1",
        "arxiv_id": "2506.23940v1",
        "authors": [
            "Yang Dai",
            "Jianxiang An",
            "Tianwei Lin",
            "Hongyang He",
            "Hongzhe Huang",
            "Wenqiao Zhang",
            "Zheqi Lv",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "submitted": "2025-06-30 15:07:41",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on integrating domain knowledge in Multimodal Large Language Models (MLLMs), which is not directly related to Information Retrieval (IR) or Search technologies. While it mentions multimodal benchmarks, the abstract does not provide any insights into query understanding, ranking models, or user behavior modeling, which are key areas of interest in IR."
    },
    {
        "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data",
        "abstract": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.",
        "url": "http://arxiv.org/abs/2506.23735v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23735v1",
        "arxiv_id": "2506.23735v1",
        "authors": [
            "JiaRu Wu",
            "Mingwei Liu"
        ],
        "submitted": "2025-06-30 11:18:56",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on robustness and generalization, the context is not relevant to the user's primary research interests in IR and NLP."
    },
    {
        "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization",
        "abstract": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.",
        "url": "http://arxiv.org/abs/2506.23714v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23714v1",
        "arxiv_id": "2506.23714v1",
        "authors": [
            "Md Moinul Islam",
            "Sofoklis Kakouros",
            "Janne HeikkilÃ¤",
            "Mourad Oussalah"
        ],
        "submitted": "2025-06-30 10:41:33",
        "source": "arxiv",
        "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a multimodal video summarization framework that integrates textual, audio, and visual cues, which is a relevant topic in Information Retrieval. However, the focus on video summarization and multimodal integration is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. While the paper touches on some related concepts, such as extracting prosodic features and identifying semantically important moments, it does not address the user's core research themes."
    },
    {
        "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative Recommendation",
        "abstract": "Generative recommendation (GR) typically encodes behavioral or semantic\naspects of item information into discrete tokens, leveraging the standard\nautoregressive (AR) generation paradigm to make predictions. However, existing\nmethods tend to overlook their intrinsic relationship, that is, the semantic\nusually provides some reasonable explainability \"$\\textbf{why}$\" for the\nbehavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To\nthis end, we present Chunk AutoRegressive Modeling (CAR), a new generation\nparadigm following the decision pattern that users usually think semantic\naspects of items (e.g. brand) and then take actions on target items (e.g.\npurchase). Our CAR, for the $\\textit{first time}$, incorporates semantics\n(SIDs) and behavior (UID) into a single autoregressive transformer from an\n``act-with-think'' dual perspective via chunk-level autoregression.\nSpecifically, CAR packs SIDs and UID into a conceptual chunk for item unified\nrepresentation, allowing each decoding step to make a holistic prediction.\nExperiments show that our CAR significantly outperforms existing methods based\non traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we\nverify the scaling effect between model performance and SIDs bit number,\ndemonstrating that CAR preliminary emulates a kind of slow-thinking style\nmechanism akin to the reasoning processes observed in large language models\n(LLMs).",
        "url": "http://arxiv.org/abs/2506.23643v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23643v1",
        "arxiv_id": "2506.23643v1",
        "authors": [
            "Yifan Wang",
            "Weinan Gan",
            "Longtao Xiao",
            "Jieming Zhu",
            "Heng Chang",
            "Haozhao Wang",
            "Rui Zhang",
            "Zhenhua Dong",
            "Ruiming Tang",
            "Ruixuan Li"
        ],
        "submitted": "2025-06-30 09:13:54",
        "source": "arxiv",
        "comment": "9 pages, 2 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a novel approach to generative recommendation, incorporating semantic and behavioral aspects. While it touches on the idea of understanding user behavior, it does not explicitly address query understanding, ranking models, or click models, which are core areas of interest in Information Retrieval. The paper's focus on recommender systems and generative models is somewhat related to the user's interests, but it does not align with the primary focus on information retrieval and deep semantic understanding."
    },
    {
        "title": "Semantic-guided Diverse Decoding for Large Language Model",
        "abstract": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.",
        "url": "http://arxiv.org/abs/2506.23601v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23601v1",
        "arxiv_id": "2506.23601v1",
        "authors": [
            "Weijie Shi",
            "Yue Cui",
            "Yaguang Wu",
            "Jingzhi Fang",
            "Shibo Zhang",
            "Mengze Li",
            "Sirui Han",
            "Jia Zhu",
            "Jiajie Xu",
            "Xiaofang Zhou"
        ],
        "submitted": "2025-06-30 08:06:49",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on diverse decoding for large language models, which is related to information retrieval and search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on semantic diversity and embedding space operations is somewhat relevant to NLP and data mining, but the connection to information retrieval is indirect and limited."
    },
    {
        "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning",
        "abstract": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.",
        "url": "http://arxiv.org/abs/2506.23524v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23524v1",
        "arxiv_id": "2506.23524v1",
        "authors": [
            "Phan Quoc Hung Mai",
            "Quang Hung Nguyen",
            "Phuong Giang Duong",
            "Hong Hanh Nguyen",
            "Nguyen Tuan Long"
        ],
        "submitted": "2025-06-30 05:19:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific domain (education) and language (Vietnamese), which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it explores multitask learning, the topics and techniques discussed are not aligned with the user's specific areas of interest, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Datasets for Fairness in Language Models: An In-Depth Survey",
        "abstract": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.",
        "url": "http://arxiv.org/abs/2506.23411v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23411v1",
        "arxiv_id": "2506.23411v1",
        "authors": [
            "Jiale Zhang",
            "Zichong Wang",
            "Avash Palikhe",
            "Zhipeng Yin",
            "Wenbin Zhang"
        ],
        "submitted": "2025-06-29 22:11:58",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on fairness in language models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it touches on related topics like NLP, the scope is limited to language models and fairness, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Density, asymmetry and citation dynamics in scientific literature",
        "abstract": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward.",
        "url": "http://arxiv.org/abs/2506.23366v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23366v1",
        "arxiv_id": "2506.23366v1",
        "authors": [
            "Nathaniel Imel",
            "Zachary Hafen"
        ],
        "submitted": "2025-06-29 18:55:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling, which are the user's primary research interests. While it touches on the topic of semantic similarity, it is focused on scientometrics and citation dynamics, which is a distinct area of research."
    },
    {
        "title": "GaussMaster: An LLM-based Database Copilot System",
        "abstract": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.",
        "url": "http://arxiv.org/abs/2506.23322v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23322v1",
        "arxiv_id": "2506.23322v1",
        "authors": [
            "Wei Zhou",
            "Ji Sun",
            "Xuanhe Zhou",
            "Guoliang Li",
            "Luyang Liu",
            "Hao Wu",
            "Tianyuan Wang"
        ],
        "submitted": "2025-06-29 16:39:31",
        "source": "arxiv",
        "comment": "We welcome contributions from the community. For reference, please\n  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster",
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on autonomous database platforms and a specific system called GaussMaster, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. Although it mentions SQL queries and anomaly detection, the context is not relevant to the user's research interests."
    },
    {
        "title": "Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences",
        "abstract": "In the online digital realm, recommendation systems are ubiquitous and play a\ncrucial role in enhancing user experience. These systems leverage user\npreferences to provide personalized recommendations, thereby helping users\nnavigate through the paradox of choice. This work focuses on personalized\nsequential recommendation, where the system considers not only a user's\nimmediate, evolving session context, but also their cumulative historical\nbehavior to provide highly relevant and timely recommendations. Through an\nempirical study conducted on diverse real-world datasets, we have observed and\nquantified the existence and impact of both short-term (immediate and\ntransient) and long-term (enduring and stable) preferences on users' historical\ninteractions. Building on these insights, we propose a framework that combines\nshort- and long-term preferences to enhance recommendation performance, namely\nCompositions of Variant Experts (CoVE). This novel framework dynamically\nintegrates short- and long-term preferences through the use of different\nspecialized recommendation models (i.e., experts). Extensive experiments\nshowcase the effectiveness of the proposed methods and ablation studies further\ninvestigate the impact of variant expert types.",
        "url": "http://arxiv.org/abs/2506.23170v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23170v1",
        "arxiv_id": "2506.23170v1",
        "authors": [
            "Jaime Hieu Do",
            "Trung-Hoang Le",
            "Hady W. Lauw"
        ],
        "submitted": "2025-06-29 10:09:33",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on personalized sequential recommendation, which is related to user behavior modeling and query understanding in information retrieval. However, it does not directly address ranking models or deep semantic understanding, which are core aspects of the user's research interests. The paper's emphasis on recommender systems and user preferences is somewhat relevant, but not a central match for the user's primary focus on information retrieval."
    },
    {
        "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions",
        "abstract": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.",
        "url": "http://arxiv.org/abs/2506.23146v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23146v1",
        "arxiv_id": "2506.23146v1",
        "authors": [
            "Dingzriui Wang",
            "Xuanliang Zhang",
            "Keyan Xu",
            "Qingfu Zhu",
            "Wanxiang Che",
            "Yang Deng"
        ],
        "submitted": "2025-06-29 08:55:37",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel metric for evaluating in-context learning effectiveness, which is a topic in Natural Language Processing. However, it does not directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are the user's primary research interests."
    },
    {
        "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models",
        "abstract": "The rise of API-only access to state-of-the-art LLMs highlights the need for\neffective black-box jailbreak methods to identify model vulnerabilities in\nreal-world settings. Without a principled objective for gradient-based\noptimization, most existing approaches rely on genetic algorithms, which are\nlimited by their initialization and dependence on manually curated prompt\npools. Furthermore, these methods require individual optimization for each\nprompt, failing to provide a comprehensive characterization of model\nvulnerabilities. To address this gap, we introduce VERA: Variational infErence\nfRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a\nvariational inference problem, training a small attacker LLM to approximate the\ntarget LLM's posterior over adversarial prompts. Once trained, the attacker can\ngenerate diverse, fluent jailbreak prompts for a target query without\nre-optimization. Experimental results show that VERA achieves strong\nperformance across a range of target LLMs, highlighting the value of\nprobabilistic inference for adversarial prompt generation.",
        "url": "http://arxiv.org/abs/2506.22666v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22666v1",
        "arxiv_id": "2506.22666v1",
        "authors": [
            "Anamika Lochab",
            "Lu Yan",
            "Patrick Pynadath",
            "Xiangyu Zhang",
            "Ruqi Zhang"
        ],
        "submitted": "2025-06-27 22:22:00",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a framework for jailbreaking large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves language models, the context and objectives are distinct from the user's primary research interests."
    },
    {
        "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
        "abstract": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
        "url": "http://arxiv.org/abs/2506.24119v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24119v1",
        "arxiv_id": "2506.24119v1",
        "authors": [
            "Bo Liu",
            "Leon Guertler",
            "Simon Yu",
            "Zichen Liu",
            "Penghui Qi",
            "Daniel Balcells",
            "Mickel Liu",
            "Cheston Tan",
            "Weiyan Shi",
            "Min Lin",
            "Wee Sun Lee",
            "Natasha Jaques"
        ],
        "submitted": "2025-06-30 17:58:13",
        "source": "arxiv",
        "comment": "Work in Progress",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multi-agent reinforcement learning and zero-sum games, which is not directly related to information retrieval, search technologies, or query understanding. While the paper mentions language models, it does not specifically address ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation",
        "abstract": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.",
        "url": "http://arxiv.org/abs/2506.23979v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23979v1",
        "arxiv_id": "2506.23979v1",
        "authors": [
            "Renren Jin",
            "Tianhao Shen",
            "Xinwei Wu",
            "Dan Shi",
            "Haoran Sun",
            "Wuwei Huang",
            "Quandong Wang",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang",
            "Deyi Xiong"
        ],
        "submitted": "2025-06-30 15:45:28",
        "source": "arxiv",
        "comment": "33 pages, 15 tables, 11 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on preference data generation for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions fine-tuning and dataset construction, the context is different from the user's primary research interests."
    },
    {
        "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders",
        "abstract": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.",
        "url": "http://arxiv.org/abs/2506.23951v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23951v1",
        "arxiv_id": "2506.23951v1",
        "authors": [
            "Mathis Le Bail",
            "JÃ©rÃ©mie Dentan",
            "Davide Buscaldi",
            "Sonia Vanier"
        ],
        "submitted": "2025-06-30 15:18:50",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of Sparse Autoencoders to extract interpretable concepts from Large Language Models for text classification, which is a topic related to Natural Language Processing. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval. The paper's focus on text classification and concept extraction is somewhat relevant to the user's research, but it does not align with their primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It",
        "abstract": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.",
        "url": "http://arxiv.org/abs/2506.23864v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23864v1",
        "arxiv_id": "2506.23864v1",
        "authors": [
            "Seyed Mahed Mousavi",
            "Edoardo Cecchinato",
            "Lucia Hornikova",
            "Giuseppe Riccardi"
        ],
        "submitted": "2025-06-30 13:57:28",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper's focus on evaluating the reliability of reasoning benchmarks and identifying flaws in their design and methodology is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on the topic of model performance, it does not specifically address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins",
        "abstract": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.",
        "url": "http://arxiv.org/abs/2506.23826v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23826v1",
        "arxiv_id": "2506.23826v1",
        "authors": [
            "LluÃ­s C. Coll",
            "Martin W. Lauer-Schmaltz",
            "Philip Cash",
            "John P. Hansen",
            "Anja Maier"
        ],
        "submitted": "2025-06-30 13:18:31",
        "source": "arxiv",
        "comment": "24 pages, 9 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Human Digital Twins and conversational AI, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs",
        "abstract": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.",
        "url": "http://arxiv.org/abs/2506.23610v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23610v1",
        "arxiv_id": "2506.23610v1",
        "authors": [
            "Manuel Pratelli",
            "Marinella Petrocchi"
        ],
        "submitted": "2025-06-30 08:16:07",
        "source": "arxiv",
        "comment": "pre-print version - paper actually under submission",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper evaluates the ability of Large Language Models (LLMs) to simulate human personality-driven susceptibility to misinformation, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on the topic of language models, it does not focus on query understanding, ranking models, or user behavior modeling, which are my primary areas of interest."
    },
    {
        "title": "Hierarchical Memory Organization for Wikipedia Generation",
        "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.",
        "url": "http://arxiv.org/abs/2506.23393v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23393v1",
        "arxiv_id": "2506.23393v1",
        "authors": [
            "Eugene J. Yu",
            "Dawei Zhu",
            "Yifan Song",
            "Xiangyu Wong",
            "Jiebin Zhang",
            "Wenxuan Shi",
            "Xiaoguang Li",
            "Qun Liu",
            "Sujian Li"
        ],
        "submitted": "2025-06-29 20:22:49",
        "source": "arxiv",
        "comment": "ACL 2025 Main Conference",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on generating Wikipedia articles using a hierarchical memory architecture, which is not directly related to information retrieval, search technologies, or query understanding. While it involves processing and organizing information, the context is different from the user's primary research interests."
    },
    {
        "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties",
        "abstract": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.",
        "url": "http://arxiv.org/abs/2506.23367v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23367v1",
        "arxiv_id": "2506.23367v1",
        "authors": [
            "Paige TuttÃ¶sÃ­",
            "H. Henny Yeung",
            "Yue Wang",
            "Jean-Julien Aucouturier",
            "Angelica Lim"
        ],
        "submitted": "2025-06-29 18:55:05",
        "source": "arxiv",
        "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of text-to-speech systems for second language speakers is outside your primary focus and does not align with your research themes."
    },
    {
        "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)",
        "abstract": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.",
        "url": "http://arxiv.org/abs/2506.23315v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23315v1",
        "arxiv_id": "2506.23315v1",
        "authors": [
            "Shouvon Sarker",
            "Xishuang Dong",
            "Lijun Qian"
        ],
        "submitted": "2025-06-29 16:17:17",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific application of BERT in the clinical domain, using electronic health records, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves natural language processing, the context is distinct from the user's primary research interests."
    },
    {
        "title": "Two Spelling Normalization Approaches Based on Large Language Models",
        "abstract": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.",
        "url": "http://arxiv.org/abs/2506.23288v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23288v1",
        "arxiv_id": "2506.23288v1",
        "authors": [
            "Miguel Domingo",
            "Francisco Casacuberta"
        ],
        "submitted": "2025-06-29 15:25:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on spelling normalization in historical documents, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models, the context and application are quite different from the user's areas of focus."
    },
    {
        "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy",
        "abstract": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.",
        "url": "http://arxiv.org/abs/2506.23149v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23149v1",
        "arxiv_id": "2506.23149v1",
        "authors": [
            "Dingzirui Wang",
            "Xuanliang Zhang",
            "Keyan Xu",
            "Qingfu Zhu",
            "Wanxiang Che",
            "Yang Deng"
        ],
        "submitted": "2025-06-29 08:57:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on synthesizing demonstrations from scratch for arbitrary tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of consistency and diversity, it does not explicitly address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research."
    },
    {
        "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format",
        "abstract": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.",
        "url": "http://arxiv.org/abs/2506.23133v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23133v1",
        "arxiv_id": "2506.23133v1",
        "authors": [
            "Dingzirui Wang",
            "Xuanliang Zhang",
            "Rongyu Cao",
            "Longxu Dou",
            "Xianzhen Luo",
            "Yingwei Ma",
            "Qingfu Zhu",
            "Wanxiang Che",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "submitted": "2025-06-29 08:11:52",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the idea of adapting formats for large language models (LLMs) to improve their reasoning capability, which is related to query understanding and ranking models. However, the focus is on generating and selecting formats rather than query understanding or ranking models, making it somewhat relevant to the user's research interests."
    },
    {
        "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models",
        "abstract": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.",
        "url": "http://arxiv.org/abs/2506.23122v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23122v1",
        "arxiv_id": "2506.23122v1",
        "authors": [
            "Shivam Sharma",
            "Tanmoy Chakraborty"
        ],
        "submitted": "2025-06-29 07:12:11",
        "source": "arxiv",
        "comment": "This work has been submitted to the IEEE for possible publication",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The topic of meme analysis and narrative role classification is outside your primary focus areas of IR, NLP, and data mining."
    }
]