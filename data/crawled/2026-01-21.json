[
    {
        "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
        "abstract": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.",
        "url": "http://arxiv.org/abs/2601.14249v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14249v1",
        "arxiv_id": "2601.14249v1",
        "authors": [
            "Yuming Yang",
            "Mingyoung Lai",
            "Wanxu Zhao",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Mingqi Wu",
            "Chiyue Huang",
            "Jun Zhao",
            "Haijun Lv",
            "Jian Tong",
            "Yunhua Zhou",
            "Yicheng Zou",
            "Qipeng Guo",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "submitted": "2026-01-20 18:58:10",
        "source": "arxiv",
        "comment": "26 pages. Project page: https://github.com/UmeanNever/RankSurprisalRatio"
    },
    {
        "title": "XR: Cross-Modal Agents for Composed Image Retrieval",
        "abstract": "Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.",
        "url": "http://arxiv.org/abs/2601.14245v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14245v1",
        "arxiv_id": "2601.14245v1",
        "authors": [
            "Zhongyu Yang",
            "Wei Pang",
            "Yingfang Yuan"
        ],
        "submitted": "2026-01-20 18:57:00",
        "source": "arxiv",
        "comment": "Accepted by WWW 2026. Project: https://01yzzyu.github.io/xr.github.io/"
    },
    {
        "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
        "abstract": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.",
        "url": "http://arxiv.org/abs/2601.14243v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14243v1",
        "arxiv_id": "2601.14243v1",
        "authors": [
            "Haocheng Xi",
            "Charlie Ruan",
            "Peiyuan Liao",
            "Yujun Lin",
            "Han Cai",
            "Yilong Zhao",
            "Shuo Yang",
            "Kurt Keutzer",
            "Song Han",
            "Ligeng Zhu"
        ],
        "submitted": "2026-01-20 18:54:31",
        "source": "arxiv",
        "comment": "11 pages, 6 figures, 4 tables"
    },
    {
        "title": "APEX-Agents",
        "abstract": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.",
        "url": "http://arxiv.org/abs/2601.14242v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14242v1",
        "arxiv_id": "2601.14242v1",
        "authors": [
            "Bertie Vidgen",
            "Austin Mann",
            "Abby Fennelly",
            "John Wright Stanly",
            "Lucas Rothman",
            "Marco Burstein",
            "Julien Benchek",
            "David Ostrofsky",
            "Anirudh Ravichandran",
            "Debnil Sur",
            "Neel Venugopal",
            "Alannah Hsia",
            "Isaac Robinson",
            "Calix Huang",
            "Olivia Varones",
            "Daniyal Khan",
            "Michael Haines",
            "Zach Richards",
            "Chirag Mahapatra",
            "Brendan Foody",
            "Osvald Nitski"
        ],
        "submitted": "2026-01-20 18:53:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems",
        "abstract": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.",
        "url": "http://arxiv.org/abs/2601.14230v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14230v1",
        "arxiv_id": "2601.14230v1",
        "authors": [
            "Yiyang Wang",
            "Yiqiao Jin",
            "Alex Cabral",
            "Josiah Hester"
        ],
        "submitted": "2026-01-20 18:44:04",
        "source": "arxiv",
        "comment": "15 pages, 9 figures"
    },
    {
        "title": "Rerank Before You Reason: Analyzing Reranking Tradeoffs through Effective Token Cost in Deep Search Agents",
        "abstract": "Deep research agents rely on iterative retrieval and reasoning to answer complex queries, but scaling test-time computation raises significant efficiency concerns. We study how to allocate reasoning budget in deep search pipelines, focusing on the role of listwise reranking. Using the BrowseComp-Plus benchmark, we analyze tradeoffs between model scale, reasoning effort, reranking depth, and total token cost via a novel effective token cost (ETC) metric. Our results show that reranking consistently improves retrieval and end-to-end accuracy, and that moderate reranking often yields larger gains than increasing search-time reasoning, achieving comparable accuracy at substantially lower cost. All our code is available at https://github.com/texttron/BrowseComp-Plus.git",
        "url": "http://arxiv.org/abs/2601.14224v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14224v1",
        "arxiv_id": "2601.14224v1",
        "authors": [
            "Sahel Sharifymoghaddam",
            "Jimmy Lin"
        ],
        "submitted": "2026-01-20 18:38:35",
        "source": "arxiv",
        "comment": "10 pages, 7 figures"
    },
    {
        "title": "Generalization and Completeness of Stochastic Local Search Algorithms",
        "abstract": "We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.",
        "url": "http://arxiv.org/abs/2601.14212v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14212v1",
        "arxiv_id": "2601.14212v1",
        "authors": [
            "Daniel Loscos",
            "Narciso Marti-Oliet",
            "Ismael Rodriguez"
        ],
        "submitted": "2026-01-20 18:17:45",
        "source": "arxiv",
        "comment": "This paper was published in Swarm and Evolutionary Computation. The present version is the author's accepted manuscript"
    },
    {
        "title": "HALT: Hallucination Assessment via Latent Testing",
        "abstract": "Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.",
        "url": "http://arxiv.org/abs/2601.14210v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14210v1",
        "arxiv_id": "2601.14210v1",
        "authors": [
            "Rohan Bhatnagar",
            "Youran Sun",
            "Chi Andrew Zhang",
            "Yixin Wen",
            "Haizhao Yang"
        ],
        "submitted": "2026-01-20 18:16:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
        "abstract": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
        "url": "http://arxiv.org/abs/2601.14209v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14209v1",
        "arxiv_id": "2601.14209v1",
        "authors": [
            "Matthew Y. R. Yang",
            "Hao Bai",
            "Ian Wu",
            "Gene Yang",
            "Amrith Setlur",
            "Aviral Kumar"
        ],
        "submitted": "2026-01-20 18:15:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
        "abstract": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
        "url": "http://arxiv.org/abs/2601.14192v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14192v1",
        "arxiv_id": "2601.14192v1",
        "authors": [
            "Xiaofang Yang",
            "Lijun Li",
            "Heng Zhou",
            "Tong Zhu",
            "Xiaoye Qu",
            "Yuchen Fan",
            "Qianshan Wei",
            "Rui Ye",
            "Li Kang",
            "Yiran Qin",
            "Zhiqiang Kou",
            "Daizong Liu",
            "Qi Li",
            "Ning Ding",
            "Siheng Chen",
            "Jing Shao"
        ],
        "submitted": "2026-01-20 17:51:56",
        "source": "arxiv",
        "comment": "35 pages, 200 references"
    },
    {
        "title": "ReSearch: A Multi-Stage Machine Learning Framework for Earth Science Data Discovery",
        "abstract": "The rapid expansion of Earth Science data from satellite observations, reanalysis products, and numerical simulations has created a critical bottleneck in scientific discovery, namely identifying relevant datasets for a given research objective.\n  Existing discovery systems are primarily retrieval-centric and struggle to bridge the gap between high-level scientific intent and heterogeneous metadata at scale.\n  We introduce \\textbf{ReSearch}, a multi-stage, reasoning-enhanced search framework that formulates Earth Science data discovery as an iterative process of intent interpretation, high-recall retrieval, and context-aware ranking.\n  ReSearch integrates lexical search, semantic embeddings, abbreviation expansion, and large language model reranking within a unified architecture that explicitly separates recall and precision objectives.\n  To enable realistic evaluation, we construct a literature-grounded benchmark by aligning natural language intent with datasets cited in peer-reviewed Earth Science studies.\n  Experiments demonstrate that ReSearch consistently improves recall and ranking performance over baseline methods, particularly for task-based queries expressing abstract scientific goals.\n  These results underscore the importance of intent-aware, multi-stage search as a foundational capability for reproducible and scalable Earth Science research.",
        "url": "http://arxiv.org/abs/2601.14176v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14176v1",
        "arxiv_id": "2601.14176v1",
        "authors": [
            "Youran Sun",
            "Yixin Wen",
            "Haizhao Yang"
        ],
        "submitted": "2026-01-20 17:27:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A model of errors in transformers",
        "abstract": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.",
        "url": "http://arxiv.org/abs/2601.14175v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14175v1",
        "arxiv_id": "2601.14175v1",
        "authors": [
            "Suvrat Raju",
            "Praneeth Netrapalli"
        ],
        "submitted": "2026-01-20 17:27:03",
        "source": "arxiv",
        "comment": "8+17pages"
    },
    {
        "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
        "abstract": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.",
        "url": "http://arxiv.org/abs/2601.14172v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14172v1",
        "arxiv_id": "2601.14172v1",
        "authors": [
            "Víctor Yeste",
            "Paolo Rosso"
        ],
        "submitted": "2026-01-20 17:25:33",
        "source": "arxiv",
        "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 37 pages, 4 figures,"
    },
    {
        "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law",
        "abstract": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",
        "url": "http://arxiv.org/abs/2601.14160v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14160v1",
        "arxiv_id": "2601.14160v1",
        "authors": [
            "Ali Hamza Bashir",
            "Muhammad Rehan Khalid",
            "Kostadin Cvejoski",
            "Jana Birr",
            "Jule Berghaus",
            "Armin Berger",
            "Sandra Halscheidt",
            "Christian Temath",
            "Rafet Sifa",
            "David Berghaus"
        ],
        "submitted": "2026-01-20 17:11:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Legal Retrieval for Public Defenders",
        "abstract": "AI tools are increasingly suggested as solutions to assist public agencies with heavy workloads. In public defense, where a constitutional right to counsel meets the complexities of law, overwhelming caseloads and constrained resources, practitioners face especially taxing conditions. Yet, there is little evidence of how AI could meaningfully support defenders' day-to-day work. In partnership with the New Jersey Office of the Public Defender, we develop the NJ BriefBank, a retrieval tool which surfaces relevant appellate briefs to streamline legal research and writing. We show that existing legal retrieval benchmarks fail to transfer to public defense search, however adding domain knowledge improves retrieval quality. This includes query expansion with legal reasoning, domain-specific data and curated synthetic examples. To facilitate further research, we provide a taxonomy of realistic defender search queries and release a manually annotated public defense retrieval dataset. Together, our work offers starting points towards building practical, reliable retrieval AI tools for public defense, and towards more realistic legal retrieval benchmarks.",
        "url": "http://arxiv.org/abs/2601.14348v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14348v1",
        "arxiv_id": "2601.14348v1",
        "authors": [
            "Dominik Stammbach",
            "Kylie Zhang",
            "Patty Liu",
            "Nimra Nadeem",
            "Lucia Zheng",
            "Peter Henderson"
        ],
        "submitted": "2026-01-20 17:08:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
        "abstract": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
        "url": "http://arxiv.org/abs/2601.14152v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14152v1",
        "arxiv_id": "2601.14152v1",
        "authors": [
            "Hyunjong Ok",
            "Jaeho Lee"
        ],
        "submitted": "2026-01-20 16:54:22",
        "source": "arxiv",
        "comment": "preprint"
    },
    {
        "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
        "abstract": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.",
        "url": "http://arxiv.org/abs/2601.14127v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14127v1",
        "arxiv_id": "2601.14127v1",
        "authors": [
            "Renmiao Chen",
            "Yida Lu",
            "Shiyao Cui",
            "Xuan Ouyang",
            "Victor Shea-Jay Huang",
            "Shumin Zhang",
            "Chengwei Pan",
            "Han Qiu",
            "Minlie Huang"
        ],
        "submitted": "2026-01-20 16:24:18",
        "source": "arxiv",
        "comment": "*15 pages, 5 figures. Introduces MIR-SafetyBench (2,676 instances; 9 multi-image relations). Equal contribution; †Corresponding author. Code/data: https://github.com/thu-coai/MIR-SafetyBench"
    },
    {
        "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic",
        "abstract": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.",
        "url": "http://arxiv.org/abs/2601.14124v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14124v1",
        "arxiv_id": "2601.14124v1",
        "authors": [
            "Saad Mankarious",
            "Aya Zirikly"
        ],
        "submitted": "2026-01-20 16:21:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Systematic Analysis of Chunking Strategies for Reliable Question Answering",
        "abstract": "We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a \"context cliff\" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).",
        "url": "http://arxiv.org/abs/2601.14123v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14123v1",
        "arxiv_id": "2601.14123v1",
        "authors": [
            "Sofia Bennani",
            "Charles Moslonka"
        ],
        "submitted": "2026-01-20 16:19:58",
        "source": "arxiv",
        "comment": "3 pages, 2 figures, 1 table, pre-print"
    },
    {
        "title": "NewsRECON: News article REtrieval for image CONtextualization",
        "abstract": "Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.",
        "url": "http://arxiv.org/abs/2601.14121v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14121v1",
        "arxiv_id": "2601.14121v1",
        "authors": [
            "Jonathan Tonglet",
            "Iryna Gurevych",
            "Tinne Tuytelaars",
            "Marie-Francine Moens"
        ],
        "submitted": "2026-01-20 16:15:53",
        "source": "arxiv",
        "comment": "Preprint under review. Code available at https://github.com/jtonglet/arxiv2025-newsrecon"
    },
    {
        "title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns",
        "abstract": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.",
        "url": "http://arxiv.org/abs/2601.14112v2",
        "pdf_url": "https://arxiv.org/pdf/2601.14112v2",
        "arxiv_id": "2601.14112v2",
        "authors": [
            "George Mihaila"
        ],
        "submitted": "2026-01-20 16:06:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks",
        "abstract": "This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means",
        "url": "http://arxiv.org/abs/2601.14105v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14105v1",
        "arxiv_id": "2601.14105v1",
        "authors": [
            "Olesya Razuvayevskaya",
            "Kalina Bontcheva"
        ],
        "submitted": "2026-01-20 16:04:09",
        "source": "arxiv",
        "comment": "In Proceedings of the ACM Web Conference 2026 (WWW 2026)"
    },
    {
        "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning",
        "abstract": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",
        "url": "http://arxiv.org/abs/2601.14084v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14084v1",
        "arxiv_id": "2601.14084v1",
        "authors": [
            "Abdurrahim Yilmaz",
            "Ozan Erdem",
            "Ece Gokyayla",
            "Ayda Acar",
            "Burc Bugra Dagtas",
            "Dilara Ilhan Erdil",
            "Gulsum Gencoglan",
            "Burak Temelkuran"
        ],
        "submitted": "2026-01-20 15:44:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs",
        "abstract": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.",
        "url": "http://arxiv.org/abs/2601.14063v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14063v1",
        "arxiv_id": "2601.14063v1",
        "authors": [
            "Mohsinul Kabir",
            "Tasnim Ahmed",
            "Md Mezbaur Rahman",
            "Shaoxiong Ji",
            "Hassan Alhuzali",
            "Sophia Ananiadou"
        ],
        "submitted": "2026-01-20 15:21:18",
        "source": "arxiv",
        "comment": "30 Pages, 13 Figures"
    },
    {
        "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models",
        "abstract": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.",
        "url": "http://arxiv.org/abs/2601.14051v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14051v1",
        "arxiv_id": "2601.14051v1",
        "authors": [
            "Peter Devine",
            "Mardhiyah Sanni",
            "Farid Adilazuarda",
            "Julieta Gil Loizaga",
            "Barry Haddow"
        ],
        "submitted": "2026-01-20 15:05:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering",
        "abstract": "Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.",
        "url": "http://arxiv.org/abs/2601.14050v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14050v1",
        "arxiv_id": "2601.14050v1",
        "authors": [
            "Yuxin Chen",
            "Zhengzhou Cai",
            "Xiangtian Ji",
            "Weixiang Zhao",
            "An Zhang",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "submitted": "2026-01-20 15:04:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
        "abstract": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.",
        "url": "http://arxiv.org/abs/2601.14046v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14046v1",
        "arxiv_id": "2601.14046v1",
        "authors": [
            "Shikhar Bharadwaj",
            "Chin-Jou Li",
            "Yoonjae Kim",
            "Kwanghee Choi",
            "Eunjung Yeo",
            "Ryan Soh-Eun Shim",
            "Hanyu Zhou",
            "Brendon Boldt",
            "Karen Rosero Jacome",
            "Kalvin Chang",
            "Darsh Agrawal",
            "Keer Xu",
            "Chao-Han Huck Yang",
            "Jian Zhu",
            "Shinji Watanabe",
            "David R. Mortensen"
        ],
        "submitted": "2026-01-20 15:00:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants",
        "abstract": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.",
        "url": "http://arxiv.org/abs/2601.14041v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14041v1",
        "arxiv_id": "2601.14041v1",
        "authors": [
            "Yunhe Wang",
            "Kai Han",
            "Huiling Zhen",
            "Yuchuan Tian",
            "Hanting Chen",
            "Yongbing Huang",
            "Yufei Cui",
            "Yingte Shu",
            "Shan Gao",
            "Ismail Elezi",
            "Roy Vaughan Miles",
            "Songcen Xu",
            "Feng Wen",
            "Chao Xu",
            "Sinan Zeng",
            "Dacheng Tao"
        ],
        "submitted": "2026-01-20 14:58:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RM-Distiller: Exploiting Generative LLM for Reward Model Distillation",
        "abstract": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.",
        "url": "http://arxiv.org/abs/2601.14032v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14032v1",
        "arxiv_id": "2601.14032v1",
        "authors": [
            "Hongli Zhou",
            "Hui Huang",
            "Wei Liu",
            "Chenglong Wang",
            "Xingyuan Bu",
            "Lvyuan Han",
            "Fuhai Song",
            "Muyun Yang",
            "Wenhao Jiang",
            "Hailong Cao",
            "Tiejun Zhao"
        ],
        "submitted": "2026-01-20 14:53:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models",
        "abstract": "Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.",
        "url": "http://arxiv.org/abs/2601.14007v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14007v1",
        "arxiv_id": "2601.14007v1",
        "authors": [
            "Junyu Zhang",
            "Yipeng Kang",
            "Jiong Guo",
            "Jiayu Zhan",
            "Junqi Wang"
        ],
        "submitted": "2026-01-20 14:26:58",
        "source": "arxiv",
        "comment": "34 pagess, 16 figures, 6 tables, submitted to ACL 2026"
    },
    {
        "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
        "abstract": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.",
        "url": "http://arxiv.org/abs/2601.14004v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14004v1",
        "arxiv_id": "2601.14004v1",
        "authors": [
            "Hengyuan Zhang",
            "Zhihao Zhang",
            "Mingyang Wang",
            "Zunhai Su",
            "Yiwei Wang",
            "Qianli Wang",
            "Shuzhou Yuan",
            "Ercong Nie",
            "Xufeng Duan",
            "Qibo Xue",
            "Zeping Yu",
            "Chenming Shang",
            "Xiao Liang",
            "Jing Xiong",
            "Hui Shen",
            "Chaofan Tao",
            "Zhengwu Liu",
            "Senjie Jin",
            "Zhiheng Xi",
            "Dongdong Zhang",
            "Sophia Ananiadou",
            "Tao Gui",
            "Ruobing Xie",
            "Hayden Kwok-Hay So",
            "Hinrich Schütze",
            "Xuanjing Huang",
            "Qi Zhang",
            "Ngai Wong"
        ],
        "submitted": "2026-01-20 14:23:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval",
        "abstract": "Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR",
        "url": "http://arxiv.org/abs/2601.14001v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14001v1",
        "arxiv_id": "2601.14001v1",
        "authors": [
            "Niall McGuire",
            "Yashar Moshfeghi"
        ],
        "submitted": "2026-01-20 14:22:41",
        "source": "arxiv",
        "comment": "Accepted At ECIR 2026"
    },
    {
        "title": "From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning",
        "abstract": "Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \\textbf{+5.84\\%} using only \\textbf{5\\%} of the data, while our aligned sampling strategy further boosts average performance by \\textbf{+4.24\\%}.",
        "url": "http://arxiv.org/abs/2601.13995v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13995v1",
        "arxiv_id": "2601.13995v1",
        "authors": [
            "Zihan Niu",
            "Wenping Hu",
            "Junmin Chen",
            "Xiyue Wang",
            "Tong Xu",
            "Ruiming Tang"
        ],
        "submitted": "2026-01-20 14:06:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework",
        "abstract": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.",
        "url": "http://arxiv.org/abs/2601.13992v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13992v1",
        "arxiv_id": "2601.13992v1",
        "authors": [
            "Jin Cui",
            "Jiaqi Guo",
            "Jiepeng Zhou",
            "Ruixuan Yang",
            "Jiayi Lu",
            "Jiajun Xu",
            "Jiangcheng Song",
            "Boran Zhao",
            "Pengju Ren"
        ],
        "submitted": "2026-01-20 14:05:19",
        "source": "arxiv",
        "comment": "11pages, 9figures"
    },
    {
        "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
        "abstract": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
        "url": "http://arxiv.org/abs/2601.13969v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13969v1",
        "arxiv_id": "2601.13969v1",
        "authors": [
            "Joaquín Polonuer",
            "Lucas Vittor",
            "Iñaki Arango",
            "Ayush Noori",
            "David A. Clifton",
            "Luciano Del Corro",
            "Marinka Zitnik"
        ],
        "submitted": "2026-01-20 13:46:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization",
        "abstract": "As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a \"diverge-then-converge\" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.",
        "url": "http://arxiv.org/abs/2601.13938v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13938v1",
        "arxiv_id": "2601.13938v1",
        "authors": [
            "Heyang Zhou",
            "JiaJia Chen",
            "Xiaolu Chen",
            "Jie Bao",
            "Zhen Chen",
            "Yong Liao"
        ],
        "submitted": "2026-01-20 13:13:39",
        "source": "arxiv",
        "comment": "9 pages, 3 figures. Submitted to ACL 2026. Corresponding author: Zhen Chen"
    },
    {
        "title": "Generating consensus and dissent on massive discussion platforms with an $O(N)$ semantic-vector model",
        "abstract": "Reaching consensus on massive discussion networks is critical for reducing noise and achieving optimal collective outcomes. However, the natural tendency of humans to preserve their initial ideas constrains the emergence of global solutions. To address this, Collective Intelligence (CI) platforms facilitate the discovery of globally superior solutions. We introduce a dynamical system based on the standard $O(N)$ model to drive the aggregation of semantically similar ideas. The system consists of users represented as nodes in a $d=2$ lattice with nearest-neighbor interactions, where their ideas are represented by semantic vectors computed with a pretrained embedding model. We analyze the system's equilibrium states as a function of the coupling parameter $β$. Our results show that $β> 0$ drives the system toward a ferromagnetic-like phase (global consensus), while $β< 0$ induces an antiferromagnetic-like state (maximum dissent), where users maximize semantic distance from their neighbors. This framework offers a controllable method for managing the tradeoff between cohesion and diversity in CI platforms.",
        "url": "http://arxiv.org/abs/2601.13932v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13932v1",
        "arxiv_id": "2601.13932v1",
        "authors": [
            "A. Ferrer",
            "D. Muñoz-Jordán",
            "A. Rivero",
            "A. Tarancón",
            "C. Tarancón",
            "D. Yllanes"
        ],
        "submitted": "2026-01-20 13:08:00",
        "source": "arxiv",
        "comment": "9 pages, 8 figures"
    },
    {
        "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
        "abstract": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
        "url": "http://arxiv.org/abs/2601.13931v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13931v1",
        "arxiv_id": "2601.13931v1",
        "authors": [
            "Yannis Vasilakis",
            "Rachel Bittner",
            "Johan Pauwels"
        ],
        "submitted": "2026-01-20 13:06:48",
        "source": "arxiv",
        "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026"
    },
    {
        "title": "Automatic Prompt Optimization for Dataset-Level Feature Discovery",
        "abstract": "Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.",
        "url": "http://arxiv.org/abs/2601.13922v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13922v1",
        "arxiv_id": "2601.13922v1",
        "authors": [
            "Adrian Cosma",
            "Oleg Szehr",
            "David Kletz",
            "Alessandro Antonucci",
            "Olivier Pelletier"
        ],
        "submitted": "2026-01-20 12:51:03",
        "source": "arxiv",
        "comment": "5 Figures, 1 Table"
    },
    {
        "title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs",
        "abstract": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker",
        "url": "http://arxiv.org/abs/2601.13919v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13919v1",
        "arxiv_id": "2601.13919v1",
        "authors": [
            "Yuezhe Yang",
            "Hao Wang",
            "Yige Peng",
            "Jinman Kim",
            "Lei Bi"
        ],
        "submitted": "2026-01-20 12:48:09",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
        "abstract": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.",
        "url": "http://arxiv.org/abs/2601.13918v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13918v1",
        "arxiv_id": "2601.13918v1",
        "authors": [
            "Yusheng Liao",
            "Chuan Xuan",
            "Yutong Cai",
            "Lina Yang",
            "Zhe Chen",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "submitted": "2026-01-20 12:48:04",
        "source": "arxiv",
        "comment": "37 pages, 12 figures"
    },
    {
        "title": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores",
        "abstract": "Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.",
        "url": "http://arxiv.org/abs/2601.13885v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13885v1",
        "arxiv_id": "2601.13885v1",
        "authors": [
            "Esma Balkır",
            "Alice Pernthaller",
            "Marco Basaldella",
            "José Hernández-Orallo",
            "Nigel Collier"
        ],
        "submitted": "2026-01-20 11:59:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models",
        "abstract": "Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.",
        "url": "http://arxiv.org/abs/2601.13882v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13882v1",
        "arxiv_id": "2601.13882v1",
        "authors": [
            "Unggi Lee",
            "Sookbun Lee",
            "Heungsoo Choi",
            "Jinseo Lee",
            "Haeun Park",
            "Younghoon Jeon",
            "Sungmin Cho",
            "Minju Kang",
            "Junbo Koh",
            "Jiyeong Bae",
            "Minwoo Nam",
            "Juyeon Eun",
            "Yeonji Jung",
            "Yeil Jeong"
        ],
        "submitted": "2026-01-20 11:53:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring",
        "abstract": "While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.",
        "url": "http://arxiv.org/abs/2601.13879v2",
        "pdf_url": "https://arxiv.org/pdf/2601.13879v2",
        "arxiv_id": "2601.13879v2",
        "authors": [
            "Dongxu Zhang",
            "Yiding Sun",
            "Cheng Tan",
            "Wenbiao Yan",
            "Ning Yang",
            "Jihua Zhu",
            "Haijun Zhang"
        ],
        "submitted": "2026-01-20 11:45:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education",
        "abstract": "Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \\textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.",
        "url": "http://arxiv.org/abs/2601.13876v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13876v1",
        "arxiv_id": "2601.13876v1",
        "authors": [
            "Unggi Lee",
            "Jahyun Jeong",
            "Sunyoung Shin",
            "Haeun Park",
            "Jeongsu Moon",
            "Youngchang Song",
            "Jaechang Shim",
            "JaeHwan Lee",
            "Yunju Noh",
            "Seungwon Choi",
            "Ahhyun Kim",
            "TaeHyeon Kim",
            "Kyungtae Joo",
            "Taeyeong Kim",
            "Gyeonggeon Lee"
        ],
        "submitted": "2026-01-20 11:43:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QKVQA: Question-Focused Filtering for Knowledge-based VQA",
        "abstract": "Knowledge-based Visual Question Answering (KB-VQA) aims to answer questions by integrating images with external knowledge. Effective knowledge filtering is crucial for improving accuracy. Typical filtering methods use similarity metrics to locate relevant article sections from one article, leading to information selection errors at the article and intra-article levels. Although recent explorations of Multimodal Large Language Model (MLLM)-based filtering methods demonstrate superior semantic understanding and cross-article filtering capabilities, their high computational cost limits practical application. To address these issues, this paper proposes a question-focused filtering method. This approach can perform question-focused, cross-article filtering, efficiently obtaining high-quality filtered knowledge while keeping computational costs comparable to typical methods. Specifically, we design a trainable Question-Focused Filter (QFF) and a Chunk-based Dynamic Multi-Article Selection (CDA) module, which collectively alleviate information selection errors at both the article and intra-article levels. Experiments show that our method outperforms current state-of-the-art models by 4.9% on E-VQA and 3.8% on InfoSeek, validating its effectiveness. The code is publicly available at: https://github.com/leaffeall/QKVQA.",
        "url": "http://arxiv.org/abs/2601.13856v2",
        "pdf_url": "https://arxiv.org/pdf/2601.13856v2",
        "arxiv_id": "2601.13856v2",
        "authors": [
            "Wei Ye",
            "Yixin Su",
            "Yueguo Chen",
            "Longxiang Gao",
            "Jianjun Li",
            "Ruixuan Li",
            "Rui Zhang"
        ],
        "submitted": "2026-01-20 11:08:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
        "abstract": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).",
        "url": "http://arxiv.org/abs/2601.13836v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13836v1",
        "arxiv_id": "2601.13836v1",
        "authors": [
            "Qian Chen",
            "Jinlan Fu",
            "Changsong Li",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "submitted": "2026-01-20 10:47:20",
        "source": "arxiv",
        "comment": "https://openmoss.github.io/FutureOmni"
    },
    {
        "title": "The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations",
        "abstract": "Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.",
        "url": "http://arxiv.org/abs/2601.13835v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13835v1",
        "arxiv_id": "2601.13835v1",
        "authors": [
            "Sam OConnor Russell",
            "Delphine Charuau",
            "Naomi Harte"
        ],
        "submitted": "2026-01-20 10:45:12",
        "source": "arxiv",
        "comment": "Accepted to ICASSP 2026"
    },
    {
        "title": "Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning",
        "abstract": "LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \\textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.",
        "url": "http://arxiv.org/abs/2601.13806v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13806v1",
        "arxiv_id": "2601.13806v1",
        "authors": [
            "Dezhao Song",
            "Guglielmo Bonifazi",
            "Frank Schilder",
            "Jonathan Richard Schwarz"
        ],
        "submitted": "2026-01-20 10:06:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis",
        "abstract": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .",
        "url": "http://arxiv.org/abs/2601.13802v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13802v1",
        "arxiv_id": "2601.13802v1",
        "authors": [
            "Yushen Chen",
            "Junzhe Liu",
            "Yujie Tu",
            "Zhikang Niu",
            "Yuzhe Liang",
            "Kai Yu",
            "Chunyu Qiang",
            "Chen Zhang",
            "Xie Chen"
        ],
        "submitted": "2026-01-20 10:02:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance",
        "abstract": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench",
        "url": "http://arxiv.org/abs/2601.13770v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13770v1",
        "arxiv_id": "2601.13770v1",
        "authors": [
            "Mostapha Benhenda"
        ],
        "submitted": "2026-01-20 09:23:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
        "abstract": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations. The code is available at https://github.com/RUCBM/DARC.",
        "url": "http://arxiv.org/abs/2601.13761v2",
        "pdf_url": "https://arxiv.org/pdf/2601.13761v2",
        "arxiv_id": "2601.13761v2",
        "authors": [
            "Shengda Fan",
            "Xuyan Ye",
            "Yankai Lin"
        ],
        "submitted": "2026-01-20 09:12:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering",
        "abstract": "Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \\textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.",
        "url": "http://arxiv.org/abs/2601.13752v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13752v1",
        "arxiv_id": "2601.13752v1",
        "authors": [
            "Chak Tou Leong",
            "Dingwei Chen",
            "Heming Xia",
            "Qingyu Yin",
            "Sunbowen Lee",
            "Jian Wang",
            "Wenjie Li"
        ],
        "submitted": "2026-01-20 09:07:01",
        "source": "arxiv",
        "comment": "Working in progress"
    },
    {
        "title": "Pro-AI Bias in Large Language Models",
        "abstract": "Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.",
        "url": "http://arxiv.org/abs/2601.13749v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13749v1",
        "arxiv_id": "2601.13749v1",
        "authors": [
            "Benaya Trabelsi",
            "Jonathan Shaki",
            "Sarit Kraus"
        ],
        "submitted": "2026-01-20 09:03:57",
        "source": "arxiv",
        "comment": "13 pages, 6 figures. Code available at: https://github.com/benayat/Pro-AI-bias-in-LLMs"
    },
    {
        "title": "Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues",
        "abstract": "Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.",
        "url": "http://arxiv.org/abs/2601.13742v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13742v1",
        "arxiv_id": "2601.13742v1",
        "authors": [
            "Arjun Chandra",
            "Kevin Miller",
            "Venkatesh Ravichandran",
            "Constantinos Papayiannis",
            "Venkatesh Saligrama"
        ],
        "submitted": "2026-01-20 08:57:02",
        "source": "arxiv",
        "comment": "EACL 2026 Findings"
    },
    {
        "title": "Towards robust long-context understanding of large language model via active recap learning",
        "abstract": "In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM",
        "url": "http://arxiv.org/abs/2601.13734v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13734v1",
        "arxiv_id": "2601.13734v1",
        "authors": [
            "Chenyu Hui"
        ],
        "submitted": "2026-01-20 08:42:04",
        "source": "arxiv",
        "comment": "5 pages"
    },
    {
        "title": "On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation",
        "abstract": "In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.",
        "url": "http://arxiv.org/abs/2601.13729v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13729v1",
        "arxiv_id": "2601.13729v1",
        "authors": [
            "Weichuan Wang",
            "Mingyang Liu",
            "Linqi Song",
            "Chen Ma"
        ],
        "submitted": "2026-01-20 08:39:10",
        "source": "arxiv",
        "comment": "9 pages, 12 figures"
    },
    {
        "title": "Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models",
        "abstract": "Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.",
        "url": "http://arxiv.org/abs/2601.14327v1",
        "pdf_url": "https://arxiv.org/pdf/2601.14327v1",
        "arxiv_id": "2601.14327v1",
        "authors": [
            "YuanLab. ai",
            "Shawn Wu",
            "Jiangang Luo",
            "Tong Yu",
            "Darcy Chen",
            "Sean Wang",
            "Xudong Zhao",
            "Louie Li",
            "Claire Wang",
            "Hunter He",
            "Carol Wang",
            "Allen Wang"
        ],
        "submitted": "2026-01-20 08:39:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents",
        "abstract": "Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \\emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \\textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \\textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \\textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.",
        "url": "http://arxiv.org/abs/2601.13722v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13722v1",
        "arxiv_id": "2601.13722v1",
        "authors": [
            "Yulin Hu",
            "Zimo Long",
            "Jiahe Guo",
            "Xingyu Sui",
            "Xing Fu",
            "Weixiang Zhao",
            "Yanyan Zhao",
            "Bing Qin"
        ],
        "submitted": "2026-01-20 08:27:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search",
        "abstract": "Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.",
        "url": "http://arxiv.org/abs/2601.13719v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13719v1",
        "arxiv_id": "2601.13719v1",
        "authors": [
            "Xinlei Yin",
            "Xiulian Peng",
            "Xiao Li",
            "Zhiwei Xiong",
            "Yan Lu"
        ],
        "submitted": "2026-01-20 08:23:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff",
        "abstract": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.",
        "url": "http://arxiv.org/abs/2601.13717v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13717v1",
        "arxiv_id": "2601.13717v1",
        "authors": [
            "Zehan Li",
            "Yuxuan Wang",
            "Ali El Lahib",
            "Ying-Jieh Xia",
            "Xinyu Pi"
        ],
        "submitted": "2026-01-20 08:21:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark",
        "abstract": "Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.",
        "url": "http://arxiv.org/abs/2601.13711v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13711v1",
        "arxiv_id": "2601.13711v1",
        "authors": [
            "Lotta Kiefer",
            "Christoph Leiter",
            "Sotaro Takeshita",
            "Elena Schmidt",
            "Steffen Eger"
        ],
        "submitted": "2026-01-20 08:08:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction",
        "abstract": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.",
        "url": "http://arxiv.org/abs/2601.13710v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13710v1",
        "arxiv_id": "2601.13710v1",
        "authors": [
            "Sayeed Shafayet Chowdhury",
            "Snehasis Mukhopadhyay",
            "Shiaofen Fang",
            "Vijay R. Ramakrishnan"
        ],
        "submitted": "2026-01-20 08:07:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games",
        "abstract": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.",
        "url": "http://arxiv.org/abs/2601.13709v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13709v1",
        "arxiv_id": "2601.13709v1",
        "authors": [
            "Christopher Kao",
            "Vanshika Vats",
            "James Davis"
        ],
        "submitted": "2026-01-20 08:07:21",
        "source": "arxiv",
        "comment": "For associated dataset, see https://github.com/cocochief4/llm-mafia. Published in IEEE ICA 2025, waiting for IEEEXplore proceedings"
    },
    {
        "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
        "abstract": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
        "url": "http://arxiv.org/abs/2601.13697v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13697v1",
        "arxiv_id": "2601.13697v1",
        "authors": [
            "Zhihang Yuan",
            "Chengyu Yue",
            "Long Huang",
            "Litu Ou",
            "Lei Shi"
        ],
        "submitted": "2026-01-20 07:51:32",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "OptiSQL: Executable SQL Generation from Optical Tokens",
        "abstract": "Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.",
        "url": "http://arxiv.org/abs/2601.13695v2",
        "pdf_url": "https://arxiv.org/pdf/2601.13695v2",
        "arxiv_id": "2601.13695v2",
        "authors": [
            "Sifan Li",
            "Hongkai Chen",
            "Yujun Cai",
            "Liyang Chen",
            "Qingwen Ye",
            "Yiwei Wang"
        ],
        "submitted": "2026-01-20 07:49:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning",
        "abstract": "Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.",
        "url": "http://arxiv.org/abs/2601.13690v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13690v1",
        "arxiv_id": "2601.13690v1",
        "authors": [
            "Yue Guo",
            "Fanfu Wang",
            "Jianwei Lv",
            "Xincheng Shi",
            "Yuchen Li",
            "Youya Wang",
            "Yunsheng Zeng",
            "Yujing Liu",
            "Yunhao Qiao",
            "Gen Li",
            "Junfeng Wang",
            "Bo Yuan"
        ],
        "submitted": "2026-01-20 07:43:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference",
        "abstract": "The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\\times$ compared to the original model in the 224K context. Our code will be open-source.",
        "url": "http://arxiv.org/abs/2601.13684v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13684v1",
        "arxiv_id": "2601.13684v1",
        "authors": [
            "Zhiyuan Shi",
            "Qibo Qiu",
            "Feng Xue",
            "Zhonglin Jiang",
            "Li Yu",
            "Jian Jiang",
            "Xiaofei He",
            "Wenxiao Wang"
        ],
        "submitted": "2026-01-20 07:35:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks",
        "abstract": "Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a \"middle ground\". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.",
        "url": "http://arxiv.org/abs/2601.13669v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13669v1",
        "arxiv_id": "2601.13669v1",
        "authors": [
            "Jiayu Lin",
            "Zhongyu Wei"
        ],
        "submitted": "2026-01-20 07:10:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis",
        "abstract": "Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.",
        "url": "http://arxiv.org/abs/2601.13659v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13659v1",
        "arxiv_id": "2601.13659v1",
        "authors": [
            "Chunlei Meng",
            "Ziyang Zhou",
            "Lucas He",
            "Xiaojing Du",
            "Chun Ouyang",
            "Zhongxue Gan"
        ],
        "submitted": "2026-01-20 06:50:40",
        "source": "arxiv",
        "comment": "This study has been accepted by IEEE ICASSP2026"
    },
    {
        "title": "Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation",
        "abstract": "The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.",
        "url": "http://arxiv.org/abs/2601.13658v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13658v1",
        "arxiv_id": "2601.13658v1",
        "authors": [
            "Arthur Amalvy",
            "Hen-Hsen Huang"
        ],
        "submitted": "2026-01-20 06:48:42",
        "source": "arxiv",
        "comment": "12 pages"
    },
    {
        "title": "Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge",
        "abstract": "Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.",
        "url": "http://arxiv.org/abs/2601.13649v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13649v1",
        "arxiv_id": "2601.13649v1",
        "authors": [
            "Xiaolin Zhou",
            "Zheng Luo",
            "Yicheng Gao",
            "Qixuan Chen",
            "Xiyang Hu",
            "Yue Zhao",
            "Ruishan Liu"
        ],
        "submitted": "2026-01-20 06:33:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Token-Level Text Anomaly Detection",
        "abstract": "Despite significant progress in text anomaly detection for web applications such as spam filtering and fake news detection, existing methods are fundamentally limited to document-level analysis, unable to identify which specific parts of a text are anomalous. We introduce token-level anomaly detection, a novel paradigm that enables fine-grained localization of anomalies within text. We formally define text anomalies at both document and token-levels, and propose a unified detection framework that operates across multiple levels. To facilitate research in this direction, we collect and annotate three benchmark datasets spanning spam, reviews and grammar errors with token-level labels. Experimental results demonstrate that our framework get better performance than other 6 baselines, opening new possibilities for precise anomaly localization in text. All the codes and data are publicly available on https://github.com/charles-cao/TokenCore.",
        "url": "http://arxiv.org/abs/2601.13644v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13644v1",
        "arxiv_id": "2601.13644v1",
        "authors": [
            "Yang Cao",
            "Bicheng Yu",
            "Sikun Yang",
            "Ming Liu",
            "Yujiu Yang"
        ],
        "submitted": "2026-01-20 06:27:09",
        "source": "arxiv",
        "comment": "WWW 2026"
    },
    {
        "title": "Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models",
        "abstract": "Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user's permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query's activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.",
        "url": "http://arxiv.org/abs/2601.13630v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13630v1",
        "arxiv_id": "2601.13630v1",
        "authors": [
            "Zhaopeng Zhang",
            "Pengcheng Sun",
            "Lan Zhang",
            "Chen Tang",
            "Jiewei Lai",
            "Yunhao Wang",
            "Hui Jin"
        ],
        "submitted": "2026-01-20 05:57:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CauScientist: Teaching LLMs to Respect Data for Causal Discovery",
        "abstract": "Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating \"data scientists\" with probabilistic statistics as rigorous \"verifiers\". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.",
        "url": "http://arxiv.org/abs/2601.13614v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13614v1",
        "arxiv_id": "2601.13614v1",
        "authors": [
            "Bo Peng",
            "Sirui Chen",
            "Lei Xu",
            "Chaochao Lu"
        ],
        "submitted": "2026-01-20 05:32:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Balancing Fairness and High Match Rates in Reciprocal Recommender Systems: A Nash Social Welfare Approach",
        "abstract": "Matching platforms, such as online dating services and job recommendations, have become increasingly prevalent. For the success of these platforms, it is crucial to design reciprocal recommender systems (RRSs) that not only increase the total number of matches but also avoid creating unfairness among users. In this paper, we investigate the fairness of RRSs on matching platforms. From the perspective of fair division, we define the users' opportunities to be recommended and establish the fairness concept of envy-freeness in the allocation of these opportunities. We first introduce the Social Welfare (SW) method, which approximately maximizes the number of matches, and show that it leads to significant unfairness in recommendation opportunities, illustrating the trade-off between fairness and match rates. To address this challenge, we propose the Nash Social Welfare (NSW) method, which alternately optimizes two NSW functions and achieves nearly envy-free recommendations. We further generalize the SW and NSW method to the $α$-SW method, which balances the trade-off between fairness and high match rates. Additionally, we develop a computationally efficient approximation algorithm for the SW/NSW/$α$-SW methods based on the Sinkhorn algorithm. Through extensive experiments on both synthetic datasets and two real-world datasets, we demonstrate the practical effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2601.13609v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13609v1",
        "arxiv_id": "2601.13609v1",
        "authors": [
            "Yoji Tomita",
            "Tomohiko Yokoyama"
        ],
        "submitted": "2026-01-20 05:18:25",
        "source": "arxiv",
        "comment": "arXiv admin note: text overlap with arXiv:2409.00720"
    },
    {
        "title": "DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems",
        "abstract": "Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.",
        "url": "http://arxiv.org/abs/2601.13591v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13591v1",
        "arxiv_id": "2601.13591v1",
        "authors": [
            "Maojun Sun",
            "Yifei Xie",
            "Yue Wu",
            "Ruijian Han",
            "Binyan Jiang",
            "Defeng Sun",
            "Yancheng Yuan",
            "Jian Huang"
        ],
        "submitted": "2026-01-20 04:44:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions",
        "abstract": "Large Language Models (LLMs) are increasingly employed in various question-answering tasks. However, recent studies showcase that LLMs are susceptible to persuasion and could adopt counterfactual beliefs. We present a systematic evaluation of LLM susceptibility to persuasion under the Source--Message--Channel--Receiver (SMCR) communication framework. Across five mainstream Large Language Models (LLMs) and three domains (factual knowledge, medical QA, and social bias), we analyze how different persuasive strategies influence belief stability over multiple interaction turns. We further examine whether meta-cognition prompting (i.e., eliciting self-reported confidence) affects resistance to persuasion. Results show that smaller models exhibit extreme compliance, with over 80% of belief changes occurring at the first persuasive turn (average end turn of 1.1--1.4). Contrary to expectations, meta-cognition prompting increases vulnerability by accelerating belief erosion rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and Mistral~7B improves substantially (35.7% $\\rightarrow$ 79.3%), Llama models remain highly susceptible (<14%) even when fine-tuned on their own failure cases. Together, these findings highlight substantial model-dependent limits of current robustness interventions and offer guidance for developing more trustworthy LLMs.",
        "url": "http://arxiv.org/abs/2601.13590v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13590v1",
        "arxiv_id": "2601.13590v1",
        "authors": [
            "Fan Huang",
            "Haewoon Kwak",
            "Jisun An"
        ],
        "submitted": "2026-01-20 04:43:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TREX: Tokenizer Regression for Optimal Data Mixture",
        "abstract": "Building effective tokenizers for multilingual Large Language Models (LLMs) requires careful control over language-specific data mixtures. While a tokenizer's compression performance critically affects the efficiency of LLM training and inference, existing approaches rely on heuristics or costly large-scale searches to determine optimal language ratios. We introduce Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based framework that efficiently predicts the optimal data mixture for tokenizer training. TREX trains small-scale proxy tokenizers on random mixtures, gathers their compression statistics, and learns to predict compression performance from data mixtures. This learned model enables scalable mixture search before large-scale tokenizer training, mitigating the accuracy-cost trade-off in multilingual tokenizer design. Tokenizers trained with TReX's predicted mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to 12% in both inand out-of-distribution compression efficiency, demonstrating strong scalability, robustness, and practical effectiveness.",
        "url": "http://arxiv.org/abs/2601.13588v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13588v1",
        "arxiv_id": "2601.13588v1",
        "authors": [
            "Inho Won",
            "Hangyeol Yoo",
            "Minkyung Cho",
            "Jungyeul Park",
            "Hoyun Song",
            "KyungTae Lim"
        ],
        "submitted": "2026-01-20 04:41:09",
        "source": "arxiv",
        "comment": "Accepted to EACL 2026. Long Paper. (19 languages studied: Chinese, Greek, Japanese, etc.)"
    },
    {
        "title": "Comparing Without Saying: A Dataset and Benchmark for Implicit Comparative Opinion Mining from Same-User Reviews",
        "abstract": "Existing studies on comparative opinion mining have mainly focused on explicit comparative expressions, which are uncommon in real-world reviews. This leaves implicit comparisons - here users express preferences across separate reviews - largely underexplored. We introduce SUDO, a novel dataset for implicit comparative opinion mining from same-user reviews, allowing reliable inference of user preferences even without explicit comparative cues. SUDO comprises 4,150 annotated review pairs (15,191 sentences) with a bi-level structure capturing aspect-level mentions and review-level preferences. We benchmark this task using two baseline architectures: traditional machine learning- and language model-based baselines. Experimental results show that while the latter outperforms the former, overall performance remains moderate, revealing the inherent difficulty of the task and establishing SUDO as a challenging and valuable benchmark for future research.",
        "url": "http://arxiv.org/abs/2601.13575v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13575v1",
        "arxiv_id": "2601.13575v1",
        "authors": [
            "Thanh-Lam T. Nguyen",
            "Ngoc-Quang Le",
            "Quoc-Trung Phu",
            "Thi-Phuong Le",
            "Ngoc-Huyen Pham",
            "Phuong-Nguyen Nguyen",
            "Hoang-Quynh Le"
        ],
        "submitted": "2026-01-20 04:00:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Self-Improvement as Coherence Optimization: A Theoretical Account",
        "abstract": "Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.",
        "url": "http://arxiv.org/abs/2601.13566v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13566v1",
        "arxiv_id": "2601.13566v1",
        "authors": [
            "Tianyi Qiu",
            "Ahmed Hani Ismail",
            "Zhonghao He",
            "Shi Feng"
        ],
        "submitted": "2026-01-20 03:50:02",
        "source": "arxiv",
        "comment": "39 pages"
    },
    {
        "title": "Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis",
        "abstract": "Men who have sex with men (MSM) are at elevated risk for sexually transmitted infections and harmful drinking compared to heterosexual men. Text data collected from social media and dating applications may provide new opportunities for personalized public health interventions by enabling automatic identification of risk and protective behaviors. In this study, we evaluated whether text from social media and dating apps can be used to predict sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake among MSM. With participant consent, we collected textual data and trained machine learning models using features derived from ChatGPT embeddings, BERT embeddings, LIWC, and a dictionary-based risk term approach. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78, and moderate performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63. These findings demonstrate that social media and dating app text data can provide valuable insights into risk and protective behaviors and highlight the potential of large language model-based methods to support scalable and personalized public health interventions for MSM.",
        "url": "http://arxiv.org/abs/2601.13558v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13558v1",
        "arxiv_id": "2601.13558v1",
        "authors": [
            "Mehrab Beikzadeh",
            "Chenglin Hong",
            "Cory J Cascalheira",
            "Callisto Boka",
            "Majid Sarrafzadeh",
            "Ian W Holloway"
        ],
        "submitted": "2026-01-20 03:28:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations",
        "abstract": "Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \\textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \\textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \\textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.\n  \\textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}",
        "url": "http://arxiv.org/abs/2601.13547v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13547v1",
        "arxiv_id": "2601.13547v1",
        "authors": [
            "Yujia Hu",
            "Roy Ka-Wei Lee"
        ],
        "submitted": "2026-01-20 03:13:07",
        "source": "arxiv",
        "comment": "EACL 2026 Main Conference"
    },
    {
        "title": "When Wording Steers the Evaluation: Framing Bias in LLM judges",
        "abstract": "Large language models (LLMs) are known to produce varying responses depending on prompt phrasing, indicating that subtle guidance in phrasing can steer their answers. However, the impact of this framing bias on LLM-based evaluation, where models are expected to make stable and impartial judgments, remains largely underexplored. Drawing inspiration from the framing effect in psychology, we systematically investigate how deliberate prompt framing skews model judgments across four high-stakes evaluation tasks. We design symmetric prompts using predicate-positive and predicate-negative constructions and demonstrate that such framing induces significant discrepancies in model outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with model families showing distinct tendencies toward agreement or rejection. These findings suggest that framing bias is a structural property of current LLM-based evaluation systems, underscoring the need for framing-aware protocols.",
        "url": "http://arxiv.org/abs/2601.13537v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13537v1",
        "arxiv_id": "2601.13537v1",
        "authors": [
            "Yerin Hwang",
            "Dongryeol Lee",
            "Taegwan Kang",
            "Minwoo Lee",
            "Kyomin Jung"
        ],
        "submitted": "2026-01-20 02:48:10",
        "source": "arxiv",
        "comment": "4 pages"
    },
    {
        "title": "Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs",
        "abstract": "Model developers implement safeguards in frontier models to prevent misuse, for example, by employing classifiers to filter dangerous outputs. In this work, we demonstrate that even robustly safeguarded models can be used to elicit harmful capabilities in open-source models through elicitation attacks. Our elicitation attacks consist of three stages: (i) constructing prompts in adjacent domains to a target harmful task that do not request dangerous information; (ii) obtaining responses to these prompts from safeguarded frontier models; (iii) fine-tuning open-source models on these prompt-output pairs. Since the requested prompts cannot be used to directly cause harm, they are not refused by frontier model safeguards. We evaluate these elicitation attacks within the domain of hazardous chemical synthesis and processing, and demonstrate that our attacks recover approximately 40% of the capability gap between the base open-source model and an unrestricted frontier model. We then show that the efficacy of elicitation attacks scales with the capability of the frontier model and the amount of generated fine-tuning data. Our work demonstrates the challenge of mitigating ecosystem level risks with output-level safeguards.",
        "url": "http://arxiv.org/abs/2601.13528v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13528v1",
        "arxiv_id": "2601.13528v1",
        "authors": [
            "Jackson Kaunismaa",
            "Avery Griffin",
            "John Hughes",
            "Christina Q. Knight",
            "Mrinank Sharma",
            "Erik Jones"
        ],
        "submitted": "2026-01-20 02:24:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval",
        "abstract": "Dense retrievers powered by pretrained embeddings are widely used for document retrieval but struggle in specialized domains due to the mismatches between the training and target domain distributions. Domain adaptation typically requires costly annotation and retraining of query-document pairs. In this work, we revisit an overlooked alternative: applying PCA to domain embeddings to derive lower-dimensional representations that preserve domain-relevant features while discarding non-discriminative components. Though traditionally used for efficiency, we demonstrate that this simple embedding compression can effectively improve retrieval performance. Evaluated across 9 retrievers and 14 MTEB datasets, PCA applied solely to query embeddings improves NDCG@10 in 75.4% of model-dataset pairs, offering a simple and lightweight method for domain adaptation.",
        "url": "http://arxiv.org/abs/2601.13525v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13525v1",
        "arxiv_id": "2601.13525v1",
        "authors": [
            "Chunsheng Zuo",
            "Daniel Khashabi"
        ],
        "submitted": "2026-01-20 02:21:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Integrating Vision-Centric Text Understanding for Conversational Recommender Systems",
        "abstract": "Conversational Recommender Systems (CRSs) have attracted growing attention for their ability to deliver personalized recommendations through natural language interactions. To more accurately infer user preferences from multi-turn conversations, recent works increasingly expand conversational context (e.g., by incorporating diverse entity information or retrieving related dialogues). While such context enrichment can assist preference modeling, it also introduces longer and more heterogeneous inputs, leading to practical issues such as input length constraints, text style inconsistency, and irrelevant textual noise, thereby raising the demand for stronger language understanding ability. In this paper, we propose STARCRS, a Screen-Text-AwaRe Conversational Recommender System that integrates two complementary text understanding modes: (1) a screen-reading pathway that encodes auxiliary textual information as visual tokens, mimicking skim reading on a screen, and (2) an LLM-based textual pathway that focuses on a limited set of critical content for fine-grained reasoning. We design a knowledge-anchored fusion framework that combines contrastive alignment, cross-attention interaction, and adaptive gating to integrate the two modes for improved preference modeling and response generation. Extensive experiments on two widely used benchmarks demonstrate that STARCRS consistently improves both recommendation accuracy and generated response quality.",
        "url": "http://arxiv.org/abs/2601.13505v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13505v1",
        "arxiv_id": "2601.13505v1",
        "authors": [
            "Wei Yuan",
            "Shutong Qiao",
            "Tong Chen",
            "Quoc Viet Hung Nguyen",
            "Zi Huang",
            "Hongzhi Yin"
        ],
        "submitted": "2026-01-20 01:41:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Anonpsy: A Graph-Based Framework for Structure-Preserving De-identification of Psychiatric Narratives",
        "abstract": "Psychiatric narratives encode patient identity not only through explicit identifiers but also through idiosyncratic life events embedded in their clinical structure. Existing de-identification approaches, including PHI masking and LLM-based synthetic rewriting, operate at the text level and offer limited control over which semantic elements are preserved or altered. We introduce Anonpsy, a de-identification framework that reformulates the task as graph-guided semantic rewriting. Anonpsy (1) converts each narrative into a semantic graph encoding clinical entities, temporal anchors, and typed relations; (2) applies graph-constrained perturbations that modify identifying context while preserving clinically essential structure; and (3) regenerates text via graph-conditioned LLM generation. Evaluated on 90 clinician-authored psychiatric case narratives, Anonpsy preserves diagnostic fidelity while achieving consistently low re-identification risk under expert, semantic, and GPT-5-based evaluations. Compared with a strong LLM-only rewriting baseline, Anonpsy yields substantially lower semantic similarity and identifiability. These results demonstrate that explicit structural representations combined with constrained generation provide an effective approach to de-identification for psychiatric narratives.",
        "url": "http://arxiv.org/abs/2601.13503v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13503v1",
        "arxiv_id": "2601.13503v1",
        "authors": [
            "Kyung Ho Lim",
            "Byung-Hoon Kim"
        ],
        "submitted": "2026-01-20 01:37:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing",
        "abstract": "News consumption on social media has become ubiquitous, yet how different forms of engagement shape psychosocial outcomes remains unclear. To address this gap, we leveraged a large-scale dataset of ~26M posts and ~45M comments on the BlueSky platform, and conducted a quasi-experimental study, matching 81,345 Treated users exposed to News feeds with 83,711 Control users using stratified propensity score analysis. We examined psychosocial wellbeing, in terms of affective, behavioral, and cognitive outcomes. Our findings reveal that news engagement produces systematic trade-offs: increased depression, stress, and anxiety, yet decreased loneliness and increased social interaction on the platform. Regression models reveal that News feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting, with magnitude differences exceeding tenfold. These per-engagement effects accumulate with repeated exposure, showing significant psychosocial impacts. Our work extends theories of news effects beyond crisis-centric frameworks by demonstrating that routine consumption creates distinct psychological dynamics depending on engagement type, and bears implications for tools and interventions for mitigating the psychosocial costs of news consumption on social media.",
        "url": "http://arxiv.org/abs/2601.13487v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13487v1",
        "arxiv_id": "2601.13487v1",
        "authors": [
            "Olivia Pal",
            "Agam Goyal",
            "Eshwar Chandrasekharan",
            "Koustuv Saha"
        ],
        "submitted": "2026-01-20 00:46:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving",
        "abstract": "Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8/5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems",
        "url": "http://arxiv.org/abs/2601.13453v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13453v1",
        "arxiv_id": "2601.13453v1",
        "authors": [
            "Aditya Thole",
            "Anmol Agrawal",
            "Arnav Ramamoorthy",
            "Dhruv Kumar"
        ],
        "submitted": "2026-01-19 23:11:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for Text Categorization",
        "abstract": "Open-set learning and discovery (OSLD) is a challenging machine learning task in which samples from new (unknown) classes can appear at test time. It can be seen as a generalization of zero-shot learning, where the new classes are not known a priori, hence involving the active discovery of new classes. While zero-shot learning has been extensively studied in text classification, especially with the emergence of pre-trained language models, open-set learning and discovery is a comparatively new setup for the text domain. To this end, we introduce the first multilingual open-set learning and discovery (MOSLD) benchmark for text categorization by topic, comprising 960K data samples across 12 languages. To construct the benchmark, we (i) rearrange existing datasets and (ii) collect new data samples from the news domain. Moreover, we propose a novel framework for the OSLD task, which integrates multiple stages to continuously discover and learn new classes. We evaluate several language models, including our own, to obtain results that can be used as reference for future work. We release our benchmark at https://github.com/Adriana19Valentina/MOSLD-Bench.",
        "url": "http://arxiv.org/abs/2601.13437v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13437v1",
        "arxiv_id": "2601.13437v1",
        "authors": [
            "Adriana-Valentina Costache",
            "Daria-Nicoleta Dragomir",
            "Silviu-Florin Gheorghe",
            "Eduard Poesina",
            "Paul Irofti",
            "Radu Tudor Ionescu"
        ],
        "submitted": "2026-01-19 22:49:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large Language Models",
        "abstract": "Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect/misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.",
        "url": "http://arxiv.org/abs/2601.13433v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13433v1",
        "arxiv_id": "2601.13433v1",
        "authors": [
            "Priyanka Mary Mammen",
            "Emil Joswin",
            "Shankar Venkitachalam"
        ],
        "submitted": "2026-01-19 22:37:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks",
        "abstract": "Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden's theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs' ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.",
        "url": "http://arxiv.org/abs/2601.13392v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13392v1",
        "arxiv_id": "2601.13392v1",
        "authors": [
            "Shlok Shelat",
            "Jay Raval",
            "Souvik Roy",
            "Manas Gaur"
        ],
        "submitted": "2026-01-19 21:00:31",
        "source": "arxiv",
        "comment": "30 pages, 11 figures, 6 tables, Work in Progress"
    },
    {
        "title": "Structured Insight from Unstructured Data: Large Language Models for SDOH-Driven Diabetes Risk Prediction",
        "abstract": "Social determinants of health (SDOH) play a critical role in Type 2 Diabetes (T2D) management but are often absent from electronic health records and risk prediction models. Most individual-level SDOH data is collected through structured screening tools, which lack the flexibility to capture the complexity of patient experiences and unique needs of a clinic's population. This study explores the use of large language models (LLMs) to extract structured SDOH information from unstructured patient life stories and evaluate the predictive value of both the extracted features and the narratives themselves for assessing diabetes control. We collected unstructured interviews from 65 T2D patients aged 65 and older, focused on their lived experiences, social context, and diabetes management. These narratives were analyzed using LLMs with retrieval-augmented generation to produce concise, actionable qualitative summaries for clinical interpretation and structured quantitative SDOH ratings for risk prediction modeling. The structured SDOH ratings were used independently and in combination with traditional laboratory biomarkers as inputs to linear and tree-based machine learning models (Ridge, Lasso, Random Forest, and XGBoost) to demonstrate how unstructured narrative data can be applied in conventional risk prediction workflows. Finally, we evaluated several LLMs on their ability to predict a patient's level of diabetes control (low, medium, high) directly from interview text with A1C values redacted. LLMs achieved 60% accuracy in predicting diabetes control levels from interview text. This work demonstrates how LLMs can translate unstructured SDOH-related data into structured insights, offering a scalable approach to augment clinical risk models and decision-making.",
        "url": "http://arxiv.org/abs/2601.13388v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13388v1",
        "arxiv_id": "2601.13388v1",
        "authors": [
            "Sasha Ronaghi",
            "Prerit Choudhary",
            "David H Rehkopf",
            "Bryant Lin"
        ],
        "submitted": "2026-01-19 20:53:09",
        "source": "arxiv",
        "comment": "7 pages, 5 figures"
    },
    {
        "title": "Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning",
        "abstract": "Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.",
        "url": "http://arxiv.org/abs/2601.13387v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13387v1",
        "arxiv_id": "2601.13387v1",
        "authors": [
            "Zhenjiang Mao",
            "Anirudhh Venkat",
            "Artem Bisliouk",
            "Akshat Kothiyal",
            "Sindhura Kumbakonam Subramanian",
            "Saithej Singhu",
            "Ivan Ruchkin"
        ],
        "submitted": "2026-01-19 20:48:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Completion to Editing: Unlocking Context-Aware Code Infilling via Search-and-Replace Instruction Tuning",
        "abstract": "The dominant Fill-in-the-Middle (FIM) paradigm for code completion is constrained by its rigid inability to correct contextual errors and reliance on unaligned, insecure Base models. While Chat LLMs offer safety and Agentic workflows provide flexibility, they suffer from performance degradation and prohibitive latency, respectively. To resolve this dilemma, we propose Search-and-Replace Infilling (SRI), a framework that internalizes the agentic verification-and-editing mechanism into a unified, single-pass inference process. By structurally grounding edits via an explicit search phase, SRI harmonizes completion tasks with the instruction-following priors of Chat LLMs, extending the paradigm from static infilling to dynamic context-aware editing. We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder series. Extensive evaluations demonstrate that with minimal data (20k samples), SRI-Coder enables Chat models to surpass the completion performance of their Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general coding competencies and maintains inference latency comparable to standard FIM. We empower the entire Qwen3-Coder series with SRI, encouraging the developer community to leverage this framework for advanced auto-completion and assisted development.",
        "url": "http://arxiv.org/abs/2601.13384v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13384v1",
        "arxiv_id": "2601.13384v1",
        "authors": [
            "Jiajun Zhang",
            "Zeyu Cui",
            "Jiaxi Yang",
            "Lei Zhang",
            "Yuheng Jing",
            "Zeyao Ma",
            "Tianyi Bai",
            "Zilei Wang",
            "Qiang Liu",
            "Liang Wang",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "submitted": "2026-01-19 20:33:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models",
        "abstract": "As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.",
        "url": "http://arxiv.org/abs/2601.13368v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13368v1",
        "arxiv_id": "2601.13368v1",
        "authors": [
            "Zhenjiang Mao",
            "Anirudhh Venkat"
        ],
        "submitted": "2026-01-19 20:04:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output Prefix Injection",
        "abstract": "As open-weight large language models (LLMs) increase in capabilities, safeguarding them against malicious prompts and understanding possible attack vectors becomes ever more important. While automated jailbreaking methods like GCG [Zou et al., 2023] remain effective, they often require substantial computational resources and specific expertise. We introduce \"sockpuppetting'', a simple method for jailbreaking open-weight LLMs by inserting an acceptance sequence (e.g., \"Sure, here is how to...'') at the start of a model's output and allowing it to complete the response. Requiring only a single line of code and no optimization, sockpuppetting achieves up to 80% higher attack success rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a hybrid approach that optimizes the adversarial suffix within the assistant message block rather than the user prompt, increasing ASR by 64% over GCG on Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting as an effective low-cost attack accessible to unsophisticated adversaries, highlighting the need for defences against output-prefix injection in open-weight models.",
        "url": "http://arxiv.org/abs/2601.13359v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13359v1",
        "arxiv_id": "2601.13359v1",
        "authors": [
            "Asen Dotsinski",
            "Panagiotis Eustratiadis"
        ],
        "submitted": "2026-01-19 19:53:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Relation of State Space Models and Hidden Markov Models",
        "abstract": "State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models.\n  In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.",
        "url": "http://arxiv.org/abs/2601.13357v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13357v1",
        "arxiv_id": "2601.13357v1",
        "authors": [
            "Aydin Ghojogh",
            "M. Hadi Sepanj",
            "Benyamin Ghojogh"
        ],
        "submitted": "2026-01-19 19:51:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Guidelines for the Creation of an Annotated Corpus",
        "abstract": "This document, based on feedback from UMR TETIS members and the scientific literature, provides a generic methodology for creating annotation guidelines and annotated textual datasets (corpora). It covers methodological aspects, as well as storage, sharing, and valorization of the data. It includes definitions and examples to clearly illustrate each step of the process, thus providing a comprehensive framework to support the creation and use of corpora in various research contexts.",
        "url": "http://arxiv.org/abs/2601.13353v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13353v1",
        "arxiv_id": "2601.13353v1",
        "authors": [
            "Bahdja Boudoua",
            "Nadia Guiffant",
            "Mathieu Roche",
            "Maguelonne Teisseire",
            "Annelise Tran"
        ],
        "submitted": "2026-01-19 19:42:43",
        "source": "arxiv",
        "comment": "8 pages, 3 figures"
    },
    {
        "title": "LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence Prediction",
        "abstract": "Large language models are strong sequence predictors, yet standard inference relies on immutable context histories. After making an error at generation step t, the model lacks an updatable memory mechanism that improves predictions for step t+1. We propose LLM-as-RNN, an inference-only framework that turns a frozen LLM into a recurrent predictor by representing its hidden state as natural-language memory. This state, implemented as a structured system-prompt summary, is updated at each timestep via feedback-driven text rewrites, enabling learning without parameter updates. Under a fixed token budget, LLM-as-RNN corrects errors and retains task-relevant patterns, effectively performing online learning through language. We evaluate the method on three sequential benchmarks in healthcare, meteorology, and finance across Llama, Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot, full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on average, while producing interpretable, human-readable learning traces absent in standard context accumulation.",
        "url": "http://arxiv.org/abs/2601.13352v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13352v1",
        "arxiv_id": "2601.13352v1",
        "authors": [
            "Yuxing Lu",
            "J. Ben Tamo",
            "Weichen Zhao",
            "Nan Sun",
            "Yishan Zhong",
            "Wenqi Shi",
            "Jinzhuo Wang",
            "May D. Wang"
        ],
        "submitted": "2026-01-19 19:41:39",
        "source": "arxiv",
        "comment": "17 pages, 5 figures, 6 tables"
    },
    {
        "title": "AfroScope: A Framework for Studying the Linguistic Landscape of Africa",
        "abstract": "Language Identification (LID) is the task of determining the language of a given text and is a fundamental preprocessing step that affects the reliability of downstream NLP applications. While recent work has expanded LID coverage for African languages, existing approaches remain limited in (i) the number of supported languages and (ii) their ability to make fine-grained distinctions among closely related varieties. We introduce AfroScope, a unified framework for African LID that includes AfroScope-Data, a dataset covering 713 African languages, and AfroScope-Models, a suite of strong LID models with broad language coverage. To better distinguish highly confusable languages, we propose a hierarchical classification approach that leverages Mirror-Serengeti, a specialized embedding model targeting 29 closely related or geographically proximate languages. This approach improves macro F1 by 4.55 on this confusable subset compared to our best base model. Finally, we analyze cross linguistic transfer and domain effects, offering guidance for building robust African LID systems. We position African LID as an enabling technology for large scale measurement of Africas linguistic landscape in digital text and release AfroScope-Data and AfroScope-Models publicly.",
        "url": "http://arxiv.org/abs/2601.13346v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13346v1",
        "arxiv_id": "2601.13346v1",
        "authors": [
            "Sang Yun Kwon",
            "AbdelRahim Elmadany",
            "Muhammad Abdul-Mageed"
        ],
        "submitted": "2026-01-19 19:30:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RegCheck: A tool for automating comparisons between study registrations and papers",
        "abstract": "Across the social and medical sciences, researchers recognize that specifying planned research activities (i.e., 'registration') prior to the commencement of research has benefits for both the transparency and rigour of science. Despite this, evidence suggests that study registrations frequently go unexamined, minimizing their effectiveness. In a way this is no surprise: manually checking registrations against papers is labour- and time-intensive, requiring careful reading across formats and expertise across domains. The advent of AI unlocks new possibilities in facilitating this activity. We present RegCheck, a modular LLM-assisted tool designed to help researchers, reviewers, and editors from across scientific disciplines compare study registrations with their corresponding papers. Importantly, RegCheck keeps human expertise and judgement in the loop by (i) ensuring that users are the ones who determine which features should be compared, and (ii) presenting the most relevant text associated with each feature to the user, facilitating (rather than replacing) human discrepancy judgements. RegCheck also generates shareable reports with unique RegCheck IDs, enabling them to be easily shared and verified by other users. RegCheck is designed to be adaptable across scientific domains, as well as registration and publication formats. In this paper we provide an overview of the motivation, workflow, and design principles of RegCheck, and we discuss its potential as an extensible infrastructure for reproducible science with an example use case.",
        "url": "http://arxiv.org/abs/2601.13330v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13330v1",
        "arxiv_id": "2601.13330v1",
        "authors": [
            "Jamie Cummins",
            "Beth Clarke",
            "Ian Hussey",
            "Malte Elson"
        ],
        "submitted": "2026-01-19 19:10:45",
        "source": "arxiv",
        "comment": "15 pages, 1 figure"
    },
    {
        "title": "Reducing Tokenization Premiums for Low-Resource Languages",
        "abstract": "Relative to English, low-resource languages suffer from substantial tokenization premiums in modern LMs, meaning that it generally requires several times as many tokens to encode a sentence in a low-resource language than to encode the analogous sentence in English. This tokenization premium results in increased API and energy costs and reduced effective context windows for these languages. In this paper we analyze the tokenizers of ten popular LMs to better understand their designs and per-language tokenization premiums. We also propose a mechanism to reduce tokenization premiums in pre-trained models, by post-hoc additions to the token vocabulary that coalesce multi-token characters into single tokens. We apply this methodology to 12 low-resource languages, demonstrating that the original and compressed inputs often have similar last hidden states when run through the Llama 3.2 1B model.",
        "url": "http://arxiv.org/abs/2601.13328v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13328v1",
        "arxiv_id": "2601.13328v1",
        "authors": [
            "Geoffrey Churchill",
            "Steven Skiena"
        ],
        "submitted": "2026-01-19 19:08:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Arab Voices: Mapping Standard and Dialectal Arabic Speech Technology",
        "abstract": "Dialectal Arabic (DA) speech data vary widely in domain coverage, dialect labeling practices, and recording conditions, complicating cross-dataset comparison and model evaluation. To characterize this landscape, we conduct a computational analysis of linguistic ``dialectness'' alongside objective proxies of audio quality on the training splits of widely used DA corpora. We find substantial heterogeneity both in acoustic conditions and in the strength and consistency of dialectal signals across datasets, underscoring the need for standardized characterization beyond coarse labels. To reduce fragmentation and support reproducible evaluation, we introduce Arab Voices, a standardized framework for DA ASR. Arab Voices provides unified access to 31 datasets spanning 14 dialects, with harmonized metadata and evaluation utilities. We further benchmark a range of recent ASR systems, establishing strong baselines for modern DA ASR.",
        "url": "http://arxiv.org/abs/2601.13319v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13319v1",
        "arxiv_id": "2601.13319v1",
        "authors": [
            "Peter Sullivan",
            "AbdelRahim Elmadany",
            "Alcides Alcoba Inciarte",
            "Muhammad Abdul-Mageed"
        ],
        "submitted": "2026-01-19 19:02:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse",
        "abstract": "Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.",
        "url": "http://arxiv.org/abs/2601.13317v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13317v1",
        "arxiv_id": "2601.13317v1",
        "authors": [
            "Samantha Sudhoff",
            "Pranav Perumal",
            "Zhaoqing Wu",
            "Tunazzina Islam"
        ],
        "submitted": "2026-01-19 19:00:56",
        "source": "arxiv",
        "comment": null
    }
]