[
    {
        "title": "State over Tokens: Characterizing the Role of Reasoning Tokens",
        "abstract": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.",
        "url": "http://arxiv.org/abs/2512.12777v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12777v1",
        "arxiv_id": "2512.12777v1",
        "authors": [
            "Mosh Levy",
            "Zohar Elyoseph",
            "Shauli Ravfogel",
            "Yoav Goldberg"
        ],
        "submitted": "2025-12-14 17:30:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions",
        "abstract": "Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.",
        "url": "http://arxiv.org/abs/2512.12775v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12775v1",
        "arxiv_id": "2512.12775v1",
        "authors": [
            "Pedro Henrique Luz de Araujo",
            "Michael A. Hedderich",
            "Ali Modarressi",
            "Hinrich Schuetze",
            "Benjamin Roth"
        ],
        "submitted": "2025-12-14 17:27:02",
        "source": "arxiv",
        "comment": "31 pages, 35 figures"
    },
    {
        "title": "Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining",
        "abstract": "Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu",
        "url": "http://arxiv.org/abs/2512.12770v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12770v1",
        "arxiv_id": "2512.12770v1",
        "authors": [
            "Thales Sales Almeida",
            "Rodrigo Nogueira",
            "Hélio Pedrini"
        ],
        "submitted": "2025-12-14 17:19:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Intelligent Scientific Literature Explorer using Machine Learning (ISLE)",
        "abstract": "The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.",
        "url": "http://arxiv.org/abs/2512.12760v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12760v1",
        "arxiv_id": "2512.12760v1",
        "authors": [
            "Sina Jani",
            "Arman Heidari",
            "Amirmohammad Anvari",
            "Zahra Rahimi"
        ],
        "submitted": "2025-12-14 16:54:24",
        "source": "arxiv",
        "comment": "18 pages, 7 figures, 3 tables"
    },
    {
        "title": "FuXi-$γ$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism",
        "abstract": "Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$γ$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$γ$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$γ$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\\times$ and inference by up to 6.18$\\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.",
        "url": "http://arxiv.org/abs/2512.12740v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12740v1",
        "arxiv_id": "2512.12740v1",
        "authors": [
            "Dezhi Yi",
            "Wei Guo",
            "Wenyang Cui",
            "Wenxuan He",
            "Huifeng Guo",
            "Yong Liu",
            "Zhenhua Dong",
            "Ye Lu"
        ],
        "submitted": "2025-12-14 15:38:14",
        "source": "arxiv",
        "comment": "Accepted by KDD 2026"
    },
    {
        "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents",
        "abstract": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.",
        "url": "http://arxiv.org/abs/2512.12730v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12730v1",
        "arxiv_id": "2512.12730v1",
        "authors": [
            "Jingzhe Ding",
            "Shengda Long",
            "Changxin Pu",
            "Huan Zhou",
            "Hongwan Gao",
            "Xiang Gao",
            "Chao He",
            "Yue Hou",
            "Fei Hu",
            "Zhaojian Li",
            "Weiran Shi",
            "Zaiyuan Wang",
            "Daoguang Zan",
            "Chenchen Zhang",
            "Xiaoxu Zhang",
            "Qizhi Chen",
            "Xianfu Cheng",
            "Bo Deng",
            "Qingshui Gu",
            "Kai Hua",
            "Juntao Lin",
            "Pai Liu",
            "Mingchen Li",
            "Xuanguang Pan",
            "Zifan Peng",
            "Yujia Qin",
            "Yong Shan",
            "Zhewen Tan",
            "Weihao Xie",
            "Zihan Wang",
            "Yishuo Yuan",
            "Jiayu Zhang",
            "Enduo Zhao",
            "Yunfei Zhao",
            "He Zhu",
            "Chenyang Zou",
            "Ming Ding",
            "Jianpeng Jiao",
            "Jiaheng Liu",
            "Minghao Liu",
            "Qian Liu",
            "Chongyao Tao",
            "Jian Yang",
            "Tong Yang",
            "Zhaoxiang Zhang",
            "Xinjie Chen",
            "Wenhao Huang",
            "Ge Zhang"
        ],
        "submitted": "2025-12-14 15:12:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning",
        "abstract": "Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.",
        "url": "http://arxiv.org/abs/2512.12716v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12716v1",
        "arxiv_id": "2512.12716v1",
        "authors": [
            "Xuanzhang Liu",
            "Jianglun Feng",
            "Zhuoran Zhuang",
            "Junzhe Zhao",
            "Maofei Que",
            "Jieting Li",
            "Dianlei Wang",
            "Hao Tong",
            "Ye Chen",
            "Pan Li"
        ],
        "submitted": "2025-12-14 14:41:29",
        "source": "arxiv",
        "comment": "Accepted to WSDM '26 Oral"
    },
    {
        "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning",
        "abstract": "Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.",
        "url": "http://arxiv.org/abs/2512.12701v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12701v1",
        "arxiv_id": "2512.12701v1",
        "authors": [
            "Xue Li",
            "Xiaonan Song",
            "Henry Hu"
        ],
        "submitted": "2025-12-14 14:11:32",
        "source": "arxiv",
        "comment": "10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results"
    },
    {
        "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment",
        "abstract": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.",
        "url": "http://arxiv.org/abs/2512.12692v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12692v1",
        "arxiv_id": "2512.12692v1",
        "authors": [
            "Mahir Labib Dihan",
            "Tanzima Hashem",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "submitted": "2025-12-14 13:56:54",
        "source": "arxiv",
        "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/"
    },
    {
        "title": "Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning",
        "abstract": "Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing \"RL over SFT\" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.",
        "url": "http://arxiv.org/abs/2512.12690v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12690v1",
        "arxiv_id": "2512.12690v1",
        "authors": [
            "Yongcan Yu",
            "Lingxiao He",
            "Shuo Lu",
            "Lijun Sheng",
            "Yinuo Xu",
            "Yanbo Wang",
            "Kuangpu Guo",
            "Jianjie Cheng",
            "Meng Wang",
            "Qianlong Xie",
            "Xingxing Wang",
            "Dapeng Hu",
            "Jian Liang"
        ],
        "submitted": "2025-12-14 13:46:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity",
        "abstract": "Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.",
        "url": "http://arxiv.org/abs/2512.12688v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12688v1",
        "arxiv_id": "2512.12688v1",
        "authors": [
            "Dongseok Kim",
            "Hyoungsun Choi",
            "Mohamed Jismy Aashik Rasool",
            "Gisung Oh"
        ],
        "submitted": "2025-12-14 13:42:20",
        "source": "arxiv",
        "comment": "24 pages"
    },
    {
        "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI",
        "abstract": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.",
        "url": "http://arxiv.org/abs/2512.12686v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12686v1",
        "arxiv_id": "2512.12686v1",
        "authors": [
            "Samarth Sarin",
            "Lovepreet Singh",
            "Bhaskarjit Sarmah",
            "Dhagash Mehta"
        ],
        "submitted": "2025-12-14 13:38:06",
        "source": "arxiv",
        "comment": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India"
    },
    {
        "title": "Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches",
        "abstract": "We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.",
        "url": "http://arxiv.org/abs/2512.12677v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12677v1",
        "arxiv_id": "2512.12677v1",
        "authors": [
            "Amirhossein Yousefiramandi",
            "Ciaran Cooney"
        ],
        "submitted": "2025-12-14 13:02:06",
        "source": "arxiv",
        "comment": "18 pages, 6 figures"
    },
    {
        "title": "Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks",
        "abstract": "Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.",
        "url": "http://arxiv.org/abs/2512.12654v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12654v1",
        "arxiv_id": "2512.12654v1",
        "authors": [
            "Hassan Mujtaba",
            "Hamza Naveed",
            "Hanzlah Munir"
        ],
        "submitted": "2025-12-14 11:59:16",
        "source": "arxiv",
        "comment": "6 pages"
    },
    {
        "title": "LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases",
        "abstract": "Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.",
        "url": "http://arxiv.org/abs/2512.12643v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12643v1",
        "arxiv_id": "2512.12643v1",
        "authors": [
            "Yida Cai",
            "Ranjuexiao Hu",
            "Huiyuan Xie",
            "Chenyang Li",
            "Yun Liu",
            "Yuxiao Ye",
            "Zhenghao Liu",
            "Weixing Shen",
            "Zhiyuan Liu"
        ],
        "submitted": "2025-12-14 11:16:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Which Pieces Does Unigram Tokenization Really Need?",
        "abstract": "The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.",
        "url": "http://arxiv.org/abs/2512.12641v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12641v1",
        "arxiv_id": "2512.12641v1",
        "authors": [
            "Sander Land",
            "Yuval Pinter"
        ],
        "submitted": "2025-12-14 11:13:49",
        "source": "arxiv",
        "comment": "10 pages, 1 figure. For associated code, see https://github.com/sanderland/script_tok"
    },
    {
        "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.",
        "url": "http://arxiv.org/abs/2512.12623v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12623v1",
        "arxiv_id": "2512.12623v1",
        "authors": [
            "Chengzhi Liu",
            "Yuzhe Yang",
            "Yue Fan",
            "Qingyue Wei",
            "Sheng Liu",
            "Xin Eric Wang"
        ],
        "submitted": "2025-12-14 10:07:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives",
        "abstract": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.",
        "url": "http://arxiv.org/abs/2512.12620v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12620v1",
        "arxiv_id": "2512.12620v1",
        "authors": [
            "Aheli Poddar",
            "Saptarshi Sahoo",
            "Sujata Ghosh"
        ],
        "submitted": "2025-12-14 09:50:10",
        "source": "arxiv",
        "comment": "9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at https://github.com/XAheli/Logic-in-LLMs"
    },
    {
        "title": "StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning",
        "abstract": "Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.",
        "url": "http://arxiv.org/abs/2512.12613v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12613v1",
        "arxiv_id": "2512.12613v1",
        "authors": [
            "Yucan Guo",
            "Saiping Guan",
            "Miao Su",
            "Zeya Zhao",
            "Xiaolong Jin",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "submitted": "2025-12-14 09:36:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching",
        "abstract": "Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/",
        "url": "http://arxiv.org/abs/2512.12610v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12610v1",
        "arxiv_id": "2512.12610v1",
        "authors": [
            "Wonseok Choi",
            "Sohwi Lim",
            "Nam Hyeon-Woo",
            "Moon Ye-Bin",
            "Dong-Ju Jeong",
            "Jinyoung Hwang",
            "Tae-Hyun Oh"
        ],
        "submitted": "2025-12-14 09:24:51",
        "source": "arxiv",
        "comment": "WACV 2026"
    },
    {
        "title": "Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery",
        "abstract": "Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.",
        "url": "http://arxiv.org/abs/2512.12608v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12608v1",
        "arxiv_id": "2512.12608v1",
        "authors": [
            "Hong Su"
        ],
        "submitted": "2025-12-14 09:12:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation",
        "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.",
        "url": "http://arxiv.org/abs/2512.12597v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12597v1",
        "arxiv_id": "2512.12597v1",
        "authors": [
            "Miriam Horovicz"
        ],
        "submitted": "2025-12-14 08:31:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning",
        "abstract": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.",
        "url": "http://arxiv.org/abs/2512.12576v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12576v1",
        "arxiv_id": "2512.12576v1",
        "authors": [
            "Xueru Wen",
            "Jie Lou",
            "Yanjiang Liu",
            "Hongyu Lin",
            "Ben He",
            "Xianpei Han",
            "Le Sun",
            "Yaojie Lu",
            "Debing Zhang"
        ],
        "submitted": "2025-12-14 07:03:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks",
        "abstract": "Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.",
        "url": "http://arxiv.org/abs/2512.12544v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12544v1",
        "arxiv_id": "2512.12544v1",
        "authors": [
            "Yiming Zeng",
            "Jinghan Cao",
            "Zexin Li",
            "Wanhao Yu",
            "Zhankai Ye",
            "Dawei Xiang",
            "Ting Hua",
            "Xin Liu",
            "Shangqian Gao",
            "Tingting Yu"
        ],
        "submitted": "2025-12-14 04:28:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data",
        "abstract": "The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.",
        "url": "http://arxiv.org/abs/2512.12537v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12537v1",
        "arxiv_id": "2512.12537v1",
        "authors": [
            "Agniva Maiti",
            "Manya Pandey",
            "Murari Mandal"
        ],
        "submitted": "2025-12-14 04:08:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings",
        "abstract": "Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.",
        "url": "http://arxiv.org/abs/2512.12492v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12492v1",
        "arxiv_id": "2512.12492v1",
        "authors": [
            "Shengkai Xu",
            "Hsiang Lun Kao",
            "Tianxiang Xu",
            "Honghui Zhang",
            "Junqiao Wang",
            "Runmeng Ding",
            "Guanyu Liu",
            "Tianyu Shi",
            "Zhenyu Yu",
            "Guofeng Pan",
            "Ziqian Bi",
            "Yuqi Ouyang"
        ],
        "submitted": "2025-12-13 23:33:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting",
        "abstract": "Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.",
        "url": "http://arxiv.org/abs/2512.12488v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12488v1",
        "arxiv_id": "2512.12488v1",
        "authors": [
            "James Luther",
            "Donald Brown"
        ],
        "submitted": "2025-12-13 23:11:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval",
        "abstract": "Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.",
        "url": "http://arxiv.org/abs/2512.12458v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12458v1",
        "arxiv_id": "2512.12458v1",
        "authors": [
            "Vihan Lakshman",
            "Blaise Munyampirwa",
            "Julian Shun",
            "Benjamin Coleman"
        ],
        "submitted": "2025-12-13 21:05:21",
        "source": "arxiv",
        "comment": "27 pages"
    },
    {
        "title": "Large language models have learned to use language",
        "abstract": "Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.",
        "url": "http://arxiv.org/abs/2512.12447v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12447v1",
        "arxiv_id": "2512.12447v1",
        "authors": [
            "Gary Lupyan"
        ],
        "submitted": "2025-12-13 20:09:10",
        "source": "arxiv",
        "comment": "Commentary on Futrell & Mahowald's How Linguistics Learned to Stop Worrying and Love the Language Models (BBS, Forthcoming)"
    },
    {
        "title": "Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors",
        "abstract": "As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.",
        "url": "http://arxiv.org/abs/2512.12444v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12444v1",
        "arxiv_id": "2512.12444v1",
        "authors": [
            "Veronica Mangiaterra",
            "Hamad Al-Azary",
            "Chiara Barattieri di San Pietro",
            "Paolo Canal",
            "Valentina Bambini"
        ],
        "submitted": "2025-12-13 19:56:31",
        "source": "arxiv",
        "comment": "30 pages, 5 figures"
    },
    {
        "title": "The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework",
        "abstract": "We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.",
        "url": "http://arxiv.org/abs/2512.12394v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12394v1",
        "arxiv_id": "2512.12394v1",
        "authors": [
            "Vladimir Berman"
        ],
        "submitted": "2025-12-13 16:58:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining",
        "abstract": "Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.",
        "url": "http://arxiv.org/abs/2512.12384v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12384v1",
        "arxiv_id": "2512.12384v1",
        "authors": [
            "Jesse Ponnock"
        ],
        "submitted": "2025-12-13 16:28:50",
        "source": "arxiv",
        "comment": "8 pages, 4 figures, 1 table"
    },
    {
        "title": "VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding",
        "abstract": "Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.",
        "url": "http://arxiv.org/abs/2512.12360v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12360v1",
        "arxiv_id": "2512.12360v1",
        "authors": [
            "Yufei Yin",
            "Qianke Meng",
            "Minghao Chen",
            "Jiajun Ding",
            "Zhenwei Shao",
            "Zhou Yu"
        ],
        "submitted": "2025-12-13 15:11:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema",
        "abstract": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.",
        "url": "http://arxiv.org/abs/2512.12337v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12337v1",
        "arxiv_id": "2512.12337v1",
        "authors": [
            "Yushen Fang",
            "Jianjun Li",
            "Mingqian Ding",
            "Chang Liu",
            "Xinchi Zou",
            "Wenqi Yang"
        ],
        "submitted": "2025-12-13 14:07:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving",
        "abstract": "Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.",
        "url": "http://arxiv.org/abs/2512.12302v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12302v1",
        "arxiv_id": "2512.12302v1",
        "authors": [
            "Huan Zheng",
            "Yucheng Zhou",
            "Tianyi Yan",
            "Jiayi Su",
            "Hongjun Chen",
            "Dubing Chen",
            "Wencheng Han",
            "Runzhou Tao",
            "Zhongying Qiu",
            "Jianfei Yang",
            "Jianbing Shen"
        ],
        "submitted": "2025-12-13 11:59:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation",
        "abstract": "This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.",
        "url": "http://arxiv.org/abs/2512.12297v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12297v1",
        "arxiv_id": "2512.12297v1",
        "authors": [
            "Radu-Gabriel Chivereanu",
            "Tiberiu Boros"
        ],
        "submitted": "2025-12-13 11:41:54",
        "source": "arxiv",
        "comment": "Accepted at The 20th International Conference on Linguistic Resources and Tools for Natural Language Processing"
    },
    {
        "title": "Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics",
        "abstract": "We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.",
        "url": "http://arxiv.org/abs/2512.12264v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12264v1",
        "arxiv_id": "2512.12264v1",
        "authors": [
            "Abhay Srivastava",
            "Sam Jung",
            "Spencer Mateega"
        ],
        "submitted": "2025-12-13 10:07:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages",
        "abstract": "The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.",
        "url": "http://arxiv.org/abs/2512.12245v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12245v1",
        "arxiv_id": "2512.12245v1",
        "authors": [
            "Anika Sharma",
            "Tianyi Niu",
            "Emma Wrenn",
            "Shashank Srivastava"
        ],
        "submitted": "2025-12-13 09:06:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Semantic Distance Measurement based on Multi-Kernel Gaussian Processes",
        "abstract": "Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.",
        "url": "http://arxiv.org/abs/2512.12238v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12238v1",
        "arxiv_id": "2512.12238v1",
        "authors": [
            "Yinzhu Cheng",
            "Haihua Xie",
            "Yaqing Wang",
            "Miao He",
            "Mingming Sun"
        ],
        "submitted": "2025-12-13 08:34:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking",
        "abstract": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.",
        "url": "http://arxiv.org/abs/2512.12218v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12218v1",
        "arxiv_id": "2512.12218v1",
        "authors": [
            "Rheeya Uppaal",
            "Phu Mon Htut",
            "Min Bai",
            "Nikolaos Pappas",
            "Zheng Qi"
        ],
        "submitted": "2025-12-13 07:04:42",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Training Versatile Coding Agents in Synthetic Environments",
        "abstract": "Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.",
        "url": "http://arxiv.org/abs/2512.12216v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12216v1",
        "arxiv_id": "2512.12216v1",
        "authors": [
            "Yiqi Zhu",
            "Apurva Gandhi",
            "Graham Neubig"
        ],
        "submitted": "2025-12-13 07:02:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search",
        "abstract": "Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.",
        "url": "http://arxiv.org/abs/2512.12207v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12207v1",
        "arxiv_id": "2512.12207v1",
        "authors": [
            "Jiangen He",
            "Jiqun Liu"
        ],
        "submitted": "2025-12-13 06:39:45",
        "source": "arxiv",
        "comment": "CHIIR 2026"
    },
    {
        "title": "Diffusion Language Model Inference with Monte Carlo Tree Search",
        "abstract": "Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.",
        "url": "http://arxiv.org/abs/2512.12168v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12168v1",
        "arxiv_id": "2512.12168v1",
        "authors": [
            "Zheng Huang",
            "Kiran Ramnath",
            "Yueyan Chen",
            "Aosong Feng",
            "Sangmin Woo",
            "Balasubramaniam Srinivasan",
            "Zhichao Xu",
            "Kang Zhou",
            "Shuai Wang",
            "Haibo Ding",
            "Lin Lee Cheong"
        ],
        "submitted": "2025-12-13 04:30:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings",
        "abstract": "So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.",
        "url": "http://arxiv.org/abs/2512.12167v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12167v1",
        "arxiv_id": "2512.12167v1",
        "authors": [
            "Yoav Gelberg",
            "Koshi Eguchi",
            "Takuya Akiba",
            "Edoardo Cetin"
        ],
        "submitted": "2025-12-13 04:23:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering",
        "abstract": "Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.",
        "url": "http://arxiv.org/abs/2512.12089v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12089v1",
        "arxiv_id": "2512.12089v1",
        "authors": [
            "Zihu Wang",
            "Boxun Xu",
            "Yuxuan Xia",
            "Peng Li"
        ],
        "submitted": "2025-12-12 23:33:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding",
        "abstract": "The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.",
        "url": "http://arxiv.org/abs/2512.12087v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12087v1",
        "arxiv_id": "2512.12087v1",
        "authors": [
            "Jiayi Yuan",
            "Cameron Shinn",
            "Kai Xu",
            "Jingze Cui",
            "George Klimiashvili",
            "Guangxuan Xiao",
            "Perkz Zheng",
            "Bo Li",
            "Yuxin Zhou",
            "Zhouhai Ye",
            "Weijie You",
            "Tian Zheng",
            "Dominic Brown",
            "Pengbo Wang",
            "Richard Cai",
            "Julien Demouth",
            "John D. Owens",
            "Xia Hu",
            "Song Han",
            "Timmy Liu",
            "Huizi Mao"
        ],
        "submitted": "2025-12-12 23:30:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL",
        "abstract": "Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.",
        "url": "http://arxiv.org/abs/2512.12084v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12084v1",
        "arxiv_id": "2512.12084v1",
        "authors": [
            "Hanzhou Liu",
            "Kai Yin",
            "Zhitong Chen",
            "Chenyue Liu",
            "Ali Mostafavi"
        ],
        "submitted": "2025-12-12 23:25:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs",
        "abstract": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.",
        "url": "http://arxiv.org/abs/2512.12072v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12072v1",
        "arxiv_id": "2512.12072v1",
        "authors": [
            "Avinash Amballa",
            "Yashas Malur Saidutta",
            "Chi-Heng Lin",
            "Vivek Kulkarni",
            "Srinivas Chappidi"
        ],
        "submitted": "2025-12-12 22:39:01",
        "source": "arxiv",
        "comment": "Arxiv Submission"
    },
    {
        "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring",
        "abstract": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.",
        "url": "http://arxiv.org/abs/2512.12069v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12069v1",
        "arxiv_id": "2512.12069v1",
        "authors": [
            "Peichun Hua",
            "Hao Li",
            "Shanghao Shi",
            "Zhiyuan Yu",
            "Ning Zhang"
        ],
        "submitted": "2025-12-12 22:31:38",
        "source": "arxiv",
        "comment": "40 pages, 13 figures"
    },
    {
        "title": "The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior",
        "abstract": "Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.",
        "url": "http://arxiv.org/abs/2512.12066v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12066v1",
        "arxiv_id": "2512.12066v1",
        "authors": [
            "Erik Larsen"
        ],
        "submitted": "2025-12-12 22:29:13",
        "source": "arxiv",
        "comment": "14 pages, 7 figures, 6 tables. Code and data available at https://github.com/erikl2/safety-refusal-stability"
    },
    {
        "title": "Benchmarking Contextual Understanding for In-Car Conversational Systems",
        "abstract": "In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.",
        "url": "http://arxiv.org/abs/2512.12042v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12042v1",
        "arxiv_id": "2512.12042v1",
        "authors": [
            "Philipp Habicht",
            "Lev Sorokin",
            "Abdullah Saydemir",
            "Ken E. Friedl",
            "Andrea Stocco"
        ],
        "submitted": "2025-12-12 21:15:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus",
        "abstract": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.",
        "url": "http://arxiv.org/abs/2512.12012v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12012v1",
        "arxiv_id": "2512.12012v1",
        "authors": [
            "Antonio Guillen-Perez"
        ],
        "submitted": "2025-12-12 20:07:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",
        "url": "http://arxiv.org/abs/2512.12008v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12008v1",
        "arxiv_id": "2512.12008v1",
        "authors": [
            "Minghui Liu",
            "Aadi Palnitkar",
            "Tahseen Rabbani",
            "Hyunwoo Jae",
            "Kyle Rui Sang",
            "Dixi Yao",
            "Shayan Shabihi",
            "Fuheng Zhao",
            "Tian Li",
            "Ce Zhang",
            "Furong Huang",
            "Kunpeng Zhang"
        ],
        "submitted": "2025-12-12 19:50:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models",
        "abstract": "Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.",
        "url": "http://arxiv.org/abs/2512.11998v1",
        "pdf_url": "https://arxiv.org/pdf/2512.11998v1",
        "arxiv_id": "2512.11998v1",
        "authors": [
            "Glenn Zhang",
            "Treasure Mayowa",
            "Jason Fan",
            "Yicheng Fu",
            "Aaron Sandoval",
            "Sean O'Brien",
            "Kevin Zhu"
        ],
        "submitted": "2025-12-12 19:29:05",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025 SRW, 5 pages body, 14 pages total"
    }
]