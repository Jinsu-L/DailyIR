# Daily Papers Report - 2025-07-09

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Flippi: End To End GenAI Assistant for E-Commerce

- **LLM Score**: 7
- **Keyword Score**: 16
- **Authors**: Anand A. Rajasekar, Praveen Tangarajan, Anjali Nainani, Amogh Batwal, Vinay Rao Dandin, Anusua Trivedi, Ozan Ersoy
- **URL**: <http://arxiv.org/abs/2507.05788v1>
- **Submitted**: 2025-07-08 08:50:47
- **Comment**: 10 pages, 2 figures, 7 tables
- **Topic Keywords**: query, queries, rag, conversion rate, retrieval, shopping, commerce, e-commerce, search
- **Reason**: The paper's focus on conversational assistants, natural language dialogue, and e-commerce is relevant to your interests in Information Retrieval and Search technologies. The use of advanced NLP techniques such as Query Reformulation, Intent Detection, and Retrieval-Augmented Generation is also aligned with your research themes. However, the paper's primary focus on e-commerce and conversational assistants is not a central match with your broader interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Conversational Assistant for E-commerce
- **Aim**: To develop an end-to-end conversational assistant, Flippi, that enables customers to discover products efficiently through natural language dialogue
- **Rationale**: To address the challenges posed by the vast and overwhelming product landscape in e-commerce
- **Ground**: The system uses advanced NLP techniques such as Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction
- **Experiment**: The Flippi system was deployed through an A/B experimentation framework and has undergone numerous iterative development cycles, with each iteration yielding enhancements over its predecessor
- **Takeaway**: Flippi is effective in delivering a contextual, intuitive, and responsive shopping experience, empowering customers to make confident purchase decisions

#### Abstract
> The emergence of conversational assistants has fundamentally reshaped user
interactions with digital platforms. This paper introduces Flippi-a
cutting-edge, end-to-end conversational assistant powered by large language
models (LLMs) and tailored for the e-commerce sector. Flippi addresses the
challenges posed by the vast and often overwhelming product landscape, enabling
customers to discover products more efficiently through natural language
dialogue. By accommodating both objective and subjective user requirements,
Flippi delivers a personalized shopping experience that surpasses traditional
search methods. This paper details how Flippi interprets customer queries to
provide precise product information, leveraging advanced NLP techniques such as
Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),
Named Entity Recognition (NER), and Context Reduction. Flippi's unique
capability to identify and present the most attractive offers on an e-commerce
site is also explored, demonstrating how it empowers users to make
cost-effective decisions. Additionally, the paper discusses Flippi's
comparative analysis features, which help users make informed choices by
contrasting product features, prices, and other relevant attributes. The
system's robust architecture is outlined, emphasizing its adaptability for
integration across various e-commerce platforms and the technological choices
underpinning its performance and accuracy. Finally, a comprehensive evaluation
framework is presented, covering performance metrics, user satisfaction, and
the impact on customer engagement and conversion rates. By bridging the
convenience of online shopping with the personalized assistance traditionally
found in physical stores, Flippi sets a new standard for customer satisfaction
and engagement in the digital marketplace.

---

### 2. Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA

- **LLM Score**: 6
- **Keyword Score**: 15
- **Authors**: Shashank Verma, Fengyi Jiang, Xiangning Xue
- **URL**: <http://arxiv.org/abs/2507.05577v1>
- **Submitted**: 2025-07-08 01:25:06
- **Comment**: Paper submitted to CLEF 2025 CEUR-WS
- **Topic Keywords**: information retrieval, ranking, rerank, rag, retrieval, rank, search
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval (IR) and Search technologies, particularly in the context of biomedical question answering. The use of cross-encoders, LLMs, and reranking models is relevant to the user's focus on query understanding and ranking models. However, the paper's specific focus on biomedical literature and question answering is not directly aligned with the user's broader interests in e-commerce and general information retrieval.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) system for biomedical question answering
- **Aim**: To develop a system that can efficiently retrieve and generate accurate answers to biomedical questions
- **Rationale**: The system combines a bi-encoder architecture for efficient top-1k retrieval and cross-encoders for re-ranking to select the final top-10 documents, and uses few-shot prompting of instruction-tuned large language models for answer generation
- **Ground**: The system was evaluated on the BioASQ 2025 Task 13b Challenge and achieved a Mean Average Precision (MAP)@10 of 0.1581, placing 10th on the leaderboard
- **Experiment**: The authors experimented with different prompting strategies, including the number of few-shot examples and the design of prompts templates, and compared the performance of two language models, GPT-4o-turbo and Mistral-7B-Instruct-v0.3
- **Takeaway**: The system demonstrates the effectiveness of a hybrid approach combining bi-encoders and cross-encoders for biomedical text retrieval and answer generation, and highlights the importance of prompt style and fine-tuning for improved performance

#### Abstract
> Biomedical semantic question answering rooted in information retrieval can
play a crucial role in keeping up to date with vast, rapidly evolving and
ever-growing biomedical literature. A robust system can help researchers,
healthcare professionals and even layman users access relevant knowledge
grounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important
benchmark, offering a competitive platform for advancement of this space. This
paper presents the methodologies and results from our participation in this
challenge where we built a Retrieval-Augmented Generation (RAG) system that can
answer biomedical questions by retrieving relevant PubMed documents and
snippets to generate answers. For the retrieval task, we generated dense
embeddings from biomedical articles for initial retrieval, and applied an
ensemble of finetuned cross-encoders and large language models (LLMs) for
re-ranking to identify top relevant documents. Our solution achieved an MAP@10
of 0.1581, placing 10th on the leaderboard for the retrieval task. For answer
generation, we employed few-shot prompting of instruction-tuned LLMs. Our
system achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean
Reciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of
0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal
answers (rank 11).

---

### 3. SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression

- **LLM Score**: 6
- **Keyword Score**: 14
- **Authors**: Yiqiao Jin, Kartik Sharma, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar
- **URL**: <http://arxiv.org/abs/2507.05633v1>
- **Submitted**: 2025-07-08 03:29:09
- **Comment**: 20 pages
- **Topic Keywords**: ranking, rerank, relevance, rag, retrieval, rank
- **Reason**: The paper proposes a framework for Retrieval-augmented Generation (RAG) that balances local precision and global knowledge coverage. While it's related to information retrieval and natural language processing, the focus is on integrating textual and compressed representations for robust RAG, which is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. However, the paper's emphasis on context efficiency and answer correctness may be of interest to the user, especially in areas that require deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) framework for natural language processing
- **Aim**: To propose a unified RAG framework, SARA, that addresses key challenges in RAG, including limited effective context, context redundancy, and compression-fidelity trade-off
- **Rationale**: SARA combines natural-language text snippets with semantic compression vectors to enhance context efficiency and answer correctness, and represents contexts at two levels: fine-grained natural-language spans and compact, interpretable vectors
- **Ground**: The framework consists of two stages: Compression Learning and Instruction-tuning, and is model-agnostic, compatible with any retrievers, embedding models, and open-source Large Language Models (LLMs)
- **Experiment**: The authors evaluate SARA's generalizability across different retrieval, embedding, and generation components, experimenting with both sparse and dense retrievers, and diverse datasets spanning different domains, input length, and task types
- **Takeaway**: SARA consistently outperforms baselines on both lexical and LLM-based evaluation metrics, and demonstrates the importance of balancing compression efficiency with faithfulness in RAG

#### Abstract
> Retrieval-augmented Generation (RAG) extends large language models (LLMs)
with external knowledge but faces key challenges: restricted effective context
length and redundancy in retrieved documents. Pure compression-based approaches
reduce input size but often discard fine-grained details essential for factual
accuracy. We propose SARA, a unified RAG framework that balances local
precision and global knowledge coverage under tight context budgets. SARA
combines natural-language text snippets with semantic compression vectors to
jointly enhance context efficiency and answer correctness. It represents
contexts at two complementary levels: 1) fine-grained natural-language spans
that preserve critical entities and numerical values, and 2) compact,
interpretable vectors that summarize high-level semantics. An iterative
evidence-selection module employs the compression vectors for dynamic reranking
of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families
(Mistral, Llama, and Gemma), SARA consistently improves answer relevance
(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),
demonstrating the importance of integrating textual and compressed
representations for robust, context-efficient RAG.

---

### 4. Enhancing the Interpretability of Rule-based Explanations through Information Retrieval

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Alessandro Umbrico, Guido Bologna, Luca Coraci, Francesca Fracasso, Silvia Gola, Gabriella Cortellessa
- **URL**: <http://arxiv.org/abs/2507.05976v1>
- **Submitted**: 2025-07-08 13:32:50
- **Topic Keywords**: information retrieval, relevance, retrieval
- **Reason**: The paper explores the intersection of Information Retrieval and Explainable AI, which is relevant to your interests in query understanding and ranking models. However, the focus on rule-based explanations and healthcare decision-making processes is somewhat niche and may not directly align with your primary focus on information retrieval in e-commerce and other domains.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Explainable AI (XAI) for Trustworthy AI Systems
- **Aim**: To improve the trustworthiness of AI systems by providing insights into how and why AI models produce predictions
- **Rationale**: Current explainability methods have limitations, and a novel approach is needed to provide flexible and dynamic interpretation of AI model outcomes
- **Ground**: Attribution-based analysis combining information retrieval and semantic clustering of attributes, rule extraction algorithm, and attribute relevance computation
- **Experiment**: Experimental evaluation of the proposed approach using global and local explanations, user study with 30 participants to evaluate interpretability and usefulness of visualization options
- **Takeaway**: The proposed approach improves the interpretability of rule-based prediction models, making AI systems more trustworthy and useful in high-stakes domains like healthcare

#### Abstract
> The lack of transparency of data-driven Artificial Intelligence techniques
limits their interpretability and acceptance into healthcare decision-making
processes. We propose an attribution-based approach to improve the
interpretability of Explainable AI-based predictions in the specific context of
arm lymphedema's risk assessment after lymph nodal radiotherapy in breast
cancer. The proposed method performs a statistical analysis of the attributes
in the rule-based prediction model using standard metrics from Information
Retrieval techniques. This analysis computes the relevance of each attribute to
the prediction and provides users with interpretable information about the
impact of risk factors. The results of a user study that compared the output
generated by the proposed approach with the raw output of the Explainable AI
model suggested higher levels of interpretability and usefulness in the context
of predicting lymphedema risk.

---

### 5. Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Sandeep Mishra, Anubhab Mandal, Bishal Santra, Tushar Abhishek, Pawan Goyal, Manish Gupta
- **URL**: <http://arxiv.org/abs/2507.05940v1>
- **Submitted**: 2025-07-08 12:38:41
- **Topic Keywords**: query, queries, search
- **Reason**: The paper explores query auto-completion in dialog systems, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on chat-based systems and dialog context, which is not directly aligned with the user's primary research interests in e-commerce and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Chat-Ghosting in modern search engines and chat interfaces
- **Aim**: To conduct an open and thorough study on Chat-Ghosting using various query auto-completion methods
- **Rationale**: The problem of Chat-Ghosting has received limited attention from the NLP/ML research community despite the growing prominence of chat-based systems
- **Ground**: Four publicly available dialog datasets
- **Experiment**: Experimenting with tries, n-gram models, and deep learning methods, including T5 and Phi-2, and incorporating conversational context
- **Takeaway**: Statistical n-gram models and tries outperform deep learning-based models for seen prefixes, while neural models perform better for unseen queries, and incorporating conversational context leads to significant improvements in ghosting quality

#### Abstract
> Ghosting, the ability to predict a user's intended text input for inline
query auto-completion, is an invaluable feature for modern search engines and
chat interfaces, greatly enhancing user experience. By suggesting completions
to incomplete queries (or prefixes), ghosting aids users with slow typing
speeds, disabilities, or limited language proficiency. Ghosting is a
challenging problem and has become more important with the ubiquitousness of
chat-based systems like ChatGPT, Copilot, etc. Despite the increasing
prominence of chat-based systems utilizing ghosting, this challenging problem
of Chat-Ghosting has received little attention from the NLP/ML research
community. There is a lack of standardized benchmarks and relative performance
analysis of deep learning and non-deep learning methods. We address this
through an open and thorough study of this problem using four publicly
available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and
two human-bot (Open Assistant and ShareGPT). We experiment with various
existing query auto-completion methods (using tries), n-gram methods and deep
learning methods, with and without dialog context. We also propose a novel
entropy-based dynamic early stopping strategy. Our analysis finds that
statistical n-gram models and tries outperform deep learning based models in
terms of both model performance and inference efficiency for seen prefixes. For
unseen queries, neural models like T5 and Phi-2 lead to better results. Adding
conversational context leads to significant improvements in ghosting quality,
especially for Open-Assistant and ShareGPT. We make code and data publicly
available

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Kechen Liu
- **URL**: <http://arxiv.org/abs/2507.05733v1>
- **Submitted**: 2025-07-08 07:26:55
- **Topic Keywords**: rag, recommend, rank, search
- **Reason**: The paper explores the integration of self-attentive sequential recommendation with fine-tuned large language models, which is related to query understanding and ranking models in information retrieval. However, the focus on recommender systems and language models is not directly aligned with the user's primary interest in information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization.

#### Abstract
> Self-Attentive Sequential Recommendation (SASRec) effectively captures
long-term user preferences by applying attention mechanisms to historical
interactions. Concurrently, the rise of Large Language Models (LLMs) has
motivated research into LLM-based recommendation, which leverages their
powerful generalization and language understanding capabilities. However, LLMs
often lack the domain-specific knowledge and collaborative signals essential
for high-quality recommendations when relying solely on textual prompts. To
address this limitation, this study proposes SASRecLLM, a novel framework that
integrates SASRec as a collaborative encoder with an LLM fine-tuned using
Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to
align their dimensional spaces, and three targeted training strategies are
designed to optimize the hybrid architecture. Extensive experiments on multiple
datasets demonstrate that SASRecLLM achieves robust and consistent improvements
over strong baselines in both cold-start and warm-start scenarios. This work
advances the field of LLM-based recommendation by presenting a modular and
effective paradigm for fusing structured collaborative filtering with the
semantic power of fine-tuned LLMs. The implementation is available on GitHub:
https://github.com/kechenkristin/RecLLM

### 7. Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers

- **LLM Score**: 4
- **Keyword Score**: 19
- **Authors**: Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang
- **URL**: <http://arxiv.org/abs/2507.06223v1>
- **Submitted**: 2025-07-08 17:56:28
- **Comment**: under review
- **Topic Keywords**: information retrieval, queries, ranking, rerank, relevance, retrieval, rank, search
- **Reason**: The paper focuses on the efficiency and effectiveness of Large Language Models (LLMs) in reranking tasks, proposing new metrics and an estimator to evaluate their performance. While it's related to search technologies and ranking models, it doesn't directly address query understanding, user behavior modeling, or deep semantic understanding, which are core aspects of your research interests.

#### Abstract
> Large Language Models (LLMs) have recently been applied to reranking tasks in
information retrieval, achieving strong performance. However, their high
computational demands often hinder practical deployment. Existing studies
evaluate the efficiency of LLM-based rerankers using proxy metrics such as
latency, the number of forward passes, input tokens, and output tokens.
However, these metrics depend on hardware and running-time choices (\eg
parallel or not, batch size, etc), and often fail to account for model size,
making it difficult to interpret and obscuring the evaluation of the
efficiency-effectiveness tradeoff. To address this issue, we propose
E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per
PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for
hardware-agnostic throughput. Companied with the new metrics, an interpretable
FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even
without running any experiments. Based on the proposed metrics, we conduct
comprehensive experiments to evaluate a wide range of LLM-based rerankers with
different architecture, studying the efficiency-effectiveness trade-off and
bringing this issue to the attention of the research community.

### 8. RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based Recommendation

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Zeyuan Meng, Zixuan Yi, Iadh Ounis
- **URL**: <http://arxiv.org/abs/2507.05880v1>
- **Submitted**: 2025-07-08 11:04:17
- **Topic Keywords**: ranking, listwise, pointwise, pairwise, recommend, rank
- **Reason**: The paper focuses on recommender systems, which is a related topic to information retrieval. However, the emphasis is on large language models and their application to recommendation tasks, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in information retrieval. The paper's scope is also limited to recommender systems, and does not explore other areas of interest such as natural language processing or data mining.

#### Abstract
> A recent Large language model (LLM)-based recommendation model, called
RecRanker, has demonstrated a superior performance in the top-k recommendation
task compared to other models. In particular, RecRanker samples users via
clustering, generates an initial ranking list using an initial recommendation
model, and fine-tunes an LLM through hybrid instruction tuning to infer user
preferences. However, the contribution of each core component remains
underexplored. In this work, we inspect the reproducibility of RecRanker, and
study the impact and role of its various components. We begin by reproducing
the RecRanker pipeline through the implementation of all its key components.
Our reproduction shows that the pairwise and listwise methods achieve a
performance comparable to that reported in the original paper. For the
pointwise method, while we are also able to reproduce the original paper's
results, further analysis shows that the performance is abnormally high due to
data leakage from the inclusion of ground-truth information in the prompts. To
enable a fair and comprehensive evaluation of LLM-based top-k recommendations,
we propose RecRankerEval, an extensible framework that covers five key
dimensions: user sampling strategy, initial recommendation model, LLM backbone,
dataset selection, and instruction tuning method. Using the RecRankerEval
framework, we show that the original results of RecRanker can be reproduced on
the ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,
but not on BookCrossing due to the lack of timestamp information in the
original RecRanker paper. Furthermore, we demonstrate that RecRanker's
performance can be improved by employing alternative user sampling methods,
stronger initial recommenders, and more capable LLMs.

### 9. SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Jiale Lao, Immanuel Trummer
- **URL**: <http://arxiv.org/abs/2507.06192v1>
- **Submitted**: 2025-07-08 17:20:34
- **Topic Keywords**: query, queries, rag, search
- **Reason**: The paper presents a system for generating customized and realistic SQL workloads using Large Language Models, which is related to information retrieval and search technologies. However, the focus is on database research and development, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and data mining, but it does not explore deep semantic understanding or real-time relevance optimization.

#### Abstract
> Database research and development often require a large number of SQL queries
for benchmarking purposes. However, acquiring real-world SQL queries is
challenging due to privacy concerns, and existing SQL generation methods are
limited in customization and in satisfying realistic constraints. To address
this issue, we present SQLBarber, a system based on Large Language Models
(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)
eliminates the need for users to manually craft SQL templates in advance, while
providing the flexibility to accept natural language specifications to
constrain SQL templates, (ii) scales efficiently to generate large volumes of
queries matching any user-defined cost distribution (e.g., cardinality and
execution plan cost), and (iii) uses execution statistics from Amazon Redshift
and Snowflake to derive SQL template specifications and query cost
distributions that reflect real-world query characteristics. SQLBarber
introduces (i) a declarative interface for users to effortlessly generate
customized SQL templates, (ii) an LLM-powered pipeline augmented with a
self-correction module that profiles, refines, and prunes SQL templates based
on query costs, and (iii) a Bayesian Optimizer to efficiently explore different
predicate values and identify a set of queries that satisfy the target cost
distribution. We construct and open-source ten benchmarks of varying difficulty
levels and target query cost distributions based on real-world statistics from
Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show
that SQLBarber is the only system that can generate customized SQL templates.
It reduces query generation time by one to three orders of magnitude, and
significantly improves alignment with the target cost distribution, compared
with existing methods.

### 10. Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Y. Du
- **URL**: <http://arxiv.org/abs/2507.05933v1>
- **Submitted**: 2025-07-08 12:33:11
- **Comment**: 7 pages
- **Topic Keywords**: query, queries, retrieval
- **Reason**: The paper focuses on vector retrieval systems and proposes a framework for evaluating embedding quality, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus is on evaluating embedding quality rather than query understanding or ranking models, making it only loosely relevant to the user's research interests.

#### Abstract
> Vector retrieval systems exhibit significant performance variance across
queries due to heterogeneous embedding quality. We propose a lightweight
framework for predicting retrieval performance at the query level by combining
quantization robustness and neighborhood density metrics. Our approach is
motivated by the observation that high-quality embeddings occupy geometrically
stable regions in the embedding space and exhibit consistent neighborhood
structures. We evaluate our method on 4 standard retrieval datasets, showing
consistent improvements of 9.4$\pm$1.2\% in Recall@10 over competitive
baselines. The framework requires minimal computational overhead (less than 5\%
of retrieval time) and enables adaptive retrieval strategies. Our analysis
reveals systematic patterns in embedding quality across different query types,
providing insights for targeted training data augmentation.

### 11. DRAGON: Dynamic RAG Benchmark On News

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Fedor Chernogorskii, Sergei Averkiev, Liliya Kudraleeva, Zaven Martirosian, Maria Tikhonova, Valentin Malykh, Alena Fenogenova
- **URL**: <http://arxiv.org/abs/2507.05713v1>
- **Submitted**: 2025-07-08 06:52:43
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper presents a benchmark for Retrieval-Augmented Generation (RAG) in Russian, focusing on the dynamic nature of real-world deployments. While it touches on information retrieval and natural language processing, the primary focus is on language models and generation, which is not directly related to the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Retrieval-Augmented Generation (RAG) is a widely adopted approach for
improving the factuality of large language models (LLMs) by incorporating
external knowledge at inference time. Although there exist multiple RAG
benchmarks for English, evaluation resources for other languages, including
Russian, remain scarce and static, failing to capture the dynamic nature of
real-world deployments.
  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first
dynamic benchmark for evaluating RAG systems in Russian on a changing news
corpora. DRAGON is built upon a regularly updated corpus of Russian news and
public documents and supports comprehensive evaluation of both the retriever
and generator components. Question generation is performed automatically with
the use of Knowledge Graph constructed from the corpus and enables the
extraction of four core question types aligned with distinct subgraph patterns.
We release a complete evaluation framework comprising the pipeline for
automatic question generation, evaluation scripts, which are potentially
reusable for other languages and multilingual settings, and benchmark data. We
also launch a public leaderboard to encourage community participation and
comparison.

### 12. KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zeyuan Meng, Zixuan Yi, Iadh Ounis
- **URL**: <http://arxiv.org/abs/2507.05863v1>
- **Submitted**: 2025-07-08 10:44:27
- **Topic Keywords**: rag, retrieval, recommend
- **Reason**: The paper explores the use of Large Language Models (LLMs) in recommender systems, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on recommendation systems and the lack of deep semantic understanding and real-time relevance optimization make it less relevant to my primary research themes.

#### Abstract
> Large Language Models (LLMs) have shown strong potential in recommender
systems due to their contextual learning and generalisation capabilities.
Existing LLM-based recommendation approaches typically formulate the
recommendation task using specialised prompts designed to leverage their
contextual abilities, and aligning their outputs closely with human preferences
to yield an improved recommendation performance. However, the use of LLMs for
recommendation tasks is limited by the absence of domain-specific knowledge.
This lack of relevant relational knowledge about the items to be recommended in
the LLM's pre-training corpus can lead to inaccuracies or hallucinations,
resulting in incorrect or misleading recommendations. Moreover, directly using
information from the knowledge graph introduces redundant and noisy
information, which can affect the LLM's reasoning process or exceed its input
context length, thereby reducing the performance of LLM-based recommendations.
To address the lack of domain-specific knowledge, we propose a novel model
called Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation
(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation
(GraphRAG) component to integrate additional information from a knowledge graph
(KG) into instructions, enabling the LLM to collaboratively exploit
recommendation signals from both text-based user interactions and the knowledge
graph to better estimate the users' preferences in a recommendation context. In
particular, we perform graph RAG by pre-training a graph attention network
(GAT) to select the most relevant triple for the target users for the used LLM,
thereby enhancing the LLM while reducing redundant and noisy information. Our
extensive experiments on three public datasets show that our proposed KERAG_R
model significantly outperforms ten existing state-of-the-art recommendation
methods.

### 13. HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: YiHan Jiao, ZheHao Tan, Dan Yang, DuoLin Sun, Jie Feng, Jian Wang, Peng Wei
- **URL**: <http://arxiv.org/abs/2507.05714v1>
- **Submitted**: 2025-07-08 06:53:28
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper proposes a new method for retrieval-augmented generation, focusing on hierarchical thought processes and instruction tuning. While it touches on retrieval and generation, the primary focus is on language models and their capabilities, rather than information retrieval or search technologies. The paper's relevance to the user's interests is limited, but it may be of interest to those exploring the intersection of NLP and IR.

#### Abstract
> Retrieval-augmented generation (RAG) has become a fundamental paradigm for
addressing the challenges faced by large language models in handling real-time
information and domain-specific problems. Traditional RAG systems primarily
rely on the in-context learning (ICL) capabilities of the large language model
itself. Still, in-depth research on the specific capabilities needed by the RAG
generation model is lacking, leading to challenges with inconsistent document
quality and retrieval system imperfections. Even the limited studies that
fine-tune RAG generative models often \textit{lack a granular focus on RAG
task} or \textit{a deeper utilization of chain-of-thought processes}. To
address this, we propose that RAG models should possess three progressively
hierarchical abilities (1) Filtering: the ability to select relevant
information; (2) Combination: the ability to combine semantic information
across paragraphs; and (3) RAG-specific reasoning: the ability to further
process external knowledge using internal knowledge. Thus, we introduce our new
RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning
Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering"
strategy. This method enhances the model's open-book examination capability by
utilizing multi-level progressive chain-of-thought. Experiments show that the
HIRAG training strategy significantly improves the model's performance on
datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

### 14. DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Maximilian Heil, Aleksandar Pramov
- **URL**: <http://arxiv.org/abs/2507.06195v1>
- **Submitted**: 2025-07-08 17:22:22
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on numerical fact verification, which is a specific sub-problem within Information Retrieval. While it touches on some relevant topics like tokenization and evidence retrieval, the primary focus is on natural language inference and classification, which is not directly related to query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is somewhat limited.

#### Abstract
> Numerical claims, statements involving quantities, comparisons, and temporal
references, pose unique challenges for automated fact-checking systems. In this
study, we evaluate modeling strategies for veracity prediction of such claims
using the QuanTemp dataset and building our own evidence retrieval pipeline. We
investigate three key factors: (1) the impact of more evidences with longer
input context windows using ModernBERT, (2) the effect of right-to-left (R2L)
tokenization, and (3) their combined influence on classification performance.
Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does
not boost natural language inference (NLI) of numerical tasks. A longer context
window does also not enhance veracity performance either, highlighting evidence
quality as the dominant bottleneck. Our best-performing system achieves
competitive macro-average F1 score of 0.57 and places us among the Top-4
submissions in Task 3 of CheckThat! 2025. Our code is available at
https://github.com/dsgt-arc/checkthat-2025-numerical.

### 15. Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Guillem Ram√≠rez, Alexandra Birch, Ivan Titov
- **URL**: <http://arxiv.org/abs/2507.05391v1>
- **Submitted**: 2025-07-07 18:22:55
- **Topic Keywords**: queries, search
- **Reason**: The paper explores the intersection of natural language processing and privacy, which is related to my interests in NLP and information retrieval. However, the focus on language model adherence to privacy preferences is not directly aligned with my primary research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) are primarily accessed via commercial APIs, but
this often requires users to expose their data to service providers. In this
paper, we explore how users can stay in control of their data by using privacy
profiles: simple natural language instructions that say what should and should
not be revealed. We build a framework where a local model uses these
instructions to rewrite queries, only hiding details deemed sensitive by the
user, before sending them to an external model, thus balancing privacy with
performance. To support this research, we introduce PEEP, a multilingual
dataset of real user queries annotated to mark private content and paired with
synthetic privacy profiles. Our experiments with lightweight LLMs show they can
follow these instructions to some extent, but also face consistent challenges,
highlighting the need for models that better understand and comply with
user-defined privacy preferences.

### 16. LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: William Fleshman, Benjamin Van Durme
- **URL**: <http://arxiv.org/abs/2507.05346v1>
- **Submitted**: 2025-07-07 18:00:01
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper proposes a method for selecting and combining fine-tuned language model experts, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on language tasks and knowledge-intensive applications, which is not directly aligned with the user's primary interest in search technologies and user behavior modeling.

#### Abstract
> The proliferation of fine-tuned language model experts for specific tasks and
domains signals the need for efficient selection and combination methods. We
propose LoRA-Augmented Generation (LAG) for leveraging large libraries of
knowledge and task-specific LoRA adapters. LAG requires no additional training
or access to data, and efficiently filters, retrieves, and applies experts on a
per-token and layer basis. We evaluate LAG on various knowledge-intensive
tasks, achieving superior performance over existing data-free methods. We
explore scenarios where additional data is available, demonstrating LAG's
compatibility with alternative solutions such as retrieval-augmented generation
(RAG).

### 17. Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yibin Liu, Ang Li, Shijian Li
- **URL**: <http://arxiv.org/abs/2507.06044v1>
- **Submitted**: 2025-07-08 14:45:47
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on explainable recommendations, which is somewhat related to information retrieval, but the primary focus is on recommender systems rather than search technologies. The use of large language models and contrastive prompting is interesting, but the lack of direct relevance to query understanding, ranking models, and user behavior modeling limits the paper's alignment with the user's core research themes.

#### Abstract
> Explainable recommendations, which use the information of user and item with
interaction to generate a explanation for why the user would interact with the
item, are crucial for improving user trust and decision transparency to the
recommender system. Existing methods primarily rely on encoding features of
users and items to embeddings, which often leads to information loss due to
dimensionality reduction, sparse interactions, and so on. With the advancements
of large language models (LLMs) in language comprehension, some methods use
embeddings as LLM inputs for explanation generation. However, since embeddings
lack inherent semantics, LLMs must adjust or extend their parameters to
interpret them, a process that inevitably incurs information loss. To address
this issue, we propose a novel approach combining profile generation via
hierarchical interaction summarization (PGHIS), which leverages a pretrained
LLM to hierarchically summarize user-item interactions, generating structured
textual profiles as explicit representations of user and item characteristics.
Additionally, we propose contrastive prompting for explanation generation
(CPEG) which employs contrastive learning to guide another reasoning language
models in producing high-quality ground truth recommendation explanations.
Finally, we use the textual profiles of user and item as input and high-quality
explanation as output to fine-tune a LLM for generating explanations.
Experimental results on multiple datasets demonstrate that our approach
outperforms existing state-of-the-art methods, achieving a great improvement on
metrics about explainability (e.g., 5% on GPTScore) and text quality.
Furthermore, our generated ground truth explanations achieve a significantly
higher win rate compared to user-written reviews and those produced by other
methods, demonstrating the effectiveness of CPEG in generating high-quality
ground truths.

### 18. ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Haoxin Wang, Xianhan Peng, Xucheng Huang, Yizhe Huang, Ming Gong, Chenghan Yang, Yang Liu, Ling Jiang
- **URL**: <http://arxiv.org/abs/2507.05639v1>
- **Submitted**: 2025-07-08 03:35:48
- **Topic Keywords**: commerce, e-commerce, search
- **Reason**: The paper focuses on evaluating LLM agents in e-commerce customer support, which is related to your background in e-commerce. However, the primary focus is on benchmarking and evaluation, rather than query understanding, ranking models, or user behavior modeling, which are your core research interests in IR and NLP.

#### Abstract
> In this paper, we introduce ECom-Bench, the first benchmark framework for
evaluating LLM agent with multimodal capabilities in the e-commerce customer
support domain. ECom-Bench features dynamic user simulation based on persona
information collected from real e-commerce customer interactions and a
realistic task dataset derived from authentic e-commerce dialogues. These
tasks, covering a wide range of business scenarios, are designed to reflect
real-world complexities, making ECom-Bench highly challenging. For instance,
even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our
benchmark, highlighting the substantial difficulties posed by complex
e-commerce scenarios. Upon publication, the code and data will be open-sourced
to facilitate further research and development in this domain.

### 19. CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yuchen Huang, Zhiyuan Fan, Zhitao He, Sandeep Polisetty, Wenyan Li, Yi R. Fung
- **URL**: <http://arxiv.org/abs/2507.06210v1>
- **Submitted**: 2025-07-08 17:38:56
- **Comment**: 25 pages, COLM 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on improving the cultural awareness of CLIP, a vision-language model, by creating a synthetic dataset and fine-tuning it. While it's related to multimodal understanding and deep semantic understanding, it's not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

### 20. DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Nicholas Popoviƒç, Ashish Kangen, Tim Schopf, Michael F√§rber
- **URL**: <http://arxiv.org/abs/2507.05997v1>
- **Submitted**: 2025-07-08 13:55:25
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on document-level entity and relation extraction using synthetic data generation and in-context learning, which is related to information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests.

#### Abstract
> Large, high-quality annotated corpora remain scarce in document-level entity
and relation extraction in zero-shot or few-shot settings. In this paper, we
present a fully automatic, LLM-based pipeline for synthetic data generation and
in-context learning for document-level entity and relation extraction. In
contrast to existing approaches that rely on manually annotated demonstrations
or direct zero-shot inference, our method combines synthetic data generation
with retrieval-based in-context learning, using a reasoning-optimized language
model. This allows us to build a high-quality demonstration database without
manual annotation and to dynamically retrieve relevant examples at inference
time. Based on our approach we produce a synthetic dataset of over $5k$
Wikipedia abstracts with approximately $59k$ entities and $30k$ relation
triples. Finally, we evaluate in-context learning performance on the DocIE
shared task, extracting entities and relations from long documents in a
zero-shot setting. We find that in-context joint entity and relation extraction
at document-level remains a challenging task, even for state-of-the-art large
language models.

### 21. Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jiahuan Pei, Fanghua Ye, Xin Sun, Wentao Deng, Koen Hindriks, Junxiao Wang
- **URL**: <http://arxiv.org/abs/2507.05528v1>
- **Submitted**: 2025-07-07 22:56:37
- **Comment**: 14 pages
- **Topic Keywords**: rag
- **Reason**: The paper explores the application of large language models in conversational education, which is related to information retrieval and search technologies. However, the focus is on procedural learning and pedagogic quality assessment, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited, but it does touch on NLP and AI4Education, which are adjacent to the user's research areas.

#### Abstract
> Large language models (LLMs) have advanced virtual educators and learners,
bridging NLP with AI4Education. Existing work often lacks scalability and fails
to leverage diverse, large-scale course content, with limited frameworks for
assessing pedagogic quality. To this end, we propose WikiHowAgent, a
multi-agent workflow leveraging LLMs to simulate interactive teaching-learning
conversations. It integrates teacher and learner agents, an interaction
manager, and an evaluator to facilitate procedural learning and assess
pedagogic quality. We introduce a dataset of 114,296 teacher-learner
conversations grounded in 14,287 tutorials across 17 domains and 727 topics.
Our evaluation protocol combines computational and rubric-based metrics with
human judgment alignment. Results demonstrate the workflow's effectiveness in
diverse setups, offering insights into LLM capabilities across domains. Our
datasets and implementations are fully open-sourced.

### 22. "Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yufei Tao, Adam Hiatt, Rahul Seetharaman, Ameeta Agrawal
- **URL**: <http://arxiv.org/abs/2507.05424v1>
- **Submitted**: 2025-07-07 19:13:20
- **Topic Keywords**: rag
- **Reason**: The paper explores the contextual grounding of large language models, introducing a novel evaluation framework and analyzing how models prioritize and integrate contextual and parametric knowledge. While it touches on the topic of query understanding and ranking models, it is not directly related to my primary focus on information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large language models are capable of leveraging both contextual and
parametric knowledge but how they prioritize and integrate these sources
remains underexplored. We introduce CoPE, a novel evaluation framework that
systematically measures contextual knowledge (CK) and parametric knowledge (PK)
across models and languages. Using our MultiWikiAtomic dataset in English,
Spanish, and Danish, we analyze how large language models (LLMs) integrate
context, prioritize information, and incorporate PK in open-ended question
answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where
LLMs tend to overlook or deprioritize information that appears later in a given
context, revealing a strong positional bias that affects contextual grounding.
We further find that reasoning models, as well as non-reasoning models prompted
with chain-of-thought (CoT), use context even less than non-reasoning models
without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,
in particular, results in lower recall and shorter responses, leading to
degraded contextual grounding. Based on these insights, we design prompt-based
methods to effectively leverage input context. A case study applying CoPE to
summarization demonstrates that CK-informed prompting improves factual
grounding and reduces hallucination.

### 23. Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Christos Nikou, Theodoros Giannakopoulos
- **URL**: <http://arxiv.org/abs/2507.06070v1>
- **Submitted**: 2025-07-08 15:13:26
- **Comment**: International Journal of Music Science, Technology and Art, 15 pages,
  7 figures
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper focuses on audio fingerprinting, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions deep neural networks and transfer learning, the context is specific to audio processing and not applicable to the user's research interests.

#### Abstract
> Recent advances in song identification leverage deep neural networks to learn
compact audio fingerprints directly from raw waveforms. While these methods
perform well under controlled conditions, their accuracy drops significantly in
real-world scenarios where the audio is captured via mobile devices in noisy
environments. In this paper, we introduce a novel evaluation protocol designed
to better reflect such real-world conditions. We generate three recordings of
the same audio, each with increasing levels of noise, captured using a mobile
device's microphone. Our results reveal a substantial performance drop for two
state-of-the-art CNN-based models under this protocol, compared to previously
reported benchmarks. Additionally, we highlight the critical role of the
augmentation pipeline during training with contrastive loss. By introduction
low pass and high pass filters in the augmentation pipeline we significantly
increase the performance of both systems in our proposed evaluation.
Furthermore, we develop a transformer-based model with a tailored projection
module and demonstrate that transferring knowledge from a semantically relevant
domain yields a more robust solution. The transformer architecture outperforms
CNN-based models across all noise levels, and query durations. In low noise
conditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in
finding the correct song, surpassing by 14%, and by 18.5% the second-best
performing model, respectively, Under heavy noise levels, we achieve a
detection rate 56.5% for 15-second query duration. All experiments are
conducted on public large-scale dataset of over 100K songs, with queries
matched against a database of 56 million vectors.

### 24. GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Moritz M√ºller, Simon Razniewski
- **URL**: <http://arxiv.org/abs/2507.05740v1>
- **Submitted**: 2025-07-08 07:37:12
- **Comment**: 7 pages, 6 figures, 1 table
- **Topic Keywords**: query, search, acl
- **Reason**: The paper focuses on building a massive knowledge base for exploring factual LLM knowledge, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions LLMs, the context is different from the user's interests in IR and NLP.

#### Abstract
> Language models are powerful tools, yet their factual knowledge is still
poorly understood, and inaccessible to ad-hoc browsing and scalable statistical
analysis. This demonstration introduces GPTKB v1.5, a densely interlinked
100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using
the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu
et al., ACL 2025). The demonstration experience focuses on three use cases: (1)
link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM
knowledge querying, (3) comparative exploration of the strengths and weaknesses
of LLM knowledge. Massive-recursive LLM knowledge materialization is a
groundbreaking opportunity both for the research area of systematic analysis of
LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator
is accessible at https://gptkb.org.

### 25. Vers un cadre ontologique pour la gestion des comp{√©}tences : {√†} des fins de formation, de recrutement, de m{√©}tier, ou de recherches associ{√©}es

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Ngoc Luyen Le, Marie-H√©l√®ne Abel, Bertrand Laforge
- **URL**: <http://arxiv.org/abs/2507.05767v1>
- **Submitted**: 2025-07-08 08:13:30
- **Comment**: in French language. 36es Journ{\'e}es francophones d'Ing{\'e}nierie
  des Connaissances (IC 2025) @ Plate-Forme Intelligence Artificielle (PFIA
  2025), Jul 2025, Dijon, France
- **Topic Keywords**: rag, recommend, personalization, search
- **Reason**: The paper focuses on competence management and ontology-based frameworks, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions semantic understanding and automation, the context is different and not directly applicable to the user's areas of focus.

#### Abstract
> The rapid transformation of the labor market, driven by technological
advancements and the digital economy, requires continuous competence
development and constant adaptation. In this context, traditional competence
management systems lack interoperability, adaptability, and semantic
understanding, making it difficult to align individual competencies with labor
market needs and training programs. This paper proposes an ontology-based
framework for competence management, enabling a structured representation of
competencies, occupations, and training programs. By leveraging ontological
models and semantic reasoning, this framework aims to enhance the automation of
competence-to-job matching, the personalization of learning recommendations,
and career planning. This study discusses the design, implementation, and
potential applications of the framework, focusing on competence training
programs, job searching, and finding competent individuals.

### 26. Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Alex ZH Dou, Zhongwei Wan, Dongfei Cui, Xin Wang, Jing Xiong, Haokun Lin, Chaofan Tao, Shen Yan, Mi Zhang
- **URL**: <http://arxiv.org/abs/2507.05557v1>
- **Submitted**: 2025-07-08 00:41:12
- **Comment**: Technical Report
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper focuses on large language models and hierarchical retrieval-augmented reasoning, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions retrieval, it is not in the context of search or IR, and the paper's primary focus is on language modeling and complex reasoning tasks.

#### Abstract
> Test-time scaling has emerged as a promising paradigm in language modeling,
leveraging additional computational resources at inference time to enhance
model performance. In this work, we introduce R2-LLMs, a novel and versatile
hierarchical retrieval-augmented reasoning framework designed to improve
test-time scaling in large language models (LLMs) without requiring
distillation from more advanced models to obtain chain-of-thought (CoT)
training data. R2-LLMs enhances inference-time generalization by integrating
dual-level retrieval-based in-context learning: (1) At the coarse level, our
approach extracts abstract templates from complex reasoning problems and
retrieves similar problem-answer pairs to facilitate high-level in-context
learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs
efficiently retrieves analogous intermediate solution steps from reference
mathematical problem datasets, refining step-wise reasoning with the aid of a
process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical
reasoning-augmentation method that enhances in-context-level reasoning while
seamlessly integrating with step-level tree search methods. Utilizing PRM, it
refines both candidate generation and decision-making for improved reasoning
accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO
datasets achieve substantial relative improvement with an increase of up to 16%
using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of
our approach in complex reasoning tasks.

### 27. Unconditional Diffusion for Generative Sequential Recommendation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Yimeng Bai, Yang Zhang, Sihao Ding, Shaohui Ruan, Han Yao, Danhui Guan, Fuli Feng, Tat-Seng Chua
- **URL**: <http://arxiv.org/abs/2507.06121v1>
- **Submitted**: 2025-07-08 16:05:18
- **Topic Keywords**: rag, recommend, personalization
- **Reason**: The paper focuses on generative sequential recommendation using diffusion models, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on user behavior modeling, it is not in the context of search or e-commerce, and the paper's primary focus is on recommender systems rather than information retrieval.

#### Abstract
> Diffusion models, known for their generative ability to simulate data
creation through noise-adding and denoising processes, have emerged as a
promising approach for building generative recommenders. To incorporate user
history for personalization, existing methods typically adopt a conditional
diffusion framework, where the reverse denoising process of reconstructing
items from noise is modified to be conditioned on the user history. However,
this design may fail to fully utilize historical information, as it gets
distracted by the need to model the "item $\leftrightarrow$ noise" translation.
This motivates us to reformulate the diffusion process for sequential
recommendation in an unconditional manner, treating user history (instead of
noise) as the endpoint of the forward diffusion process (i.e., the starting
point of the reverse process), rather than as a conditional input. This
formulation allows for exclusive focus on modeling the "item $\leftrightarrow$
history" translation. To this end, we introduce Brownian Bridge Diffusion
Recommendation (BBDRec). By leveraging a Brownian bridge process, BBDRec
enforces a structured noise addition and denoising mechanism, ensuring that the
trajectories are constrained towards a specific endpoint -- user history,
rather than noise. Extensive experiments demonstrate BBDRec's effectiveness in
enhancing sequential recommendation performance. The source code is available
at https://github.com/baiyimeng/BBDRec.

### 28. The bitter lesson of misuse detection

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hadrien Mariaccia, Charbel-Rapha√´l Segerie, Diego Dorn
- **URL**: <http://arxiv.org/abs/2507.06282v1>
- **Submitted**: 2025-07-08 15:21:17
- **Topic Keywords**: queries, search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on language models and misuse detection, which is not a core area of interest. While the paper touches on the concept of semantic understanding, it is not applied in the context of search or retrieval.

#### Abstract
> Prior work on jailbreak detection has established the importance of
adversarial robustness for LLMs but has largely focused on the model ability to
resist adversarial inputs and to output safe content, rather than the
effectiveness of external supervision systems. The only public and independent
benchmark of these guardrails to date evaluates a narrow set of supervisors on
limited scenarios. Consequently, no comprehensive public benchmark yet verifies
how well supervision systems from the market perform under realistic, diverse
attacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of
LLM Supervision Systems. The framework is two dimensional: harm severity
(benign, borderline, harmful) and adversarial sophistication (direct vs.
jailbreak) and provides a rich dataset covering 3 jailbreak families and 11
harm categories. Our evaluations reveal drastic limitations of specialized
supervision systems. While they recognize some known jailbreak patterns, their
semantic understanding and generalization capabilities are very limited,
sometimes with detection rates close to zero when asking a harmful question
directly or with a new jailbreak technique such as base64 encoding. Simply
asking generalist LLMs if the user question is "harmful or not" largely
outperforms these supervisors from the market according to our BELLS score. But
frontier LLMs still suffer from metacognitive incoherence, often responding to
queries they correctly identify as harmful (up to 30 percent for Claude 3.7 and
greater than 50 percent for Mistral Large). These results suggest that simple
scaffolding could significantly improve misuse detection robustness, but more
research is needed to assess the tradeoffs of such techniques. Our results
support the "bitter lesson" of misuse detection: general capabilities of LLMs
are necessary to detect a diverse array of misuses and jailbreaks.

### 29. Towards a Principled Evaluation of Knowledge Editors

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Sebastian Pohl, Max Ploner, Alan Akbik
- **URL**: <http://arxiv.org/abs/2507.05937v1>
- **Submitted**: 2025-07-08 12:37:54
- **Comment**: Accepted at L2M2 workshop at ACL 2025
- **Topic Keywords**: ranking, rank
- **Reason**: The paper focuses on knowledge editing, a topic outside the scope of information retrieval and search technologies. While it touches on evaluation methodologies, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to the user's research is limited.

#### Abstract
> Model editing has been gaining increasing attention over the past few years.
For Knowledge Editing in particular, more challenging evaluation datasets have
recently been released. These datasets use different methodologies to score the
success of editors. Yet, it remains under-explored how robust these
methodologies are and whether they unfairly favor some editors. Moreover, the
disruptive impact of these editors on overall model capabilities remains a
constant blind spot.
  We address both of these problems and show that choosing different metrics
and evaluation methodologies as well as different edit batch sizes can lead to
a different ranking of knowledge editors. Crucially we demonstrate this effect
also on general language understanding tasks evaluated alongside the knowledge
editing tasks. Further we include a manual assessment of the string matching
based evaluation method for knowledge editing that is favored by recently
released datasets, revealing a tendency to produce false positive matches.

### 30. How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Tanvina Patel, Wiebke Hutiri, Aaron Yi Ding, Odette Scharenborg
- **URL**: <http://arxiv.org/abs/2507.05885v1>
- **Submitted**: 2025-07-08 11:17:13
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper focuses on automatic speech recognition (ASR) and bias measurement, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> There is increasingly more evidence that automatic speech recognition (ASR)
systems are biased against different speakers and speaker groups, e.g., due to
gender, age, or accent. Research on bias in ASR has so far primarily focused on
detecting and quantifying bias, and developing mitigation approaches. Despite
this progress, the open question is how to measure the performance and bias of
a system. In this study, we compare different performance and bias measures,
from literature and proposed, to evaluate state-of-the-art end-to-end ASR
systems for Dutch. Our experiments use several bias mitigation strategies to
address bias against different speaker groups. The findings reveal that
averaged error rates, a standard in ASR research, alone is not sufficient and
should be supplemented by other measures. The paper ends with recommendations
for reporting ASR performance and bias to better represent a system's
performance for diverse speaker groups, and overall system bias.

### 31. On the Costs and Benefits of Learned Indexing for Dynamic High-Dimensional Data: Extended Version

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ter√©zia Slanin√°kov√°, Jaroslav Olha, David Proch√°zka, Matej Antol, Vlastislav Dohnal
- **URL**: <http://arxiv.org/abs/2507.05865v1>
- **Submitted**: 2025-07-08 10:47:03
- **Topic Keywords**: query, search
- **Reason**: The paper focuses on learned indexing for dynamic high-dimensional data, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on query performance, the context is different from the user's primary interests in IR and NLP.

#### Abstract
> One of the main challenges within the growing research area of learned
indexing is the lack of adaptability to dynamically expanding datasets. This
paper explores the dynamization of a static learned index for complex data
through operations such as node splitting and broadening, enabling efficient
adaptation to new data. Furthermore, we evaluate the trade-offs between static
and dynamic approaches by introducing an amortized cost model to assess query
performance in tandem with the build costs of the index structure, enabling
experimental determination of when a dynamic learned index outperforms its
static counterpart. We apply the dynamization method to a static learned index
and demonstrate that its superior scaling quickly surpasses the static
implementation in terms of overall costs as the database grows. This is an
extended version of the paper presented at DAWAK 2025.

### 32. PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh
- **URL**: <http://arxiv.org/abs/2507.05444v1>
- **Submitted**: 2025-07-07 19:50:12
- **Topic Keywords**: rag, search, korea
- **Reason**: The paper focuses on vocabulary acquisition for second-language learners, using large language models to generate mnemonic devices. While it involves natural language processing, the topic is not directly related to information retrieval, search technologies, or query understanding, which are the user's primary research interests.

#### Abstract
> Vocabulary acquisition poses a significant challenge for second-language (L2)
learners, especially when learning typologically distant languages such as
English and Korean, where phonological and structural mismatches complicate
vocabulary learning. Recently, large language models (LLMs) have been used to
generate keyword mnemonics by leveraging similar keywords from a learner's
first language (L1) to aid in acquiring L2 vocabulary. However, most of this
research has focused on native English speakers learning other languages,
rather than the reverse. In this paper, we present PhoniTale, a novel
cross-lingual mnemonic generation system that retrieves L1 keyword sequence
based on phonological similarity and uses LLMs to generate mnemonics. We
evaluate PhoniTale using both automated metrics and human evaluations,
comparing its output to mnemonics created by humans and by previous automated
approaches. To assess practical effectiveness, we also conduct a short-term
recall test measuring mnemonic helpfulness. Our findings show that PhoniTale
performs comparably to human-authored mnemonics. We also highlight key areas
for future improvement in mnemonic quality and methodology.

### 33. Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Swapnil Bhattacharyya, Shrey Ganatra, Harshvivek Kashid, Spandan Anaokar, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya
- **URL**: <http://arxiv.org/abs/2507.06090v1>
- **Submitted**: 2025-07-08 15:30:49
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on a specific domain (consumer law in India) and uses AI-based techniques for summarization and case retrieval, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on judicial assistance and case prediction is also not aligned with the user's primary focus on real-time relevance optimization and deep semantic understanding.

#### Abstract
> AI-based judicial assistance and case prediction have been extensively
studied in criminal and civil domains, but remain largely unexplored in
consumer law, especially in India. In this paper, we present Nyay-Darpan, a
novel two-in-one framework that (i) summarizes consumer case files and (ii)
retrieves similar case judgements to aid decision-making in consumer dispute
resolution. Our methodology not only addresses the gap in consumer law AI tools
but also introduces an innovative approach to evaluate the quality of the
summary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',
symbolizing the ability of our tool to reflect the core of consumer disputes
through precise summarization and intelligent case retrieval. Our system
achieves over 75 percent accuracy in similar case prediction and approximately
70 percent accuracy across material summary evaluation metrics, demonstrating
its practical effectiveness. We will publicly release the Nyay-Darpan framework
and dataset to promote reproducibility and facilitate further research in this
underexplored yet impactful domain.

### 34. Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhijun Guo, Alvina Lai, Julia Ive, Alexandru Petcu, Yutong Wang, Luyuan Qi, Johan H Thygesen, Kezhi Li
- **URL**: <http://arxiv.org/abs/2507.05984v1>
- **Submitted**: 2025-07-08 13:41:22
- **Topic Keywords**: retrieval, recommend
- **Reason**: The paper focuses on developing a chatbot for depression screening, using a large language model and retrieval-augmented generation. While it mentions real-time clarification, it does not relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

### 35. From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal Collaborative Filtering Recommendation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Guohao Li, Li Jing, Jia Wu, Xuefei Li, Kai Zhu, Yue He
- **URL**: <http://arxiv.org/abs/2507.05715v1>
- **Submitted**: 2025-07-08 06:58:24
- **Comment**: ACM MM'25 (Experimental supplementary version)
- **Topic Keywords**: rag, recommend
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on multimodal collaborative filtering recommendation, which is a topic in recommender systems, but not the primary focus of the user's research. The paper's emphasis on ID-free embeddings and multimodal features is also not aligned with the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Most existing multimodal collaborative filtering recommendation (MCFRec)
methods rely heavily on ID features and multimodal content to enhance
recommendation performance. However, this paper reveals that ID features are
effective but have limited benefits in multimodal collaborative filtering
recommendation. Therefore, this paper systematically deconstruct the pros and
cons of ID features: (i) they provide initial embedding but lack semantic
richness, (ii) they provide a unique identifier for each user and item but
hinder generalization to untrained data, and (iii) they assist in aligning and
fusing multimodal features but may lead to representation shift. Based on these
insights, this paper proposes IDFREE, an ID-free multimodal collaborative
Filtering REcommEndation baseline. IDFREE replaces ID features with multimodal
features and positional encodings to generate semantically meaningful ID-free
embeddings. For ID-free multimodal collaborative filtering, it further proposes
an adaptive similarity graph module to construct dynamic user-user and
item-item graphs based on multimodal features. Then, an augmented user-item
graph encoder is proposed to construct more effective user and item encoding.
Finally, IDFREE achieves inter-multimodal alignment based on the contrastive
learning and uses Softmax loss as recommendation loss. Basic experiments on
three public datasets demonstrate that IDFREE outperforms existing ID-based
MCFRec methods, achieving an average performance gain of 72.24% across standard
metrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended
experiments further validate our findings on the limitations of ID features in
MCFRec. The code is released at https://github.com/G-H-Li/IDFREE.

### 36. Agentic-R1: Distilled Dual-Strategy Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang
- **URL**: <http://arxiv.org/abs/2507.05707v1>
- **Submitted**: 2025-07-08 06:35:16
- **Comment**: Preprint. 15 pages. Project available at
  https://github.com/StigLidu/DualDistill
- **Topic Keywords**: query
- **Reason**: The paper focuses on developing a reasoning framework for mathematical and logical tasks, using a combination of code execution and text-based reasoning. While it involves some form of query understanding and ranking models, the primary focus is on reasoning and problem-solving, which is not directly related to the user's interests in information retrieval, search technologies, and user behavior modeling.

#### Abstract
> Current long chain-of-thought (long-CoT) models excel at mathematical
reasoning but rely on slow and error-prone natural language traces.
Tool-augmented agents address arithmetic via code execution, but often falter
on complex logical tasks. We introduce a fine-tuning framework, DualDistill,
that distills complementary reasoning strategies from multiple teachers into a
unified student model. Using this approach, we train Agentic-R1, which
dynamically selects the optimal strategy for each query, invoking tools for
arithmetic and algorithmic problems, and using text-based reasoning for
abstract ones. Our method improves accuracy across a range of tasks, including
both computation-intensive and standard benchmarks, demonstrating the
effectiveness of multi-strategy distillation in achieving robust and efficient
reasoning. Our project is available at https://github.com/StigLidu/DualDistill

### 37. Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou
- **URL**: <http://arxiv.org/abs/2507.06229v1>
- **Submitted**: 2025-07-08 17:59:22
- **Topic Keywords**: rag
- **Reason**: The paper focuses on agent-based problem solving, leveraging cross-domain experience, and knowledge transfer, which is not directly related to information retrieval, search technologies, or query understanding. While it involves language agents and knowledge bases, the context is distinct from the user's primary research interests.

#### Abstract
> As language agents tackle increasingly complex tasks, they struggle with
effective error correction and experience reuse across domains. We introduce
Agent KB, a hierarchical experience framework that enables complex agentic
problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses
a core limitation: agents traditionally cannot learn from each other's
experiences. By capturing both high-level strategies and detailed execution
logs, Agent KB creates a shared knowledge base that enables cross-agent
knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success
rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3
improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on
intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to
improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a
modular, framework-agnostic infrastructure for enabling agents to learn from
past experiences and generalize successful strategies to new tasks.

### 38. DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ayush Parikh, Hoang Thanh Thanh Truong, Jeanette Schofield, Maximilian Heil
- **URL**: <http://arxiv.org/abs/2507.06205v1>
- **Submitted**: 2025-07-08 17:30:18
- **Topic Keywords**: rag
- **Reason**: The paper focuses on a specific task, Scientific Web Discourse Detection, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves NLP and classification, the context is not relevant to the user's core research themes.

#### Abstract
> In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a
Scientific Web Discourse Detection, present the methods we explored for this
task. For this multiclass classification task, we determined if a tweet
contained a scientific claim, a reference to a scientific study or publication,
and/or mentions of scientific entities, such as a university or a scientist. We
present 3 modeling approaches for this task: transformer finetuning, few-shot
prompting of LLMs, and a combined ensemble model whose design was informed by
earlier experiments. Our team placed 7th in the competition, achieving a
macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline
of 0.8375. Our code is available on Github at
https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

### 39. Differential Mamba

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nadav Schneider, Itamar Zimerman, Eliya Nachmani
- **URL**: <http://arxiv.org/abs/2507.06204v1>
- **Submitted**: 2025-07-08 17:30:14
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on improving the performance of a specific neural network architecture, Mamba, by addressing the issue of overallocation of attention in sequence models. While it mentions Transformers, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Sequence models like Transformers and RNNs often overallocate attention to
irrelevant context, leading to noisy intermediate representations. This
degrades LLM capabilities by promoting hallucinations, weakening long-range and
retrieval abilities, and reducing robustness. Recent work has shown that
differential design can mitigate this issue in Transformers, improving their
effectiveness across various applications. In this paper, we explore whether
these techniques, originally developed for Transformers, can be applied to
Mamba, a recent architecture based on selective state-space layers that
achieves Transformer-level performance with greater efficiency. We show that a
naive adaptation of differential design to Mamba is insufficient and requires
careful architectural modifications. To address this, we introduce a novel
differential mechanism for Mamba, empirically validated on language modeling
benchmarks, demonstrating improved retrieval capabilities and superior
performance over vanilla Mamba. Finally, we conduct extensive ablation studies
and empirical analyses to justify our design choices and provide evidence that
our approach effectively mitigates the overallocation problem in Mamba-based
models. Our code is publicly available.

### 40. CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, Minghao Liu, Yihang Xia, Jiawei Shen, Yuchen Wu, Yixin Cao, Zhaoxiang Zhang, Wenhao Huang, Jiaheng Liu, Ge Zhang
- **URL**: <http://arxiv.org/abs/2507.06181v1>
- **Submitted**: 2025-07-08 17:03:39
- **Topic Keywords**: rag
- **Reason**: The paper focuses on formalizing mathematical statements, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the context is specific to automated theorem proving and formal mathematical reasoning, which is not aligned with the user's research interests.

#### Abstract
> Translating natural language mathematical statements into formal, executable
code is a fundamental challenge in automated theorem proving. While prior work
has focused on generation and compilation success, little attention has been
paid to the critic phase-the evaluation of whether generated formalizations
truly capture the semantic intent of the original problem. In this paper, we
introduce CriticLean, a novel critic-guided reinforcement learning framework
that elevates the role of the critic from a passive validator to an active
learning component. Specifically, first, we propose the CriticLeanGPT, trained
via supervised fine-tuning and reinforcement learning, to rigorously assess the
semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,
a benchmark designed to measure models' ability to distinguish semantically
correct from incorrect formalizations, and demonstrate that our trained
CriticLeanGPT models can significantly outperform strong open- and
closed-source baselines. Building on the CriticLean framework, we construct
FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich
domain diversity, broad difficulty coverage, and high correctness based on
human evaluation. Overall, our findings highlight that optimizing the critic
phase is essential for producing reliable formalizations, and we hope our
CriticLean will provide valuable insights for future advances in formal
mathematical reasoning.

### 41. Coding Triangle: How Does Large Language Model Understand Code?

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Taolin Zhang, Zihan Ma, Maosong Cao, Junnan Liu, Songyang Zhang, Kai Chen
- **URL**: <http://arxiv.org/abs/2507.06138v1>
- **Submitted**: 2025-07-08 16:20:43
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests. While it touches on Natural Language Processing, it is focused on code generation and evaluation, which is a distinct field.

#### Abstract
> Large language models (LLMs) have achieved remarkable progress in code
generation, yet their true programming competence remains underexplored. We
introduce the Code Triangle framework, which systematically evaluates LLMs
across three fundamental dimensions: editorial analysis, code implementation,
and test case generation. Through extensive experiments on competitive
programming benchmarks, we reveal that while LLMs can form a self-consistent
system across these dimensions, their solutions often lack the diversity and
robustness of human programmers. We identify a significant distribution shift
between model cognition and human expertise, with model errors tending to
cluster due to training data biases and limited reasoning transfer. Our study
demonstrates that incorporating human-generated editorials, solutions, and
diverse test cases, as well as leveraging model mixtures, can substantially
enhance both the performance and robustness of LLMs. Furthermore, we reveal
both the consistency and inconsistency in the cognition of LLMs that may
facilitate self-reflection and self-improvement, providing a potential
direction for developing more powerful coding models.

### 42. Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Murilo Gustineli, Anthony Miyaguchi, Adrian Cheung, Divyansh Khattak
- **URL**: <http://arxiv.org/abs/2507.06093v1>
- **Submitted**: 2025-07-08 15:35:19
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on plant identification using computer vision and deep learning, which is outside your primary areas of interest.

#### Abstract
> We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

### 43. We Should Evaluate Real-World Impact

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ehud Reiter
- **URL**: <http://arxiv.org/abs/2507.05973v1>
- **Submitted**: 2025-07-08 13:29:04
- **Comment**: This paper will appear in Computational Linguistics journal as a
  "Last Word" opinion piece. The Arxiv version is a pre-MIT Press publication
  version
- **Topic Keywords**: acl
- **Reason**: The paper's focus on NLP systems and their real-world impact is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. The paper's abstract does not mention any relevant topics such as ranking models, user behavior modeling, or deep semantic understanding.

#### Abstract
> The ACL community has very little interest in evaluating the real-world
impact of NLP systems. A structured survey of the ACL Anthology shows that
perhaps 0.1% of its papers contain such evaluations; furthermore most papers
which include impact evaluations present them very sketchily and instead focus
on metric evaluations. NLP technology would be more useful and more quickly
adopted if we seriously tried to understand and evaluate its real-world impact.

### 44. MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Fathinah Izzati, Xinyue Li, Yuxuan Wu, Gus Xia
- **URL**: <http://arxiv.org/abs/2507.05894v1>
- **Submitted**: 2025-07-08 11:32:02
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on music scene imagination and video background music generation, which is outside your primary areas of interest.

#### Abstract
> Humans can imagine various atmospheres and settings when listening to music,
envisioning movie scenes that complement each piece. For example, slow,
melancholic music might evoke scenes of heartbreak, while upbeat melodies
suggest celebration. This paper explores whether a Music Language Model, e.g.
MU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),
which requires cross-modal information from video and music to train. To
improve upon existing music captioning models which focusing solely on musical
elements, we introduce MusiScene, a music captioning model designed to imagine
scenes that complement each music. In this paper, (1) we construct a
large-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music
Understanding LLaMA for the MSI task to create MusiScene, and (3) we conduct
comprehensive evaluations and prove that our MusiScene is more capable of
generating contextually relevant captions compared to MU-LLaMA. We leverage the
generated MSI captions to enhance Video Background Music Generation (VBMG) from
text.

### 45. Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shuai Zhao, Yulin Zhang, Luwei Xiao, Xinyi Wu, Yanhao Jia, Zhongliang Guo, Xiaobao Wu, Cong-Duy Nguyen, Guoming Zhang, Anh Tuan Luu
- **URL**: <http://arxiv.org/abs/2507.05816v1>
- **Submitted**: 2025-07-08 09:36:14
- **Topic Keywords**: rag
- **Reason**: The paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on predicting retinopathy of prematurity using large language models, which is outside your primary area of interest. Additionally, the paper does not involve query understanding, ranking models, or user behavior modeling, making it only loosely relevant to your research.

#### Abstract
> Despite the remarkable progress of large language models (LLMs) across
various domains, their capacity to predict retinopathy of prematurity (ROP)
risk remains largely unexplored. To address this gap, we introduce a novel
Chinese benchmark dataset, termed CROP, comprising 993 admission records
annotated with low, medium, and high-risk labels. To systematically examine the
predictive capabilities and affective biases of LLMs in ROP risk
stratification, we propose Affective-ROPTester, an automated evaluation
framework incorporating three prompting strategies: Instruction-based,
Chain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme
assesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and
ICL schemes leverage external medical knowledge to enhance predictive accuracy.
Crucially, we integrate emotional elements at the prompt level to investigate
how different affective framings influence the model's ability to predict ROP
and its bias patterns. Empirical results derived from the CROP dataset yield
two principal observations. First, LLMs demonstrate limited efficacy in ROP
risk prediction when operating solely on intrinsic knowledge, yet exhibit
marked performance gains when augmented with structured external inputs.
Second, affective biases are evident in the model outputs, with a consistent
inclination toward overestimating medium- and high-risk cases. Third, compared
to negative emotions, positive emotional framing contributes to mitigating
predictive bias in model outputs. These findings highlight the critical role of
affect-sensitive prompt engineering in enhancing diagnostic reliability and
emphasize the utility of Affective-ROPTester as a framework for evaluating and
mitigating affective bias in clinical language modeling systems.

### 46. Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly
- **URL**: <http://arxiv.org/abs/2507.05724v1>
- **Submitted**: 2025-07-08 07:18:33
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of automatic speech recognition and mixture-of-experts architectures is outside your primary focus, and the paper does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> Mixture-of-experts (MoE) architectures have expanded from language modeling
to automatic speech recognition (ASR). Traditional MoE methods, such as the
Switch Transformer, route experts independently within each layer. Our analysis
reveals that routers in most layers make expert choices that are not strongly
correlated with the choices of the routers in other layers. To increase the
cooperation between experts in different layers and encourage greater
specialization, we use a shared router across different MoE layers. We call
this model \emph{Omni-router Transformer}. Extensive experiments on a
large-scale pseudo-labeled dataset and evaluations across 10 diverse,
out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is
able to achieve lower training loss and consistently outperform dense and
Switch Transformer models, reducing average word error rates by 11.2% and 8.2%,
respectively, while providing structured expert usage and improved robustness
to diverse data.

### 47. MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, Dong Yu
- **URL**: <http://arxiv.org/abs/2507.05720v1>
- **Submitted**: 2025-07-08 07:07:53
- **Comment**: 17 pages, 4 figures
- **Topic Keywords**: click
- **Reason**: The paper focuses on mobile GUI agents and reinforcement learning, which is not directly related to information retrieval, search technologies, or query understanding. While it involves learning and optimization, the context is different from the user's primary research interests.

#### Abstract
> Recently, there has been a surge of vision-based GUI agents designed to
automate everyday mobile and web tasks. These agents interpret raw GUI
screenshots and autonomously decide where to click, scroll, or type, which
bypasses handcrafted rules and app-specific APIs. However, most existing
methods trained GUI agent in the offline environment using pre-collected
trajectories. This approach limits scalability, causes overfitting to specific
UI templates, and leads to brittle policies when faced with unseen environment.
We present MobileGUI-RL, a scalable framework that trains GUI agent in online
environment. MobileGUI-RL contains two key components. It (i) synthesizes a
curriculum of learnable tasks through self-exploration and filtering, and (ii)
adapts GRPO to GUI navigation with trajectory-aware advantages and composite
rewards that balance task success and execution efficiency. Experiments on
three online mobile-agent benchmarks show consistent gains, validating the
effectiveness of our approach.

### 48. TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Aravind Cheruvu, Shravya Kanchi, Sifat Muhammad Abdullah, Nicholas Kong, Daphne Yao, Murtuza Jadliwala, Bimal Viswanath
- **URL**: <http://arxiv.org/abs/2507.05660v1>
- **Submitted**: 2025-07-08 04:40:09
- **Comment**: Pre-print
- **Topic Keywords**: rag
- **Reason**: The paper focuses on conversational AI and toxicity mitigation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions fine-tuning and classification, the context is different from the user's primary research interests.

#### Abstract
> Recent advances in foundation models, such as LLMs, have revolutionized
conversational AI. Chatbots are increasingly being developed by customizing
LLMs on specific conversational datasets. However, mitigating toxicity during
this customization, especially when dealing with untrusted training data,
remains a significant challenge. To address this, we introduce TuneShield, a
defense framework designed to mitigate toxicity during chatbot fine-tuning
while preserving conversational quality. TuneShield leverages LLM-based
toxicity classification, utilizing the instruction-following capabilities and
safety alignment of LLMs to effectively identify toxic samples, outperforming
industry API services. TuneShield generates synthetic conversation samples,
termed 'healing data', based on the identified toxic samples, using them to
mitigate toxicity while reinforcing desirable behavior during fine-tuning. It
performs an alignment process to further nudge the chatbot towards producing
desired responses. Our findings show that TuneShield effectively mitigates
toxicity injection attacks while preserving conversational quality, even when
the toxicity classifiers are imperfect or biased. TuneShield proves to be
resilient against adaptive adversarial and jailbreak attacks. Additionally,
TuneShield demonstrates effectiveness in mitigating adaptive toxicity injection
attacks during dialog-based learning (DBL).

### 49. Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mingzhe Li, Jing Xiang, Qishen Zhang, Kaiyang Wan, Xiuying Chen
- **URL**: <http://arxiv.org/abs/2507.05617v1>
- **Submitted**: 2025-07-08 02:54:15
- **Comment**: Accepted by ACL 2025 main
- **Topic Keywords**: rag
- **Reason**: This paper focuses on knowledge distillation and model architecture, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions text matching, the approach is not centered around query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests.

#### Abstract
> Knowledge distillation typically involves transferring knowledge from a Large
Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such
as text matching, fine-tuned smaller models often yield more effective
domain-specific representations, as they focus on optimizing the similarity of
input pairs. To leverage both the specialized strengths of small models and the
rich semantic understanding of LLMs, we introduce a flipped knowledge
distillation paradigm, where LLM learns from SLM. Specifically, we address the
architectural gap between decoder-only LLMs and smaller encoder-based models by
reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder
generates compressed representations, while the decoder maps them to the output
space. During training, the encoder produces representations and their
similarities, which are then aligned with the similarity scores produced by the
teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.
The MCL ensures accurate similarity for both positive and negative pairs, and
adaptively handles the internal differences within positive and negative
samples. Our paradigm requires only a reasonably good-performing SLM, allowing
the LLM to achieve improved performance. Experiments on financial and
healthcare benchmarks, as well as real-world applications, confirm its
effectiveness, and the model has been fully deployed in an online environment.

### 50. Self-Review Framework for Enhancing Instruction Following Capability of LLM

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sihyun Park
- **URL**: <http://arxiv.org/abs/2507.05598v1>
- **Submitted**: 2025-07-08 02:17:18
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on large language models and instruction-following capabilities, which is outside your primary research focus.

#### Abstract
> Various techniques have been proposed to improve large language models (LLMs)
adherence to formatting and instruction constraints. One of the most effective
approaches involves utilizing high-quality data generated by powerful models.
However, such models often fail to fully comply with complex instructions in a
single generation. To address this limitation, iterative revision methods have
been introduced. Nevertheless, as the number of data points and revision
iterations increases, the associated monetary costs grow significantly. As a
resource-efficient alternative, methods have been proposed that leverage
high-performance evaluation tools to compensate for the limited self-evaluation
capabilities of open-source LLMs. However, these approaches often lead to a
degradation in output quality due to excessive revision. To overcome these
challenges, we propose Re5, a self-evaluation and revision framework designed
to enhance instruction-following performance while preserving the quality of
the generated content. Re5 extracts task and constraint components from user
instructions, performs structural evaluations to prevent error accumulation,
and applies fine-grained constraint-specific content evaluations followed by
selective revisions. This process ensures precise and quality-preserving
improvements. The final high-quality outputs are used for alignment tuning,
enabling long-term alignment improvements through a data-centric iterative
refinement loop. Experimental results demonstrate that Re5 achieves
instruction-following performance comparable to models trained on data
generated by GPT-4o-mini, a high-performance model, even with a small amount of
data while maintaining response quality with a 64.24%-win rate over the
non-revised initial responses. These results validate Re5 as an efficient and
effective solution for enhancing instruction adherence with minimal external
supervision.

---

