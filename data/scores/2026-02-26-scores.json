[
    {
        "title": "Revisiting Text Ranking in Deep Research",
        "abstract": "Deep research has emerged as an important task that aims to address hard queries through extensive open-web exploration. To tackle it, most prior work equips large language model (LLM)-based agents with opaque web search APIs, enabling agents to iteratively issue search queries, retrieve external evidence, and reason over it. Despite search's essential role in deep research, black-box web search APIs hinder systematic analysis of search components, leaving the behaviour of established text ranking methods in deep research largely unclear. To fill this gap, we reproduce a selection of key findings and best practices for IR text ranking methods in the deep research setting. In particular, we examine their effectiveness from three perspectives: (i) retrieval units (documents vs. passages), (ii) pipeline configurations (different retrievers, re-rankers, and re-ranking depths), and (iii) query characteristics (the mismatch between agent-issued queries and the training queries of text rankers). We perform experiments on BrowseComp-Plus, a deep research dataset with a fixed corpus, evaluating 2 open-source agents, 5 retrievers, and 3 re-rankers across diverse setups. We find that agent-issued queries typically follow web-search-style syntax (e.g., quoted exact matches), favouring lexical, learned sparse, and multi-vector retrievers; passage-level units are more efficient under limited context windows, and avoid the difficulties of document length normalisation in lexical retrieval; re-ranking is highly effective; translating agent-issued queries into natural-language questions significantly bridges the query mismatch.",
        "url": "http://arxiv.org/abs/2602.21456v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21456v1",
        "arxiv_id": "2602.21456v1",
        "authors": [
            "Chuan Meng",
            "Litu Ou",
            "Sean MacAvaney",
            "Jeff Dalton"
        ],
        "submitted": "2026-02-25 00:18:07",
        "source": "arxiv",
        "comment": null,
        "score": 17,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of deep research and text ranking methods. The study examines the effectiveness of various IR text ranking methods in a deep research setting, which aligns with your focus on query understanding and ranking models. The paper's emphasis on systematic analysis and evaluation of search components also resonates with your interest in user behavior modeling and click models."
    },
    {
        "title": "AQR-HNSW: Accelerating Approximate Nearest Neighbor Search via Density-aware Quantization and Multi-stage Re-ranking",
        "abstract": "Approximate Nearest Neighbor (ANN) search has become fundamental to modern AI infrastructure, powering recommendation systems, search engines, and large language models across industry leaders from Google to OpenAI. Hierarchical Navigable Small World (HNSW) graphs have emerged as the dominant ANN algorithm, widely adopted in production systems due to their superior recall versus latency balance. However, as vector databases scale to billions of embeddings, HNSW faces critical bottlenecks: memory consumption expands, distance computation overhead dominates query latency, and it suffers suboptimal performance on heterogeneous data distributions. This paper presents Adaptive Quantization and Rerank HNSW (AQR-HNSW), a novel framework that synergistically integrates three strategies to enhance HNSW scalability. AQR-HNSW introduces (1) density-aware adaptive quantization, achieving 4x compression while preserving distance relationships; (2) multi-state re-ranking that reduces unnecessary computations by 35%; and (3) quantization-optimized SIMD implementations delivering 16-64 operations per cycle across architectures. Evaluation on standard benchmarks demonstrates 2.5-3.3x higher queries per second (QPS) than state-of-the-art HNSW implementations while maintaining over 98% recall, with 75% memory reduction for the index graph and 5x faster index construction.",
        "url": "http://arxiv.org/abs/2602.21600v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21600v1",
        "arxiv_id": "2602.21600v1",
        "authors": [
            "Ganap Ashit Tewary",
            "Nrusinga Charan Gantayat",
            "Jeff Zhang"
        ],
        "submitted": "2026-02-25 05:58:16",
        "source": "arxiv",
        "comment": "Accepted at DAC 2026",
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on improving the efficiency of Approximate Nearest Neighbor Search, which is related to Information Retrieval and Search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on scalability and efficiency in vector databases is somewhat relevant to the e-commerce domain, but it does not strongly align with the user's primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval",
        "abstract": "Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM",
        "url": "http://arxiv.org/abs/2602.22278v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22278v1",
        "arxiv_id": "2602.22278v1",
        "authors": [
            "Dawei Su",
            "Dongsheng Wang"
        ],
        "submitted": "2026-02-25 10:31:32",
        "source": "arxiv",
        "comment": "5 pages, 2 figure",
        "score": 11,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of multimodal information retrieval, which requires deep semantic understanding. The use of large language models and the proposed framework, RetLLM, aligns with your focus on query understanding and ranking models. The paper's emphasis on scalability and simplicity also resonates with your interest in real-time relevance optimization."
    },
    {
        "title": "Revisiting RAG Retrievers: An Information Theoretic Benchmark",
        "abstract": "Retrieval-Augmented Generation (RAG) systems rely critically on the retriever module to surface relevant context for large language models. Although numerous retrievers have recently been proposed, each built on different ranking principles such as lexical matching, dense embeddings, or graph citations, there remains a lack of systematic understanding of how these mechanisms differ and overlap. Existing benchmarks primarily compare entire RAG pipelines or introduce new datasets, providing little guidance on selecting or combining retrievers themselves. Those that do compare retrievers directly use a limited set of evaluation tools which fail to capture complementary and overlapping strengths. This work presents MIGRASCOPE, a Mutual Information based RAG Retriever Analysis Scope. We revisit state-of-the-art retrievers and introduce principled metrics grounded in information and statistical estimation theory to quantify retrieval quality, redundancy, synergy, and marginal contribution. We further show that if chosen carefully, an ensemble of retrievers outperforms any single retriever. We leverage the developed tools over major RAG corpora to provide unique insights on contribution levels of the state-of-the-art retrievers. Our findings provide a fresh perspective on the structure of modern retrieval techniques and actionable guidance for designing robust and efficient RAG systems.",
        "url": "http://arxiv.org/abs/2602.21553v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21553v1",
        "arxiv_id": "2602.21553v1",
        "authors": [
            "Wenqing Zheng",
            "Dmitri Kalaev",
            "Noah Fatsi",
            "Daniel Barcklow",
            "Owen Reinert",
            "Igor Melnyk",
            "Senthil Kumar",
            "C. Bayan Bruss"
        ],
        "submitted": "2026-02-25 04:19:06",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) systems. The focus on ranking principles, retriever analysis, and ensemble methods aligns with your interests in query understanding and ranking models. However, the paper's primary focus on RAG systems might not be a central match with your broader interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "Offline Reasoning for Efficient Recommendation: LLM-Empowered Persona-Profiled Item Indexing",
        "abstract": "Recent advances in large language models (LLMs) offer new opportunities for recommender systems by capturing the nuanced semantics of user interests and item characteristics through rich semantic understanding and contextual reasoning. In particular, LLMs have been employed as rerankers that reorder candidate items based on inferred user-item relevance. However, these approaches often require expensive online inference-time reasoning, leading to high latency that hampers real-world deployment. In this work, we introduce Persona4Rec, a recommendation framework that performs offline reasoning to construct interpretable persona representations of items, enabling lightweight and scalable real-time inference. In the offline stage, Persona4Rec leverages LLMs to reason over item reviews, inferring diverse user motivations that explain why different types of users may engage with an item; these inferred motivations are materialized as persona representations, providing multiple, human-interpretable views of each item. Unlike conventional approaches that rely on a single item representation, Persona4Rec learns to align user profiles with the most plausible item-side persona through a dedicated encoder, effectively transforming user-item relevance into user-persona relevance. At the online stage, this persona-profiled item index allows fast relevance computation without invoking expensive LLM reasoning. Extensive experiments show that Persona4Rec achieves performance comparable to recent LLM-based rerankers while substantially reducing inference time. Moreover, qualitative analysis confirms that persona representations not only drive efficient scoring but also provide intuitive, review-grounded explanations. These results demonstrate that Persona4Rec offers a practical and interpretable solution for next-generation recommender systems.",
        "url": "http://arxiv.org/abs/2602.21756v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21756v1",
        "arxiv_id": "2602.21756v1",
        "authors": [
            "Deogyong Kim",
            "Junseong Lee",
            "Jeongeun Lee",
            "Changhoe Kim",
            "Junguel Lee",
            "Jungseok Lee",
            "Dongha Lee"
        ],
        "submitted": "2026-02-25 10:14:30",
        "source": "arxiv",
        "comment": "Under review",
        "score": 10,
        "keyword_reasons": [
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval and Search technologies, particularly in the area of recommender systems and query understanding. Although it focuses on recommender systems, the use of large language models and persona representations aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the paper's primary focus on recommender systems rather than search technologies prevents it from receiving a perfect score."
    },
    {
        "title": "Trie-Aware Transformers for Generative Recommendation",
        "abstract": "Generative recommendation (GR) aligns with advances in generative AI by casting next-item prediction as token-level generation rather than score-based ranking. Most GR methods adopt a two-stage pipeline: (i) \\textit{item tokenization}, which maps each item to a sequence of discrete, hierarchically organized tokens; and (ii) \\textit{autoregressive generation}, which predicts the next item's tokens conditioned on the tokens of user's interaction history. Although hierarchical tokenization induces a prefix tree (trie) over items, standard autoregressive modeling with conventional Transformers often flattens item tokens into a linear stream and overlooks the underlying topology.\n  To address this, we propose TrieRec, a trie-aware generative recommendation method that augments Transformers with structural inductive biases via two positional encodings. First, a \\textit{trie-aware absolute positional encoding} aggregates a token's (node's) local structural context (\\eg depth, ancestors, and descendants) into the token representation. Second, a \\textit{topology-aware relative positional encoding} injects pairwise structural relations into self-attention to capture topology-induced semantic relatedness. TrieRec is also model-agnostic, efficient, and hyperparameter-free. In our experiments, we implement TrieRec within three representative GR backbones, achieving notably improvements of 8.83\\% on average across four real-world datasets.",
        "url": "http://arxiv.org/abs/2602.21677v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21677v1",
        "arxiv_id": "2602.21677v1",
        "authors": [
            "Zhenxiang Xu",
            "Jiawei Chen",
            "Sirui Chen",
            "Yong He",
            "Jieyu Yang",
            "Chuan Yuan",
            "Ke Ding",
            "Can Wang"
        ],
        "submitted": "2026-02-25 08:25:16",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to your research interests in Information Retrieval and Search technologies, as it deals with recommendation systems and generative AI. However, it focuses more on the NLP and recommender systems aspects, which are not your primary focus. The paper's emphasis on deep semantic understanding and real-time relevance optimization is somewhat relevant to your interests, but it is not a central match."
    },
    {
        "title": "DynamicGTR: Leveraging Graph Topology Representation Preferences to Boost VLM Capabilities on Graph QAs",
        "abstract": "Vision-Language Models (VLMs) have emerged as versatile solutions for zero-shot question answering (QA) across various domains. However, enabling VLMs to effectively comprehend structured graphs and perform accurate, efficient QA remains challenging. Existing approaches typically rely on one single graph topology representation (GTR), such as fixed-style visual images or unified text descriptions. This ``one-size-fits-all'' strategy often neglects model-specific and task-specific preferences, resulting in inaccurate or over-lengthy responses to graph-related queries. To address this, we propose the $\\mbox{DynamicGTR}$ framework, which dynamically selects the optimal GTR for each query during inference, thereby enhancing the zero-shot graph QA capabilities of VLMs with a customizable accuracy and brevity trade-off. Extensive experiments show that DynamicGTR not only improves VLM-based graph algorithm QA performance but also successfully transfers the experience trained from synthetic graph algorithm tasks to real-world applications like link prediction and node classification, without any additional training. Additionally, DynamicGTR demonstrates strong transferability across tasks, domains, and models, suggesting its potential as a flexible solution for broad graph scenarios.",
        "url": "http://arxiv.org/abs/2602.21864v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21864v1",
        "arxiv_id": "2602.21864v1",
        "authors": [
            "Yanbin Wei",
            "Jiangyue Yan",
            "Chun Kang",
            "Yang Chen",
            "Hua Liu",
            "James Kwok",
            "Yu Zhang"
        ],
        "submitted": "2026-02-25 12:45:45",
        "source": "arxiv",
        "comment": "CVPR 2026",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Vision-Language Models and graph question answering, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it touches on aspects of query understanding and ranking models, the context is quite different from your core areas of interest."
    },
    {
        "title": "Retrieval Challenges in Low-Resource Public Service Information: A Case Study on Food Pantry Access",
        "abstract": "Public service information systems are often fragmented, inconsistently formatted, and outdated. These characteristics create low-resource retrieval environments that hinder timely access to critical services. We investigate retrieval challenges in such settings through the domain of food pantry access, a socially urgent problem given persistent food insecurity. We develop an AI-powered conversational retrieval system that scrapes and indexes publicly available pantry data and employs a Retrieval-Augmented Generation (RAG) pipeline to support natural language queries via a web interface. We conduct a pilot evaluation study using community-sourced queries to examine system behavior in realistic scenarios. Our analysis reveals key limitations in retrieval robustness, handling underspecified queries, and grounding over inconsistent knowledge bases. This ongoing work exposes fundamental IR challenges in low-resource environments and motivates future research on robust conversational retrieval to improve access to critical public resources.",
        "url": "http://arxiv.org/abs/2602.21598v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21598v1",
        "arxiv_id": "2602.21598v1",
        "authors": [
            "Touseef Hasan",
            "Laila Cure",
            "Souvika Sarkar"
        ],
        "submitted": "2026-02-25 05:48:15",
        "source": "arxiv",
        "comment": "3 pages, 1 figure",
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of low-resource environments and conversational retrieval systems. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on public service information systems and food pantry access is also somewhat niche compared to the user's broader interests in e-commerce and general search technologies."
    },
    {
        "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
        "abstract": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.",
        "url": "http://arxiv.org/abs/2602.22207v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22207v1",
        "arxiv_id": "2602.22207v1",
        "authors": [
            "Hanna Yukhymenko",
            "Anton Alexandrov",
            "Martin Vechev"
        ],
        "submitted": "2026-02-25 18:58:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multilingual Large Language Model evaluation, specifically addressing translation quality issues. While it involves ranking methods (T-RANK), the context is not directly related to query understanding, ranking models, or user behavior modeling in the Information Retrieval domain. The paper's emphasis on NLP and language translation does not strongly align with your primary research interests."
    },
    {
        "title": "DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs",
        "abstract": "Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.",
        "url": "http://arxiv.org/abs/2602.22175v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22175v1",
        "arxiv_id": "2602.22175v1",
        "authors": [
            "Xi Ye",
            "Wuwei Zhang",
            "Fangcong Yin",
            "Howard Yen",
            "Danqi Chen"
        ],
        "submitted": "2026-02-25 18:21:35",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper proposes a novel decoding algorithm for improving long-context reasoning in language models, leveraging retrieval heads to identify task-relevant tokens and dynamically adjust attention. While it's related to Natural Language Processing and deep semantic understanding, it's not directly focused on information retrieval or search technologies, which are the primary areas of interest. The paper's relevance is somewhat related but not a central match."
    },
    {
        "title": "Personalized Graph-Empowered Large Language Model for Proactive Information Access",
        "abstract": "Since individuals may struggle to recall all life details and often confuse events, establishing a system to assist users in recalling forgotten experiences is essential. While numerous studies have proposed memory recall systems, these primarily rely on deep learning techniques that require extensive training and often face data scarcity due to the limited availability of personal lifelogs. As lifelogs grow over time, systems must also adapt quickly to newly accumulated data. Recently, large language models (LLMs) have demonstrated remarkable capabilities across various tasks, making them promising for personalized applications. In this work, we present a framework that leverages LLMs for proactive information access, integrating personal knowledge graphs to enhance the detection of access needs through a refined decision-making process. Our framework offers high flexibility, enabling the replacement of base models and the modification of fact retrieval methods for continuous improvement. Experimental results demonstrate that our approach effectively identifies forgotten events, supporting users in recalling past experiences more efficiently.",
        "url": "http://arxiv.org/abs/2602.21862v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21862v1",
        "arxiv_id": "2602.21862v1",
        "authors": [
            "Chia Cheng Chang",
            "An-Zi Yen",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "submitted": "2026-02-25 12:43:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a memory recall system using large language models and personal knowledge graphs, which is not directly related to information retrieval, search technologies, or query understanding. While it involves NLP, the application domain is distinct from the user's primary research interests."
    },
    {
        "title": "Prompt Architecture Determines Reasoning Quality: A Variable Isolation Study on the Car Wash Problem",
        "abstract": "Large language models consistently fail the \"car wash problem,\" a viral reasoning benchmark requiring implicit physical constraint inference. We present a variable isolation study (n=20 per condition, 6 conditions, 120 total trials) examining which prompt architecture layers in a production system enable correct reasoning. Using Claude 3.5 Sonnet with controlled hyperparameters (temperature 0.7, top_p 1.0), we find that the STAR (Situation-Task-Action-Result) reasoning framework alone raises accuracy from 0% to 85% (p=0.001, Fisher's exact test, odds ratio 13.22). Adding user profile context via vector database retrieval provides a further 10 percentage point gain, while RAG context contributes an additional 5 percentage points, achieving 100% accuracy in the full-stack condition. These results suggest that structured reasoning scaffolds -- specifically, forced goal articulation before inference -- matter substantially more than context injection for implicit constraint reasoning tasks.",
        "url": "http://arxiv.org/abs/2602.21814v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21814v1",
        "arxiv_id": "2602.21814v1",
        "authors": [
            "Heejin Jo"
        ],
        "submitted": "2026-02-25 11:40:15",
        "source": "arxiv",
        "comment": "9 pages, 4 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be related to Natural Language Processing (NLP) and large language models, but it does not align with the user's primary focus on Information Retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization. The paper's focus on prompt architecture and reasoning quality in a specific benchmark (the 'car wash problem') does not match the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences",
        "abstract": "Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.",
        "url": "http://arxiv.org/abs/2602.21585v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21585v1",
        "arxiv_id": "2602.21585v1",
        "authors": [
            "Sweta Karlekar",
            "Carolina Zheng",
            "Magnus Saebo",
            "Nicolas Beltran-Velez",
            "Shuyang Yu",
            "John Bowlan",
            "Michal Kucer",
            "David Blei"
        ],
        "submitted": "2026-02-25 05:16:11",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses an optimization algorithm for Large Language Models (LLMs) that leverages pairwise comparisons to guide search. While it touches on aspects of search and optimization, it doesn't directly relate to the user's core research themes in Information Retrieval, query understanding, or ranking models. The focus on LLMs and pairwise comparisons is somewhat relevant, but not central to the user's interests."
    },
    {
        "title": "One Brain, Omni Modalities: Towards Unified Non-Invasive Brain Decoding with Large Language Models",
        "abstract": "Deciphering brain function through non-invasive recordings requires synthesizing complementary high-frequency electromagnetic (EEG/MEG) and low-frequency metabolic (fMRI) signals. However, despite their shared neural origins, extreme discrepancies have traditionally confined these modalities to isolated analysis pipelines, hindering a holistic interpretation of brain activity. To bridge this fragmentation, we introduce \\textbf{NOBEL}, a \\textbf{n}euro-\\textbf{o}mni-modal \\textbf{b}rain-\\textbf{e}ncoding \\textbf{l}arge language model (LLM) that unifies these heterogeneous signals within the LLM's semantic embedding space. Our architecture integrates a unified encoder for EEG and MEG with a novel dual-path strategy for fMRI, aligning non-invasive brain signals and external sensory stimuli into a shared token space, then leverages an LLM as a universal backbone. Extensive evaluations demonstrate that NOBEL serves as a robust generalist across standard single-modal tasks. We also show that the synergistic fusion of electromagnetic and metabolic signals yields higher decoding accuracy than unimodal baselines, validating the complementary nature of multiple neural modalities. Furthermore, NOBEL exhibits strong capabilities in stimulus-aware decoding, effectively interpreting visual semantics from multi-subject fMRI data on the NSD and HAD datasets while uniquely leveraging direct stimulus inputs to verify causal links between sensory signals and neural responses. NOBEL thus takes a step towards unifying non-invasive brain decoding, demonstrating the promising potential of omni-modal brain understanding.",
        "url": "http://arxiv.org/abs/2602.21522v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21522v1",
        "arxiv_id": "2602.21522v1",
        "authors": [
            "Changli Tang",
            "Shurui Li",
            "Junliang Wang",
            "Qinfan Xiao",
            "Zhonghao Zhai",
            "Lei Bai",
            "Yu Qiao",
            "Bowen Zhou",
            "Wen Wu",
            "Yuanning Li",
            "Chao Zhang"
        ],
        "submitted": "2026-02-25 03:24:54",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not related to Information Retrieval, Search technologies, or any of the user's core research themes. It appears to be focused on brain decoding and neural signals, which falls under a different domain."
    },
    {
        "title": "Adversarial Intent is a Latent Variable: Stateful Trust Inference for Securing Multimodal Agentic RAG",
        "abstract": "Current stateless defences for multimodal agentic RAG fail to detect adversarial strategies that distribute malicious semantics across retrieval, planning, and generation components. We formulate this security challenge as a Partially Observable Markov Decision Process (POMDP), where adversarial intent is a latent variable inferred from noisy multi-stage observations. We introduce MMA-RAG^T, an inference-time control framework governed by a Modular Trust Agent (MTA) that maintains an approximate belief state via structured LLM reasoning. Operating as a model-agnostic overlay, MMA-RAGT mediates a configurable set of internal checkpoints to enforce stateful defence-in-depth. Extensive evaluation on 43,774 instances demonstrates a 6.50x average reduction factor in Attack Success Rate relative to undefended baselines, with negligible utility cost. Crucially, a factorial ablation validates our theoretical bounds: while statefulness and spatial coverage are individually necessary (26.4 pp and 13.6 pp gains respectively), stateless multi-point intervention can yield zero marginal benefit under homogeneous stateless filtering when checkpoint detections are perfectly correlated.",
        "url": "http://arxiv.org/abs/2602.21447v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21447v1",
        "arxiv_id": "2602.21447v1",
        "authors": [
            "Inderjeet Singh",
            "Vikas Pahuja",
            "Aishvariya Priya Rathina Sabapathy",
            "Chiara Picardi",
            "Amit Giloni",
            "Roman Vainshtein",
            "Andr√©s Murillo",
            "Hisashi Kojima",
            "Motoyoshi Sekiya",
            "Yuki Unno",
            "Junichi Suga"
        ],
        "submitted": "2026-02-24 23:52:27",
        "source": "arxiv",
        "comment": "13 pages, 2 figures, 5 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal agentic RAG security, using Partially Observable Markov Decision Process and Modular Trust Agent to detect adversarial intent. While it involves retrieval and planning components, the primary focus is on security and trust inference, which is not directly related to your core research interests in Information Retrieval, query understanding, and ranking models."
    },
    {
        "title": "SumTablets: A Transliteration Dataset of Sumerian Tablets",
        "abstract": "Sumerian transliteration is a conventional system for representing a scholar's interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet's cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration.\n  To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub.\n  Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph's possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.",
        "url": "http://arxiv.org/abs/2602.22200v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22200v1",
        "arxiv_id": "2602.22200v1",
        "authors": [
            "Cole Simmons",
            "Richard Diehl Martinez",
            "Dan Jurafsky"
        ],
        "submitted": "2026-02-25 18:50:42",
        "source": "arxiv",
        "comment": "11 pages with 3 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing, as it focuses on creating a dataset for Sumerian transliteration and evaluating NLP methods for this specific task."
    },
    {
        "title": "Sparsity Induction for Accurate Post-Training Pruning of Large Language Models",
        "abstract": "Large language models have demonstrated capabilities in text generation, while their increasing parameter scales present challenges in computational and memory efficiency. Post-training sparsity (PTS), which reduces model cost by removing weights from dense networks, is an effective approach. However, native dense matrices lack high sparsity, making existing approaches that directly remove weights disrupt model states, resulting in unsatisfactory performance recovery even with post-tuning. We propose Sparsity Induction, which promotes models toward higher sparsity at both distribution and feature levels before pruning, to push the limits of PTS. At the distribution level, we enhance distributional sparsity through mathematically equivalent scaling transformations, which are fully absorbable and incur no extra parameters or inference-time overhead. At the feature level, we introduce Spectral Norm Loss to promote feature sparsity from a low-rank perspective. Experiments across diverse model architectures and tasks demonstrate that our method further enhances sparsity-friendliness, achieving superior pruning performance over existing approaches.",
        "url": "http://arxiv.org/abs/2602.21652v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21652v1",
        "arxiv_id": "2602.21652v1",
        "authors": [
            "Minhao Jiang",
            "Zhikai Li",
            "Xuewen Liu",
            "Jing Zhang",
            "Mengjuan Chen",
            "Qingyi Gu"
        ],
        "submitted": "2026-02-25 07:25:01",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, 4 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on post-training pruning of large language models, which is a topic outside the user's primary research interests in Information Retrieval and Search technologies. While it involves NLP, the specific application and methodology are not directly related to the user's core themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Scalable Multilingual Multimodal Machine Translation with Speech-Text Fusion",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved notable success in enhancing translation performance by integrating multimodal information. However, existing research primarily focuses on image-guided methods, whose applicability is constrained by the scarcity of multilingual image-text pairs. The speech modality overcomes this limitation due to its natural alignment with text and the abundance of existing speech datasets, which enable scalable language coverage. In this paper, we propose a Speech-guided Machine Translation (SMT) framework that integrates speech and text as fused inputs into an MLLM to improve translation quality. To mitigate reliance on low-resource data, we introduce a Self-Evolution Mechanism. The core components of this framework include a text-to-speech model, responsible for generating synthetic speech, and an MLLM capable of classifying synthetic speech samples and iteratively optimizing itself using positive samples. Experimental results demonstrate that our framework surpasses all existing methods on the Multi30K multimodal machine translation benchmark, achieving new state-of-the-art results. Furthermore, on general machine translation datasets, particularly the FLORES-200, it achieves average state-of-the-art performance in 108 translation directions. Ablation studies on CoVoST-2 confirms that differences between synthetic and authentic speech have negligible impact on translation quality. The code and models are released at https://github.com/yxduir/LLM-SRT.",
        "url": "http://arxiv.org/abs/2602.21646v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21646v1",
        "arxiv_id": "2602.21646v1",
        "authors": [
            "Yexing Du",
            "Youcheng Pan",
            "Zekun Wang",
            "Zheng Chu",
            "Yichong Huang",
            "Kaiyuan Liu",
            "Bo Yang",
            "Yang Xiang",
            "Ming Liu",
            "Bing Qin"
        ],
        "submitted": "2026-02-25 07:19:34",
        "source": "arxiv",
        "comment": "Accepted in ICLR 2026",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine translation, specifically using speech and text fusion for translation quality improvement. While it involves multimodal information, it doesn't directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Multi-dimensional Assessment and Explainable Feedback for Counselor Responses to Client Resistance in Text-based Counseling with LLMs",
        "abstract": "Effectively addressing client resistance is a sophisticated clinical skill in psychological counseling, yet practitioners often lack timely and scalable supervisory feedback to refine their approaches. Although current NLP research has examined overall counseling quality and general therapeutic skills, it fails to provide granular evaluations of high-stakes moments where clients exhibit resistance. In this work, we present a comprehensive pipeline for the multi-dimensional evaluation of human counselors' interventions specifically targeting client resistance in text-based therapy. We introduce a theory-driven framework that decomposes counselor responses into four distinct communication mechanisms. Leveraging this framework, we curate and share an expert-annotated dataset of real-world counseling excerpts, pairing counselor-client interactions with professional ratings and explanatory rationales. Using this data, we perform full-parameter instruction tuning on a Llama-3.1-8B-Instruct backbone to model fine-grained evaluative judgments of response quality and generate explanations underlying. Experimental results show that our approach can effectively distinguish the quality of different communication mechanisms (77-81% F1), substantially outperforming GPT-4o and Claude-3.5-Sonnet (45-59% F1). Moreover, the model produces high-quality explanations that closely align with expert references and receive near-ceiling ratings from human experts (2.8-2.9/3.0). A controlled experiment with 43 counselors further confirms that receiving these AI-generated feedback significantly improves counselors' ability to respond effectively to client resistance.",
        "url": "http://arxiv.org/abs/2602.21638v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21638v1",
        "arxiv_id": "2602.21638v1",
        "authors": [
            "Anqi Li",
            "Ruihan Wang",
            "Zhaoming Chen",
            "Yuqian Chen",
            "Yu Lu",
            "Yi Zhu",
            "Yuan Xie",
            "Zhenzhong Lan"
        ],
        "submitted": "2026-02-25 07:05:05",
        "source": "arxiv",
        "comment": "8 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies, but it does involve Natural Language Processing (NLP) and deep semantic understanding. However, the focus on text-based counseling and LLMs is quite distinct from the user's interests in e-commerce, query understanding, and ranking models."
    },
    {
        "title": "When More Is Less: A Systematic Analysis of Spatial and Commonsense Information for Visual Spatial Reasoning",
        "abstract": "Visual spatial reasoning (VSR) remains challenging for modern vision-language models (VLMs), despite advances in multimodal architectures. A common strategy is to inject additional information at inference time, such as explicit spatial cues, external commonsense knowledge, or chain-of-thought (CoT) reasoning instructions. However, it remains unclear when such information genuinely improves reasoning and when it introduces noise. In this paper, we conduct a hypothesis-driven analysis of information injection for VSR across three representative VLMs and two public benchmarks. We examine (i) the type and number of spatial contexts, (ii) the amount and relevance of injected commonsense knowledge, and (iii) the interaction between spatial grounding and CoT prompting. Our results reveal a consistent pattern: more information does not necessarily yield better reasoning. Targeted single spatial cues outperform multi-context aggregation, excessive or weakly relevant commonsense knowledge degrades performance, and CoT prompting improves accuracy only when spatial grounding is sufficiently precise. These findings highlight the importance of selective, task-aligned information injection and provide practical guidance for designing reliable multimodal reasoning pipelines.",
        "url": "http://arxiv.org/abs/2602.21619v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21619v1",
        "arxiv_id": "2602.21619v1",
        "authors": [
            "Muku Akasaka",
            "Soyeon Caren Han"
        ],
        "submitted": "2026-02-25 06:22:48",
        "source": "arxiv",
        "comment": "5 pages, 6 figures, Under review",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it deals with multimodal architectures and visual spatial reasoning. However, the focus on visual spatial reasoning and multimodal architectures is not directly aligned with the user's primary research themes in query understanding, ranking models, and user behavior modeling. The paper's findings on information injection and selective information use may be of some interest, but it is not a central match for the user's research interests."
    },
    {
        "title": "MrBERT: Modern Multilingual Encoders via Vocabulary, Domain, and Dimensional Adaptation",
        "abstract": "We introduce MrBERT, a family of 150M-300M parameter encoders built on the ModernBERT architecture and pre-trained on 35 languages and code. Through targeted adaptation, this model family achieves state-of-the-art results on Catalan- and Spanish-specific tasks, while establishing robust performance across specialized biomedical and legal domains. To bridge the gap between research and production, we incorporate Matryoshka Representation Learning (MRL), enabling flexible vector sizing that significantly reduces inference and storage costs. Ultimately, the MrBERT family demonstrates that modern encoder architectures can be optimized for both localized linguistic excellence and efficient, high-stakes domain specialization. We open source the complete model family on Huggingface.",
        "url": "http://arxiv.org/abs/2602.21379v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21379v1",
        "arxiv_id": "2602.21379v1",
        "authors": [
            "Daniel Tamayo",
            "I√±aki Lacunza",
            "Paula Rivera-Hidalgo",
            "Severino Da Dalt",
            "Javier Aula-Blasco",
            "Aitor Gonzalez-Agirre",
            "Marta Villegas"
        ],
        "submitted": "2026-02-24 21:19:40",
        "source": "arxiv",
        "comment": "24 pages, 14 tables and 4 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a multilingual encoder architecture, which is related to Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on domain adaptation and efficiency in specialized domains is not directly aligned with the user's core research themes in Information Retrieval and Search technologies. While the paper touches on NLP, it is more focused on encoder architectures and adaptation techniques rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives",
        "abstract": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.",
        "url": "http://arxiv.org/abs/2602.21351v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21351v1",
        "arxiv_id": "2602.21351v1",
        "authors": [
            "Dmitrii Pantiukhin",
            "Ivan Kuznetsov",
            "Boris Shapkin",
            "Antonia Anna Jost",
            "Thomas Jung",
            "Nikolay Koldunov"
        ],
        "submitted": "2026-02-24 20:37:38",
        "source": "arxiv",
        "comment": "20 pages, 6 figures, 7 tables, supplementary material included",
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves data analysis and querying, the focus is on geoscientific data archives and multi-agent systems, which does not align with your primary research interests."
    },
    {
        "title": "Decoding the Hook: A Multimodal LLM Framework for Analyzing the Hooking Period of Video Ads",
        "abstract": "Video-based ads are a vital medium for brands to engage consumers, with social media platforms leveraging user data to optimize ad delivery and boost engagement. A crucial but under-explored aspect is the 'hooking period', the first three seconds that capture viewer attention and influence engagement metrics. Analyzing this brief window is challenging due to the multimodal nature of video content, which blends visual, auditory, and textual elements. Traditional methods often miss the nuanced interplay of these components, requiring advanced frameworks for thorough evaluation.\n  This study presents a framework using transformer-based multimodal large language models (MLLMs) to analyze the hooking period of video ads. It tests two frame sampling strategies, uniform random sampling and key frame selection, to ensure balanced and representative acoustic feature extraction, capturing the full range of design elements. The hooking video is processed by state-of-the-art MLLMs to generate descriptive analyses of the ad's initial impact, which are distilled into coherent topics using BERTopic for high-level abstraction. The framework also integrates features such as audio attributes and aggregated ad targeting information, enriching the feature set for further analysis.\n  Empirical validation on large-scale real-world data from social media platforms demonstrates the efficacy of our framework, revealing correlations between hooking period features and key performance metrics like conversion per investment. The results highlight the practical applicability and predictive power of the approach, offering valuable insights for optimizing video ad strategies. This study advances video ad analysis by providing a scalable methodology for understanding and enhancing the initial moments of video advertisements.",
        "url": "http://arxiv.org/abs/2602.22299v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22299v1",
        "arxiv_id": "2602.22299v1",
        "authors": [
            "Kunpeng Zhang",
            "Poppy Zhang",
            "Shawndra Hill",
            "Amel Awadelkarim"
        ],
        "submitted": "2026-02-25 18:24:06",
        "source": "arxiv",
        "comment": "11 pages, 5 figures, 3 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval and search technologies, but its focus on video ad analysis and multimodal large language models is not directly aligned with your core research themes. While it involves deep semantic understanding and real-time relevance optimization, the context is more specific to video advertising and not broadly applicable to information retrieval."
    },
    {
        "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
        "abstract": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring that evaluates dialogues along latent axes while remaining agnostic to the specific personality models, their dimensions, transition mechanisms, or LLMs used. These scores function as dynamic state variables that systematically reconfigure the system prompt, steering behavioral alignment throughout the interaction.We evaluate this framework by operationalizing the Interpersonal Circumplex (IPC) in a medical education setting. Results demonstrate that the system successfully adapts its personality state to user inputs, but also influences user behavior, thereby facilitating de-escalation training. Notably, the scoring pipeline maintains comparable precision even when utilizing lightweight, fine-tuned classifiers instead of large-scale LLMs. This work demonstrates the feasibility of modular, personality-adaptive architectures for education, customer support, and broader human-computer interaction.",
        "url": "http://arxiv.org/abs/2602.22157v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22157v1",
        "arxiv_id": "2602.22157v1",
        "authors": [
            "Leon Pielage",
            "Ole H√§tscher",
            "Mitja Back",
            "Bernhard Marschall",
            "Benjamin Risse"
        ],
        "submitted": "2026-02-25 18:05:11",
        "source": "arxiv",
        "comment": "22 pages, 5 figures, submitted to ICPR 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on adapting personality expression in Large Language Models using state machines, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context and application are quite different from your areas of focus."
    },
    {
        "title": "When AI Writes, Whose Voice Remains? Quantifying Cultural Marker Erasure Across World English Varieties in Large Language Models",
        "abstract": "Large Language Models (LLMs) are increasingly used to ``professionalize'' workplace communication, often at the cost of linguistic identity. We introduce \"Cultural Ghosting\", the systematic erasure of linguistic markers unique to non-native English varieties during text processing. Through analysis of 22,350 LLM outputs generated from 1,490 culturally marked texts (Indian, Singaporean,& Nigerian English) processed by five models under three prompt conditions, we quantify this phenomenon using two novel metrics: Identity Erasure Rate (IER) & Semantic Preservation Score (SPS). Across all prompts, we find an overall IER of 10.26%, with model-level variation from 3.5% to 20.5% (5.9x range). Crucially, we identify a Semantic Preservation Paradox: models maintain high semantic similarity (mean SPS = 0.748) while systematically erasing cultural markers. Pragmatic markers (politeness conventions) are 1.9x more vulnerable than lexical markers (71.5% vs. 37.1% erasure). Our experiments demonstrate that explicit cultural-preservation prompts reduce erasure by 29% without sacrificing semantic quality.",
        "url": "http://arxiv.org/abs/2602.22145v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22145v1",
        "arxiv_id": "2602.22145v1",
        "authors": [
            "Satyam Kumar Navneet",
            "Joydeep Chandra",
            "Yong Zhang"
        ],
        "submitted": "2026-02-25 17:54:42",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on the cultural implications of Large Language Models, specifically the erasure of linguistic markers in non-native English varieties. While it touches on the topic of language processing, it does not seem to be directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "SWE-Prot√©g√©: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
        "abstract": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot√©g√©, a post-training framework that reframes software repair as an expert-prot√©g√© collaboration problem. In SWE-Prot√©g√©, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).",
        "url": "http://arxiv.org/abs/2602.22124v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22124v1",
        "arxiv_id": "2602.22124v1",
        "authors": [
            "Patrick Tser Jern Kon",
            "Archana Pradeep",
            "Ang Chen",
            "Alexander P. Ellis",
            "Warren Hunt",
            "Zijian Wang",
            "John Yang",
            "Samuel Thompson"
        ],
        "submitted": "2026-02-25 17:11:49",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on software engineering tasks and the application of small language models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves learning and collaboration, the context is specific to software engineering and does not align with the user's core themes."
    },
    {
        "title": "Learning to Collaborate via Structures: Cluster-Guided Item Alignment for Federated Recommendation",
        "abstract": "Federated recommendation facilitates collaborative model training across distributed clients while keeping sensitive user interaction data local. Conventional approaches typically rely on synchronizing high-dimensional item representations between the server and clients. This paradigm implicitly assumes that precise geometric alignment of embedding coordinates is necessary for collaboration across clients. We posit that establishing relative semantic relationships among items is more effective than enforcing shared representations. Specifically, global semantic relations serve as structural constraints for items. Within these constraints, the framework allows item representations to vary locally on each client, which flexibility enables the model to capture fine-grained user personalization while maintaining global consistency. To this end, we propose Cluster-Guided FedRec framework (CGFedRec), a framework that transforms uploaded embeddings into compact cluster labels. In this framework, the server functions as a global structure discoverer to learn item clusters and distributes only the resulting labels. This mechanism explicitly cuts off the downstream transmission of item embeddings, relieving clients from maintaining global shared item embeddings. Consequently, CGFedRec achieves the effective injection of global collaborative signals into local item representations without transmitting full embeddings. Extensive experiments demonstrate that our approach significantly improves communication efficiency while maintaining superior recommendation accuracy across multiple datasets.",
        "url": "http://arxiv.org/abs/2602.21957v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21957v1",
        "arxiv_id": "2602.21957v1",
        "authors": [
            "Yuchun Tu",
            "Zhiwei Li",
            "Bingli Sun",
            "Yixuan Li",
            "Xiao Song"
        ],
        "submitted": "2026-02-25 14:39:47",
        "source": "arxiv",
        "comment": "18 pages, 9 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a federated recommendation framework that focuses on collaborative model training across distributed clients. While it touches on recommendation systems, which are related to information retrieval, the primary focus is on recommender systems rather than information retrieval. The paper's emphasis on federated learning and item alignment does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
        "abstract": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.",
        "url": "http://arxiv.org/abs/2602.21951v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21951v1",
        "arxiv_id": "2602.21951v1",
        "authors": [
            "Bo Xue",
            "Yuan Jin",
            "Luoyi Fu",
            "Jiaxin Ding",
            "Xinbing Wang"
        ],
        "submitted": "2026-02-25 14:34:02",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The use of Large Language Models (LLMs) for knowledge graph reasoning and the focus on discriminative relational reasoning aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the specific application to knowledge graph reasoning is somewhat outside your primary focus on e-commerce and general search technologies."
    },
    {
        "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
        "abstract": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.",
        "url": "http://arxiv.org/abs/2602.21887v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21887v1",
        "arxiv_id": "2602.21887v1",
        "authors": [
            "Changjiang Gao",
            "Zixian Huang",
            "Kaichen Yang",
            "Jiajun Chen",
            "Jixing Li",
            "Shujian Huang"
        ],
        "submitted": "2026-02-25 13:10:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper ExpLang explores the use of multilingual thinking in large language models, which is somewhat related to information retrieval and search technologies. However, the focus on language selection and reinforcement learning is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While the paper touches on the idea of exploration and exploitation, it is more relevant to the NLP domain than information retrieval."
    },
    {
        "title": "FewMMBench: A Benchmark for Multimodal Few-Shot Learning",
        "abstract": "As multimodal large language models (MLLMs) advance in handling interleaved image-text data, assessing their few-shot learning capabilities remains an open challenge. In this paper, we introduce FewMMBench, a comprehensive benchmark designed to evaluate MLLMs under few-shot conditions, with a focus on In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting. Covering a diverse suite of multimodal understanding tasks, from attribute recognition to temporal reasoning, FewMMBench enables systematic analysis across task types, model families, and prompting strategies. We evaluate 26 open-weight MLLMs from six model families across zero-shot, few-shot, and CoT-augmented few-shot settings. Our findings reveal that instruction-tuned models exhibit strong zero-shot performance but benefit minimally, or even regress, with additional demonstrations or CoT reasoning. Retrieval-based demonstrations and increased context size also yield limited gains. These results highlight FewMMBench as a rigorous testbed for diagnosing and advancing few-shot capabilities in multimodal LLMs. The data is available at: https://huggingface.co/datasets/mustafaa/FewMMBench",
        "url": "http://arxiv.org/abs/2602.21854v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21854v1",
        "arxiv_id": "2602.21854v1",
        "authors": [
            "Mustafa Dogan",
            "Ilker Kesen",
            "Iacer Calixto",
            "Aykut Erdem",
            "Erkut Erdem"
        ],
        "submitted": "2026-02-25 12:30:18",
        "source": "arxiv",
        "comment": "Preprint. 49 pages, 38 Figures, 5 Tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves multimodal large language models, the focus is on few-shot learning capabilities, which is not a central theme in your research."
    },
    {
        "title": "Explore-on-Graph: Incentivizing Autonomous Exploration of Large Language Models on Knowledge Graphs with Path-refined Reward Modeling",
        "abstract": "The reasoning process of Large Language Models (LLMs) is often plagued by hallucinations and missing facts in question-answering tasks. A promising solution is to ground LLMs' answers in verifiable knowledge sources, such as Knowledge Graphs (KGs). Prevailing KG-enhanced methods typically constrained LLM reasoning either by enforcing rules during generation or by imitating paths from a fixed set of demonstrations. However, they naturally confined the reasoning patterns of LLMs within the scope of prior experience or fine-tuning data, limiting their generalizability to out-of-distribution graph reasoning problems. To tackle this problem, in this paper, we propose Explore-on-Graph (EoG), a novel framework that encourages LLMs to autonomously explore a more diverse reasoning space on KGs. To incentivize exploration and discovery of novel reasoning paths, we propose to introduce reinforcement learning during training, whose reward is the correctness of the reasoning paths' final answers. To enhance the efficiency and meaningfulness of the exploration, we propose to incorporate path information as additional reward signals to refine the exploration process and reduce futile efforts. Extensive experiments on five KGQA benchmark datasets demonstrate that, to the best of our knowledge, our method achieves state-of-the-art performance, outperforming not only open-source but also even closed-source LLMs.",
        "url": "http://arxiv.org/abs/2602.21728v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21728v1",
        "arxiv_id": "2602.21728v1",
        "authors": [
            "Shiqi Yan",
            "Yubo Chen",
            "Ruiqi Zhou",
            "Zhengxi Yao",
            "Shuai Chen",
            "Tianyi Zhang",
            "Shijie Zhang",
            "Wei Qiang Zhang",
            "Yongfeng Huang",
            "Haixin Duan",
            "Yunqi Zhang"
        ],
        "submitted": "2026-02-25 09:35:18",
        "source": "arxiv",
        "comment": "Published as a conference paper at ICLR 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of reinforcement learning on large language models to improve their reasoning capabilities on knowledge graphs. While it touches on aspects of query understanding and ranking models, the primary focus is on a specific area of NLP, which is relevant but not central to your research interests. The paper's emphasis on knowledge graphs and path-refined reward modeling is somewhat related to your interests in information retrieval and semantic understanding, but it does not directly align with your core themes."
    },
    {
        "title": "DWA-KD: Dual-Space Weighting and Time-Warped Alignment for Cross-Tokenizer Knowledge Distillation",
        "abstract": "Knowledge Distillation (KD) has emerged as a crucial technique for compressing Large Language Models (LLMs). Although existing cross-tokenizer KD methods have made notable progress, their effectiveness remains constrained by suboptimal alignment across sequence and vocabulary levels. To address these limitations, we introduce Dual-Space Weighting and Time-Warped Alignment (DWA-KD), a novel cross-tokenizer distillation framework that enhances token-wise distillation through dual-space entropy-based weighting and achieves precise sequence-level alignment by leveraging both lexical and semantic information. At the token level, DWA-KD maps teacher representations into the student space and vice versa, performing dual-space KD via Kullback-Leibler divergence (KL). The process is modulated by dual-space weights that up-weight tokens where the student is uncertain and the teacher is confident, thereby focusing learning on informative tokens rather than treating all positions equally. At the sequence level, DWA-KD applies Soft Dynamic Time Warping (Soft-DTW) to both the embedding and final hidden-state layers, enabling robust alignment of lexical and contextual semantics between teacher and student sequences. Extensive experiments across diverse NLP benchmarks demonstrate that DWA-KD outperforms state-of-the-art KD baselines, while ablation studies confirm the complementary contributions of entropy-based token weighting and embedding and final hidden state layer Soft-DTW alignment.",
        "url": "http://arxiv.org/abs/2602.21669v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21669v1",
        "arxiv_id": "2602.21669v1",
        "authors": [
            "Duc Trung Vu",
            "Pham Khanh Chi",
            "Dat Phi Van",
            "Linh Ngo Van",
            "Sang Dinh",
            "Trung Le"
        ],
        "submitted": "2026-02-25 08:04:44",
        "source": "arxiv",
        "comment": "EACL Findings",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on knowledge distillation for Large Language Models, which is a topic in Natural Language Processing (NLP). However, it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "RuCL: Stratified Rubric-Based Curriculum Learning for Multimodal Large Language Model Reasoning",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a prevailing paradigm for enhancing reasoning in Multimodal Large Language Models (MLLMs). However, relying solely on outcome supervision risks reward hacking, where models learn spurious reasoning patterns to satisfy final answer checks. While recent rubric-based approaches offer fine-grained supervision signals, they suffer from high computational costs of instance-level generation and inefficient training dynamics caused by treating all rubrics as equally learnable. In this paper, we propose Stratified Rubric-based Curriculum Learning (RuCL), a novel framework that reformulates curriculum learning by shifting the focus from data selection to reward design. RuCL generates generalized rubrics for broad applicability and stratifies them based on the model's competence. By dynamically adjusting rubric weights during training, RuCL guides the model from mastering foundational perception to tackling advanced logical reasoning. Extensive experiments on various visual reasoning benchmarks show that RuCL yields a remarkable +7.83% average improvement over the Qwen2.5-VL-7B model, achieving a state-of-the-art accuracy of 60.06%.",
        "url": "http://arxiv.org/abs/2602.21628v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21628v1",
        "arxiv_id": "2602.21628v1",
        "authors": [
            "Yukun Chen",
            "Jiaming Li",
            "Longze Chen",
            "Ze Gong",
            "Jingpeng Li",
            "Zhen Qin",
            "Hengyu Chang",
            "Ancheng Xu",
            "Zhihao Yang",
            "Hamid Alinejad-Rokny",
            "Qiang Qu",
            "Bo Zheng",
            "Min Yang"
        ],
        "submitted": "2026-02-25 06:46:24",
        "source": "arxiv",
        "comment": "8 pages",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal large language model reasoning, using reinforcement learning and curriculum learning to improve model performance. While it touches on aspects of model training and evaluation, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "MixSarc: A Bangla-English Code-Mixed Corpus for Implicit Meaning Identification",
        "abstract": "Bangla-English code-mixing is widespread across South Asian social media, yet resources for implicit meaning identification in this setting remain scarce. Existing sentiment and sarcasm models largely focus on monolingual English or high-resource languages and struggle with transliteration variation, cultural references, and intra-sentential language switching. To address this gap, we introduce MixSarc, the first publicly available Bangla-English code-mixed corpus for implicit meaning identification. The dataset contains 9,087 manually annotated sentences labeled for humor, sarcasm, offensiveness, and vulgarity. We construct the corpus through targeted social media collection, systematic filtering, and multi-annotator validation. We benchmark transformer-based models and evaluate zero-shot large language models under structured prompting. Results show strong performance on humor detection but substantial degradation on sarcasm, offense, and vulgarity due to class imbalance and pragmatic complexity. Zero-shot models achieve competitive micro-F1 scores but low exact match accuracy. Further analysis reveals that over 42\\% of negative sentiment instances in an external dataset exhibit sarcastic characteristics. MixSarc provides a foundational resource for culturally aware NLP and supports more reliable multi-label modeling in code-mixed environments.",
        "url": "http://arxiv.org/abs/2602.21608v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21608v1",
        "arxiv_id": "2602.21608v1",
        "authors": [
            "Kazi Samin Yasar Alam",
            "Md Tanbir Chowdhury",
            "Tamim Ahmed",
            "Ajwad Abrar",
            "Md Rafid Haque"
        ],
        "submitted": "2026-02-25 06:12:06",
        "source": "arxiv",
        "comment": "Under Review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a Bangla-English code-mixed corpus for implicit meaning identification, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the specific context and task are quite different from the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "iMiGUE-Speech: A Spontaneous Speech Dataset for Affective Analysis",
        "abstract": "This work presents iMiGUE-Speech, an extension of the iMiGUE dataset that provides a spontaneous affective corpus for studying emotional and affective states. The new release focuses on speech and enriches the original dataset with additional metadata, including speech transcripts, speaker-role separation between interviewer and interviewee, and word-level forced alignments. Unlike existing emotional speech datasets that rely on acted or laboratory-elicited emotions, iMiGUE-Speech captures spontaneous affect arising naturally from real match outcomes. To demonstrate the utility of the dataset and establish initial benchmarks, we introduce two evaluation tasks for comparative assessment: speech emotion recognition and transcript-based sentiment analysis. These tasks leverage state-of-the-art pre-trained representations to assess the dataset's ability to capture spontaneous affective states from both acoustic and linguistic modalities. iMiGUE-Speech can also be synchronously paired with micro-gesture annotations from the original iMiGUE dataset, forming a uniquely multimodal resource for studying speech-gesture affective dynamics. The extended dataset is available at https://github.com/CV-AC/imigue-speech.",
        "url": "http://arxiv.org/abs/2602.21464v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21464v1",
        "arxiv_id": "2602.21464v1",
        "authors": [
            "Sofoklis Kakouros",
            "Fang Kang",
            "Haoyu Chen"
        ],
        "submitted": "2026-02-25 00:38:19",
        "source": "arxiv",
        "comment": "Accepted to Speech Prosody 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on affective analysis and speech datasets, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves speech transcripts and sentiment analysis, the context is more aligned with affective computing and multimodal analysis rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Small Language Models for Privacy-Preserving Clinical Information Extraction in Low-Resource Languages",
        "abstract": "Extracting clinical information from medical transcripts in low-resource languages remains a significant challenge in healthcare natural language processing (NLP). This study evaluates a two-step pipeline combining Aya-expanse-8B as a Persian-to-English translation model with five open-source small language models (SLMs) -- Qwen2.5-7B-Instruct, Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Qwen2.5-1.5B-Instruct, and Gemma-3-1B-it -- for binary extraction of 13 clinical features from 1,221 anonymized Persian transcripts collected at a cancer palliative care call center. Using a few-shot prompting strategy without fine-tuning, models were assessed on macro-averaged F1-score, Matthews Correlation Coefficient (MCC), sensitivity, and specificity to account for class imbalance. Qwen2.5-7B-Instruct achieved the highest overall performance (median macro-F1: 0.899; MCC: 0.797), while Gemma-3-1B-it showed the weakest results. Larger models (7B--8B parameters) consistently outperformed smaller counterparts in sensitivity and MCC. A bilingual analysis of Aya-expanse-8B revealed that translating Persian transcripts to English improved sensitivity, reduced missing outputs, and boosted metrics robust to class imbalance, though at the cost of slightly lower specificity and precision. Feature-level results showed reliable extraction of physiological symptoms across most models, whereas psychological complaints, administrative requests, and complex somatic features remained challenging. These findings establish a practical, privacy-preserving blueprint for deploying open-source SLMs in multilingual clinical NLP settings with limited infrastructure and annotation resources, and highlight the importance of jointly optimizing model scale and input language strategy for sensitive healthcare applications.",
        "url": "http://arxiv.org/abs/2602.21374v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21374v1",
        "arxiv_id": "2602.21374v1",
        "authors": [
            "Mohammadreza Ghaffarzadeh-Esfahani",
            "Nahid Yousefian",
            "Ebrahim Heidari-Farsani",
            "Ali Akbar Omidvarian",
            "Sepehr Ghahraei",
            "Atena Farangi",
            "AmirBahador Boroumand"
        ],
        "submitted": "2026-02-24 21:10:29",
        "source": "arxiv",
        "comment": "16 pages, 3 figures, 2 supplementary files",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on clinical information extraction using small language models in low-resource languages, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and context are quite different from your areas of focus."
    },
    {
        "title": "Black-Box Reliability Certification for AI Agents via Self-Consistency Sampling and Conformal Calibration",
        "abstract": "Given a black-box AI system and a task, at what confidence level can a practitioner trust the system's output? We answer with a reliability level -- a single number per system-task pair, derived from self-consistency sampling and conformal calibration, that serves as a black-box deployment gate with exact, finite-sample, distribution-free guarantees. Self-consistency sampling reduces uncertainty exponentially; conformal calibration guarantees correctness within 1/(n+1) of the target level, regardless of the system's errors -- made transparently visible through larger answer sets for harder questions. Weaker models earn lower reliability levels (not accuracy -- see Definition 2.4): GPT-4.1 earns 94.6% on GSM8K and 96.8% on TruthfulQA, while GPT-4.1-nano earns 89.8% on GSM8K and 66.5% on MMLU. We validate across five benchmarks, five models from three families, and both synthetic and real data. Conditional coverage on solvable items exceeds 0.93 across all configurations; sequential stopping reduces API costs by around 50%.",
        "url": "http://arxiv.org/abs/2602.21368v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21368v1",
        "arxiv_id": "2602.21368v1",
        "authors": [
            "Charafeddine Mouzouni"
        ],
        "submitted": "2026-02-24 21:03:50",
        "source": "arxiv",
        "comment": "41 pages, 11 figures, 10 tables, including appendices",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on reliability certification for AI agents, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it touches on AI systems, the context is more about model reliability and deployment, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment",
        "abstract": "Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.",
        "url": "http://arxiv.org/abs/2602.21346v1",
        "pdf_url": "https://arxiv.org/pdf/2602.21346v1",
        "arxiv_id": "2602.21346v1",
        "authors": [
            "Mengxuan Hu",
            "Vivek V. Datla",
            "Anoop Kumar",
            "Zihan Guan",
            "Sheng Li",
            "Alfy Samuel",
            "Daben Liu"
        ],
        "submitted": "2026-02-24 20:30:51",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the safety of large language models through alignment techniques, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on aspects of NLP, the primary focus on safety and alignment in language models makes it less relevant to your research areas."
    },
    {
        "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
        "abstract": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.",
        "url": "http://arxiv.org/abs/2602.22190v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22190v1",
        "arxiv_id": "2602.22190v1",
        "authors": [
            "Rui Yang",
            "Qianhui Wu",
            "Zhaoyang Wang",
            "Hanyang Chen",
            "Ke Yang",
            "Hao Cheng",
            "Huaxiu Yao",
            "Baoling Peng",
            "Huan Zhang",
            "Jianfeng Gao",
            "Tong Zhang"
        ],
        "submitted": "2026-02-25 18:34:57",
        "source": "arxiv",
        "comment": "57 pages, 17 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves training agents to reason and act, the focus is on GUI agents and action-aware supervision, which is not a central match to your interests."
    },
    {
        "title": "DLT-Corpus: A Large-Scale Text Collection for the Distributed Ledger Technology Domain",
        "abstract": "We introduce DLT-Corpus, the largest domain-specific text collection for Distributed Ledger Technology (DLT) research to date: 2.98 billion tokens from 22.12 million documents spanning scientific literature (37,440 publications), United States Patent and Trademark Office (USPTO) patents (49,023 filings), and social media (22 million posts). Existing Natural Language Processing (NLP) resources for DLT focus narrowly on cryptocurrencies price prediction and smart contracts, leaving domain-specific language under explored despite the sector's ~$3 trillion market capitalization and rapid technological evolution.\n  We demonstrate DLT-Corpus' utility by analyzing technology emergence patterns and market-innovation correlations. Findings reveal that technologies originate in scientific literature before reaching patents and social media, following traditional technology transfer patterns. While social media sentiment remains overwhelmingly bullish even during crypto winters, scientific and patent activity grow independently of market fluctuations, tracking overall market expansion in a virtuous cycle where research precedes and enables economic growth that funds further innovation.\n  We publicly release the full DLT-Corpus; LedgerBERT, a domain-adapted model achieving 23% improvement over BERT-base on a DLT-specific Named Entity Recognition (NER) task; and all associated tools and code.",
        "url": "http://arxiv.org/abs/2602.22045v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22045v1",
        "arxiv_id": "2602.22045v1",
        "authors": [
            "Walter Hernandez Cruz",
            "Peter Devine",
            "Nikhil Vadgama",
            "Paolo Tasca",
            "Jiahua Xu"
        ],
        "submitted": "2026-02-25 15:53:41",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a large-scale text collection for Distributed Ledger Technology, which is a specific domain. While it involves Natural Language Processing (NLP) and utilizes a BERT-based model, the focus is more on the DLT domain rather than general IR or search technologies. The paper's relevance to user's interests is somewhat related, but not central to their core research themes."
    },
    {
        "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
        "abstract": "Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recognition, a translation-guided ASR framework that utilizes multilingual translation embeddings to enhance recognition performance in low-resource environments. The framework is centered around the parallel gated cross-attention (PGCA) mechanism, which adaptively integrates embeddings from various auxiliary languages into the ASR decoder. This mechanism facilitates robust cross-linguistic semantic guidance while ensuring stable optimization and minimizing interference between languages. To support ongoing research initiatives, we present YT-THDC, a 30-hour corpus of Taiwanese Hokkien drama speech with aligned Mandarin subtitles and manually verified Taiwanese Hokkien transcriptions. Comprehensive experiments and analyses identify the auxiliary languages that most effectively enhance ASR performance, achieving a 14.77% relative reduction in character error rate and demonstrating the efficacy of translation-guided learning for underrepresented languages in practical applications.",
        "url": "http://arxiv.org/abs/2602.22039v1",
        "pdf_url": "https://arxiv.org/pdf/2602.22039v1",
        "arxiv_id": "2602.22039v1",
        "authors": [
            "Cheng-Yeh Yang",
            "Chien-Chun Wang",
            "Li-Wei Chen",
            "Hung-Shin Lee",
            "Hsin-Min Wang",
            "Berlin Chen"
        ],
        "submitted": "2026-02-25 15:47:34",
        "source": "arxiv",
        "comment": "Accepted to LREC 2026",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on low-resource automatic speech recognition using translation-guided learning and parallel gated cross-attention. While it involves NLP and deep semantic understanding, it is not directly related to information retrieval, search technologies, or query understanding, which are the primary areas of interest."
    },
    {
        "title": "Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?",
        "abstract": "Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.\n  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.",
        "url": "http://arxiv.org/abs/2602.21480v2",
        "pdf_url": "https://arxiv.org/pdf/2602.21480v2",
        "arxiv_id": "2602.21480v2",
        "authors": [
            "Germ√°n T. Eizaguirre",
            "Lars Tissen",
            "Marc S√°nchez-Artigas"
        ],
        "submitted": "2026-02-25 01:12:35",
        "source": "arxiv",
        "comment": "11 pages, 4 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the intersection of Text-to-SQL and Big Data, introducing novel metrics for evaluating Text-to-Big SQL. While it touches on aspects of query understanding and real-time relevance optimization, its primary focus is on the performance implications of text-to-SQL systems at scale, which is somewhat related to the user's interests in Information Retrieval and Search technologies."
    }
]