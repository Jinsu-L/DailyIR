[
    {
        "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
        "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
        "url": "http://arxiv.org/abs/2512.19682v2",
        "pdf_url": "https://arxiv.org/pdf/2512.19682v2",
        "arxiv_id": "2512.19682v2",
        "authors": [
            "Jiacheng Guo",
            "Ling Yang",
            "Peter Chen",
            "Qixin Xiao",
            "Yinjie Wang",
            "Xinzhe Juan",
            "Jiahao Qiu",
            "Ke Shen",
            "Mengdi Wang"
        ],
        "submitted": "2025-12-22 18:57:13",
        "source": "arxiv",
        "comment": "Our codes are available at https://github.com/Gen-Verse/GenEnv"
    },
    {
        "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
        "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
        "url": "http://arxiv.org/abs/2512.19673v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
        "arxiv_id": "2512.19673v1",
        "authors": [
            "Yuqiao Tan",
            "Minzheng Wang",
            "Shizhu He",
            "Huanxuan Liao",
            "Chengfeng Zhao",
            "Qiunan Lu",
            "Tian Liang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "submitted": "2025-12-22 18:51:48",
        "source": "arxiv",
        "comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO"
    },
    {
        "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
        "abstract": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
        "url": "http://arxiv.org/abs/2512.19651v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19651v1",
        "arxiv_id": "2512.19651v1",
        "authors": [
            "Filippos Ventirozos",
            "Peter Appleby",
            "Matthew Shardlow"
        ],
        "submitted": "2025-12-22 18:23:37",
        "source": "arxiv",
        "comment": "9 pages, 3 figures, 3 tables"
    },
    {
        "title": "Diacritic Restoration for Low-Resource Indigenous Languages: Case Study with Bribri and Cook Islands Māori",
        "abstract": "We present experiments on diacritic restoration, a form of text normalization essential for natural language processing (NLP) tasks. Our study focuses on two extremely under-resourced languages: Bribri, a Chibchan language spoken in Costa Rica, and Cook Islands Māori, a Polynesian language spoken in the Cook Islands. Specifically, this paper: (i) compares algorithms for diacritics restoration in under-resourced languages, including tonal diacritics, (ii) examines the amount of data required to achieve target performance levels, (iii) contrasts results across varying resource conditions, and (iv) explores the related task of diacritic correction. We find that fine-tuned, character-level LLMs perform best, likely due to their ability to decompose complex characters into their UTF-8 byte representations. In contrast, massively multilingual models perform less effectively given our data constraints. Across all models, reliable performance begins to emerge with data budgets of around 10,000 words. Zero-shot approaches perform poorly in all cases. This study responds both to requests from the language communities and to broader NLP research questions concerning model performance and generalization in under-resourced contexts.",
        "url": "http://arxiv.org/abs/2512.19630v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19630v1",
        "arxiv_id": "2512.19630v1",
        "authors": [
            "Rolando Coto-Solano",
            "Daisy Li",
            "Manoela Teleginski Ferraz",
            "Olivia Sasse",
            "Cha Krupka",
            "Sharid Loáiciga",
            "Sally Akevai Tenamu Nicholas"
        ],
        "submitted": "2025-12-22 18:04:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring the features used for summary evaluation by Human and GPT",
        "abstract": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
        "url": "http://arxiv.org/abs/2512.19620v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
        "arxiv_id": "2512.19620v1",
        "authors": [
            "Zahra Sadeghi",
            "Evangelos Milios",
            "Frank Rudzicz"
        ],
        "submitted": "2025-12-22 17:54:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MauBERT: Universal Phonetic Inductive Biases for Few-Shot Acoustic Units Discovery",
        "abstract": "This paper introduces MauBERT, a multilingual extension of HuBERT that leverages articulatory features for robust cross-lingual phonetic representation learning. We continue HuBERT pre-training with supervision based on a phonetic-to-articulatory feature mapping in 55 languages. Our models learn from multilingual data to predict articulatory features or phones, resulting in language-independent representations that capture multilingual phonetic properties. Through comprehensive ABX discriminability testing, we show MauBERT models produce more context-invariant representations than state-of-the-art multilingual self-supervised learning models. Additionally, the models effectively adapt to unseen languages and casual speech with minimal self-supervised fine-tuning (10 hours of speech). This establishes an effective approach for instilling linguistic inductive biases in self-supervised speech models.",
        "url": "http://arxiv.org/abs/2512.19612v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19612v1",
        "arxiv_id": "2512.19612v1",
        "authors": [
            "Angelo Ortiz Tandazo",
            "Manel Khentout",
            "Youssef Benchekroun",
            "Thomas Hueber",
            "Emmanuel Dupoux"
        ],
        "submitted": "2025-12-22 17:47:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Increasing the Thinking Budget is Not All You Need",
        "abstract": "Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",
        "url": "http://arxiv.org/abs/2512.19585v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19585v1",
        "arxiv_id": "2512.19585v1",
        "authors": [
            "Ignacio Iacobacci",
            "Zhaozhi Qian",
            "Faroq AL-Tam",
            "Muhammad AL-Qurishi",
            "Riad Souissi"
        ],
        "submitted": "2025-12-22 17:12:04",
        "source": "arxiv",
        "comment": "4 pages, 4 figures, 3 tables"
    },
    {
        "title": "Algerian Dialect",
        "abstract": "We present Algerian Dialect, a large-scale sentiment-annotated dataset consisting of 45,000 YouTube comments written in Algerian Arabic dialect. The comments were collected from more than 30 Algerian press and media channels using the YouTube Data API. Each comment is manually annotated into one of five sentiment categories: very negative, negative, neutral, positive, and very positive. In addition to sentiment labels, the dataset includes rich metadata such as collection timestamps, like counts, video URLs, and annotation dates. This dataset addresses the scarcity of publicly available resources for Algerian dialect and aims to support research in sentiment analysis, dialectal Arabic NLP, and social media analytics. The dataset is publicly available on Mendeley Data under a CC BY 4.0 license at https://doi.org/10.17632/zzwg3nnhsz.2.",
        "url": "http://arxiv.org/abs/2512.19543v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19543v1",
        "arxiv_id": "2512.19543v1",
        "authors": [
            "Zakaria Benmounah",
            "Abdennour Boulesnane"
        ],
        "submitted": "2025-12-22 16:26:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Event Extraction in Large Language Model",
        "abstract": "Large language models (LLMs) and multimodal LLMs are changing event extraction (EE): prompting and generation can often produce structured outputs in zero shot or few shot settings. Yet LLM based pipelines face deployment gaps, including hallucinations under weak constraints, fragile temporal and causal linking over long contexts and across documents, and limited long horizon knowledge management within a bounded context window. We argue that EE should be viewed as a system component that provides a cognitive scaffold for LLM centered solutions. Event schemas and slot constraints create interfaces for grounding and verification; event centric structures act as controlled intermediate representations for stepwise reasoning; event links support relation aware retrieval with graph based RAG; and event stores offer updatable episodic and agent memory beyond the context window. This survey covers EE in text and multimodal settings, organizing tasks and taxonomy, tracing method evolution from rule based and neural models to instruction driven and generative frameworks, and summarizing formulations, decoding strategies, architectures, representations, datasets, and evaluation. We also review cross lingual, low resource, and domain specific settings, and highlight open challenges and future directions for reliable event centric systems. Finally, we outline open challenges and future directions that are central to the LLM era, aiming to evolve EE from static extraction into a structurally reliable, agent ready perception and memory layer for open world systems.",
        "url": "http://arxiv.org/abs/2512.19537v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19537v1",
        "arxiv_id": "2512.19537v1",
        "authors": [
            "Bobo Li",
            "Xudong Han",
            "Jiang Liu",
            "Yuzhe Ding",
            "Liqiang Jing",
            "Zhaoqi Zhang",
            "Jinheng Li",
            "Xinya Du",
            "Fei Li",
            "Meishan Zhang",
            "Min Zhang",
            "Aixin Sun",
            "Philip S. Yu",
            "Hao Fei"
        ],
        "submitted": "2025-12-22 16:22:14",
        "source": "arxiv",
        "comment": "38 pages, 9 Figures, 5 Tables"
    },
    {
        "title": "A Large-Language-Model Framework for Automated Humanitarian Situation Reporting",
        "abstract": "Timely and accurate situational reports are essential for humanitarian decision-making, yet current workflows remain largely manual, resource intensive, and inconsistent. We present a fully automated framework that uses large language models (LLMs) to transform heterogeneous humanitarian documents into structured and evidence-grounded reports. The system integrates semantic text clustering, automatic question generation, retrieval augmented answer extraction with citations, multi-level summarization, and executive summary generation, supported by internal evaluation metrics that emulate expert reasoning. We evaluated the framework across 13 humanitarian events, including natural disasters and conflicts, using more than 1,100 documents from verified sources such as ReliefWeb. The generated questions achieved 84.7 percent relevance, 84.0 percent importance, and 76.4 percent urgency. The extracted answers reached 86.3 percent relevance, with citation precision and recall both exceeding 76 percent. Agreement between human and LLM based evaluations surpassed an F1 score of 0.80. Comparative analysis shows that the proposed framework produces reports that are more structured, interpretable, and actionable than existing baselines. By combining LLM reasoning with transparent citation linking and multi-level evaluation, this study demonstrates that generative AI can autonomously produce accurate, verifiable, and operationally useful humanitarian situation reports.",
        "url": "http://arxiv.org/abs/2512.19475v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19475v1",
        "arxiv_id": "2512.19475v1",
        "authors": [
            "Ivan Decostanzi",
            "Yelena Mejova",
            "Kyriaki Kalimeri"
        ],
        "submitted": "2025-12-22 15:28:55",
        "source": "arxiv",
        "comment": "18 pages, 3 figures"
    },
    {
        "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
        "abstract": "Large language models (LLMs) are widely described as artificial intelligence, yet their epistemic profile diverges sharply from human cognition. Here we show that the apparent alignment between human and machine outputs conceals a deeper structural mismatch in how judgments are produced. Tracing the historical shift from symbolic AI and information filtering systems to large-scale generative transformers, we argue that LLMs are not epistemic agents but stochastic pattern-completion systems, formally describable as walks on high-dimensional graphs of linguistic transitions rather than as systems that form beliefs or models of the world. By systematically mapping human and artificial epistemic pipelines, we identify seven epistemic fault lines, divergences in grounding, parsing, experience, motivation, causal reasoning, metacognition, and value. We call the resulting condition Epistemia: a structural situation in which linguistic plausibility substitutes for epistemic evaluation, producing the feeling of knowing without the labor of judgment. We conclude by outlining consequences for evaluation, governance, and epistemic literacy in societies increasingly organized around generative AI.",
        "url": "http://arxiv.org/abs/2512.19466v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19466v1",
        "arxiv_id": "2512.19466v1",
        "authors": [
            "Walter Quattrociocchi",
            "Valerio Capraro",
            "Matjaž Perc"
        ],
        "submitted": "2025-12-22 15:20:21",
        "source": "arxiv",
        "comment": "16 pages, 1 figure"
    },
    {
        "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
        "abstract": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
        "url": "http://arxiv.org/abs/2512.19456v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19456v1",
        "arxiv_id": "2512.19456v1",
        "authors": [
            "Jinwei Chi",
            "Ke Wang",
            "Yu Chen",
            "Xuanye Lin",
            "Qiang Xu"
        ],
        "submitted": "2025-12-22 15:01:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation",
        "abstract": "Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines translated high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.",
        "url": "http://arxiv.org/abs/2512.19455v2",
        "pdf_url": "https://arxiv.org/pdf/2512.19455v2",
        "arxiv_id": "2512.19455v2",
        "authors": [
            "Thittipat Pairatsuppawat",
            "Abhibhu Tachaapornchai",
            "Paweekorn Kusolsomboon",
            "Chutikan Chaiwong",
            "Thodsaporn Chay-intr",
            "Kobkrit Viriyayudhakorn",
            "Nongnuch Ketui",
            "Aslan B. Wong"
        ],
        "submitted": "2025-12-22 15:00:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive, and MCP-Augmented Environments",
        "abstract": "Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. To bridge this gap, we introduce MobileWorld, a substantially more challenging benchmark designed to better reflect real-world mobile usage, comprising 201 tasks across 20 applications, while maintaining the same level of reproducible evaluation as AndroidWorld. The difficulty of MobileWorld is twofold. First, it emphasizes long-horizon tasks with cross-application interactions: MobileWorld requires nearly twice as many task-completion steps on average (27.8 vs. 14.3) and includes far more multi-application tasks (62.2% vs. 9.5%) compared to AndroidWorld. Second, MobileWorld extends beyond standard GUI manipulation by introducing novel task categories, including agent-user interaction and MCP-augmented tasks. To ensure robust evaluation, we provide snapshot-based container environment and precise functional verifications, including backend database inspection and task callback APIs. We further develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively. Our analysis shows that current models struggle significantly with user interaction and MCP calls, offering a strategic roadmap toward more robust, next-generation mobile intelligence.",
        "url": "http://arxiv.org/abs/2512.19432v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19432v1",
        "arxiv_id": "2512.19432v1",
        "authors": [
            "Quyu Kong",
            "Xu Zhang",
            "Zhenyu Yang",
            "Nolan Gao",
            "Chen Liu",
            "Panrong Tong",
            "Chenglin Cai",
            "Hanzhang Zhou",
            "Jianan Zhang",
            "Liangyu Chen",
            "Zhidan Liu",
            "Steven Hoi",
            "Yue Wang"
        ],
        "submitted": "2025-12-22 14:31:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
        "abstract": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
        "url": "http://arxiv.org/abs/2512.19424v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19424v1",
        "arxiv_id": "2512.19424v1",
        "authors": [
            "Jian Yang",
            "Wei Zhang",
            "Yizhi Li",
            "Shawn Guo",
            "Haowen Wang",
            "Aishan Liu",
            "Ge Zhang",
            "Zili Wang",
            "Zhoujun Li",
            "Xianglong Liu",
            "Weifeng Lv"
        ],
        "submitted": "2025-12-22 14:27:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Retrieval to Reasoning: A Framework for Cyber Threat Intelligence NER with Explicit and Adaptive Instructions",
        "abstract": "The automation of Cyber Threat Intelligence (CTI) relies heavily on Named Entity Recognition (NER) to extract critical entities from unstructured text. Currently, Large Language Models (LLMs) primarily address this task through retrieval-based In-Context Learning (ICL). This paper analyzes this mainstream paradigm, revealing a fundamental flaw: its success stems not from global semantic similarity but largely from the incidental overlap of entity types within retrieved examples. This exposes the limitations of relying on unreliable implicit induction. To address this, we propose TTPrompt, a framework shifting from implicit induction to explicit instruction. TTPrompt maps the core concepts of CTI's Tactics, Techniques, and Procedures (TTPs) into an instruction hierarchy: formulating task definitions as Tactics, guiding strategies as Techniques, and annotation guidelines as Procedures. Furthermore, to handle the adaptability challenge of static guidelines, we introduce Feedback-driven Instruction Refinement (FIR). FIR enables LLMs to self-refine guidelines by learning from errors on minimal labeled data, adapting to distinct annotation dialects. Experiments on five CTI NER benchmarks demonstrate that TTPrompt consistently surpasses retrieval-based baselines. Notably, with refinement on just 1% of training data, it rivals models fine-tuned on the full dataset. For instance, on LADDER, its Micro F1 of 71.96% approaches the fine-tuned baseline, and on the complex CTINexus, its Macro F1 exceeds the fine-tuned ACLM model by 10.91%.",
        "url": "http://arxiv.org/abs/2512.19414v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19414v1",
        "arxiv_id": "2512.19414v1",
        "authors": [
            "Jiaren Peng",
            "Hongda Sun",
            "Xuan Tian",
            "Cheng Huang",
            "Zeqing Li",
            "Rui Yan"
        ],
        "submitted": "2025-12-22 14:13:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Kunnafonidilaw ka Cadeau: an ASR dataset of present-day Bambara",
        "abstract": "We present Kunkado, a 160-hour Bambara ASR dataset compiled from Malian radio archives to capture present-day spontaneous speech across a wide range of topics. It includes code-switching, disfluencies, background noise, and overlapping speakers that practical ASR systems encounter in real-world use. We finetuned Parakeet-based models on a 33.47-hour human-reviewed subset and apply pragmatic transcript normalization to reduce variability in number formatting, tags, and code-switching annotations. Evaluated on two real-world test sets, finetuning with Kunkado reduces WER from 44.47\\% to 37.12\\% on one and from 36.07\\% to 32.33\\% on the other. In human evaluation, the resulting model also outperforms a comparable system with the same architecture trained on 98 hours of cleaner, less realistic speech. We release the data and models to support robust ASR for predominantly oral languages.",
        "url": "http://arxiv.org/abs/2512.19400v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19400v1",
        "arxiv_id": "2512.19400v1",
        "authors": [
            "Yacouba Diarra",
            "Panga Azazia Kamate",
            "Nouhoum Souleymane Coulibaly",
            "Michael Leventhal"
        ],
        "submitted": "2025-12-22 13:52:33",
        "source": "arxiv",
        "comment": "7 pages, 2 figures"
    },
    {
        "title": "Brain-Grounded Axes for Reading and Steering LLM States",
        "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
        "url": "http://arxiv.org/abs/2512.19399v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19399v1",
        "arxiv_id": "2512.19399v1",
        "authors": [
            "Sandro Andric"
        ],
        "submitted": "2025-12-22 13:51:03",
        "source": "arxiv",
        "comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States"
    },
    {
        "title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models",
        "abstract": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.",
        "url": "http://arxiv.org/abs/2512.19378v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19378v1",
        "arxiv_id": "2512.19378v1",
        "authors": [
            "Zhiqing Hu",
            "Chenxu Zhao",
            "Jiazhong Lu",
            "Xiaolei Liu"
        ],
        "submitted": "2025-12-22 13:23:29",
        "source": "arxiv",
        "comment": "Camera-ready version of the paper accepted for oral presentation at the 11th International Conference on Computer and Communications (ICCC 2025)"
    },
    {
        "title": "Generative vector search to improve pathology foundation models across multimodal vision-language tasks",
        "abstract": "Retrieval-augmented generation improves large language models by grounding outputs in external knowledge sources, reducing hallucinations and addressing knowledge cutoffs. However, standard embedding-based retrieval fails to capture the complexity of multi-concept queries, particularly in domains like biomedicine, where biological data are inherently high-dimensional. For example,omics datasets, and clinical reports simultaneously exhibit numerous molecular, cellular, and physiological features. We present Stochastic Latent Matching (STHLM), a generative vector search method that samples query-conditioned embeddings from text or image inputs to enhance retrieval performance. Analogous to how Chain-of-Thought reasoning enables language models to \"think longer\" on complex problems, STHLM allows retrieval systems to \"search wider\" through iterative sampling. STHLM demonstrates critical improvements over classical vector retrieval across diverse benchmarks, including scientific literature, clinical notes, and tissue images, boosting retrieval performance by 10-30% through test-time compute (trading latency for accuracy), while enabling up to a 10-fold compression of embedding dimensions.",
        "url": "http://arxiv.org/abs/2512.19360v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19360v1",
        "arxiv_id": "2512.19360v1",
        "authors": [
            "Markus Ekvall",
            "Ludvig Bergenstråhle",
            "Patrick Truong",
            "Ben Murrell",
            "Joakim Lundeberg"
        ],
        "submitted": "2025-12-22 12:59:23",
        "source": "arxiv",
        "comment": "13 pages main (54 total), 2 main figures (9 total)"
    },
    {
        "title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
        "abstract": "The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC",
        "url": "http://arxiv.org/abs/2512.19320v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19320v1",
        "arxiv_id": "2512.19320v1",
        "authors": [
            "Yayuan Li",
            "Jian Zhang",
            "Jintao Guo",
            "Zihan Cheng",
            "Lei Qi",
            "Yinghuan Shi",
            "Yang Gao"
        ],
        "submitted": "2025-12-22 12:13:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CienaLLM: Generative Climate-Impact Extraction from News Articles with Autoregressive LLMs",
        "abstract": "Understanding and monitoring the socio-economic impacts of climate hazards requires extracting structured information from heterogeneous news articles on a large scale. To that end, we have developed CienaLLM, a modular framework based on schema-guided Generative Information Extraction. CienaLLM uses open-weight Large Language Models for zero-shot information extraction from news articles, and supports configurable prompts and output schemas, multi-step pipelines, and cloud or on-premise inference. To systematically assess how the choice of LLM family, size, precision regime, and prompting strategy affect performance, we run a large factorial study in models, precisions, and prompt engineering techniques. An additional response parsing step nearly eliminates format errors while preserving accuracy; larger models deliver the strongest and most stable performance, while quantization offers substantial efficiency gains with modest accuracy trade-offs; and prompt strategies show heterogeneous, model-specific effects. CienaLLM matches or outperforms the supervised baseline in accuracy for extracting drought impacts from Spanish news, although at a higher inference cost. While evaluated in droughts, the schema-driven and model-agnostic design is suitable for adapting to related information extraction tasks (e.g., other hazards, sectors, or languages) by editing prompts and schemas rather than retraining. We release code, configurations, and schemas to support reproducible use.",
        "url": "http://arxiv.org/abs/2512.19305v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19305v1",
        "arxiv_id": "2512.19305v1",
        "authors": [
            "Javier Vela-Tambo",
            "Jorge Gracia",
            "Fernando Dominguez-Castro"
        ],
        "submitted": "2025-12-22 11:53:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
        "abstract": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
        "url": "http://arxiv.org/abs/2512.19247v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
        "arxiv_id": "2512.19247v1",
        "authors": [
            "Do Minh Duc",
            "Quan Xuan Truong",
            "Nguyen Tat Dat",
            "Nguyen Van Vinh"
        ],
        "submitted": "2025-12-22 10:29:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
        "abstract": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
        "url": "http://arxiv.org/abs/2512.19240v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19240v1",
        "arxiv_id": "2512.19240v1",
        "authors": [
            "Mingxu Zhang",
            "Dazhong Shen",
            "Qi Zhang",
            "Ying Sun"
        ],
        "submitted": "2025-12-22 10:21:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
        "abstract": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
        "url": "http://arxiv.org/abs/2512.19238v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19238v1",
        "arxiv_id": "2512.19238v1",
        "authors": [
            "Anna-Maria Gueorguieva",
            "Aylin Caliskan"
        ],
        "submitted": "2025-12-22 10:20:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation",
        "abstract": "Current chart-specific tasks, such as chart question answering, chart parsing, and chart generation, are typically studied in isolation, preventing models from learning the shared semantics that link chart generation and interpretation. We introduce CycleChart, a consistency-based learning framework for bidirectional chart understanding and generation. CycleChart adopts a schema-centric formulation as a common interface across tasks. We construct a consistent multi-task dataset, where each chart sample includes aligned annotations for schema prediction, data parsing, and question answering. To learn cross-directional chart semantics, CycleChart introduces a generate-parse consistency objective: the model generates a chart schema from a table and a textual query, then learns to recover the schema and data from the generated chart, enforcing semantic alignment across directions. CycleChart achieves strong results on chart generation, chart parsing, and chart question answering, demonstrating improved cross-task generalization and marking a step toward more general chart understanding models.",
        "url": "http://arxiv.org/abs/2512.19173v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19173v1",
        "arxiv_id": "2512.19173v1",
        "authors": [
            "Dazhen Deng",
            "Sen Yang",
            "Yuchen He",
            "Yuan Tian",
            "Yingcai Wu"
        ],
        "submitted": "2025-12-22 09:07:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation",
        "abstract": "While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.",
        "url": "http://arxiv.org/abs/2512.19171v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19171v1",
        "arxiv_id": "2512.19171v1",
        "authors": [
            "Bingyang Kelvin Liu",
            "Ziyu Patrick Chen"
        ],
        "submitted": "2025-12-22 09:05:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Speech to Subtitles: Evaluating ASR Models in Subtitling Italian Television Programs",
        "abstract": "Subtitles are essential for video accessibility and audience engagement. Modern Automatic Speech Recognition (ASR) systems, built upon Encoder-Decoder neural network architectures and trained on massive amounts of data, have progressively reduced transcription errors on standard benchmark datasets. However, their performance in real-world production environments, particularly for non-English content like long-form Italian videos, remains largely unexplored. This paper presents a case study on developing a professional subtitling system for an Italian media company. To inform our system design, we evaluated four state-of-the-art ASR models (Whisper Large v2, AssemblyAI Universal, Parakeet TDT v3 0.6b, and WhisperX) on a 50-hour dataset of Italian television programs. The study highlights their strengths and limitations, benchmarking their performance against the work of professional human subtitlers. The findings indicate that, while current models cannot meet the media industry's accuracy needs for full autonomy, they can serve as highly effective tools for enhancing human productivity. We conclude that a human-in-the-loop (HITL) approach is crucial and present the production-grade, cloud-based infrastructure we designed to support this workflow.",
        "url": "http://arxiv.org/abs/2512.19161v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19161v1",
        "arxiv_id": "2512.19161v1",
        "authors": [
            "Alessandro Lucca",
            "Francesco Pierri"
        ],
        "submitted": "2025-12-22 08:57:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation",
        "abstract": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.",
        "url": "http://arxiv.org/abs/2512.19134v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19134v1",
        "arxiv_id": "2512.19134v1",
        "authors": [
            "Dehai Min",
            "Kailin Zhang",
            "Tongtong Wu",
            "Lu Cheng"
        ],
        "submitted": "2025-12-22 08:28:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
        "abstract": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
        "url": "http://arxiv.org/abs/2512.19126v2",
        "pdf_url": "https://arxiv.org/pdf/2512.19126v2",
        "arxiv_id": "2512.19126v2",
        "authors": [
            "Zihan Lin",
            "Xiaohan Wang",
            "Hexiong Yang",
            "Jiajun Chai",
            "Jie Cao",
            "Guojun Yin",
            "Wei Lin",
            "Ran He"
        ],
        "submitted": "2025-12-22 08:07:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SAP: Syntactic Attention Pruning for Transformer-based Language Models",
        "abstract": "This paper introduces Syntactic Attention Pruning (SAP), a novel method for effectively pruning attention heads in Transformer models. Unlike conventional approaches that rely solely on mathematical analysis of model weights and activations, SAP incorporates both the syntactic structure and attention patterns of sentences to guide the pruning process. By leveraging these linguistic features, SAP not only achieves performance comparable to state-of-the-art methods but also enhances the interpretability of model behavior. To further improve robustness, we propose Candidate Filtering (CF), a mechanism that prioritizes heads based on their contribution to model performance, mitigating degradation during pruning. Experimental results indicate that SAP effectively preserves critical heads of a high density of strong attention values, outperforming existing head pruning strategies in retrain-free settings. These findings position SAP as a promising foundation for a new direction in model compression research, offering high flexibility for pruning across all transformer-based language models.",
        "url": "http://arxiv.org/abs/2512.19125v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19125v1",
        "arxiv_id": "2512.19125v1",
        "authors": [
            "Tzu-Yun Lee",
            "Ding-Yong Hong",
            "Jan-Jan Wu"
        ],
        "submitted": "2025-12-22 08:05:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation",
        "abstract": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.",
        "url": "http://arxiv.org/abs/2512.19122v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19122v1",
        "arxiv_id": "2512.19122v1",
        "authors": [
            "Mahir Labib Dihan",
            "Sadif Ahmed",
            "Md Nafiu Rahman"
        ],
        "submitted": "2025-12-22 07:53:16",
        "source": "arxiv",
        "comment": "Accepted at BLP Workshop @ IJCNLP-AACL 2025. Code is available at https://github.com/mahirlabibdihan/BanglaForge"
    },
    {
        "title": "Stop saying LLM: Large Discourse Models (LDM) and Artificial Discursive Agent (ADA)?",
        "abstract": "This paper proposes an epistemological shift in the analysis of large generative models, replacing the category ''Large Language Models'' (LLM) with that of ''Large Discourse Models'' (LDM), and then with that of Artificial Discursive Agent (ADA). The theoretical framework is based on an ontological triad distinguishing three regulatory instances: the apprehension of the phenomenal regularities of the referential world, the structuring of embodied cognition, and the structural-linguistic sedimentation of the utterance within a socio-historical context. LDMs, operating on the product of these three instances (the document), model the discursive projection of a portion of human experience reified by the learning corpus. The proposed program aims to replace the ''fascination/fear'' dichotomy with public trials and procedures that make the place, uses, and limits of artificial discursive agents in contemporary social space decipherable, situating this approach within a perspective of governance and co-regulation involving the State, industry, civil society, and academia.",
        "url": "http://arxiv.org/abs/2512.19117v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19117v1",
        "arxiv_id": "2512.19117v1",
        "authors": [
            "Amar Lakel"
        ],
        "submitted": "2025-12-22 07:43:43",
        "source": "arxiv",
        "comment": "in French language"
    },
    {
        "title": "A Large Language Model Based Method for Complex Logical Reasoning over Knowledge Graphs",
        "abstract": "Reasoning over knowledge graphs (KGs) with first-order logic (FOL) queries is challenging due to the inherent incompleteness of real-world KGs and the compositional complexity of logical query structures. Most existing methods rely on embedding entities and relations into continuous geometric spaces and answer queries via differentiable set operations. While effective for simple query patterns, these approaches often struggle to generalize to complex queries involving multiple operators, deeper reasoning chains, or heterogeneous KG schemas. We propose ROG (Reasoning Over knowledge Graphs with large language models), an ensemble-style framework that combines query-aware KG neighborhood retrieval with large language model (LLM)-based chain-of-thought reasoning. ROG decomposes complex FOL queries into sequences of simpler sub-queries, retrieves compact, query-relevant subgraphs as contextual evidence, and performs step-by-step logical inference using an LLM, avoiding the need for task-specific embedding optimization. Experiments on standard KG reasoning benchmarks demonstrate that ROG consistently outperforms strong embedding-based baselines in terms of mean reciprocal rank (MRR), with particularly notable gains on high-complexity query types. These results suggest that integrating structured KG retrieval with LLM-driven logical reasoning offers a robust and effective alternative for complex KG reasoning tasks.",
        "url": "http://arxiv.org/abs/2512.19092v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19092v1",
        "arxiv_id": "2512.19092v1",
        "authors": [
            "Ziyan Zhang",
            "Chao Wang",
            "Zhuo Chen",
            "Lei Chen",
            "Chiyi Li",
            "Kai Song"
        ],
        "submitted": "2025-12-22 07:01:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding",
        "abstract": "Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)",
        "url": "http://arxiv.org/abs/2512.19070v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19070v1",
        "arxiv_id": "2512.19070v1",
        "authors": [
            "Ruiqi Ma",
            "Yu Yan",
            "Chunhong Zhang",
            "Minghao Yin",
            "XinChao Liu",
            "Zhihong Jin",
            "Zheng Hu"
        ],
        "submitted": "2025-12-22 06:20:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
        "abstract": "Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.",
        "url": "http://arxiv.org/abs/2512.19012v2",
        "pdf_url": "https://arxiv.org/pdf/2512.19012v2",
        "arxiv_id": "2512.19012v2",
        "authors": [
            "Shijian Ma",
            "Yunqi Huang",
            "Yan Lin"
        ],
        "submitted": "2025-12-22 04:03:01",
        "source": "arxiv",
        "comment": "Project page: https://dramabench.pages.dev/"
    },
    {
        "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
        "abstract": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
        "url": "http://arxiv.org/abs/2512.19011v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19011v1",
        "arxiv_id": "2512.19011v1",
        "authors": [
            "Akshaj Prashanth Rao",
            "Advait Singh",
            "Saumya Kumaar Saksena",
            "Dhruv Kumar"
        ],
        "submitted": "2025-12-22 04:00:35",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
        "abstract": "Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.",
        "url": "http://arxiv.org/abs/2512.19004v1",
        "pdf_url": "https://arxiv.org/pdf/2512.19004v1",
        "arxiv_id": "2512.19004v1",
        "authors": [
            "Tongyuan Miao",
            "Gary Huang",
            "Kai Jun Han",
            "Annie Jiang"
        ],
        "submitted": "2025-12-22 03:45:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework",
        "abstract": "When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.",
        "url": "http://arxiv.org/abs/2512.18999v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18999v1",
        "arxiv_id": "2512.18999v1",
        "authors": [
            "Jinyan Liu",
            "Zikang Chen",
            "Qinchuan Wang",
            "Tan Xie",
            "Heming Zheng",
            "Xudong Lv"
        ],
        "submitted": "2025-12-22 03:33:43",
        "source": "arxiv",
        "comment": "10 pages,3 figures,conference ICCBB2025"
    },
    {
        "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation",
        "abstract": "Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks",
        "url": "http://arxiv.org/abs/2512.18996v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18996v1",
        "arxiv_id": "2512.18996v1",
        "authors": [
            "Chong Liu",
            "Ming Zhang",
            "Fei Li",
            "Hao Zhou",
            "Xiaoshuang Chen",
            "Ye Yuan"
        ],
        "submitted": "2025-12-22 03:24:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
        "abstract": "In this study, we address the problem of open-vocabulary mobile manipulation, where a robot is required to carry a wide range of objects to receptacles based on free-form natural language instructions. This task is challenging, as it involves understanding visual semantics and the affordance of manipulation actions. To tackle these challenges, we propose Affordance RAG, a zero-shot hierarchical multimodal retrieval framework that constructs Affordance-Aware Embodied Memory from pre-explored images. The model retrieves candidate targets based on regional and visual semantics and reranks them with affordance scores, allowing the robot to identify manipulation options that are likely to be executable in real-world environments. Our method outperformed existing approaches in retrieval performance for mobile manipulation instruction in large-scale indoor environments. Furthermore, in real-world experiments where the robot performed mobile manipulation in indoor environments based on free-form instructions, the proposed method achieved a task success rate of 85%, outperforming existing methods in both retrieval performance and overall task success.",
        "url": "http://arxiv.org/abs/2512.18987v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18987v1",
        "arxiv_id": "2512.18987v1",
        "authors": [
            "Ryosuke Korekata",
            "Quanting Xie",
            "Yonatan Bisk",
            "Komei Sugiura"
        ],
        "submitted": "2025-12-22 02:55:25",
        "source": "arxiv",
        "comment": "Accepted to IEEE RA-L, with presentation at ICRA 2026"
    },
    {
        "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
        "abstract": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
        "url": "http://arxiv.org/abs/2512.18940v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18940v1",
        "arxiv_id": "2512.18940v1",
        "authors": [
            "Wen-Long Jin"
        ],
        "submitted": "2025-12-22 01:19:50",
        "source": "arxiv",
        "comment": "13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3"
    },
    {
        "title": "Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations",
        "abstract": "Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.",
        "url": "http://arxiv.org/abs/2512.18906v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18906v1",
        "arxiv_id": "2512.18906v1",
        "authors": [
            "Shaomu Tan",
            "Ryosuke Mitani",
            "Ritvik Choudhary",
            "Qiyu Wu",
            "Toshiyuki Sekiya",
            "Christof Monz"
        ],
        "submitted": "2025-12-21 22:37:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
        "abstract": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
        "url": "http://arxiv.org/abs/2512.18880v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18880v1",
        "arxiv_id": "2512.18880v1",
        "authors": [
            "Ming Li",
            "Han Chen",
            "Yunze Xiao",
            "Jian Chen",
            "Hong Jiao",
            "Tianyi Zhou"
        ],
        "submitted": "2025-12-21 20:41:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Application of deep learning approaches for medieval historical documents transcription",
        "abstract": "Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.",
        "url": "http://arxiv.org/abs/2512.18865v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18865v1",
        "arxiv_id": "2512.18865v1",
        "authors": [
            "Maksym Voloshchuk",
            "Bohdana Zarembovska",
            "Mykola Kozlenko"
        ],
        "submitted": "2025-12-21 19:43:30",
        "source": "arxiv",
        "comment": "15 pages, 15 figures, 4 tables. Originally published by CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073), available: https://ceur-ws.org/Vol-4133/S_05_Kozlenko.pdf"
    },
    {
        "title": "Merge on workspaces as Hopf algebra Markov chain",
        "abstract": "We study the dynamical properties of a Hopf algebra Markov chain with state space the binary rooted forests with labelled leaves. This Markovian dynamical system describes the core computational process of structure formation and transformation in syntax via the Merge operation, according to Chomsky's Minimalism model of generative linguistics. The dynamics decomposes into an ergodic dynamical system with uniform stationary distribution, given by the action of Internal Merge, while the contributions of External Merge and (a minimal form of) Sideward Merge reduce to a simpler Markov chain with state space the set of partitions and with combinatorial weights. The Sideward Merge part of the dynamics prevents convergence to fully formed connected structures (trees), unless the different forms of Merge are weighted by a cost function, as predicted by linguistic theory. Results on the asymptotic behavior of the Perron-Frobenius eigenvalue and eigenvector in this weighted case, obtained in terms of an associated Perron-Frobenius problem in the tropical semiring, show that the usual cost functions (Minimal Search and Resource Restrictions) proposed in the linguistic literature do not suffice to obtain convergence to the tree structures, while an additional optimization property based on the Shannon entropy achieves the expected result for the dynamics. We also comment on the introduction of continuous parameters related to semantic embedding and other computational models, and also on some filtering of the dynamics by coloring rules that model the linguistic filtering by theta roles and phase structure, and on parametric variation and the process of parameter setting in Externalization.",
        "url": "http://arxiv.org/abs/2512.18861v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18861v1",
        "arxiv_id": "2512.18861v1",
        "authors": [
            "Matilde Marcolli",
            "David Skigin"
        ],
        "submitted": "2025-12-21 19:26:41",
        "source": "arxiv",
        "comment": "80 pages, LaTeX, 1 png figure"
    },
    {
        "title": "Toward Human-Centered AI-Assisted Terminology Work",
        "abstract": "The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's capabilities, rather than replacing them. The framework is organized around three interrelated dimensions: the augmented terminologist, ethical AI, and human-centered design. Together, these dimensions emphasize the compatibility of high automation with strong human control, the central role of terminologists in bias mitigation, and the importance of designing AI tools and workflows around the needs, values, and well-being of the terminologist. The paper concludes by stressing that current choices in AI adoption will shape not only terminological practice, but also the preservation of accuracy, adequacy, and diversity in terminology and specialized knowledge.",
        "url": "http://arxiv.org/abs/2512.18859v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18859v1",
        "arxiv_id": "2512.18859v1",
        "authors": [
            "Antonio San Martin"
        ],
        "submitted": "2025-12-21 19:16:40",
        "source": "arxiv",
        "comment": null
    }
]