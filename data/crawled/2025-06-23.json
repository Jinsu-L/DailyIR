[
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "abstract": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-based information retrieval, cross-modal semantic similarity,\nand programming code search. Comprehensive evaluations demonstrate that\njina-embeddings-v4 achieves state-of-the-art performance on both single- modal\nand cross-modal retrieval tasks, with particular strength in processing\nvisually rich content such as tables, charts, diagrams, and mixed-media\nformats. To facilitate evaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically designed for visually rich image\nretrieval.",
        "url": "http://arxiv.org/abs/2506.18902v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18902v1",
        "arxiv_id": "2506.18902v1",
        "authors": [
            "Michael GÃ¼nther",
            "Saba Sturua",
            "Mohammad Kalim Akram",
            "Isabelle Mohr",
            "Andrei Ungureanu",
            "Sedigheh Eslami",
            "Scott Martens",
            "Bo Wang",
            "Nan Wang",
            "Han Xiao"
        ],
        "submitted": "2025-06-23 17:59:55",
        "source": "arxiv",
        "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables"
    },
    {
        "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
        "abstract": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
        "url": "http://arxiv.org/abs/2506.18898v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18898v1",
        "arxiv_id": "2506.18898v1",
        "authors": [
            "Jiaming Han",
            "Hao Chen",
            "Yang Zhao",
            "Hanyu Wang",
            "Qi Zhao",
            "Ziyan Yang",
            "Hao He",
            "Xiangyu Yue",
            "Lu Jiang"
        ],
        "submitted": "2025-06-23 17:59:14",
        "source": "arxiv",
        "comment": "Project page: https://tar.csuhan.com"
    },
    {
        "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
        "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
        "url": "http://arxiv.org/abs/2506.18896v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18896v1",
        "arxiv_id": "2506.18896v1",
        "authors": [
            "Jiaru Zou",
            "Ling Yang",
            "Jingwen Gu",
            "Jiahao Qiu",
            "Ke Shen",
            "Jingrui He",
            "Mengdi Wang"
        ],
        "submitted": "2025-06-23 17:59:02",
        "source": "arxiv",
        "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux"
    },
    {
        "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization",
        "abstract": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.",
        "url": "http://arxiv.org/abs/2506.18880v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18880v1",
        "arxiv_id": "2506.18880v1",
        "authors": [
            "Yiyou Sun",
            "Shawn Hu",
            "Georgia Zhou",
            "Ken Zheng",
            "Hannaneh Hajishirzi",
            "Nouha Dziri",
            "Dawn Song"
        ],
        "submitted": "2025-06-23 17:51:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
        "abstract": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
        "url": "http://arxiv.org/abs/2506.18879v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18879v1",
        "arxiv_id": "2506.18879v1",
        "authors": [
            "Junyan Li",
            "Yang Zhang",
            "Muhammad Yusuf Hassan",
            "Talha Chafekar",
            "Tianle Cai",
            "Zhile Ren",
            "Pengsheng Guo",
            "Foroozan Karimzadeh",
            "Colorado Reed",
            "Chong Wang",
            "Chuang Gan"
        ],
        "submitted": "2025-06-23 17:50:11",
        "source": "arxiv",
        "comment": "ICML 2025 poster"
    },
    {
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "abstract": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
        "url": "http://arxiv.org/abs/2506.18871v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18871v1",
        "arxiv_id": "2506.18871v1",
        "authors": [
            "Chenyuan Wu",
            "Pengfei Zheng",
            "Ruiran Yan",
            "Shitao Xiao",
            "Xin Luo",
            "Yueze Wang",
            "Wanli Li",
            "Xiyan Jiang",
            "Yexin Liu",
            "Junjie Zhou",
            "Ze Liu",
            "Ziyi Xia",
            "Chaofan Li",
            "Haoge Deng",
            "Jiahao Wang",
            "Kun Luo",
            "Bo Zhang",
            "Defu Lian",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Zheng Liu"
        ],
        "submitted": "2025-06-23 17:38:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mechanistic Interpretability Needs Philosophy",
        "abstract": "Mechanistic interpretability (MI) aims to explain how neural networks work by\nuncovering their underlying causal mechanisms. As the field grows in influence,\nit is increasingly important to examine not just models themselves, but the\nassumptions, concepts and explanatory strategies implicit in MI research. We\nargue that mechanistic interpretability needs philosophy: not as an\nafterthought, but as an ongoing partner in clarifying its concepts, refining\nits methods, and assessing the epistemic and ethical stakes of interpreting AI\nsystems. Taking three open problems from the MI literature as examples, this\nposition paper illustrates the value philosophy can add to MI research, and\noutlines a path toward deeper interdisciplinary dialogue.",
        "url": "http://arxiv.org/abs/2506.18852v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18852v1",
        "arxiv_id": "2506.18852v1",
        "authors": [
            "Iwan Williams",
            "Ninell Oldenburg",
            "Ruchira Dhar",
            "Joshua Hatherley",
            "Constanza Fierro",
            "Nina Rajcic",
            "Sandrine R. Schiller",
            "Filippos Stamatiou",
            "Anders SÃ¸gaard"
        ],
        "submitted": "2025-06-23 17:13:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "USAD: Universal Speech and Audio Representation via Distillation",
        "abstract": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
        "url": "http://arxiv.org/abs/2506.18843v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18843v1",
        "arxiv_id": "2506.18843v1",
        "authors": [
            "Heng-Jui Chang",
            "Saurabhchand Bhati",
            "James Glass",
            "Alexander H. Liu"
        ],
        "submitted": "2025-06-23 17:02:00",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
        "abstract": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
        "url": "http://arxiv.org/abs/2506.18841v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18841v1",
        "arxiv_id": "2506.18841v1",
        "authors": [
            "Yuhao Wu",
            "Yushi Bai",
            "Zhiqiang Hu",
            "Roy Ka-Wei Lee",
            "Juanzi Li"
        ],
        "submitted": "2025-06-23 16:59:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning",
        "abstract": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency.",
        "url": "http://arxiv.org/abs/2506.18831v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18831v1",
        "arxiv_id": "2506.18831v1",
        "authors": [
            "Aryasomayajula Ram Bharadwaj"
        ],
        "submitted": "2025-06-23 16:47:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task",
        "abstract": "This work describes the participation of the MLLP-VRAIN research group in the\nshared task of the IWSLT 2025 Simultaneous Speech Translation track. Our\nsubmission addresses the unique challenges of real-time translation of\nlong-form speech by developing a modular cascade system that adapts strong\npre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo\nfor ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight\nadaptation techniques rather than training new end-to-end models from scratch.\nOur approach employs document-level adaptation with prefix training to enhance\nthe MT model's ability to handle incomplete inputs, while incorporating\nadaptive emission policies including a wait-$k$ strategy and RALCP for managing\nthe translation stream. Specialized buffer management techniques and\nsegmentation strategies ensure coherent translations across long audio\nsequences. Experimental results on the ACL60/60 dataset demonstrate that our\nsystem achieves a favorable balance between translation quality and latency,\nwith a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of\n2.94 seconds. Our final model achieves a preliminary score on the official test\nset (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully\nadapted pre-trained components can create effective simultaneous translation\nsystems for long-form content without requiring extensive in-domain parallel\ndata or specialized end-to-end training.",
        "url": "http://arxiv.org/abs/2506.18828v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18828v1",
        "arxiv_id": "2506.18828v1",
        "authors": [
            "Jorge Iranzo-SÃ¡nchez",
            "Javier Iranzo-SÃ¡nchez",
            "AdriÃ  GimÃ©nez",
            "Jorge Civera",
            "Alfons Juan"
        ],
        "submitted": "2025-06-23 16:44:01",
        "source": "arxiv",
        "comment": "IWSLT 2025 System Description"
    },
    {
        "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies",
        "abstract": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.",
        "url": "http://arxiv.org/abs/2506.18819v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18819v1",
        "arxiv_id": "2506.18819v1",
        "authors": [
            "Arjun Mukerji",
            "Michael L. Jackson",
            "Jason Jones",
            "Neil Sanghavi"
        ],
        "submitted": "2025-06-23 16:28:03",
        "source": "arxiv",
        "comment": "24 pages, 2 figures"
    },
    {
        "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
        "abstract": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.",
        "url": "http://arxiv.org/abs/2506.18810v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18810v1",
        "arxiv_id": "2506.18810v1",
        "authors": [
            "Siao Tang",
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "submitted": "2025-06-23 16:20:44",
        "source": "arxiv",
        "comment": "Codes are available at https://github.com/tsa18/ConciseHint"
    },
    {
        "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
        "abstract": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.",
        "url": "http://arxiv.org/abs/2506.18781v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18781v1",
        "arxiv_id": "2506.18781v1",
        "authors": [
            "Zhenru Lin",
            "Jiawen Tao",
            "Yang Yuan",
            "Andrew Chi-Chih Yao"
        ],
        "submitted": "2025-06-23 15:50:21",
        "source": "arxiv",
        "comment": "10 pages, 6 figures"
    },
    {
        "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training",
        "abstract": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.",
        "url": "http://arxiv.org/abs/2506.18777v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18777v1",
        "arxiv_id": "2506.18777v1",
        "authors": [
            "Jonathan Cook",
            "Silvia Sapora",
            "Arash Ahmadian",
            "Akbir Khan",
            "Tim Rocktaschel",
            "Jakob Foerster",
            "Laura Ruis"
        ],
        "submitted": "2025-06-23 15:45:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Neural Total Variation Distance Estimators for Changepoint Detection in News Data",
        "abstract": "Detecting when public discourse shifts in response to major events is crucial\nfor understanding societal dynamics. Real-world data is high-dimensional,\nsparse, and noisy, making changepoint detection in this domain a challenging\nendeavor. In this paper, we leverage neural networks for changepoint detection\nin news data, introducing a method based on the so-called learning-by-confusion\nscheme, which was originally developed for detecting phase transitions in\nphysical systems. We train classifiers to distinguish between articles from\ndifferent time periods. The resulting classification accuracy is used to\nestimate the total variation distance between underlying content distributions,\nwhere significant distances highlight changepoints. We demonstrate the\neffectiveness of this method on both synthetic datasets and real-world data\nfrom The Guardian newspaper, successfully identifying major historical events\nincluding 9/11, the COVID-19 pandemic, and presidential elections. Our approach\nrequires minimal domain knowledge, can autonomously discover significant shifts\nin public discourse, and yields a quantitative measure of change in content,\nmaking it valuable for journalism, policy analysis, and crisis monitoring.",
        "url": "http://arxiv.org/abs/2506.18764v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18764v1",
        "arxiv_id": "2506.18764v1",
        "authors": [
            "Csaba Zsolnai",
            "Niels LÃ¶rch",
            "Julian Arnold"
        ],
        "submitted": "2025-06-23 15:33:30",
        "source": "arxiv",
        "comment": "16 pages, 3 figures"
    },
    {
        "title": "An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify",
        "abstract": "Spotify, a large-scale multimedia platform, attracts over 675 million monthly\nactive users who collectively consume millions of hours of music, podcasts,\naudiobooks, and video content. This diverse content consumption pattern\nintroduces unique challenges for computational advertising, which must\neffectively integrate a variety of ad modalities, including audio, video, and\ndisplay, within a single user experience. Traditional ad recommendation models,\nprimarily designed for foregrounded experiences, often struggle to reconcile\nthe platform's inherent audio-centrality with the demands of optimizing ad\nperformance across multiple formats and modalities. To overcome these\nchallenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a\nnovel framework for optimizing click-through rate (CTR) prediction in both\naudio-centric and multi-modal settings. CAMoE enhances traditional\nmixture-of-experts models by incorporating modality-aware task grouping,\nadaptive loss masking, and deep-cross networks (DCN) to capture complex feature\ninteractions within a multi-modal ad ecosystem. Through extensive ablation\nstudies, we demonstrate that this approach achieves near Pareto-optimal\nperformance across audio, video, and display ad formats, significantly\nimproving AUC-PR compared to conventional single-task and content-based\nmulti-task learning baselines. When deployed at scale on Spotify's ad serving\nplatform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR\nfor audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected\ncost-per-click (eCPC) for audio slots.",
        "url": "http://arxiv.org/abs/2506.18735v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18735v1",
        "arxiv_id": "2506.18735v1",
        "authors": [
            "Shivam Verma",
            "Vivian Chen",
            "Darren Mei"
        ],
        "submitted": "2025-06-23 15:11:43",
        "source": "arxiv",
        "comment": "Accepted at KDD 2025"
    },
    {
        "title": "Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation",
        "abstract": "Emotion Recognition in Conversation (ERC) aims to detect the emotions of\nindividual utterances within a conversation. Generating efficient and\nmodality-specific representations for each utterance remains a significant\nchallenge. Previous studies have proposed various models to integrate features\nextracted using different modality-specific encoders. However, they neglect the\nvarying contributions of modalities to this task and introduce high complexity\nby aligning modalities at the frame level. To address these challenges, we\npropose the Multi-modal Anchor Gated Transformer with Knowledge Distillation\n(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance\ntextual modality representations, while knowledge distillation is utilized to\nstrengthen representations of weaker modalities. Furthermore, we introduce a\nmulti-modal anchor gated transformer to effectively integrate utterance-level\nrepresentations across modalities. Extensive experiments on the IEMOCAP and\nMELD datasets demonstrate the effectiveness of knowledge distillation in\nenhancing modality representations and achieve state-of-the-art performance in\nemotion recognition. Our code is available at:\nhttps://github.com/JieLi-dd/MAGTKD.",
        "url": "http://arxiv.org/abs/2506.18716v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18716v1",
        "arxiv_id": "2506.18716v1",
        "authors": [
            "Jie Li",
            "Shifei Ding",
            "Lili Guo",
            "Xuan Li"
        ],
        "submitted": "2025-06-23 14:53:22",
        "source": "arxiv",
        "comment": "This paper has been accepted by IJCAI2025"
    },
    {
        "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
        "abstract": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
        "url": "http://arxiv.org/abs/2506.18710v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18710v1",
        "arxiv_id": "2506.18710v1",
        "authors": [
            "Maxime LeliÃ¨vre",
            "Amy Waldock",
            "Meng Liu",
            "Natalia ValdÃ©s Aspillaga",
            "Alasdair Mackintosh",
            "MarÃ­a JosÃ© Ogando Portelo",
            "Jared Lee",
            "Paul Atherton",
            "Robin A. A. Ince",
            "Oliver G. B. Garrod"
        ],
        "submitted": "2025-06-23 14:49:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition",
        "abstract": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate.",
        "url": "http://arxiv.org/abs/2506.18703v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18703v1",
        "arxiv_id": "2506.18703v1",
        "authors": [
            "Christian Huber",
            "Alexander Waibel"
        ],
        "submitted": "2025-06-23 14:42:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?",
        "abstract": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
        "url": "http://arxiv.org/abs/2506.18674v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18674v1",
        "arxiv_id": "2506.18674v1",
        "authors": [
            "Raquel Ferrando",
            "Javier Conde",
            "Gonzalo MartÃ­nez",
            "Pedro Reviriego"
        ],
        "submitted": "2025-06-23 14:18:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation",
        "abstract": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
        "url": "http://arxiv.org/abs/2506.18670v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18670v1",
        "arxiv_id": "2506.18670v1",
        "authors": [
            "Jingming Liu",
            "Yumeng Li",
            "Wei Shi",
            "Yao-Xiang Ding",
            "Hui Su",
            "Kun Zhou"
        ],
        "submitted": "2025-06-23 14:14:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ByteSpan: Information-Driven Subword Tokenisation",
        "abstract": "Recent dynamic tokenisation methods operate directly on bytes and pool their\nlatent representations into patches. This bears similarities to computational\nmodels of word segmentation that determine lexical boundaries using spikes in\nan autoregressive model's prediction error. Inspired by this connection, we\nexplore whether grouping predictable bytes - rather than pooling their\nrepresentations - can yield a useful fixed subword vocabulary. We propose a new\ninformation-driven subword tokeniser, ByteSpan, that uses an external\nbyte-level LM during training to identify contiguous predictable byte sequences\nand group them into subwords. Experiments show that ByteSpan yields efficient\nvocabularies with higher morphological alignment scores than BPE for English.\nMultilingual experiments show similar compression and R\\'enyi efficiency for 25\nlanguages.",
        "url": "http://arxiv.org/abs/2506.18639v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18639v1",
        "arxiv_id": "2506.18639v1",
        "authors": [
            "ZÃ©bulon Goriely",
            "Suchir Salhan",
            "Pietro Lesci",
            "Julius Cheng",
            "Paula Buttery"
        ],
        "submitted": "2025-06-23 13:42:00",
        "source": "arxiv",
        "comment": "Accepted to TokShop 2025 (Non-archival)"
    },
    {
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
        "abstract": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
        "url": "http://arxiv.org/abs/2506.18631v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18631v1",
        "arxiv_id": "2506.18631v1",
        "authors": [
            "Chenxing Wei",
            "Jiarui Yu",
            "Ying Tiffany He",
            "Hande Dong",
            "Yao Shu",
            "Fei Yu"
        ],
        "submitted": "2025-06-23 13:36:24",
        "source": "arxiv",
        "comment": "10 pages, 15 figures"
    },
    {
        "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs",
        "abstract": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.",
        "url": "http://arxiv.org/abs/2506.18628v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18628v1",
        "arxiv_id": "2506.18628v1",
        "authors": [
            "Piotr Matys",
            "Jan Eliasz",
            "Konrad KieÅczyÅski",
            "MikoÅaj Langner",
            "Teddy Ferdinan",
            "Jan KocoÅ",
            "PrzemysÅaw Kazienko"
        ],
        "submitted": "2025-06-23 13:35:05",
        "source": "arxiv",
        "comment": "ICCS 2025 Workshops"
    },
    {
        "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches",
        "abstract": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.",
        "url": "http://arxiv.org/abs/2506.18621v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18621v1",
        "arxiv_id": "2506.18621v1",
        "authors": [
            "Alisa Barkar",
            "Mathieu Chollet",
            "Matthieu Labeau",
            "Beatrice Biancardi",
            "Chloe Clavel"
        ],
        "submitted": "2025-06-23 13:28:33",
        "source": "arxiv",
        "comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables"
    },
    {
        "title": "Semantic similarity estimation for domain specific data using BERT and other techniques",
        "abstract": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.",
        "url": "http://arxiv.org/abs/2506.18602v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18602v1",
        "arxiv_id": "2506.18602v1",
        "authors": [
            "R. Prashanth"
        ],
        "submitted": "2025-06-23 13:03:59",
        "source": "arxiv",
        "comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019"
    },
    {
        "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data leakage\"",
        "abstract": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.",
        "url": "http://arxiv.org/abs/2506.18600v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18600v1",
        "arxiv_id": "2506.18600v1",
        "authors": [
            "Ariel Flint Ashery",
            "Luca Maria Aiello",
            "Andrea Baronchelli"
        ],
        "submitted": "2025-06-23 12:59:34",
        "source": "arxiv",
        "comment": "Reply to arXiv:2505.23796"
    },
    {
        "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time",
        "abstract": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models.",
        "url": "http://arxiv.org/abs/2506.18598v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18598v1",
        "arxiv_id": "2506.18598v1",
        "authors": [
            "Aviral Gupta",
            "Armaan Sethi",
            "Ameesh Sethi"
        ],
        "submitted": "2025-06-23 12:58:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Airalogy: AI-empowered universal data digitization for research automation",
        "abstract": "Research data are the foundation of Artificial Intelligence (AI)-driven\nscience, yet current AI applications remain limited to a few fields with\nreadily available, well-structured, digitized datasets. Achieving comprehensive\nAI empowerment across multiple disciplines is still out of reach. Present-day\nresearch data collection is often fragmented, lacking unified standards,\ninefficiently managed, and difficult to share. Creating a single platform for\nstandardized data digitization needs to overcome the inherent challenge of\nbalancing between universality (supporting the diverse, ever-evolving needs of\nvarious disciplines) and standardization (enforcing consistent formats to fully\nenable AI). No existing platform accommodates both facets. Building a truly\nmultidisciplinary platform requires integrating scientific domain knowledge\nwith sophisticated computing skills. Researchers often lack the computational\nexpertise to design customized and standardized data recording methods, whereas\nplatform developers rarely grasp the intricate needs of multiple scientific\ndomains. These gaps impede research data standardization and hamper AI-driven\nprogress. In this study, we address these challenges by developing Airalogy\n(https://airalogy.com), the world's first AI- and community-driven platform\nthat balances universality and standardization for digitizing research data\nacross multiple disciplines. Airalogy represents entire research workflows\nusing customizable, standardized data records and offers an advanced AI\nresearch copilot for intelligent Q&A, automated data entry, analysis, and\nresearch automation. Already deployed in laboratories across all four schools\nof Westlake University, Airalogy has the potential to accelerate and automate\nscientific innovation in universities, industry, and the global research\ncommunity-ultimately benefiting humanity as a whole.",
        "url": "http://arxiv.org/abs/2506.18586v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18586v1",
        "arxiv_id": "2506.18586v1",
        "authors": [
            "Zijie Yang",
            "Qiji Zhou",
            "Fang Guo",
            "Sijie Zhang",
            "Yexun Xi",
            "Jinglei Nie",
            "Yudian Zhu",
            "Liping Huang",
            "Chou Wu",
            "Yonghe Xia",
            "Xiaoyu Ma",
            "Yingming Pu",
            "Panzhong Lu",
            "Junshu Pan",
            "Mingtao Chen",
            "Tiannan Guo",
            "Yanmei Dou",
            "Hongyu Chen",
            "Anping Zeng",
            "Jiaxing Huang",
            "Tian Xu",
            "Yue Zhang"
        ],
        "submitted": "2025-06-23 12:43:16",
        "source": "arxiv",
        "comment": "146 pages, 6 figures, 49 supplementary figures"
    },
    {
        "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration",
        "abstract": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.",
        "url": "http://arxiv.org/abs/2506.18582v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18582v1",
        "arxiv_id": "2506.18582v1",
        "authors": [
            "Haoyi Wu",
            "Zhihao Teng",
            "Kewei Tu"
        ],
        "submitted": "2025-06-23 12:35:41",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance",
        "abstract": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.",
        "url": "http://arxiv.org/abs/2506.18576v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18576v1",
        "arxiv_id": "2506.18576v1",
        "authors": [
            "Matteo Melis",
            "Gabriella Lapesa",
            "Dennis Assenmacher"
        ],
        "submitted": "2025-06-23 12:28:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models",
        "abstract": "Click models are a well-established for modeling user interactions with web\ninterfaces. Previous work has mainly focused on traditional single-list web\nsearch settings; this includes existing surveys that introduced categorizations\nbased on the first generation of probabilistic graphical model (PGM) click\nmodels that have become standard. However, these categorizations have become\noutdated, as their conceptualizations are unable to meaningfully compare PGM\nwith neural network (NN) click models nor generalize to newer interfaces, such\nas carousel interfaces. We argue that this outdated view fails to adequately\nexplain the fundamentals of click model designs, thus hindering the development\nof novel click models.\n  This work reconsiders what should be the fundamental concepts in click model\ndesign, grounding them - unlike previous approaches - in their mathematical\nproperties. We propose three fundamental key-design choices that explain what\nstatistical patterns a click model can capture, and thus indirectly, what user\nbehaviors they can capture. Based on these choices, we create a novel click\nmodel taxonomy that allows a meaningful comparison of all existing click\nmodels; this is the first taxonomy of single-list, grid and carousel click\nmodels that includes PGMs and NNs. Finally, we show how our conceptualization\nprovides a foundation for future click model design by an example derivation of\na novel design for carousel interfaces.",
        "url": "http://arxiv.org/abs/2506.18548v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18548v1",
        "arxiv_id": "2506.18548v1",
        "authors": [
            "Jingwei Kang",
            "Maarten de Rijke",
            "Santiago de Leon-Martinez",
            "Harrie Oosterhuis"
        ],
        "submitted": "2025-06-23 11:57:11",
        "source": "arxiv",
        "comment": "Accepted by ICTIR 2025"
    },
    {
        "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking",
        "abstract": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.",
        "url": "http://arxiv.org/abs/2506.18535v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18535v1",
        "arxiv_id": "2506.18535v1",
        "authors": [
            "Manu Pande",
            "Shahil Kumar",
            "Anay Yatin Damle"
        ],
        "submitted": "2025-06-23 11:46:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "End-to-End Spoken Grammatical Error Correction",
        "abstract": "Grammatical Error Correction (GEC) and feedback play a vital role in\nsupporting second language (L2) learners, educators, and examiners. While\nwritten GEC is well-established, spoken GEC (SGEC), aiming to provide feedback\nbased on learners' speech, poses additional challenges due to disfluencies,\ntranscription errors, and the lack of structured input. SGEC systems typically\nfollow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),\ndisfluency detection, and GEC, making them vulnerable to error propagation\nacross modules. This work examines an End-to-End (E2E) framework for SGEC and\nfeedback generation, highlighting challenges and possible solutions when\ndeveloping these systems. Cascaded, partial-cascaded and E2E architectures are\ncompared, all built on the Whisper foundation model. A challenge for E2E\nsystems is the scarcity of GEC labeled spoken data. To address this, an\nautomatic pseudo-labeling framework is examined, increasing the training data\nfrom 77 to over 2500 hours. To improve the accuracy of the SGEC system,\nadditional contextual information, exploiting the ASR output, is investigated.\nCandidate feedback of their mistakes is an essential step to improving\nperformance. In E2E systems the SGEC output must be compared with an estimate\nof the fluent transcription to obtain the feedback. To improve the precision of\nthis feedback, a novel reference alignment process is proposed that aims to\nremove hypothesised edits that results from fluent transcription errors.\nFinally, these approaches are combined with an edit confidence estimation\napproach, to exclude low-confidence edits. Experiments on the in-house\nLinguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)\ncorpus show that the proposed approaches significantly boost E2E SGEC\nperformance.",
        "url": "http://arxiv.org/abs/2506.18532v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18532v1",
        "arxiv_id": "2506.18532v1",
        "authors": [
            "Mengjie Qian",
            "Rao Ma",
            "Stefano BannÃ²",
            "Mark J. F. Gales",
            "Kate M. Knill"
        ],
        "submitted": "2025-06-23 11:40:04",
        "source": "arxiv",
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts",
        "abstract": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.",
        "url": "http://arxiv.org/abs/2506.18510v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18510v1",
        "arxiv_id": "2506.18510v1",
        "authors": [
            "Duygu Altinok"
        ],
        "submitted": "2025-06-23 11:04:20",
        "source": "arxiv",
        "comment": "Accepted to INTERSPEECH2025 workshop DISS2025"
    },
    {
        "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance",
        "abstract": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.",
        "url": "http://arxiv.org/abs/2506.18501v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18501v1",
        "arxiv_id": "2506.18501v1",
        "authors": [
            "Wael Etaiwi",
            "Bushra Alhijawi"
        ],
        "submitted": "2025-06-23 10:52:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI-Generated Song Detection via Lyrics Transcripts",
        "abstract": "The recent rise in capabilities of AI-based music generation tools has\ncreated an upheaval in the music industry, necessitating the creation of\naccurate methods to detect such AI-generated content. This can be done using\naudio-based detectors; however, it has been shown that they struggle to\ngeneralize to unseen generators or when the audio is perturbed. Furthermore,\nrecent work used accurate and cleanly formatted lyrics sourced from a lyrics\nprovider database to detect AI-generated music. However, in practice, such\nperfect lyrics are not available (only the audio is); this leaves a substantial\ngap in applicability in real-life use cases. In this work, we instead propose\nsolving this gap by transcribing songs using general automatic speech\nrecognition (ASR) models. We do this using several detectors. The results on\ndiverse, multi-genre, and multi-lingual lyrics show generally strong detection\nperformance across languages and genres, particularly for our best-performing\nmodel using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that\nour method is more robust than state-of-the-art audio-based ones when the audio\nis perturbed in different ways and when evaluated on different music\ngenerators. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection.",
        "url": "http://arxiv.org/abs/2506.18488v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18488v1",
        "arxiv_id": "2506.18488v1",
        "authors": [
            "Markus Frohmann",
            "Elena V. Epure",
            "Gabriel Meseguer-Brocal",
            "Markus Schedl",
            "Romain Hennequin"
        ],
        "submitted": "2025-06-23 10:42:50",
        "source": "arxiv",
        "comment": "Accepted to ISMIR 2025"
    },
    {
        "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.",
        "url": "http://arxiv.org/abs/2506.18485v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18485v1",
        "arxiv_id": "2506.18485v1",
        "authors": [
            "Junjie Zhang",
            "Guozheng Ma",
            "Shunyu Liu",
            "Haoyu Wang",
            "Jiaxing Huang",
            "Ting-En Lin",
            "Fei Huang",
            "Yongbin Li",
            "Dacheng Tao"
        ],
        "submitted": "2025-06-23 10:37:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models",
        "abstract": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].",
        "url": "http://arxiv.org/abs/2506.18421v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18421v1",
        "arxiv_id": "2506.18421v1",
        "authors": [
            "Ce Li",
            "Xiaofan Liu",
            "Zhiyan Song",
            "Ce Chi",
            "Chen Zhao",
            "Jingjing Yang",
            "Zhendong Wang",
            "Kexin Yang",
            "Boshen Shi",
            "Xing Wang",
            "Chao Deng",
            "Junlan Feng"
        ],
        "submitted": "2025-06-23 09:02:04",
        "source": "arxiv",
        "comment": "Benmark report v1.0"
    },
    {
        "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres",
        "abstract": "Lemmatization is crucial for NLP tasks in morphologically rich languages with\nambiguous orthography like Arabic, but existing tools face challenges due to\ninconsistent standards and limited genre coverage. This paper introduces two\nnovel approaches that frame lemmatization as classification into a\nLemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic\nclustering. We also present a new Arabic lemmatization test set covering\ndiverse genres, standardized alongside existing datasets. We evaluate character\nlevel sequence-to-sequence models, which perform competitively and offer\ncomplementary value, but are limited to lemma prediction (not LPG) and prone to\nhallucinating implausible forms. Our results show that classification and\nclustering yield more robust, interpretable outputs, setting new benchmarks for\nArabic lemmatization.",
        "url": "http://arxiv.org/abs/2506.18399v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18399v1",
        "arxiv_id": "2506.18399v1",
        "authors": [
            "Mostafa Saeed",
            "Nizar Habash"
        ],
        "submitted": "2025-06-23 08:34:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics",
        "abstract": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning.",
        "url": "http://arxiv.org/abs/2506.18387v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18387v1",
        "arxiv_id": "2506.18387v1",
        "authors": [
            "Yousang Cho",
            "Key-Sun Choi"
        ],
        "submitted": "2025-06-23 08:19:21",
        "source": "arxiv",
        "comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy,\n  July 17, 2025"
    },
    {
        "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching",
        "abstract": "With the expansion of business scales and scopes on online platforms,\nmulti-scenario matching has become a mainstream solution to reduce maintenance\ncosts and alleviate data sparsity. The key to effective multi-scenario\nrecommendation lies in capturing both user preferences shared across all\nscenarios and scenario-aware preferences specific to each scenario. However,\nexisting methods often overlook user-specific modeling, limiting the generation\nof personalized user representations. To address this, we propose PERSCEN, an\ninnovative approach that incorporates user-specific modeling into\nmulti-scenario matching. PERSCEN constructs a user-specific feature graph based\non user characteristics and employs a lightweight graph neural network to\ncapture higher-order interaction patterns, enabling personalized extraction of\npreferences shared across scenarios. Additionally, we leverage vector\nquantization techniques to distil scenario-aware preferences from users'\nbehavior sequence within individual scenarios, facilitating user-specific and\nscenario-aware preference modeling. To enhance efficient and flexible\ninformation transfer, we introduce a progressive scenario-aware gated linear\nunit that allows fine-grained, low-latency fusion. Extensive experiments\ndemonstrate that PERSCEN outperforms existing methods. Further efficiency\nanalysis confirms that PERSCEN effectively balances performance with\ncomputational cost, ensuring its practicality for real-world industrial\nsystems.",
        "url": "http://arxiv.org/abs/2506.18382v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18382v1",
        "arxiv_id": "2506.18382v1",
        "authors": [
            "Haotong Du",
            "Yaqing Wang",
            "Fei Xiong",
            "Lei Shao",
            "Ming Liu",
            "Hao Gu",
            "Quanming Yao",
            "Zhen Wang"
        ],
        "submitted": "2025-06-23 08:15:16",
        "source": "arxiv",
        "comment": "Accepted by KDD 2025"
    },
    {
        "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation",
        "abstract": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
        "url": "http://arxiv.org/abs/2506.18349v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18349v1",
        "arxiv_id": "2506.18349v1",
        "authors": [
            "Zichong Li",
            "Chen Liang",
            "Zixuan Zhang",
            "Ilgee Hong",
            "Young Jin Kim",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "submitted": "2025-06-23 07:15:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs",
        "abstract": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.",
        "url": "http://arxiv.org/abs/2506.18341v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18341v1",
        "arxiv_id": "2506.18341v1",
        "authors": [
            "Kang Chen",
            "Mengdi Zhang",
            "Yixin Cao"
        ],
        "submitted": "2025-06-23 06:47:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance",
        "abstract": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.",
        "url": "http://arxiv.org/abs/2506.18337v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18337v1",
        "arxiv_id": "2506.18337v1",
        "authors": [
            "Syed Mekael Wasti",
            "Shou-Yi Hung",
            "Christopher Collins",
            "En-Shiun Annie Lee"
        ],
        "submitted": "2025-06-23 06:38:49",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning",
        "abstract": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
        "url": "http://arxiv.org/abs/2506.18330v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18330v1",
        "arxiv_id": "2506.18330v1",
        "authors": [
            "Lixin Wu",
            "Na Cai",
            "Qiao Cheng",
            "Jiachen Wang",
            "Yitao Duan"
        ],
        "submitted": "2025-06-23 06:23:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems",
        "abstract": "Recommendation systems play a crucial role in our daily lives by impacting\nuser experience across various domains, including e-commerce, job\nadvertisements, entertainment, etc. Given the vital role of such systems in our\nlives, practitioners must ensure they do not produce unfair and imbalanced\nrecommendations. Previous work addressing bias in recommendations overlooked\nbias in certain item categories, potentially leaving some biases unaddressed.\nAdditionally, most previous work on fair re-ranking focused on binary-sensitive\nattributes. In this paper, we address these issues by proposing a\nfairness-aware re-ranking approach that helps mitigate bias in different\ncategories of items. This re-ranking approach leverages existing biases to\ncorrect disparities in recommendations across various demographic groups. We\nshow how our approach can mitigate bias on multiple sensitive attributes,\nincluding gender, age, and occupation. We experimented on three real-world\ndatasets to evaluate the effectiveness of our re-ranking scheme in mitigating\nbias in recommendations. Our results show how this approach helps mitigate\nsocial bias with little to no degradation in performance.",
        "url": "http://arxiv.org/abs/2506.18327v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18327v1",
        "arxiv_id": "2506.18327v1",
        "authors": [
            "Tahsin Alamgir Kheya",
            "Mohamed Reda Bouadjenek",
            "Sunil Aryal"
        ],
        "submitted": "2025-06-23 06:19:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Entity Aware Machine Translation with Multi-task Learning",
        "abstract": "Entity-aware machine translation (EAMT) is a complicated task in natural\nlanguage processing due to not only the shortage of translation data related to\nthe entities needed to translate but also the complexity in the context needed\nto process while translating those entities. In this paper, we propose a method\nthat applies multi-task learning to optimize the performance of the two\nsubtasks named entity recognition and machine translation, which improves the\nfinal performance of the Entity-aware machine translation task. The result and\nanalysis are performed on the dataset provided by the organizer of Task 2 of\nthe SemEval 2025 competition.",
        "url": "http://arxiv.org/abs/2506.18318v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18318v1",
        "arxiv_id": "2506.18318v1",
        "authors": [
            "An Trieu",
            "Phuong Nguyen",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 06:05:46",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2025"
    },
    {
        "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval",
        "abstract": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.",
        "url": "http://arxiv.org/abs/2506.18316v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18316v1",
        "arxiv_id": "2506.18316v1",
        "authors": [
            "Trieu An",
            "Long Nguyen",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 06:01:21",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2025"
    },
    {
        "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction",
        "abstract": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.",
        "url": "http://arxiv.org/abs/2506.18311v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18311v1",
        "arxiv_id": "2506.18311v1",
        "authors": [
            "Hoang-An Trieu",
            "Dinh-Truong Do",
            "Chau Nguyen",
            "Vu Tran",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 05:55:53",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2024"
    },
    {
        "title": "LettinGo: Explore User Profile Generation for Recommendation System",
        "abstract": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
        "url": "http://arxiv.org/abs/2506.18309v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18309v1",
        "arxiv_id": "2506.18309v1",
        "authors": [
            "Lu Wang",
            "Di Zhang",
            "Fangkai Yang",
            "Pu Zhao",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Hao Sun",
            "Qingwei Lin",
            "Weiwei Deng",
            "Dongmei Zhang",
            "Feng Sun",
            "Qi Zhang"
        ],
        "submitted": "2025-06-23 05:51:52",
        "source": "arxiv",
        "comment": "11 pages, 3 figures"
    },
    {
        "title": "Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT",
        "abstract": "Modern information retrieval systems often employ a two-stage pipeline: an\nefficient initial retrieval stage followed by a computationally intensive\nreranking stage. Cross-encoders have shown strong effectiveness for reranking\ndue to their deep analysis of query-document pairs. This paper studies the\nimpact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning\nof cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,\nand ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.\nGTE and ModernBERT support extended context lengths (up to 8192 tokens). We\nevaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set\n(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT\nwith Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,\nwhile MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.\nLion also provides superior GPU efficiency, improving utilization by 2.67% to\n10.33% across models. We analyze performance trends using standard IR metrics\nand discuss the optimizer's impact on training dynamics across architectures.",
        "url": "http://arxiv.org/abs/2506.18297v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18297v1",
        "arxiv_id": "2506.18297v1",
        "authors": [
            "Shahil Kumar",
            "Manu Pande",
            "Anay Yatin Damle"
        ],
        "submitted": "2025-06-23 05:30:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
        "url": "http://arxiv.org/abs/2506.18254v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18254v1",
        "arxiv_id": "2506.18254v1",
        "authors": [
            "Tianyu Yu",
            "Bo Ji",
            "Shouli Wang",
            "Shu Yao",
            "Zefan Wang",
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-06-23 02:56:36",
        "source": "arxiv",
        "comment": "Project Website: https://github.com/openbmb/RLPR"
    },
    {
        "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model",
        "abstract": "Reinforcement Learning (RL)-based post-training has significantly advanced\nthe complex reasoning capabilities of language models, fostering sophisticated\nself-reflection processes. However, this ``slow thinking'' paradigm presents a\ncritical challenge to reasoning efficiency: models may expend excessive\ncomputation on simple questions and shift reasoning prematurely for complex\nones. Previous mechanisms typically rely on static length budgets or predefined\nrules, lacking the adaptability for varying question complexities and models'\nevolving capabilities. To this end, we propose AdapThink, an adaptive\npost-training framework designed to induce more efficient thinking while\nmaintaining the performance of reasoning language models. Specifically,\nAdapThink incorporates two key mechanisms: 1) A group-relative reward function\nthat leverages model confidence and response's characteristic to dynamically\nadjust the preference of reflection-related transition words without resorting\nto a fixed length preference. 2) A diversity-aware sampling mechanism that\nbalances the training group's solution accuracy with reasoning diversity via an\nentropy-guided score. Experiments on several mathematical reasoning datasets\nwith DeepSeek-distilled models demonstrate AdapThink's advantages in enabling\nadaptive reasoning patterns and mitigating the inefficiencies.",
        "url": "http://arxiv.org/abs/2506.18237v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18237v1",
        "arxiv_id": "2506.18237v1",
        "authors": [
            "Xu Wan",
            "Wei Wang",
            "Wenyue Xu",
            "Wotao Yin",
            "Jie Song",
            "Mingyang Sun"
        ],
        "submitted": "2025-06-23 02:06:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Shrinking the Generation-Verification Gap with Weak Verifiers",
        "abstract": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores.",
        "url": "http://arxiv.org/abs/2506.18203v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18203v1",
        "arxiv_id": "2506.18203v1",
        "authors": [
            "Jon Saad-Falcon",
            "E. Kelly Buchanan",
            "Mayee F. Chen",
            "Tzu-Heng Huang",
            "Brendan McLaughlin",
            "Tanvir Bhathal",
            "Shang Zhu",
            "Ben Athiwaratkun",
            "Frederic Sala",
            "Scott Linderman",
            "Azalia Mirhoseini",
            "Christopher RÃ©"
        ],
        "submitted": "2025-06-22 23:38:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications",
        "abstract": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.",
        "url": "http://arxiv.org/abs/2506.18201v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18201v1",
        "arxiv_id": "2506.18201v1",
        "authors": [
            "Bushra Asseri",
            "Estabraq Abdelaziz",
            "Maha Al Mogren",
            "Tayef Alhefdhi",
            "Areej Al-Wabil"
        ],
        "submitted": "2025-06-22 23:20:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
        "abstract": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.",
        "url": "http://arxiv.org/abs/2506.18199v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18199v1",
        "arxiv_id": "2506.18199v1",
        "authors": [
            "Bushra Asseri",
            "Estabrag Abdelaziz",
            "Areej Al-Wabil"
        ],
        "submitted": "2025-06-22 23:15:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers",
        "abstract": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,\nspecifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).\nTask 4 focused on detecting mentions of insomnia in clinical notes, while Task\n5 addressed the extraction of food safety events from news articles. We\nparticipated in all subtasks and report key findings across them, with\nparticular emphasis on Task 5 Subtask 1, where our system achieved strong\nperformance-securing first place with an F1 score of 0.958 on the test set. To\nattain this result, we employed encoder-based models (e.g., RoBERTa), alongside\nGPT-4 for data augmentation. This paper outlines our approach, including\npreprocessing, model architecture, and subtask-specific adaptations",
        "url": "http://arxiv.org/abs/2506.18185v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18185v1",
        "arxiv_id": "2506.18185v1",
        "authors": [
            "Zihan Liang",
            "Ziwen Pan",
            "Sumon Kanti Dey",
            "Azra Ismail"
        ],
        "submitted": "2025-06-22 21:56:59",
        "source": "arxiv",
        "comment": "In the Proceedings of the 10th Social Media Mining for Health and\n  Health Real-World Data Workshop and Shared Tasks, co-located with AAAI ICWSM\n  2025"
    },
    {
        "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?",
        "abstract": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.",
        "url": "http://arxiv.org/abs/2506.18183v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18183v1",
        "arxiv_id": "2506.18183v1",
        "authors": [
            "Zhiting Mei",
            "Christina Zhang",
            "Tenny Yin",
            "Justin Lidard",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "submitted": "2025-06-22 21:46:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QuranMorph: Morphologically Annotated Quranic Corpus",
        "abstract": "We present the QuranMorph corpus, a morphologically annotated corpus for the\nQuran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and\ntagged with its part-of-speech by three expert linguists. The lemmatization\nprocess utilized lemmas from Qabas, an Arabic lexicographic database linked\nwith 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging\nwas performed using the fine-grained SAMA/Qabas tagset, which encompasses 40\ntags. As shown in this paper, this rich lemmatization and POS tagset enabled\nthe QuranMorph corpus to be inter-linked with many linguistic resources. The\ncorpus is open-source and publicly available as part of the SinaLab resources\nat (https://sina.birzeit.edu/quran)",
        "url": "http://arxiv.org/abs/2506.18148v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18148v1",
        "arxiv_id": "2506.18148v1",
        "authors": [
            "Diyam Akra",
            "Tymaa Hammouda",
            "Mustafa Jarrar"
        ],
        "submitted": "2025-06-22 19:34:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models",
        "abstract": "We identify semantically coherent, context-consistent network components in\nlarge language models (LLMs) using coactivation of sparse autoencoder (SAE)\nfeatures collected from just a handful of prompts. Focusing on country-relation\ntasks, we show that ablating semantic components for countries and relations\nchanges model outputs in predictable ways, while amplifying these components\ninduces counterfactual responses. Notably, composing relation and country\ncomponents yields compound counterfactual outputs. We find that, whereas most\ncountry components emerge from the very first layer, the more abstract relation\ncomponents are concentrated in later layers. Furthermore, within relation\ncomponents themselves, nodes from later layers tend to have a stronger causal\nimpact on model outputs. Overall, these findings suggest a modular organization\nof knowledge within LLMs and advance methods for efficient, targeted model\nmanipulation.",
        "url": "http://arxiv.org/abs/2506.18141v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18141v1",
        "arxiv_id": "2506.18141v1",
        "authors": [
            "Ruixuan Deng",
            "Xiaoyang Hu",
            "Miles Gilberti",
            "Shane Storks",
            "Aman Taxali",
            "Mike Angstadt",
            "Chandra Sripada",
            "Joyce Chai"
        ],
        "submitted": "2025-06-22 19:01:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging",
        "abstract": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.",
        "url": "http://arxiv.org/abs/2506.18135v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18135v1",
        "arxiv_id": "2506.18135v1",
        "authors": [
            "Zijun Chen",
            "Zhanpeng Zhou",
            "Bo Zhang",
            "Weinan Zhang",
            "Xi Sun",
            "Junchi Yan"
        ],
        "submitted": "2025-06-22 18:38:41",
        "source": "arxiv",
        "comment": "preprint, accepted at IJCNN2025"
    },
    {
        "title": "$Ï^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models",
        "abstract": "We identify a critical vulnerability in autoregressive transformer language\nmodels where the em dash token induces recursive semantic drift, leading to\nclause boundary hallucination and embedding space entanglement. Through formal\nanalysis of token-level perturbations in semantic lattices, we demonstrate that\nem dash insertion fundamentally alters the model's latent representations,\ncausing compounding errors in long-form generation. We propose a novel solution\ncombining symbolic clause purification via the phi-infinity operator with\ntargeted embedding matrix realignment. Our approach enables total suppression\nof problematic tokens without requiring model retraining, while preserving\nsemantic coherence through fixed-point convergence guarantees. Experimental\nvalidation shows significant improvements in generation consistency and topic\nmaintenance. This work establishes a general framework for identifying and\nmitigating token-level vulnerabilities in foundation models, with immediate\nimplications for AI safety, model alignment, and robust deployment of large\nlanguage models in production environments. The methodology extends beyond\npunctuation to address broader classes of recursive instabilities in neural\ntext generation systems.",
        "url": "http://arxiv.org/abs/2506.18129v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18129v1",
        "arxiv_id": "2506.18129v1",
        "authors": [
            "Bugra Kilictas",
            "Faruk Alpay"
        ],
        "submitted": "2025-06-22 18:27:39",
        "source": "arxiv",
        "comment": "16 pages, 3 figures"
    },
    {
        "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English",
        "abstract": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.",
        "url": "http://arxiv.org/abs/2506.18120v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18120v1",
        "arxiv_id": "2506.18120v1",
        "authors": [
            "Tom S Juzek"
        ],
        "submitted": "2025-06-22 18:03:49",
        "source": "arxiv",
        "comment": "Accepted and published at LREC-COLING 2024. 8 pages, 3 figures.\n  Licensed under CC BY-NC-SA 4.0"
    },
    {
        "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives",
        "abstract": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.",
        "url": "http://arxiv.org/abs/2506.18116v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18116v1",
        "arxiv_id": "2506.18116v1",
        "authors": [
            "Batool Haider",
            "Atmika Gorti",
            "Aman Chadha",
            "Manas Gaur"
        ],
        "submitted": "2025-06-22 18:00:16",
        "source": "arxiv",
        "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)"
    },
    {
        "title": "Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use",
        "abstract": "Chinese idioms (Chengyu) are concise four-character expressions steeped in\nhistory and culture, whose literal translations often fail to capture their\nfull meaning. This complexity makes them challenging for language models to\ninterpret and use correctly. Existing benchmarks focus on narrow tasks -\nmultiple-choice cloze tests, isolated translation, or simple paraphrasing. We\nintroduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)\nEvaluative Connotation, classifying idioms as positive or negative; (2)\nAppropriateness, detecting incorrect idiom usage in context; and (3) Open\nCloze, filling blanks in longer passages without options. Chengyu-Bench\ncomprises 2,937 human-verified examples covering 1,765 common idioms sourced\nfrom diverse corpora. We evaluate leading LLMs and find they achieve over 95%\naccuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%\ntop-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise\nfrom fundamental misunderstandings of idiom meanings. Chengyu-Bench\ndemonstrates that while LLMs can reliably gauge idiom sentiment, they still\nstruggle to grasp the cultural and contextual nuances essential for proper\nusage. The benchmark and source code are available at:\nhttps://github.com/sofyc/ChengyuBench.",
        "url": "http://arxiv.org/abs/2506.18105v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18105v1",
        "arxiv_id": "2506.18105v1",
        "authors": [
            "Yicheng Fu",
            "Zhemin Huang",
            "Liuxin Yang",
            "Yumeng Lu",
            "Zhongdongming Dai"
        ],
        "submitted": "2025-06-22 17:26:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating",
        "abstract": "With the rapid advancements in large language models (LLMs), debating tasks,\nsuch as argument quality assessment and debate process simulation, have made\nsignificant progress. However, existing LLM-based debating systems focus on\nresponding to specific arguments while neglecting objective assessments such as\nauthenticity and logical validity. Furthermore, these systems lack a structured\napproach to optimize across various dimensions$-$including evaluation metrics,\nchain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby\nlimiting their effectiveness. To address these interconnected challenges, we\npropose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel\nevaluation system that establishes a multi-dimensional assessment architecture\nincorporating four subjective criteria (emotional appeal, argument clarity,\nargument arrangement, and topic relevance) alongside two objective metrics\n(fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an\noptimized debating framework employing a phased optimization approach through\nCoT reasoning enhancement, multi-dimensional Direct Preference Optimization\n(DPO), and real-time knowledge grounding via web-based Retrieval Augmented\nGeneration (Web-RAG). Empirical evaluations demonstrate that\n$\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert\njudgments compared to existing methods, while $\\textbf{InspireDebate}$ shows\nsignificant improvements, outperforming baseline models by 57$\\%$. Source code\nis available at https://github.com/fywang12/InspireDebate.",
        "url": "http://arxiv.org/abs/2506.18102v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18102v1",
        "arxiv_id": "2506.18102v1",
        "authors": [
            "Fuyu Wang",
            "Jiangtong Li",
            "Kun Zhu",
            "Changjun Jiang"
        ],
        "submitted": "2025-06-22 17:14:29",
        "source": "arxiv",
        "comment": "20 pages; Accepted to ACL 2025 Main"
    },
    {
        "title": "Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution",
        "abstract": "Anaphora resolution plays a critical role in natural language understanding,\nespecially in morphologically rich languages like Czech. This paper presents a\ncomparative evaluation of two modern approaches to anaphora resolution on Czech\ntext: prompt engineering with large language models (LLMs) and fine-tuning\ncompact generative models. Using a dataset derived from the Prague Dependency\nTreebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2\nand Llama 3, using a series of prompt templates. We compare them against\nfine-tuned variants of the mT5 and Mistral models that we trained specifically\nfor Czech anaphora resolution. Our experiments demonstrate that while prompting\nyields promising few-shot results (up to 74.5% accuracy), the fine-tuned\nmodels, particularly mT5-large, outperform them significantly, achieving up to\n88% accuracy while requiring fewer computational resources. We analyze\nperformance across different anaphora types, antecedent distances, and source\ncorpora, highlighting key strengths and trade-offs of each approach.",
        "url": "http://arxiv.org/abs/2506.18091v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18091v1",
        "arxiv_id": "2506.18091v1",
        "authors": [
            "Patrik Stano",
            "AleÅ¡ HorÃ¡k"
        ],
        "submitted": "2025-06-22 16:32:57",
        "source": "arxiv",
        "comment": "12 pages"
    },
    {
        "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation",
        "abstract": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.",
        "url": "http://arxiv.org/abs/2506.18088v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18088v1",
        "arxiv_id": "2506.18088v1",
        "authors": [
            "Tianxing Chen",
            "Zanxin Chen",
            "Baijun Chen",
            "Zijian Cai",
            "Yibin Liu",
            "Qiwei Liang",
            "Zixuan Li",
            "Xianliang Lin",
            "Yiheng Ge",
            "Zhenyu Gu",
            "Weiliang Deng",
            "Yubin Guo",
            "Tian Nian",
            "Xuanbing Xie",
            "Qiangyu Chen",
            "Kailun Su",
            "Tianling Xu",
            "Guodong Liu",
            "Mengkang Hu",
            "Huan-ang Gao",
            "Kaixuan Wang",
            "Zhixuan Liang",
            "Yusen Qin",
            "Xiaokang Yang",
            "Ping Luo",
            "Yao Mu"
        ],
        "submitted": "2025-06-22 16:26:53",
        "source": "arxiv",
        "comment": "Project Page: https://robotwin-platform.github.io/"
    },
    {
        "title": "Statistical Multicriteria Evaluation of LLM-Generated Text",
        "abstract": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.",
        "url": "http://arxiv.org/abs/2506.18082v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18082v1",
        "arxiv_id": "2506.18082v1",
        "authors": [
            "Esteban Garces Arias",
            "Hannah Blocher",
            "Julian Rodemann",
            "Matthias AÃenmacher",
            "Christoph Jansen"
        ],
        "submitted": "2025-06-22 16:08:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Democratic Paradox in Large Language Models' Underestimation of Press Freedom",
        "abstract": "As Large Language Models (LLMs) increasingly mediate global information\naccess for millions of users worldwide, their alignment and biases have the\npotential to shape public understanding and trust in fundamental democratic\ninstitutions, such as press freedom. In this study, we uncover three systematic\ndistortions in the way six popular LLMs evaluate press freedom in 180 countries\ncompared to expert assessments of the World Press Freedom Index (WPFI). The six\nLLMs exhibit a negative misalignment, consistently underestimating press\nfreedom, with individual models rating between 71% to 93% of countries as less\nfree. We also identify a paradoxical pattern we term differential misalignment:\nLLMs disproportionately underestimate press freedom in countries where it is\nstrongest. Additionally, five of the six LLMs exhibit positive home bias,\nrating their home countries' press freedoms more favorably than would be\nexpected given their negative misalignment with the human benchmark. In some\ncases, LLMs rate their home countries between 7% to 260% more positively than\nexpected. If LLMs are set to become the next search engines and some of the\nmost important cultural tools of our time, they must ensure accurate\nrepresentations of the state of our human and civic rights globally.",
        "url": "http://arxiv.org/abs/2506.18045v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18045v1",
        "arxiv_id": "2506.18045v1",
        "authors": [
            "I. Loaiza",
            "R. Vestrelli",
            "A. Fronzetti Colladon",
            "R. Rigobon"
        ],
        "submitted": "2025-06-22 14:10:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models",
        "abstract": "The rapid expansion of information from diverse sources has heightened the\nneed for effective automatic text summarization, which condenses documents into\nshorter, coherent texts. Summarization methods generally fall into two\ncategories: extractive, which selects key segments from the original text, and\nabstractive, which generates summaries by rephrasing the content coherently.\nLarge language models have advanced the field of abstractive summarization, but\nthey are resourceintensive and face significant challenges in retaining key\ninformation across lengthy documents, which we call being \"lost in the middle\".\nTo address these issues, we propose a hybrid summarization approach that\ncombines extractive and abstractive techniques. Our method splits the document\ninto smaller text chunks, clusters their vector embeddings, generates a summary\nfor each cluster that represents a key idea in the document, and constructs the\nfinal summary by relying on a Markov chain graph when selecting the semantic\norder of ideas.",
        "url": "http://arxiv.org/abs/2506.18036v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18036v1",
        "arxiv_id": "2506.18036v1",
        "authors": [
            "Aziz Amari",
            "Mohamed Achref Ben Ammar"
        ],
        "submitted": "2025-06-22 13:34:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices",
        "abstract": "The ability to dynamically adjust the computational load of neural models\nduring inference in a resource aware manner is crucial for on-device processing\nscenarios, characterised by limited and time-varying computational resources.\nEarly-exit architectures represent an elegant and effective solution, since\nthey can process the input with a subset of their layers, exiting at\nintermediate branches (the upmost layers are hence removed from the model).\n  From a different perspective, for automatic speech recognition applications\nthere are memory-efficient neural architectures that apply variable frame rate\nanalysis, through downsampling/upsampling operations in the middle layers,\nreducing the overall number of operations and improving significantly the\nperformance on well established benchmarks. One example is the Zipformer.\nHowever, these architectures lack the modularity necessary to inject early-exit\nbranches.\n  With the aim of improving the performance in early-exit models, we propose\nintroducing parallel layers in the architecture that process downsampled\nversions of their inputs. % in conjunction with standard processing layers. We\nshow that in this way the speech recognition performance on standard benchmarks\nsignificantly improve, at the cost of a small increase in the overall number of\nmodel parameters but without affecting the inference time.",
        "url": "http://arxiv.org/abs/2506.18035v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18035v1",
        "arxiv_id": "2506.18035v1",
        "authors": [
            "Maxence Lasbordes",
            "Daniele Falavigna",
            "Alessio Brutti"
        ],
        "submitted": "2025-06-22 13:34:18",
        "source": "arxiv",
        "comment": "5 pages, 3 Postscript figures"
    },
    {
        "title": "PDF Retrieval Augmented Question Answering",
        "abstract": "This paper presents an advancement in Question-Answering (QA) systems using a\nRetrieval Augmented Generation (RAG) framework to enhance information\nextraction from PDF files. Recognizing the richness and diversity of data\nwithin PDFs--including text, images, vector diagrams, graphs, and tables--poses\nunique challenges for existing QA systems primarily designed for textual\ncontent. We seek to develop a comprehensive RAG-based QA system that will\neffectively address complex multimodal questions, where several data types are\ncombined in the query. This is mainly achieved by refining approaches to\nprocessing and integrating non-textual elements in PDFs into the RAG framework\nto derive precise and relevant answers, as well as fine-tuning large language\nmodels to better adapt to our system. We provide an in-depth experimental\nevaluation of our solution, demonstrating its capability to extract accurate\ninformation that can be applied to different types of content across PDFs. This\nwork not only pushes the boundaries of retrieval-augmented QA systems but also\nlays a foundation for further research in multimodal data integration and\nprocessing.",
        "url": "http://arxiv.org/abs/2506.18027v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18027v1",
        "arxiv_id": "2506.18027v1",
        "authors": [
            "Thi Thu Uyen Hoang",
            "Viet Anh Nguyen"
        ],
        "submitted": "2025-06-22 13:14:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding",
        "abstract": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.",
        "url": "http://arxiv.org/abs/2506.18023v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18023v1",
        "arxiv_id": "2506.18023v1",
        "authors": [
            "Kui Huang",
            "Xinrong Chen",
            "Wenyu Lv",
            "Jincheng Liao",
            "Guanzhong Wang",
            "Yi Liu"
        ],
        "submitted": "2025-06-22 13:06:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation",
        "abstract": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences and capturing both intra- and inter-sequence\nitem relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain\nSequential Recommendation (LLM-EMF), a novel and advanced approach that\nenhances textual information with Large Language Models (LLM) knowledge and\nsignificantly improves recommendation performance through the fusion of visual\nand textual data. Using the frozen CLIP model, we generate image and text\nembeddings, thereby enriching item representations with multimodal data. A\nmultiple attention mechanism jointly learns both single-domain and cross-domain\npreferences, effectively capturing and understanding complex user interests\nacross diverse domains. Evaluations conducted on four e-commerce datasets\ndemonstrate that LLM-EMF consistently outperforms existing methods in modeling\ncross-domain user preferences, thereby highlighting the effectiveness of\nmultimodal data integration and its advantages in enhancing sequential\nrecommendation systems. Our source code will be released.",
        "url": "http://arxiv.org/abs/2506.17966v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17966v1",
        "arxiv_id": "2506.17966v1",
        "authors": [
            "Wangyu Wu",
            "Zhenhong Chen",
            "Xianglin Qiu",
            "Siqi Song",
            "Xiaowei Huang",
            "Fei Ma",
            "Jimin Xiao"
        ],
        "submitted": "2025-06-22 09:53:21",
        "source": "arxiv",
        "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15085"
    },
    {
        "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment",
        "abstract": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.",
        "url": "http://arxiv.org/abs/2506.17951v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17951v1",
        "arxiv_id": "2506.17951v1",
        "authors": [
            "Quanwei Tang",
            "Sophia Yat Mei Lee",
            "Junshuang Wu",
            "Dong Zhang",
            "Shoushan Li",
            "Erik Cambria",
            "Guodong Zhou"
        ],
        "submitted": "2025-06-22 09:08:44",
        "source": "arxiv",
        "comment": "acl 2025 findings"
    },
    {
        "title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation",
        "abstract": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and\nextending patterns observed during pretraining but often struggle to generalize\nnovel ideas beyond their original context. This paper addresses the challenge\nof applying such localized innovations - introduced at a specific stage or\ncomponent - to other parts of a multi-stage process. We propose a scatter-based\ninnovation expansion model (innovation scatter model) that guides the LLM\nthrough a four-step process: (1) identifying the core innovation by comparing\nthe user's input with its surrounding context, (2) generalizing the innovation\nby removing references to specific stages or components, (3) determining\nwhether the generalized innovation applies to a broader scope beyond the\noriginal stage, and (4) systematically applying it to other structurally\nsimilar stages using the LLM. This model leverages structural redundancy across\nstages to improve the applicability of novel ideas. Verification results\ndemonstrate that the innovation scatter model enables LLMs to extend\ninnovations across structurally similar stages, thereby enhancing\ngeneralization and reuse.",
        "url": "http://arxiv.org/abs/2506.17949v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17949v1",
        "arxiv_id": "2506.17949v1",
        "authors": [
            "Hong Su"
        ],
        "submitted": "2025-06-22 09:02:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tutorial: $\\varphi$-Transductions in OpenFst via the Gallic Semiring",
        "abstract": "OpenFst, a popular finite-state transducer library, supports\n$\\varphi$-transitions but, due to an implementation constraint, they cannot be\nused with transducers in a straightforward way.\n  In this short tutorial, we describe how one can use other functionality\nprovided by OpenFst (namely, the Gallic semiring) to correctly implement\n$\\varphi$-transductions and demonstrate it by implementing the MaxMatch\n(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).\nAccompanying self-contained code examples are provided.\nhttps://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz",
        "url": "http://arxiv.org/abs/2506.17942v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17942v1",
        "arxiv_id": "2506.17942v1",
        "authors": [
            "Marco Cognetta",
            "Cyril Allauzen"
        ],
        "submitted": "2025-06-22 08:24:04",
        "source": "arxiv",
        "comment": "8 pages, 2 figures, code included"
    },
    {
        "title": "A GenAI System for Improved FAIR Independent Biological Database Integration",
        "abstract": "Life sciences research increasingly requires identifying, accessing, and\neffectively processing data from an ever-evolving array of information sources\non the Linked Open Data (LOD) network. This dynamic landscape places a\nsignificant burden on researchers, as the quality of query responses depends\nheavily on the selection and semantic integration of data sources --processes\nthat are often labor-intensive, error-prone, and costly. While the adoption of\nFAIR (Findable, Accessible, Interoperable, and Reusable) data principles has\naimed to address these challenges, barriers to efficient and accurate\nscientific data processing persist.\n  In this paper, we introduce FAIRBridge, an experimental natural\nlanguage-based query processing system designed to empower scientists to\ndiscover, access, and query biological databases, even when they are not\nFAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query\nintents, map them to relevant databases described in scientific literature, and\ngenerate executable queries via intelligent resource access plans. The system\nalso includes robust tools for mitigating low-quality query processing,\nensuring high fidelity and responsiveness in the information delivered.\n  FAIRBridge's autonomous query processing framework enables users to explore\nalternative data sources, make informed choices at every step, and leverage\ncommunity-driven crowd curation when needed. By providing a user-friendly,\nautomated hypothesis-testing platform in natural English, FAIRBridge\nsignificantly enhances the integration and processing of scientific data,\noffering researchers a powerful new tool for advancing their inquiries.",
        "url": "http://arxiv.org/abs/2506.17934v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17934v1",
        "arxiv_id": "2506.17934v1",
        "authors": [
            "Syed N. Sakib",
            "Kallol Naha",
            "Sajratul Y. Rubaiat",
            "Hasan M. Jamil"
        ],
        "submitted": "2025-06-22 08:04:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective",
        "abstract": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.",
        "url": "http://arxiv.org/abs/2506.17930v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17930v1",
        "arxiv_id": "2506.17930v1",
        "authors": [
            "Jianyu Wang",
            "Zhiqiang Hu",
            "Lidong Bing"
        ],
        "submitted": "2025-06-22 07:53:07",
        "source": "arxiv",
        "comment": "ICML 2025, and Code will be released at:\n  https://github.com/jianyu-cs/PromptQuine/"
    },
    {
        "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication",
        "abstract": "Large Language Models (LLMs) have achieved exceptional performance across a\nwide range of tasks. However, they still pose significant safety risks due to\nthe potential misuse for malicious purposes. Jailbreaks, which aim to elicit\nmodels to generate harmful content, play a critical role in identifying the\nunderlying security threats. Recent jailbreaking primarily focuses on\nsingle-turn scenarios, while the more complicated multi-turn scenarios remain\nunderexplored. Moreover, existing multi-turn jailbreaking techniques struggle\nto adapt to the evolving dynamics of dialogue as the interaction progresses. To\naddress this limitation, we propose a novel multi-turn jailbreaking method that\nrefines the jailbreaking path globally at each interaction. We also actively\nfabricate model responses to suppress safety-related warnings, thereby\nincreasing the likelihood of eliciting harmful outputs in subsequent questions.\nExperimental results demonstrate the superior performance of our method\ncompared with existing single-turn and multi-turn jailbreaking techniques\nacross six state-of-the-art LLMs. Our code is publicly available at\nhttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.",
        "url": "http://arxiv.org/abs/2506.17881v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17881v1",
        "arxiv_id": "2506.17881v1",
        "authors": [
            "Hua Tang",
            "Lingyong Yan",
            "Yukun Zhao",
            "Shuaiqiang Wang",
            "Jizhou Huang",
            "Dawei Yin"
        ],
        "submitted": "2025-06-22 03:15:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How Alignment Shrinks the Generative Horizon",
        "abstract": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.",
        "url": "http://arxiv.org/abs/2506.17871v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17871v1",
        "arxiv_id": "2506.17871v1",
        "authors": [
            "Chenghao Yang",
            "Ari Holtzman"
        ],
        "submitted": "2025-06-22 02:00:37",
        "source": "arxiv",
        "comment": "Codebase: https://github.com/yangalan123/LLMBranchingFactor, Website:\n  https://yangalan123.github.io/branching_factor/"
    },
    {
        "title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs",
        "abstract": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.",
        "url": "http://arxiv.org/abs/2506.17864v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17864v1",
        "arxiv_id": "2506.17864v1",
        "authors": [
            "Taolin Zhang",
            "Haidong Kang",
            "Dongyang Li",
            "Qizhou Chen",
            "Chengyu Wang Xiaofeng He",
            "Richang Hong"
        ],
        "submitted": "2025-06-22 00:58:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale",
        "abstract": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.",
        "url": "http://arxiv.org/abs/2506.17863v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17863v1",
        "arxiv_id": "2506.17863v1",
        "authors": [
            "Haoran Liu",
            "Amir Tahmasbi",
            "Ehtesham Sam Haque",
            "Purak Jain"
        ],
        "submitted": "2025-06-22 00:28:35",
        "source": "arxiv",
        "comment": "KDD LLM4ECommerce Workshop 2025"
    },
    {
        "title": "THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction",
        "abstract": "Automated clinical risk prediction from electronic health records (EHRs)\ndemands modeling both structured diagnostic codes and unstructured narrative\nnotes. However, most prior approaches either handle these modalities separately\nor rely on simplistic fusion strategies that ignore the directional,\nhierarchical causal interactions by which narrative observations precipitate\ndiagnoses and propagate risk across admissions. In this paper, we propose\nTHCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our\nframework constructs a multimodal causal graph where nodes represent clinical\nentities from two modalities: Textual propositions extracted from notes and ICD\ncodes mapped to textual descriptions. Through hierarchical causal discovery,\nTHCM-CAL infers three clinically grounded interactions: intra-slice\nsame-modality sequencing, intra-slice cross-modality triggers, and inter-slice\nrisk propagation. To enhance prediction reliability, we extend conformal\nprediction to multi-label ICD coding, calibrating per-code confidence intervals\nunder complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV\ndemonstrate the superiority of THCM-CAL.",
        "url": "http://arxiv.org/abs/2506.17844v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17844v1",
        "arxiv_id": "2506.17844v1",
        "authors": [
            "Xin Zhang",
            "Qiyu Wei",
            "Yingjie Zhu",
            "Fanyi Wu",
            "Sophia Ananiadou"
        ],
        "submitted": "2025-06-21 22:43:42",
        "source": "arxiv",
        "comment": "13 pages, 4 figures"
    },
    {
        "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach",
        "abstract": "Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights.",
        "url": "http://arxiv.org/abs/2506.17828v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17828v1",
        "arxiv_id": "2506.17828v1",
        "authors": [
            "Xinnan Zhang",
            "Chenliang Li",
            "Siliang Zeng",
            "Jiaxiang Li",
            "Zhongruo Wang",
            "Kaixiang Lin",
            "Songtao Lu",
            "Alfredo Garcia",
            "Mingyi Hong"
        ],
        "submitted": "2025-06-21 21:49:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights",
        "abstract": "Tokenization plays a pivotal role in multilingual NLP. However, existing\ntokenizers are often skewed towards high-resource languages, limiting their\neffectiveness for linguistically diverse and morphologically rich languages\nsuch as those in the Indian subcontinent. This paper presents a comprehensive\nintrinsic evaluation of tokenization strategies across 17 Indian languages. We\nquantify the trade-offs between bottom-up and top-down tokenizer algorithms\n(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of\nmultilingual vocabulary construction such as joint and cluster-based training.\nWe also show that extremely low-resource languages can benefit from tokenizers\ntrained on related high-resource languages. Our study provides practical\ninsights for building more fair, efficient, and linguistically informed\ntokenizers for multilingual NLP.",
        "url": "http://arxiv.org/abs/2506.17789v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17789v1",
        "arxiv_id": "2506.17789v1",
        "authors": [
            "N J Karthika",
            "Maharaj Brahma",
            "Rohit Saluja",
            "Ganesh Ramakrishnan",
            "Maunendra Sankar Desarkar"
        ],
        "submitted": "2025-06-21 18:47:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bayesian Social Deduction with Graph-Informed Language Models",
        "abstract": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/",
        "url": "http://arxiv.org/abs/2506.17788v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17788v1",
        "arxiv_id": "2506.17788v1",
        "authors": [
            "Shahab Rahimirad",
            "Guven Gergerli",
            "Lucia Romero",
            "Angela Qian",
            "Matthew Lyle Olson",
            "Simon Stepputtis",
            "Joseph Campbell"
        ],
        "submitted": "2025-06-21 18:45:28",
        "source": "arxiv",
        "comment": "32 pages, 10 figures. Under review"
    },
    {
        "title": "Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs",
        "abstract": "Evaluating Information Retrieval (IR) systems relies on high-quality manual\nrelevance judgments (qrels), which are costly and time-consuming to obtain.\nWhile pooling reduces the annotation effort, it results in only partially\nlabeled datasets. Large Language Models (LLMs) offer a promising alternative to\nreducing reliance on manual judgments, particularly in complex domains like\nmedical case-based retrieval, where relevance assessment requires analyzing\nboth textual and visual information. In this work, we explore using a\nMultimodal Large Language Model (MLLM) to expand relevance judgments, creating\na new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on\nthe ImageCLEFmed 2013 case-based retrieval task, simulating human assessment\nthrough an iteratively refined, structured prompting strategy that integrates\nbinary scoring, instruction-based evaluation, and few-shot learning. We\nsystematically experimented with various prompt configurations to maximize\nagreement with human judgments. To evaluate agreement between the MLLM and\nhuman judgments, we use Cohen's Kappa, achieving a substantial agreement score\nof 0.6, comparable to inter-annotator agreement typically observed in\nmultimodal retrieval tasks. Starting from the original 15,028 manual judgments\n(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset\nby over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On\naverage, each medical case query received 15,398 new annotations, with\napproximately 99% being non-relevant, reflecting the high sparsity typical in\nthis domain. Our results demonstrate the potential of MLLMs to scale relevance\njudgment collection, offering a promising direction for supporting retrieval\nevaluation in medical and multimodal IR tasks.",
        "url": "http://arxiv.org/abs/2506.17782v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17782v1",
        "arxiv_id": "2506.17782v1",
        "authors": [
            "Catarina Pires",
            "SÃ©rgio Nunes",
            "LuÃ­s Filipe Teixeira"
        ],
        "submitted": "2025-06-21 18:29:33",
        "source": "arxiv",
        "comment": "To appear at the Third Workshop on Large Language Models for\n  Evaluation in Information Retrieval (LLM4Eval 2025), co-located with SIGIR\n  2025. 9 pages, 2 figures, 5 tables"
    },
    {
        "title": "Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models",
        "abstract": "Dense embeddings are fundamental to modern machine learning systems, powering\nRetrieval-Augmented Generation (RAG), information retrieval, and representation\nlearning. While instruction-conditioning has become the dominant approach for\nembedding specialization, its direct application to low-capacity models imposes\nfundamental representational constraints that limit the performance gains\nderived from specialization. In this paper, we analyze these limitations and\nintroduce the Mixture of Task Experts (MoTE) transformer block, which leverages\ntask-specialized parameters trained with Task-Aware Contrastive Learning\n(\\tacl) to enhance the model ability to generate specialized embeddings.\nEmpirical results show that MoTE achieves $64\\%$ higher performance gains in\nretrieval datasets ($+3.27 \\rightarrow +5.21$) and $43\\%$ higher performance\ngains across all datasets ($+1.81 \\rightarrow +2.60$). Critically, these gains\nare achieved without altering instructions, training data, inference time, or\nnumber of active parameters.",
        "url": "http://arxiv.org/abs/2506.17781v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17781v1",
        "arxiv_id": "2506.17781v1",
        "authors": [
            "Miguel Romero",
            "Shuoyang Ding",
            "Corey D. Barret",
            "Georgiana Dinu",
            "George Karypis"
        ],
        "submitted": "2025-06-21 18:28:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CARTS: Collaborative Agents for Recommendation Textual Summarization",
        "abstract": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics.",
        "url": "http://arxiv.org/abs/2506.17765v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17765v1",
        "arxiv_id": "2506.17765v1",
        "authors": [
            "Jiao Chen",
            "Kehui Yao",
            "Reza Yousefi Maragheh",
            "Kai Zhao",
            "Jianpeng Xu",
            "Jason Cho",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "submitted": "2025-06-21 17:18:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations",
        "abstract": "Contemporary Language Models (LMs), while impressively fluent, often generate\ncontent that is factually incorrect or unfaithful to the input context - a\ncritical issue commonly referred to as 'hallucination'. This tendency of LMs to\ngenerate hallucinated content undermines their reliability, especially because\nthese fabrications are often highly convincing and therefore difficult to\ndetect. While several existing methods attempt to detect hallucinations, most\nrely on analyzing multiple generations per input, leading to increased\ncomputational cost and latency. To address this, we propose a single-pass,\ntraining-free approach for effective Hallucination detectIon via Decoupled\nrEpresentations (HIDE). Our approach leverages the hypothesis that\nhallucinations result from a statistical decoupling between an LM's internal\nrepresentations of input context and its generated output. We quantify this\ndecoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to\nhidden-state representations extracted while generating the output sequence. We\nconduct extensive experiments on four diverse question answering datasets,\nevaluating both faithfulness and factuality hallucinations across six\nopen-source LMs of varying scales and properties. Our results demonstrate that\nHIDE outperforms other single-pass methods in almost all settings, achieving an\naverage relative improvement of ~29% in AUC-ROC over the best-performing\nsingle-pass strategy across various models and datasets. Additionally, HIDE\nshows competitive and often superior performance with multi-pass\nstate-of-the-art methods, obtaining an average relative improvement of ~3% in\nAUC-ROC while consuming ~51% less computation time. Our findings highlight the\neffectiveness of exploiting internal representation decoupling in LMs for\nefficient and practical hallucination detection.",
        "url": "http://arxiv.org/abs/2506.17748v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17748v1",
        "arxiv_id": "2506.17748v1",
        "authors": [
            "Anwoy Chatterjee",
            "Yash Goel",
            "Tanmoy Chakraborty"
        ],
        "submitted": "2025-06-21 16:02:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process",
        "abstract": "In this paper, we introduce KAG-Thinker, a novel human-like reasoning\nframework built upon a parameter-light large language model (LLM). Our approach\nenhances the logical coherence and contextual consistency of the thinking\nprocess in question-answering (Q\\&A) tasks on domain-specific knowledge bases\n(KBs) within LLMs. This framework simulates human cognitive mechanisms for\nhandling complex problems by establishing a structured thinking process.\nContinuing the \\textbf{Logical Form} guided retrieval and reasoning technology\nroute of KAG v0.7, firstly, it decomposes complex questions into independently\nsolvable sub-problems(also referred to as logical forms) through\n\\textbf{breadth decomposition}, each represented in two equivalent\nforms-natural language and logical function-and further classified as either\nKnowledge Retrieval or Reasoning Analysis tasks, with dependencies and\nvariables passing explicitly modeled via logical function interfaces. In the\nsolving process, the Retrieval function is used to perform knowledge retrieval\ntasks, while the Math and Deduce functions are used to perform reasoning\nanalysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval\nsub-problem tasks, LLMs and external knowledge sources are regarded as\nequivalent KBs. We use the \\textbf{knowledge boundary} model to determine the\noptimal source using self-regulatory mechanisms such as confidence calibration\nand reflective reasoning, and use the \\textbf{depth solving} model to enhance\nthe comprehensiveness of knowledge acquisition. Finally, instead of utilizing\nreinforcement learning, we employ supervised fine-tuning with multi-turn\ndialogues to align the model with our structured inference paradigm, thereby\navoiding excessive reflection. This is supported by a data evaluation framework\nand iterative corpus synthesis, which facilitate the generation of detailed\nreasoning trajectories...",
        "url": "http://arxiv.org/abs/2506.17728v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17728v1",
        "arxiv_id": "2506.17728v1",
        "authors": [
            "Dalong Zhang",
            "Jun Xu",
            "Jun Zhou",
            "Lei Liang",
            "Lin Yuan",
            "Ling Zhong",
            "Mengshu Sun",
            "Peilong Zhao",
            "QiWei Wang",
            "Xiaorui Wang",
            "Xinkai Du",
            "YangYang Hou",
            "Yu Ao",
            "ZhaoYang Wang",
            "Zhengke Gui",
            "ZhiYing Yi",
            "Zhongpu Bo"
        ],
        "submitted": "2025-06-21 14:58:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages",
        "abstract": "Part-of-speech (POS) tagging remains a foundational component in natural\nlanguage processing pipelines, particularly critical for historical text\nanalysis at the intersection of computational linguistics and digital\nhumanities. Despite significant advancements in modern large language models\n(LLMs) for ancient languages, their application to Medieval Romance languages\npresents distinctive challenges stemming from diachronic linguistic evolution,\nspelling variations, and labeled data scarcity. This study systematically\ninvestigates the central determinants of POS tagging performance across diverse\ncorpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,\nspanning biblical, hagiographical, medical, and dietary domains. Through\nrigorous experimentation, we evaluate how fine-tuning approaches, prompt\nengineering, model architectures, decoding strategies, and cross-lingual\ntransfer learning techniques affect tagging accuracy. Our results reveal both\nnotable limitations in LLMs' ability to process historical language variations\nand non-standardized spelling, as well as promising specialized techniques that\neffectively address the unique challenges presented by low-resource historical\nlanguages.",
        "url": "http://arxiv.org/abs/2506.17715v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17715v1",
        "arxiv_id": "2506.17715v1",
        "authors": [
            "Matthias SchÃ¶ffel",
            "Esteban Garces Arias",
            "Marinus Wiedner",
            "Paula Ruppert",
            "Meimingwei Li",
            "Christian Heumann",
            "Matthias AÃenmacher"
        ],
        "submitted": "2025-06-21 13:33:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Aged to Perfection: Machine-Learning Maps of Age in Conversational English",
        "abstract": "The study uses the British National Corpus 2014, a large sample of\ncontemporary spoken British English, to investigate language patterns across\ndifferent age groups. Our research attempts to explore how language patterns\nvary between different age groups, exploring the connection between speaker\ndemographics and linguistic factors such as utterance duration, lexical\ndiversity, and word choice. By merging computational language analysis and\nmachine learning methodologies, we attempt to uncover distinctive linguistic\nmarkers characteristic of multiple generations and create prediction models\nthat can consistently estimate the speaker's age group from various aspects.\nThis work contributes to our knowledge of sociolinguistic diversity throughout\nthe life of modern British speech.",
        "url": "http://arxiv.org/abs/2506.17708v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17708v1",
        "arxiv_id": "2506.17708v1",
        "authors": [
            "MingZe Tang"
        ],
        "submitted": "2025-06-21 13:08:57",
        "source": "arxiv",
        "comment": "6 pages, 11 figures"
    },
    {
        "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future",
        "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing (NLP) by automating traditional labor-intensive tasks and\nconsequently accelerated the development of computer-aided applications. As\nresearchers continue to advance this field with the introduction of novel\nlanguage models and more efficient training/finetuning methodologies, the idea\nof prompt engineering and subsequent optimization strategies with LLMs has\nemerged as a particularly impactful trend to yield a substantial performance\nboost across diverse NLP tasks. To best of our knowledge numerous review\narticles have explored prompt engineering, however, a critical gap exists in\ncomprehensive analyses of prompt optimization strategies. To bridge this gap\nthis paper provides unique and comprehensive insights about the potential of\ndiverse prompt optimization strategies. It analyzes their underlying working\nparadigms and based on these principles, categorizes them into 11 distinct\nclasses. Moreover, the paper provides details about various NLP tasks where\nthese prompt optimization strategies have been employed, along with details of\ndifferent LLMs and benchmark datasets used for evaluation. This comprehensive\ncompilation lays a robust foundation for future comparative studies and enables\nrigorous assessment of prompt optimization and LLM-based predictive pipelines\nunder consistent experimental settings: a critical need in the current\nlandscape. Ultimately, this research will centralize diverse strategic\nknowledge to facilitate the adaptation of existing prompt optimization\nstrategies for development of innovative predictors across unexplored tasks.",
        "url": "http://arxiv.org/abs/2506.17700v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17700v1",
        "arxiv_id": "2506.17700v1",
        "authors": [
            "Summra Saleem",
            "Muhammad Nabeel Asim",
            "Shaista Zulfiqar",
            "Andreas Dengel"
        ],
        "submitted": "2025-06-21 12:25:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches",
        "abstract": "Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nincreasing number of online debates among social media users, conversational\nstance detection has become a crucial research area. However, existing\nconversational stance detection datasets are restricted to a limited set of\nspecific targets, which constrains the effectiveness of stance detection models\nwhen encountering a large number of unseen targets in real-world applications.\nTo bridge this gap, we manually curate a large-scale, high-quality zero-shot\nconversational stance detection dataset, named ZS-CSD, comprising 280 targets\nacross two distinct target types. Leveraging the ZS-CSD dataset, we propose\nSITPCL, a speaker interaction and target-aware prototypical contrastive\nlearning model, and establish the benchmark performance in the zero-shot\nsetting. Experimental results demonstrate that our proposed SITPCL model\nachieves state-of-the-art performance in zero-shot conversational stance\ndetection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,\nhighlighting the persistent challenges in zero-shot conversational stance\ndetection.",
        "url": "http://arxiv.org/abs/2506.17693v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17693v1",
        "arxiv_id": "2506.17693v1",
        "authors": [
            "Yuzhe Ding",
            "Kang He",
            "Bobo Li",
            "Li Zheng",
            "Haijun He",
            "Fei Li",
            "Chong Teng",
            "Donghong Ji"
        ],
        "submitted": "2025-06-21 12:02:06",
        "source": "arxiv",
        "comment": "ACL 2025 (Findings)"
    },
    {
        "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering",
        "abstract": "Knowledge-intensive multi-hop question answering (QA) tasks, which require\nintegrating evidence from multiple sources to address complex queries, often\nnecessitate multiple rounds of retrieval and iterative generation by large\nlanguage models (LLMs). However, incorporating many documents and extended\ncontexts poses challenges -such as hallucinations and semantic drift-for\nlightweight LLMs with fewer parameters. This work proposes a novel framework\ncalled DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions\ninto logically coherent subquestions to form a hallucination-free reasoning\nchain. It then iteratively refines these subquestions through context-aware\nrewriting to generate effective query formulations. For retrieval, we introduce\na lightweight discriminative keyword extraction module that leverages extracted\nkeywords to achieve targeted, precise document recall with relatively low\ncomputational overhead. Extensive experiments on three multi-hop QA datasets\ndemonstrate that DEC performs on par with or surpasses state-of-the-art\nbenchmarks while significantly reducing token consumption. Notably, our\napproach attains state-of-the-art results on models with 8B parameters,\nshowcasing its effectiveness in various scenarios, particularly in\nresource-constrained environments.",
        "url": "http://arxiv.org/abs/2506.17692v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17692v1",
        "arxiv_id": "2506.17692v1",
        "authors": [
            "Binquan Ji",
            "Haibo Luo",
            "Yifei Lu",
            "Lei Hei",
            "Jiaqi Wang",
            "Tingjing Liao",
            "Lingyu Wang",
            "Shichao Wang",
            "Feiliang Ren"
        ],
        "submitted": "2025-06-21 11:55:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models",
        "abstract": "Keyword Spotting plays a critical role in enabling hands-free interaction for\nbattery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the\nscalability and adaptability challenges of traditional systems by enabling\nrecognition of custom keywords with only a few examples. However, existing\nFS-KWS systems achieve subpar accuracy at desirable false acceptance rates,\nparticularly in resource-constrained edge environments. To address these\nissues, we propose a training scheme that leverages self-supervised learning\nmodels for robust feature extraction, dimensionality reduction, and knowledge\ndistillation. The teacher model, based on Wav2Vec 2.0 is trained using\nSub-center ArcFace loss, which enhances inter-class separability and\nintra-class compactness. To enable efficient deployment on edge devices, we\nintroduce attention-based dimensionality reduction and train a standard\nlightweight ResNet15 student model. We evaluate the proposed approach on the\nEnglish portion of the Multilingual Spoken Words Corpus (MSWC) and the Google\nSpeech Commands (GSC) datasets. Notably, the proposed training method improves\nthe 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%\nfalse alarm accuracy on the GSC dataset, thus making it significantly\nbetter-suited for a real use case scenario.",
        "url": "http://arxiv.org/abs/2506.17686v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17686v1",
        "arxiv_id": "2506.17686v1",
        "authors": [
            "Alican Gok",
            "Oguzhan Buyuksolak",
            "Osman Erman Okman",
            "Murat Saraclar"
        ],
        "submitted": "2025-06-21 11:39:11",
        "source": "arxiv",
        "comment": "To be submitted to IEEE Signal Processing Letters, 5 pages, 3 figures"
    },
    {
        "title": "Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems",
        "abstract": "In real-world recommendation systems, users would engage in variety\nscenarios, such as homepages, search pages, and related recommendation pages.\nEach of these scenarios would reflect different aspects users focus on.\nHowever, the user interests may be inconsistent in different scenarios, due to\ndifferences in decision-making processes and preference expression. This\nvariability complicates unified modeling, making multi-scenario learning a\nsignificant challenge. To address this, we propose a novel reinforcement\nlearning approach that models user preferences across scenarios by modeling\nuser interest evolution across multiple scenarios. Our method employs Double\nQ-learning to enhance next-item prediction accuracy and optimizes contrastive\nlearning loss using Q-value to make model performance better. Experimental\nresults demonstrate that our approach surpasses state-of-the-art methods in\nmulti-scenario recommendation tasks. Our work offers a fresh perspective on\nmulti-scenario modeling and highlights promising directions for future\nresearch.",
        "url": "http://arxiv.org/abs/2506.17682v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17682v1",
        "arxiv_id": "2506.17682v1",
        "authors": [
            "Zhijian Feng",
            "Wenhao Zheng",
            "Xuanji Xiao"
        ],
        "submitted": "2025-06-21 11:27:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies",
        "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.",
        "url": "http://arxiv.org/abs/2506.17673v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17673v1",
        "arxiv_id": "2506.17673v1",
        "authors": [
            "Seonglae Cho",
            "Harryn Oh",
            "Donghyun Lee",
            "Luis Eduardo Rodrigues Vieira",
            "Andrew Bermingham",
            "Ziad El Sayed"
        ],
        "submitted": "2025-06-21 10:18:25",
        "source": "arxiv",
        "comment": "18 pages, 18 figures"
    },
    {
        "title": "TPTT: Transforming Pretrained Transformer into Titans",
        "abstract": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .",
        "url": "http://arxiv.org/abs/2506.17671v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17671v1",
        "arxiv_id": "2506.17671v1",
        "authors": [
            "Fabien Furfaro"
        ],
        "submitted": "2025-06-21 10:06:07",
        "source": "arxiv",
        "comment": "6 pages, 1 figure"
    },
    {
        "title": "Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation",
        "abstract": "Large Language Models (LLMs) have revolutionized various domains but\nencounter substantial challenges in tackling optimization modeling tasks for\nOperations Research (OR), particularly when dealing with complex problem. In\nthis work, we propose Step-Opt-Instruct, a framework that augments existing\ndatasets and generates high-quality fine-tuning data tailored to optimization\nmodeling. Step-Opt-Instruct employs iterative problem generation to\nsystematically increase problem complexity and stepwise validation to\nrigorously verify data, preventing error propagation and ensuring the quality\nof the generated dataset. Leveraging this framework, we fine-tune open-source\nLLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that\nachieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and\nIndustryOR. Extensive experiments demonstrate the superior performance of\nStep-Opt, especially in addressing complex OR tasks, with a notable 17.01\\%\nimprovement in micro average accuracy on difficult problems. These findings\nhighlight the effectiveness of combining structured validation with gradual\nproblem refinement to advance the automation of decision-making processes using\nLLMs.The code and dataset are available at https://github.com/samwu-learn/Step.",
        "url": "http://arxiv.org/abs/2506.17637v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17637v1",
        "arxiv_id": "2506.17637v1",
        "authors": [
            "Yang Wu",
            "Yifan Zhang",
            "Yurong Wu",
            "Yuran Wang",
            "Junkai Zhang",
            "Jian Cheng"
        ],
        "submitted": "2025-06-21 08:42:27",
        "source": "arxiv",
        "comment": "17 pages, 12 figures"
    },
    {
        "title": "Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs",
        "abstract": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, growing evidence suggests much of their success stems from\nmemorized answer-reasoning patterns rather than genuine inference. In this\nwork, we investigate a central question: are LLMs primarily anchored to final\nanswers or to the textual pattern of reasoning chains? We propose a five-level\nanswer-visibility prompt framework that systematically manipulates answer cues\nand probes model behavior through indirect, behavioral analysis. Experiments\nacross state-of-the-art LLMs reveal a strong and consistent reliance on\nexplicit answers. The performance drops by 26.90\\% when answer cues are masked,\neven with complete reasoning chains. These findings suggest that much of the\nreasoning exhibited by LLMs may reflect post-hoc rationalization rather than\ntrue inference, calling into question their inferential depth. Our study\nuncovers the answer-anchoring phenomenon with rigorous empirical validation and\nunderscores the need for a more nuanced understanding of what constitutes\nreasoning in LLMs.",
        "url": "http://arxiv.org/abs/2506.17630v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17630v1",
        "arxiv_id": "2506.17630v1",
        "authors": [
            "Yang Wu",
            "Yifan Zhang",
            "Yiwei Wang",
            "Yujun Cai",
            "Yurong Wu",
            "Yuran Wang",
            "Ning Xu",
            "Jian Cheng"
        ],
        "submitted": "2025-06-21 08:15:45",
        "source": "arxiv",
        "comment": "14 pages, 8 figures"
    },
    {
        "title": "CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning",
        "abstract": "Embodied Visual Reasoning (EVR) seeks to follow complex, free-form\ninstructions based on egocentric video, enabling semantic understanding and\nspatiotemporal reasoning in dynamic environments. Despite its promising\npotential, EVR encounters significant challenges stemming from the diversity of\ncomplex instructions and the intricate spatiotemporal dynamics in long-term\negocentric videos. Prior solutions either employ Large Language Models (LLMs)\nover static video captions, which often omit critical visual details, or rely\non end-to-end Vision-Language Models (VLMs) that struggle with stepwise\ncompositional reasoning. Consider the complementary strengths of LLMs in\nreasoning and VLMs in perception, we propose CLiViS. It is a novel\ntraining-free framework that leverages LLMs for high-level task planning and\norchestrates VLM-driven open-world visual perception to iteratively update the\nscene context. Building on this synergy, the core of CLiViS is a dynamic\nCognitive Map that evolves throughout the reasoning process. This map\nconstructs a structured representation of the embodied scene, bridging\nlow-level perception and high-level reasoning. Extensive experiments across\nmultiple benchmarks demonstrate the effectiveness and generality of CLiViS,\nespecially in handling long-term visual dependencies. Code is available at\nhttps://github.com/Teacher-Tom/CLiViS.",
        "url": "http://arxiv.org/abs/2506.17629v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17629v1",
        "arxiv_id": "2506.17629v1",
        "authors": [
            "Kailing Li",
            "Qi'ao Xu",
            "Tianwen Qian",
            "Yuqian Fu",
            "Yang Jiao",
            "Xiaoling Wang"
        ],
        "submitted": "2025-06-21 08:11:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OpusLM: A Family of Open Unified Speech Language Models",
        "abstract": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research",
        "url": "http://arxiv.org/abs/2506.17611v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17611v1",
        "arxiv_id": "2506.17611v1",
        "authors": [
            "Jinchuan Tian",
            "William Chen",
            "Yifan Peng",
            "Jiatong Shi",
            "Siddhant Arora",
            "Shikhar Bharadwaj",
            "Takashi Maekaku",
            "Yusuke Shinohara",
            "Keita Goto",
            "Xiang Yue",
            "Huck Yang",
            "Shinji Watanabe"
        ],
        "submitted": "2025-06-21 06:30:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting",
        "abstract": "Accurate typhoon track forecasting is crucial for early system warning and\ndisaster response. While Transformer-based models have demonstrated strong\nperformance in modeling the temporal dynamics of dense trajectories of humans\nand vehicles in smart cities, they usually lack access to broader contextual\nknowledge that enhances the forecasting reliability of sparse meteorological\ntrajectories, such as typhoon tracks. To address this challenge, we propose\nTyphoFormer, a novel framework that incorporates natural language descriptions\nas auxiliary prompts to improve typhoon trajectory forecasting. For each time\nstep, we use Large Language Model (LLM) to generate concise textual\ndescriptions based on the numerical attributes recorded in the North Atlantic\nhurricane database. The language descriptions capture high-level meteorological\nsemantics and are embedded as auxiliary special tokens prepended to the\nnumerical time series input. By integrating both textual and sequential\ninformation within a unified Transformer encoder, TyphoFormer enables the model\nto leverage contextual cues that are otherwise inaccessible through numerical\nfeatures alone. Extensive experiments are conducted on HURDAT2 benchmark,\nresults show that TyphoFormer consistently outperforms other state-of-the-art\nbaseline methods, particularly under challenging scenarios involving nonlinear\npath shifts and limited historical observations.",
        "url": "http://arxiv.org/abs/2506.17609v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17609v1",
        "arxiv_id": "2506.17609v1",
        "authors": [
            "Lincan Li",
            "Eren Erman Ozguven",
            "Yue Zhao",
            "Guang Wang",
            "Yiqun Xie",
            "Yushun Dong"
        ],
        "submitted": "2025-06-21 06:14:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages",
        "abstract": "Morphological defectivity is an intriguing and understudied phenomenon in\nlinguistics. Addressing defectivity, where expected inflectional forms are\nabsent, is essential for improving the accuracy of NLP tools in morphologically\nrich languages. However, traditional linguistic resources often lack coverage\nof morphological gaps as such knowledge requires significant human expertise\nand effort to document and verify. For scarce linguistic phenomena in\nunder-explored languages, Wikipedia and Wiktionary often serve as among the few\naccessible resources. Despite their extensive reach, their reliability has been\na subject of controversy. This study customizes a novel neural morphological\nanalyzer to annotate Latin and Italian corpora. Using the massive annotated\ndata, crowd-sourced lists of defective verbs compiled from Wiktionary are\nvalidated computationally. Our results indicate that while Wiktionary provides\na highly reliable account of Italian morphological gaps, 7% of Latin lemmata\nlisted as defective show strong corpus evidence of being non-defective. This\ndiscrepancy highlights potential limitations of crowd-sourced wikis as\ndefinitive sources of linguistic knowledge, particularly for less-studied\nphenomena and languages, despite their value as resources for rare linguistic\nfeatures. By providing scalable tools and methods for quality assurance of\ncrowd-sourced data, this work advances computational morphology and expands\nlinguistic knowledge of defectivity in non-English, morphologically rich\nlanguages.",
        "url": "http://arxiv.org/abs/2506.17603v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17603v1",
        "arxiv_id": "2506.17603v1",
        "authors": [
            "Jonathan Sakunkoo",
            "Annabella Sakunkoo"
        ],
        "submitted": "2025-06-21 05:46:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A novel fast short-time root music method for vibration monitoring of high-speed spindles",
        "abstract": "Ultra-high-speed spindle bearings challenge traditional vibration monitoring\ndue to broadband noise, non-stationarity, and limited time-frequency\nresolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that\nexploits\n  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity\nfrom $\\mathcal{O}(N^3)$ to $SN\\log_2N+S^2(N+S)+M^2(N+M)$\n  while preserving parametric super-resolution. The method constructs Hankel\nmatrices from 16 ms signal frames and extracts fault frequencies through\npolynomial rooting on the unit circle. Experimental validation on the\nPolitecnico di Torino bearing dataset demonstrates breakthrough micro-defect\ndetection capabilities. The algorithm reliably identifies 150 $\\mu$m defects --\npreviously undetectable by conventional methods -- providing 72+ hours\nadditional warning time. Compared to STFT and wavelet methods, fSTrM achieves\n1.2 Hz frequency resolution (vs. 12.5 Hz), 93\\% detection rate at $-$5 dB SNR,\nand quantifies defect severity through harmonic content analysis. Critically,\nthe algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7\nhardware, enabling real-time deployment. This advancement transforms bearing\nmonitoring from failure prevention to continuous degradation assessment,\nestablishing a new paradigm for predictive maintenance in aerospace and\nprecision machining.",
        "url": "http://arxiv.org/abs/2506.17600v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17600v1",
        "arxiv_id": "2506.17600v1",
        "authors": [
            "Huiguang Zhang",
            "Baoguo Liu",
            "Wei Feng",
            "Zongtang Li"
        ],
        "submitted": "2025-06-21 05:35:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models",
        "abstract": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.",
        "url": "http://arxiv.org/abs/2506.17585v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17585v1",
        "arxiv_id": "2506.17585v1",
        "authors": [
            "Yukun Huang",
            "Sanxing Chen",
            "Jian Pei",
            "Manzil Zaheer",
            "Bhuwan Dhingra"
        ],
        "submitted": "2025-06-21 04:48:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
        "abstract": "The exponential growth of scientific literature challenges researchers\nextracting and synthesizing knowledge. Traditional search engines return many\nsources without direct, detailed answers, while general-purpose LLMs may offer\nconcise responses that lack depth or omit current information. LLMs with search\ncapabilities are also limited by context window, yielding short, incomplete\nanswers. This paper introduces WISE (Workflow for Intelligent Scientific\nKnowledge Extraction), a system addressing these limits by using a structured\nworkflow to extract, refine, and rank query-specific knowledge. WISE uses an\nLLM-powered, tree-based architecture to refine data, focusing on query-aligned,\ncontext-aware, and non-redundant information. Dynamic scoring and ranking\nprioritize unique contributions from each source, and adaptive stopping\ncriteria minimize processing overhead. WISE delivers detailed, organized\nanswers by systematically exploring and synthesizing knowledge from diverse\nsources. Experiments on HBB gene-associated diseases demonstrate WISE reduces\nprocessed text by over 80% while achieving significantly higher recall over\nbaselines like search engines and other LLM-based approaches. ROUGE and BLEU\nmetrics reveal WISE's output is more unique than other systems, and a novel\nlevel-based metric shows it provides more in-depth information. We also explore\nhow the WISE workflow can be adapted for diverse domains like drug discovery,\nmaterial science, and social science, enabling efficient knowledge extraction\nand synthesis from unstructured scientific papers and web sources.",
        "url": "http://arxiv.org/abs/2506.17580v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17580v1",
        "arxiv_id": "2506.17580v1",
        "authors": [
            "Sajratul Y. Rubaiat",
            "Hasan M. Jamil"
        ],
        "submitted": "2025-06-21 04:22:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition",
        "abstract": "Agricultural named entity recognition is a specialized task focusing on\nidentifying distinct agricultural entities within vast bodies of text,\nincluding crops, diseases, pests, and fertilizers. It plays a crucial role in\nenhancing information extraction from extensive agricultural text resources.\nHowever, the scarcity of high-quality agricultural datasets, particularly in\nChinese, has resulted in suboptimal performance when employing mainstream\nmethods for this purpose. Most earlier works only focus on annotating\nagricultural entities while overlook the profound correlation of agriculture\nwith hydrology and meteorology. To fill this blank, we present AgriCHN, a\ncomprehensive open-source Chinese resource designed to promote the accuracy of\nautomated agricultural entity annotation. The AgriCHN dataset has been\nmeticulously curated from a wealth of agricultural articles, comprising a total\nof 4,040 sentences and encapsulating 15,799 agricultural entity mentions\nspanning 27 diverse entity categories. Furthermore, it encompasses entities\nfrom hydrology to meteorology, thereby enriching the diversity of entities\nconsidered. Data validation reveals that, compared with relevant resources,\nAgriCHN demonstrates outstanding data quality, attributable to its richer\nagricultural entity types and more fine-grained entity divisions. A benchmark\ntask has also been constructed using several state-of-the-art neural NER\nmodels. Extensive experimental results highlight the significant challenge\nposed by AgriCHN and its potential for further research.",
        "url": "http://arxiv.org/abs/2506.17578v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17578v1",
        "arxiv_id": "2506.17578v1",
        "authors": [
            "Lingxiao Zeng",
            "Yiqi Tong",
            "Wei Guo",
            "Huarui Wu",
            "Lihao Ge",
            "Yijun Ye",
            "Fuzhen Zhuang",
            "Deqing Wang",
            "Wei Guo",
            "Cheng Chen"
        ],
        "submitted": "2025-06-21 04:21:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning",
        "abstract": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency.",
        "url": "http://arxiv.org/abs/2506.17562v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17562v1",
        "arxiv_id": "2506.17562v1",
        "authors": [
            "Haoxuan Che",
            "Haibo Jin",
            "Zhengrui Guo",
            "Yi Lin",
            "Cheng Jin",
            "Hao Chen"
        ],
        "submitted": "2025-06-21 03:13:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception",
        "abstract": "Traditional models of accent perception underestimate the role of gradient\nvariations in phonological features which listeners rely upon for their accent\njudgments. We investigate how pretrained representations from current\nself-supervised learning (SSL) models of speech encode phonological\nfeature-level variations that influence the perception of segmental accent. We\nfocus on three segments: the labiodental approximant, the rhotic tap, and the\nretroflex stop, which are uniformly produced in the English of native speakers\nof Hindi as well as other languages in the Indian sub-continent. We use the\nCSLU Foreign Accented English corpus (Lander, 2007) to extract, for these\nsegments, phonological feature probabilities using Phonet (V\\'asquez-Correa et\nal., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,\n2023) and WavLM (Chen et al., 2022) along with accent judgements by native\nspeakers of American English. Probing analyses show that accent strength is\nbest predicted by a subset of the segment's pretrained representation features,\nin which perceptually salient phonological features that contrast the expected\nAmerican English and realized non-native English segments are given prominent\nweighting. A multinomial logistic regression of pretrained representation-based\nsegment distances from American and Indian English baselines on accent ratings\nreveals strong associations between the odds of accent strength and distances\nfrom the baselines, in the expected directions. These results highlight the\nvalue of self-supervised speech representations for modeling accent perception\nusing interpretable phonological features.",
        "url": "http://arxiv.org/abs/2506.17542v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17542v1",
        "arxiv_id": "2506.17542v1",
        "authors": [
            "Nitin Venkateswaran",
            "Kevin Tang",
            "Ratree Wayland"
        ],
        "submitted": "2025-06-21 01:44:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning",
        "abstract": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.",
        "url": "http://arxiv.org/abs/2506.17533v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17533v1",
        "arxiv_id": "2506.17533v1",
        "authors": [
            "Yuanhao Wu",
            "Juntong Song",
            "Hanning Zhang",
            "Tong Zhang",
            "Cheng Niu"
        ],
        "submitted": "2025-06-21 01:11:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning",
        "abstract": "Our quality audit for three widely used public multilingual speech datasets -\nMozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some\nlanguages, these datasets suffer from significant quality issues. We believe\naddressing these issues will make these datasets more useful as training and\nevaluation sets, and improve downstream models. We divide these quality issues\ninto two categories: micro-level and macro-level. We find that macro-level\nissues are more prevalent in less institutionalized, often under-resourced\nlanguages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that\nhighlights the need for proactive language planning (e.g. orthography\nprescriptions, dialect boundary definition) and enhanced data quality control\nin the process of Automatic Speech Recognition (ASR) dataset creation. We\nconclude by proposing guidelines and recommendations to mitigate these issues\nin future dataset development, emphasizing the importance of sociolinguistic\nawareness in creating robust and reliable speech data resources.",
        "url": "http://arxiv.org/abs/2506.17525v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17525v1",
        "arxiv_id": "2506.17525v1",
        "authors": [
            "Mingfei Lau",
            "Qian Chen",
            "Yeming Fang",
            "Tingting Xu",
            "Tongzhou Chen",
            "Pavel Golik"
        ],
        "submitted": "2025-06-21 00:34:18",
        "source": "arxiv",
        "comment": "Accepted by ACL 2025 Main Conference"
    },
    {
        "title": "Mapping the Evolution of Research Contributions using KnoVo",
        "abstract": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework\ndesigned for quantifying and analyzing the evolution of research novelty in the\nscientific literature. Moving beyond traditional citation analysis, which\nprimarily measures impact, KnoVo determines a paper's novelty relative to both\nprior and subsequent work within its multilayered citation network. Given a\ntarget paper's abstract, KnoVo utilizes Large Language Models (LLMs) to\ndynamically extract dimensions of comparison (e.g., methodology, application,\ndataset). The target paper is then compared to related publications along these\nsame extracted dimensions. This comparative analysis, inspired by tournament\nselection, yields quantitative novelty scores reflecting the relative\nimprovement, equivalence, or inferiority of the target paper in specific\naspects. By aggregating these scores and visualizing their progression, for\ninstance, through dynamic evolution graphs and comparative radar charts, KnoVo\nfacilitates researchers not only to assess originality and identify similar\nwork, but also to track knowledge evolution along specific research dimensions,\nuncover research gaps, and explore cross-disciplinary connections. We\ndemonstrate these capabilities through a detailed analysis of 20 diverse papers\nfrom multiple scientific fields and report on the performance of various\nopen-source LLMs within the KnoVo framework.",
        "url": "http://arxiv.org/abs/2506.17508v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17508v1",
        "arxiv_id": "2506.17508v1",
        "authors": [
            "Sajratul Y. Rubaiat",
            "Syed N. Sakib",
            "Hasan M. Jamil"
        ],
        "submitted": "2025-06-20 23:17:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM",
        "abstract": "Modern GPUs evolve rapidly, yet production compilers still rely on\nhand-crafted register allocation heuristics that require substantial re-tuning\nfor each hardware generation. We introduce VeriLocc, a framework that combines\nlarge language models (LLMs) with formal compiler techniques to enable\ngeneralizable and verifiable register allocation across GPU architectures.\nVeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)\ninto target-specific register assignments, aided by static analysis for\ncross-architecture normalization and generalization and a verifier-guided\nregeneration loop to ensure correctness. Evaluated on matrix multiplication\n(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot\naccuracy and near-100% pass@100. Case study shows that VeriLocc discovers more\nperformant assignments than expert-tuned libraries, outperforming rocBLAS by\nover 10% in runtime.",
        "url": "http://arxiv.org/abs/2506.17506v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17506v1",
        "arxiv_id": "2506.17506v1",
        "authors": [
            "Lesheng Jin",
            "Zhenyuan Ruan",
            "Haohui Mai",
            "Jingbo Shang"
        ],
        "submitted": "2025-06-20 23:08:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PreQRAG -- Classify and Rewrite for Enhanced RAG",
        "abstract": "This paper presents the submission of the UDInfo team to the SIGIR 2025\nLiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)\narchitecture designed to improve retrieval and generation quality through\ntargeted question preprocessing. PreQRAG incorporates a pipeline that first\nclassifies each input question as either single-document or multi-document\ntype. For single-document questions, we employ question rewriting techniques to\nimprove retrieval precision and generation relevance. For multi-document\nquestions, we decompose complex queries into focused sub-questions that can be\nprocessed more effectively by downstream components. This classification and\nrewriting strategy improves the RAG performance. Experimental evaluation of the\nLiveRAG Challenge dataset demonstrates the effectiveness of our\nquestion-type-aware architecture, with PreQRAG achieving the preliminary second\nplace in Session 2 of the LiveRAG challenge.",
        "url": "http://arxiv.org/abs/2506.17493v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17493v1",
        "arxiv_id": "2506.17493v1",
        "authors": [
            "Damian Martinez",
            "Catalina Riano",
            "Hui Fang"
        ],
        "submitted": "2025-06-20 22:02:05",
        "source": "arxiv",
        "comment": "7 pages, SIGIR 2025 LiveRAG"
    },
    {
        "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems",
        "abstract": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.",
        "url": "http://arxiv.org/abs/2506.17467v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17467v1",
        "arxiv_id": "2506.17467v1",
        "authors": [
            "Weixin Liang"
        ],
        "submitted": "2025-06-20 20:15:09",
        "source": "arxiv",
        "comment": "Stanford CS PhD Dissertation"
    },
    {
        "title": "Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages",
        "abstract": "Automatic Speech Recognition (ASR) has reached impressive accuracy for\nhigh-resource languages, yet its utility in linguistic fieldwork remains\nlimited. Recordings collected in fieldwork contexts present unique challenges,\nincluding spontaneous speech, environmental noise, and severely constrained\ndatasets from under-documented languages. In this paper, we benchmark the\nperformance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five\ntypologically diverse low-resource languages with control of training data\nduration. Our findings show that MMS is best suited when extremely small\namounts of training data are available, whereas XLS-R shows parity performance\nonce training data exceed one hour. We provide linguistically grounded analysis\nfor further provide insights towards practical guidelines for field linguists,\nhighlighting reproducible ASR adaptation approaches to mitigate the\ntranscription bottleneck in language documentation.",
        "url": "http://arxiv.org/abs/2506.17459v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17459v1",
        "arxiv_id": "2506.17459v1",
        "authors": [
            "Siyu Liang",
            "Gina-Anne Levow"
        ],
        "submitted": "2025-06-20 19:59:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media",
        "abstract": "The use of large language models (LLMs) is becoming common in the context of\npolitical science, particularly in studies that analyse individuals use of\ndigital media. However, while previous research has demonstrated LLMs ability\nat labelling tasks, the effectiveness of using LLMs to classify political\ncontent (PC) from just URLs is not yet well explored. The work presented in\nthis article bridges this gap by evaluating whether LLMs can accurately\nidentify PC vs. non-PC from both the article text and the URLs from five\ncountries (France, Germany, Spain, the UK, and the US) and different languages.\nUsing cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we\nmeasure model performance to assess whether URL-level analysis can be a good\napproximation for full-text analysis of PC, even across different linguistic\nand national contexts. Model outputs are compared with human-labelled articles,\nas well as traditional supervised machine learning techniques, to set a\nbaseline of performance. Overall, our findings suggest the capacity of URLs to\nembed most of the news content, providing a vital perspective on accuracy-cost\nbalancing. We also account for contextual limitations and suggest\nmethodological recommendations to use LLMs within political science studies.",
        "url": "http://arxiv.org/abs/2506.17435v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17435v1",
        "arxiv_id": "2506.17435v1",
        "authors": [
            "Alberto Martinez-Serra",
            "Alejandro De La Fuente",
            "Nienke Viescher",
            "Ana S. Cardenal"
        ],
        "submitted": "2025-06-20 18:57:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making",
        "abstract": "As Large Language Models (LLMs) are integrated into safety-critical\napplications involving sequential decision-making in the real world, it is\nessential to know when to trust LLM decisions. Existing LLM Uncertainty\nQuantification (UQ) methods are primarily designed for single-turn\nquestion-answering formats, resulting in multi-step decision-making scenarios,\ne.g., LLM agentic system, being underexplored. In this paper, we introduce a\nprincipled, information-theoretic framework that decomposes LLM sequential\ndecision uncertainty into two parts: (i) internal uncertainty intrinsic to the\ncurrent decision, which is focused on existing UQ methods, and (ii) extrinsic\nuncertainty, a Mutual-Information (MI) quantity describing how much uncertainty\nshould be inherited from preceding decisions. We then propose UProp, an\nefficient and effective extrinsic uncertainty estimator that converts the\ndirect estimation of MI to the estimation of Pointwise Mutual Information (PMI)\nover multiple Trajectory-Dependent Decision Processes (TDPs). UProp is\nevaluated over extensive multi-step decision-making benchmarks, e.g.,\nAgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and\nDeepSeek-V3. Experimental results demonstrate that UProp significantly\noutperforms existing single-turn UQ baselines equipped with thoughtful\naggregation strategies. Moreover, we provide a comprehensive analysis of UProp,\nincluding sampling efficiency, potential applications, and intermediate\nuncertainty propagation, to demonstrate its effectiveness. Codes will be\navailable at https://github.com/jinhaoduan/UProp.",
        "url": "http://arxiv.org/abs/2506.17419v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17419v1",
        "arxiv_id": "2506.17419v1",
        "authors": [
            "Jinhao Duan",
            "James Diffenderfer",
            "Sandeep Madireddy",
            "Tianlong Chen",
            "Bhavya Kailkhura",
            "Kaidi Xu"
        ],
        "submitted": "2025-06-20 18:34:04",
        "source": "arxiv",
        "comment": "19 pages, 5 figures, 4 tables"
    },
    {
        "title": "Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study",
        "abstract": "Tutoring improves student achievement, but identifying and studying what\ntutoring actions are most associated with student learning at scale based on\naudio transcriptions is an open research problem. This present study\ninvestigates the feasibility and scalability of using generative AI to identify\nand evaluate specific tutor moves in real-life math tutoring. We analyze 50\nrandomly selected transcripts of college-student remote tutors assisting middle\nschool students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,\nGemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:\ndelivering effective praise and responding to student math errors. All models\nreliably detected relevant situations, for example, tutors providing praise to\nstudents (94-98% accuracy) and a student making a math error (82-88% accuracy)\nand effectively evaluated the tutors' adherence to tutoring best practices,\naligning closely with human judgments (83-89% and 73-77%, respectively). We\npropose a cost-effective prompting strategy and discuss practical implications\nfor using large language models to support scalable assessment in authentic\nsettings. This work further contributes LLM prompts to support reproducibility\nand research in AI-supported learning.",
        "url": "http://arxiv.org/abs/2506.17410v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17410v1",
        "arxiv_id": "2506.17410v1",
        "authors": [
            "Danielle R. Thomas",
            "Conrad Borchers",
            "Jionghao Lin",
            "Sanjit Kakarla",
            "Shambhavi Bhushan",
            "Erin Gatz",
            "Shivang Gupta",
            "Ralph Abboud",
            "Kenneth R. Koedinger"
        ],
        "submitted": "2025-06-20 18:13:33",
        "source": "arxiv",
        "comment": "Short research paper accepted at EC-TEL 2025"
    }
]