[
    {
        "title": "Reasoning-enhanced Query Understanding through Decomposition and Interpretation",
        "abstract": "Accurate inference of user intent is crucial for enhancing document retrieval\nin modern search engines. While large language models (LLMs) have made\nsignificant strides in this area, their effectiveness has predominantly been\nassessed with short, keyword-based queries. As AI-driven search evolves,\nlong-form queries with intricate intents are becoming more prevalent, yet they\nremain underexplored in the context of LLM-based query understanding (QU). To\nbridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query\nunderstanding through Decomposition and Interpretation. ReDI leverages the\nreasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)\nit breaks down complex queries into targeted sub-queries to accurately capture\nuser intent; (ii) it enriches each sub-query with detailed semantic\ninterpretations to improve the query-document matching; and (iii) it\nindependently retrieves documents for each sub-query and employs a fusion\nstrategy to aggregate the results for the final ranking. We compiled a\nlarge-scale dataset of real-world complex queries from a major search engine\nand distilled the query understanding capabilities of teacher models into\nsmaller models for practical application. Experiments on BRIGHT and BEIR\ndemonstrate that ReDI consistently surpasses strong baselines in both sparse\nand dense retrieval paradigms, affirming its effectiveness.",
        "url": "http://arxiv.org/abs/2509.06544v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06544v1",
        "arxiv_id": "2509.06544v1",
        "authors": [
            "Yunfei Zhong",
            "Jun Yang",
            "Yixing Fan",
            "Jiafeng Guo",
            "Lixin Su",
            "Maarten de Rijke",
            "Ruqing Zhang",
            "Dawei Yin",
            "Xueqi Cheng"
        ],
        "submitted": "2025-09-08 10:58:42",
        "source": "arxiv",
        "comment": null,
        "score": 18,
        "keyword_reasons": [
            "Found 'dense retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is extremely relevant to your research interests in Information Retrieval, particularly query understanding and ranking models. The proposed approach, ReDI, leverages large language models to decompose and interpret complex queries, which aligns with your focus on deep semantic understanding and real-time relevance optimization. The evaluation on BRIGHT and BEIR datasets further supports its relevance to your research areas."
    },
    {
        "title": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce",
        "abstract": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems.",
        "url": "http://arxiv.org/abs/2509.05570v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05570v1",
        "arxiv_id": "2509.05570v1",
        "authors": [
            "Yipeng Zhang",
            "Bowen Liu",
            "Xiaoshuang Zhang",
            "Aritra Mandal",
            "Zhe Wu",
            "Canran Xu"
        ],
        "submitted": "2025-09-06 02:54:13",
        "source": "arxiv",
        "comment": null,
        "score": 16,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your interests in Information Retrieval, particularly in query understanding and ranking models. The focus on e-commerce search and real-time relevance optimization also resonates with your background and expertise. However, the emphasis on Learning to Rank and user behavior modeling is not as prominent in this paper."
    },
    {
        "title": "Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs",
        "abstract": "Conversational recommender systems promise rich interactions for e-commerce,\nbut balancing exploration (clarifying user needs) and exploitation (making\nrecommendations) remains challenging, especially when deploying large language\nmodels (LLMs) with vast product catalogs. We address this challenge by modeling\nthe breadth of user interest via the entropy of retrieval score distributions.\nOur method uses a neural retriever to fetch relevant items for a user query and\ncomputes the entropy of the re-ranked scores to dynamically route the dialogue\npolicy: low-entropy (specific) queries trigger direct recommendations, whereas\nhigh-entropy (ambiguous) queries prompt exploratory questions. This simple yet\neffective strategy allows an LLM-driven agent to remain aware of an arbitrarily\nlarge catalog in real-time without bloating its context window.",
        "url": "http://arxiv.org/abs/2509.06185v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06185v1",
        "arxiv_id": "2509.06185v1",
        "authors": [
            "Firas Jarboui",
            "Issa Memari"
        ],
        "submitted": "2025-09-07 19:30:09",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your interests in Information Retrieval, particularly in the context of conversational recommender systems and large product catalogs. The use of entropy-driven dialogue policy and neural retrievers aligns with your focus on query understanding and ranking models. While the e-commerce domain is mentioned, the paper's emphasis on real-time relevance optimization and semantic understanding makes it a strong match for your research interests."
    },
    {
        "title": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking",
        "abstract": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy.",
        "url": "http://arxiv.org/abs/2509.06472v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06472v1",
        "arxiv_id": "2509.06472v1",
        "authors": [
            "Haoxiang Jin",
            "Ronghan Li",
            "Qiguang Miao",
            "Zixiang Lu"
        ],
        "submitted": "2025-09-08 09:37:20",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The authors propose a novel approach to leveraging Large Language Models (LLMs) for post-retrieval confidence and dynamic retrieval, which aligns with your focus on deep semantic understanding and real-time relevance optimization. While the paper's primary focus is on LLMs and NLP, its contributions to retrieval and reranking make it a useful read for your IR research."
    },
    {
        "title": "AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation",
        "abstract": "Spotify has recently introduced audiobooks as part of its catalog,\ncomplementing its music and podcast offering. Search is often the first entry\npoint for users to access new items, and an important goal for Spotify is to\nsupport users in the exploration of the audiobook catalog. More specifically,\nwe would like to enable users without a specific item in mind to broadly search\nby topic, genre, story tropes, decade, and discover audiobooks, authors and\npublishers they may like. To do this, we need to 1) inspire users to type more\nexploratory queries for audiobooks and 2) augment our retrieval systems to\nbetter deal with exploratory audiobook queries. This is challenging in a\ncold-start scenario, where we have a retrievabiliy bias due to the little\namount of user interactions with audiobooks compared to previously available\nitems such as music and podcast content. To address this, we propose\nAudioBoost, a system to boost audiobook retrievability in Spotify's Search via\nsynthetic query generation. AudioBoost leverages Large Language Models (LLMs)\nto generate synthetic queries conditioned on audiobook metadata. The synthetic\nqueries are indexed both in the Query AutoComplete (QAC) and in the Search\nRetrieval engine to improve query formulation and retrieval at the same time.\nWe show through offline evaluation that synthetic queries increase\nretrievability and are of high quality. Moreover, results from an online A/B\ntest show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in\naudiobook clicks, and +1.82% in audiobook exploratory query completions.",
        "url": "http://arxiv.org/abs/2509.06452v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06452v1",
        "arxiv_id": "2509.06452v1",
        "authors": [
            "Enrico Palumbo",
            "Gustavo Penha",
            "Alva Liu",
            "Marcus Eltscheminov",
            "Jefferson Carvalho dos Santos",
            "Alice Wang",
            "Hugues Bouchard",
            "Humberto Jesús Corona Pampin",
            "Michelle Tran Luu"
        ],
        "submitted": "2025-09-08 08:57:03",
        "source": "arxiv",
        "comment": "EARL Workshop @ RecSys25",
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of synthetic query generation and Large Language Models to improve query formulation and retrieval aligns with your focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods",
        "abstract": "Language fairness in multilingual information retrieval (MLIR) systems is\ncrucial for ensuring equitable access to information across diverse languages.\nThis paper sheds light on the issue, based on the assumption that queries in\ndifferent languages, but with identical semantics, should yield equivalent\nranking lists when retrieving on the same multilingual documents. We evaluate\nthe degree of fairness using both traditional retrieval methods, and a DPR\nneural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a\nnovel loss designed to mitigate language biases in neural MLIR approaches. Our\nanalysis exposes intrinsic language biases in current MLIR technologies, with\nnotable disparities across the retrieval methods, and the effectiveness of\nLaKDA in enhancing language fairness.",
        "url": "http://arxiv.org/abs/2509.06195v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06195v1",
        "arxiv_id": "2509.06195v1",
        "authors": [
            "Jinrui Yang",
            "Fan Jiang",
            "Timothy Baldwin"
        ],
        "submitted": "2025-09-07 20:10:49",
        "source": "arxiv",
        "comment": "Accepted at EMNLP MRL 2024",
        "score": 12,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval, specifically addressing language bias in multilingual systems, which is a critical aspect of query understanding and ranking models. The use of neural rankers and a novel loss function to mitigate language biases aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the focus on multilingual systems and language fairness is somewhat specific, but still falls within your broader IR and NLP interests."
    },
    {
        "title": "UniSearch: Rethinking Search System with a Unified Generative Architecture",
        "abstract": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications.",
        "url": "http://arxiv.org/abs/2509.06887v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06887v1",
        "arxiv_id": "2509.06887v1",
        "authors": [
            "Jiahui Chen",
            "Xiaoze Jiang",
            "Zhibo Wang",
            "Quanzhi Zhu",
            "Junyao Zhao",
            "Feng Hu",
            "Kang Pan",
            "Ao Xie",
            "Maohua Pei",
            "Zhiheng Qin",
            "Hongjing Zhang",
            "Zhixin Zhai",
            "Xiaobo Guo",
            "Runbin Zhou",
            "Kefeng Wang",
            "Mingyang Geng",
            "Cheng Chen",
            "Jingshan Lv",
            "Yupeng Huang",
            "Xiao Liang",
            "Han Li"
        ],
        "submitted": "2025-09-08 17:08:26",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a unified generative search framework, UniSearch, which integrates a search generator and a video encoder to improve representation quality and generation accuracy. While its focus is on video search, the use of a unified architecture and the introduction of Search Preference Optimization (SPO) aligns with the user's interests in information retrieval and ranking models. However, the specific application to video search and the use of a video encoder may limit its direct relevance to the user's core research themes."
    },
    {
        "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval",
        "abstract": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.",
        "url": "http://arxiv.org/abs/2509.06650v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06650v1",
        "arxiv_id": "2509.06650v1",
        "authors": [
            "Hao Lin",
            "Peitong Xie",
            "Jingxue Chen",
            "Jie Lin",
            "Qingkun Tang",
            "Qianchun Lu"
        ],
        "submitted": "2025-09-08 13:04:07",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel method for optimizing Retrieval-Augmented Generation (RAG) systems, focusing on domain-aware retrieval and query enhancement. While it doesn't directly address ranking models or user behavior modeling, it's clearly related to information retrieval and query understanding, making it a useful contribution to the field."
    },
    {
        "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)",
        "abstract": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems.",
        "url": "http://arxiv.org/abs/2509.05505v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05505v1",
        "arxiv_id": "2509.05505v1",
        "authors": [
            "Mansi Garg",
            "Lee-Chi Wang",
            "Bhavesh Ghanchi",
            "Sanjana Dumpala",
            "Shreyash Kakde",
            "Yen Chih Chen"
        ],
        "submitted": "2025-09-05 21:29:52",
        "source": "arxiv",
        "comment": "10 pages, 6 figures, 3 tables",
        "score": 11,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of biomedical literature and question answering. However, it focuses more on the biomedical domain and does not directly address your core areas of interest in e-commerce, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Beamforming-LLM: What, Where and When Did I Miss?",
        "abstract": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing.",
        "url": "http://arxiv.org/abs/2509.06221v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06221v1",
        "arxiv_id": "2509.06221v1",
        "authors": [
            "Vishal Choudhari"
        ],
        "submitted": "2025-09-07 21:52:26",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel application of spatial audio capture and retrieval-augmented generation in a multi-speaker environment. While it touches on natural language queries and summarization, its primary focus is on auditory memory systems, which is somewhat related to information retrieval but not a central match to your research interests."
    },
    {
        "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too",
        "abstract": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.",
        "url": "http://arxiv.org/abs/2509.05440v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05440v1",
        "arxiv_id": "2509.05440v1",
        "authors": [
            "Logan Lawrence",
            "Ashton Williamson",
            "Alexander Shelton"
        ],
        "submitted": "2025-09-05 18:48:34",
        "source": "arxiv",
        "comment": "12 pages, 18 tables, 1 figure",
        "score": 9,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on Natural Language Generation (NLG) evaluation. While it involves large-language models, the context is evaluation rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's broader interests in NLP and data mining is limited."
    },
    {
        "title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research",
        "abstract": "Chemical and materials research has traditionally relied heavily on knowledge\nnarrative, with progress often driven by language-based descriptions of\nprinciples, mechanisms, and experimental experiences, rather than tables,\nlimiting what conventional databases and ML can exploit. We present a\nlanguage-native database for boron nitride nanosheet (BNNS) polymer thermally\nconductive composites that captures lightly structured information from papers\nacross preparation, characterization, theory-computation, and mechanistic\nreasoning, with evidence-linked snippets. Records are organized in a\nheterogeneous database and queried via composite retrieval with semantics, key\nwords and value filters. The system can synthesizes literature into accurate,\nverifiable, and expert style guidance. This substrate enables high fidelity\nefficient Retrieval Augmented Generation (RAG) and tool augmented agents to\ninterleave retrieval with reasoning and deliver actionable SOP. The framework\nsupplies the language rich foundation required for LLM-driven materials\ndiscovery.",
        "url": "http://arxiv.org/abs/2509.06093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06093v1",
        "arxiv_id": "2509.06093v1",
        "authors": [
            "Yuze Liu",
            "Zhaoyuan Zhang",
            "Xiangsheng Zeng",
            "Yihe Zhang",
            "Leping Yu",
            "Lejia Wang",
            "Xi Yu"
        ],
        "submitted": "2025-09-07 15:15:55",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on a specific domain (chemical and materials research) and a novel database design, which doesn't align with your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does mention retrieval and LLMs, the context is quite different from your areas of focus."
    },
    {
        "title": "DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers",
        "abstract": "We present DISTRIBUTEDANN, a distributed vector search service that makes it\npossible to search over a single 50 billion vector graph index spread across\nover a thousand machines that offers 26ms median query latency and processes\nover 100,000 queries per second. This is 6x more efficient than existing\npartitioning and routing strategies that route the vector query to a subset of\npartitions in a scale out vector search system. DISTRIBUTEDANN is built using\ntwo well-understood components: a distributed key-value store and an in-memory\nANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for\nserving the Bing search engine, and we share our experience from making this\ntransition.",
        "url": "http://arxiv.org/abs/2509.06046v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06046v1",
        "arxiv_id": "2509.06046v1",
        "authors": [
            "Philip Adams",
            "Menghao Li",
            "Shi Zhang",
            "Li Tan",
            "Qi Chen",
            "Mingqin Li",
            "Zengzhong Li",
            "Knut Risvik",
            "Harsha Vardhan Simhadri"
        ],
        "submitted": "2025-09-07 13:13:02",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on distributed vector search and scalability, which is not directly related to the user's core research themes in Information Retrieval, query understanding, ranking models, and user behavior modeling. While it touches on search technologies, it's more about system architecture and scalability rather than semantic understanding or ranking models."
    },
    {
        "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding",
        "abstract": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.",
        "url": "http://arxiv.org/abs/2509.05657v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05657v1",
        "arxiv_id": "2509.05657v1",
        "authors": [
            "Yuxuan Hu",
            "Jihao Liu",
            "Ke Wang",
            "Jinliang Zhen",
            "Weikang Shi",
            "Manyuan Zhang",
            "Qi Dou",
            "Rui Liu",
            "Aojun Zhou",
            "Hongsheng Li"
        ],
        "submitted": "2025-09-06 09:26:39",
        "source": "arxiv",
        "comment": "EMNLP2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores Neural Architecture Search (NAS) using Large Language Models (LLMs), which is somewhat related to Information Retrieval, but the focus is on architecture search rather than query understanding or ranking models. While it involves deep semantic understanding, the application domain is different from e-commerce and information retrieval."
    },
    {
        "title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics",
        "abstract": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution.",
        "url": "http://arxiv.org/abs/2509.05617v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05617v1",
        "arxiv_id": "2509.05617v1",
        "authors": [
            "Shay Dahary",
            "Avi Edana",
            "Alexander Apartsin",
            "Yehudit Aperstein"
        ],
        "submitted": "2025-09-06 06:28:28",
        "source": "arxiv",
        "comment": "5 pages, 2 figures",
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies, as it focuses on emotion estimation in song lyrics using Natural Language Processing techniques. While it touches on music information retrieval, it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization in IR."
    },
    {
        "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents",
        "abstract": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.",
        "url": "http://arxiv.org/abs/2509.06917v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06917v1",
        "arxiv_id": "2509.06917v1",
        "authors": [
            "Jiacheng Miao",
            "Joe R. Davis",
            "Jonathan K. Pritchard",
            "James Zou"
        ],
        "submitted": "2025-09-08 17:28:42",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper introduces a framework for converting research papers into AI agents, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves natural language and knowledge dissemination, the focus is on AI agents and knowledge sharing rather than query understanding, ranking models, or real-time relevance optimization."
    },
    {
        "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
        "abstract": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.",
        "url": "http://arxiv.org/abs/2509.06501v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06501v1",
        "arxiv_id": "2509.06501v1",
        "authors": [
            "Junteng Liu",
            "Yunji Li",
            "Chi Zhang",
            "Jingyang Li",
            "Aili Chen",
            "Ke Ji",
            "Weiyu Cheng",
            "Zijia Wu",
            "Chengyu Du",
            "Qidi Xu",
            "Jiayuan Song",
            "Zhengmao Zhu",
            "Wenhu Chen",
            "Pengyu Zhao",
            "Junxian He"
        ],
        "submitted": "2025-09-08 10:07:03",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores web browsing capabilities and large language models for information seeking, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on web agents and long-horizon problem solving is not a central match to your primary research themes, but still relevant and potentially useful for your broader interests in NLP and data mining."
    },
    {
        "title": "Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints",
        "abstract": "The widespread deployment of LLMs across enterprise services has created a\ncritical security blind spot. Organizations operate multiple LLM services\nhandling billions of queries daily, yet regulatory compliance boundaries\nprevent these services from sharing threat intelligence about prompt injection\nattacks, the top security risk for LLMs. When an attack is detected in one\nservice, the same threat may persist undetected in others for months, as\nprivacy regulations prohibit sharing user prompts across compliance boundaries.\n  We present BinaryShield, the first privacy-preserving threat intelligence\nsystem that enables secure sharing of attack fingerprints across compliance\nboundaries. BinaryShield transforms suspicious prompts through a unique\npipeline combining PII redaction, semantic embedding, binary quantization, and\nrandomized response mechanism to potentially generate non-invertible\nfingerprints that preserve attack patterns while providing privacy. Our\nevaluations demonstrate that BinaryShield achieves an F1-score of 0.94,\nsignificantly outperforming SimHash (0.77), the privacy-preserving baseline,\nwhile achieving 64x storage reduction and 38x faster similarity search compared\nto dense embeddings.",
        "url": "http://arxiv.org/abs/2509.05608v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05608v1",
        "arxiv_id": "2509.05608v1",
        "authors": [
            "Waris Gill",
            "Natalie Isak",
            "Matthew Dressman"
        ],
        "submitted": "2025-09-06 05:57:20",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. The focus on threat intelligence and security in LLM services is outside the user's primary areas of interest."
    },
    {
        "title": "Modelling Intertextuality with N-gram Embeddings",
        "abstract": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships.",
        "url": "http://arxiv.org/abs/2509.06637v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06637v1",
        "arxiv_id": "2509.06637v1",
        "authors": [
            "Yi Xing"
        ],
        "submitted": "2025-09-08 12:54:38",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on literary studies and intertextuality, which is unrelated to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP techniques (n-gram embeddings), the context and application are not aligned with your interests."
    },
    {
        "title": "Compare: A Framework for Scientific Comparisons",
        "abstract": "Navigating the vast and rapidly increasing sea of academic publications to\nidentify institutional synergies, benchmark research contributions and pinpoint\nkey research contributions has become an increasingly daunting task, especially\nwith the current exponential increase in new publications. Existing tools\nprovide useful overviews or single-document insights, but none supports\nstructured, qualitative comparisons across institutions or publications.\n  To address this, we demonstrate Compare, a novel framework that tackles this\nchallenge by enabling sophisticated long-context comparisons of scientific\ncontributions. Compare empowers users to explore and analyze research overlaps\nand differences at both the institutional and publication granularity, all\ndriven by user-defined questions and automatic retrieval over online resources.\nFor this we leverage on Retrieval-Augmented Generation over evolving data\nsources to foster long context knowledge synthesis. Unlike traditional\nscientometric tools, Compare goes beyond quantitative indicators by providing\nqualitative, citation-supported comparisons.",
        "url": "http://arxiv.org/abs/2509.06412v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06412v1",
        "arxiv_id": "2509.06412v1",
        "authors": [
            "Moritz Staudinger",
            "Wojciech Kusa",
            "Matteo Cancellieri",
            "David Pride",
            "Petr Knoth",
            "Allan Hanbury"
        ],
        "submitted": "2025-09-08 08:05:26",
        "source": "arxiv",
        "comment": "Accepted at CIKM 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a scientific comparison framework, which, although related to information retrieval, does not directly align with your core research themes of query understanding, ranking models, and user behavior modeling. The paper's emphasis on scientometrics and citation-supported comparisons also diverges from your primary focus on deep semantic understanding and real-time relevance optimization in the e-commerce domain."
    },
    {
        "title": "A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives",
        "abstract": "Recommender systems have generated tremendous value for both users and\nbusinesses, drawing significant attention from academia and industry alike.\nHowever, due to practical constraints, academic research remains largely\nconfined to offline dataset optimizations, lacking access to real user data and\nlarge-scale recommendation platforms. This limitation reduces practical\nrelevance, slows technological progress, and hampers a full understanding of\nthe key challenges in recommender systems. In this survey, we provide a\nsystematic review of industrial recommender systems and contrast them with\ntheir academic counterparts. We highlight key differences in data scale,\nreal-time requirements, and evaluation methodologies, and we summarize major\nreal-world recommendation scenarios along with their associated challenges. We\nthen examine how industry practitioners address these challenges in\nTransaction-Oriented Recommender Systems and Content-Oriented Recommender\nSystems, a new classification grounded in item characteristics and\nrecommendation objectives. Finally, we outline promising research directions,\nincluding the often-overlooked role of user decision-making, the integration of\neconomic and psychological theories, and concrete suggestions for advancing\nacademic research. Our goal is to enhance academia's understanding of practical\nrecommender systems, bridge the growing development gap, and foster stronger\ncollaboration between industry and academia.",
        "url": "http://arxiv.org/abs/2509.06002v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06002v1",
        "arxiv_id": "2509.06002v1",
        "authors": [
            "Kuan Zou",
            "Aixin Sun"
        ],
        "submitted": "2025-09-07 10:29:41",
        "source": "arxiv",
        "comment": "Working paper",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on recommender systems, which is a related but secondary interest of yours. While it touches on real-world applications, it does not delve into information retrieval, query understanding, or ranking models, which are your core research themes."
    },
    {
        "title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning",
        "abstract": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.05635v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05635v1",
        "arxiv_id": "2509.05635v1",
        "authors": [
            "Liang Zhang",
            "Yuan Li",
            "Shijie Zhang",
            "Zheng Zhang",
            "Xitong Li"
        ],
        "submitted": "2025-09-06 07:41:47",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores query intent detection in conversational systems, leveraging relation-aware prompt learning. While it touches on query understanding, it is more focused on intent detection and conversational systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the specific application and methodology differ from your primary focus on e-commerce and real-time relevance optimization."
    },
    {
        "title": "The Majority is not always right: RL training for solution aggregation",
        "abstract": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.",
        "url": "http://arxiv.org/abs/2509.06870v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06870v1",
        "arxiv_id": "2509.06870v1",
        "authors": [
            "Wenting Zhao",
            "Pranjal Aggarwal",
            "Swarnadeep Saha",
            "Asli Celikyilmaz",
            "Jason Weston",
            "Ilia Kulikov"
        ],
        "submitted": "2025-09-08 16:39:38",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores solution aggregation using reinforcement learning, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on large language models and solution aggregation does not directly align with the user's primary interests in IR, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation",
        "abstract": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.",
        "url": "http://arxiv.org/abs/2509.06631v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06631v1",
        "arxiv_id": "2509.06631v1",
        "authors": [
            "Özgür Uğur",
            "Musa Yılmaz",
            "Esra Şavirdi",
            "Özay Ezerceli",
            "Mahmut El Huseyni",
            "Selva Taş",
            "Reyhan Bayraktar"
        ],
        "submitted": "2025-09-08 12:51:40",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores Retrieval-Augmented Generation (RAG) systems, which is a related topic to Information Retrieval. However, the focus on Large Language Models and structured output generation is more aligned with NLP, making it somewhat relevant to your research interests."
    },
    {
        "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula",
        "abstract": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.",
        "url": "http://arxiv.org/abs/2509.05867v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05867v1",
        "arxiv_id": "2509.05867v1",
        "authors": [
            "ZiXuan Zhang",
            "Bowen Hao",
            "Yingjie Li",
            "Hongzhi Yin"
        ],
        "submitted": "2025-09-06 23:48:46",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a specific domain (Traditional Chinese Medicine) and proposes a framework for generating summaries and explanations of TCM formulas, which is not directly related to the user's core research themes in Information Retrieval and Search technologies, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems",
        "abstract": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field.",
        "url": "http://arxiv.org/abs/2509.05716v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05716v1",
        "arxiv_id": "2509.05716v1",
        "authors": [
            "Manoj Madushanka Perera",
            "Adnan Mahmood",
            "Kasun Eranda Wijethilake",
            "Fahmida Islam",
            "Maryam Tahermazandarani",
            "Quan Z. Sheng"
        ],
        "submitted": "2025-09-06 13:38:03",
        "source": "arxiv",
        "comment": "42 pages, 12 figures, 4 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and conversational systems, but it does not directly focus on information retrieval, query understanding, or ranking models. The paper's emphasis on conversational question answering systems and large language models may be of interest, but it is not a central match for your core research themes."
    },
    {
        "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning",
        "abstract": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.",
        "url": "http://arxiv.org/abs/2509.06888v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06888v1",
        "arxiv_id": "2509.06888v1",
        "authors": [
            "Marc Marone",
            "Orion Weller",
            "William Fleshman",
            "Eugene Yang",
            "Dawn Lawrie",
            "Benjamin Van Durme"
        ],
        "submitted": "2025-09-08 17:08:42",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a novel multilingual encoder model, mmBERT, which shows promise in classification and retrieval tasks. While it touches on retrieval, the focus is on classification and language learning, which is somewhat related to your interests in Information Retrieval and NLP. However, the emphasis on classification and multilingual models is not a central match for your research themes."
    },
    {
        "title": "A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs",
        "abstract": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.",
        "url": "http://arxiv.org/abs/2509.06813v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06813v1",
        "arxiv_id": "2509.06813v1",
        "authors": [
            "Max Malyi",
            "Jonathan Shek",
            "Alasdair McDonald",
            "Andre Biscaya"
        ],
        "submitted": "2025-09-08 15:48:17",
        "source": "arxiv",
        "comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves Large Language Models, the focus is on a specific industrial application (wind turbine maintenance logs) and does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem",
        "abstract": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1",
        "url": "http://arxiv.org/abs/2509.06809v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06809v1",
        "arxiv_id": "2509.06809v1",
        "authors": [
            "Valentin Quesnel",
            "Damien Sileo"
        ],
        "submitted": "2025-09-08 15:43:29",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models (LLMs) and automated theorem proving, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep semantic understanding, the context is specific to mathematical reasoning and theorem proving, making it somewhat tangential to your primary focus."
    },
    {
        "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML",
        "abstract": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
        "url": "http://arxiv.org/abs/2509.06806v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06806v1",
        "arxiv_id": "2509.06806v1",
        "authors": [
            "Haoyu Dong",
            "Pengkun Zhang",
            "Mingzhe Lu",
            "Yanzhen Shen",
            "Guolin Ke"
        ],
        "submitted": "2025-09-08 15:38:31",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on continued pretraining of language models for in-context machine learning tasks, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the primary goal is to improve their ability to learn from in-context examples, which is more relevant to the NLP community than your specific focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models",
        "abstract": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting.",
        "url": "http://arxiv.org/abs/2509.06100v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06100v1",
        "arxiv_id": "2509.06100v1",
        "authors": [
            "Kefan Cao",
            "Shuaicheng Wu"
        ],
        "submitted": "2025-09-07 15:29:46",
        "source": "arxiv",
        "comment": "13 pages, 3 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on continual learning of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves deep learning and optimization, the context is not aligned with the user's primary research interests."
    },
    {
        "title": "Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models",
        "abstract": "Complementary recommendations play a crucial role in e-commerce by enhancing\nuser experience through suggestions of compatible items. Accurate\nclassification of complementary item relationships requires reliable labels,\nbut their creation presents a dilemma. Behavior-based labels are widely used\nbecause they can be easily generated from interaction logs; however, they often\ncontain significant noise and lack reliability. While function-based labels\n(FBLs) provide high-quality definitions of complementary relationships by\ncarefully articulating them based on item functions, their reliance on costly\nmanual annotation severely limits a model's ability to generalize to diverse\nitems. To resolve this trade-off, we propose Knowledge-Augmented Relation\nLearning (KARL), a framework that strategically fuses active learning with\nlarge language models (LLMs). KARL efficiently expands a high-quality FBL\ndataset at a low cost by selectively sampling data points that the classifier\nfinds the most difficult and uses the label extension of the LLM. Our\nexperiments showed that in out-of-distribution (OOD) settings, an unexplored\nitem feature space, KARL improved the baseline accuracy by up to 37%. In\ncontrast, in in-distribution (ID) settings, the learned item feature space, the\nimprovement was less than 0.5%, with prolonged learning could degrade accuracy.\nThese contrasting results are due to the data diversity driven by KARL's\nknowledge expansion, suggesting the need for a dynamic sampling strategy that\nadjusts diversity based on the prediction context (ID or OOD).",
        "url": "http://arxiv.org/abs/2509.05564v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05564v1",
        "arxiv_id": "2509.05564v1",
        "authors": [
            "Chihiro Yamasaki",
            "Kai Sugahara",
            "Kazushi Okamoto"
        ],
        "submitted": "2025-09-06 02:20:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, specifically complementary recommendations, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the emphasis on large language models and knowledge augmentation is not a central match for the user's core research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Calibrated Recommendations with Contextual Bandits",
        "abstract": "Spotify's Home page features a variety of content types, including music,\npodcasts, and audiobooks. However, historical data is heavily skewed toward\nmusic, making it challenging to deliver a balanced and personalized content\nmix. Moreover, users' preference towards different content types may vary\ndepending on the time of day, the day of week, or even the device they use. We\npropose a calibration method that leverages contextual bandits to dynamically\nlearn each user's optimal content type distribution based on their context and\npreferences. Unlike traditional calibration methods that rely on historical\naverages, our approach boosts engagement by adapting to how users interests in\ndifferent content types varies across contexts. Both offline and online results\ndemonstrate improved precision and user engagement with the Spotify Home page,\nin particular with under-represented content types such as podcasts.",
        "url": "http://arxiv.org/abs/2509.05460v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05460v1",
        "arxiv_id": "2509.05460v1",
        "authors": [
            "Diego Feijer",
            "Himan Abdollahpouri",
            "Sanket Gupta",
            "Alexander Clare",
            "Yuxiao Wen",
            "Todd Wasson",
            "Maria Dimakopoulou",
            "Zahra Nazari",
            "Kyle Kretschman",
            "Mounia Lalmas"
        ],
        "submitted": "2025-09-05 19:28:08",
        "source": "arxiv",
        "comment": "Accepted at ACM RecSys '25, CONSEQUENCES workshop",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores contextual bandits for personalized content recommendations, which is somewhat related to information retrieval and user behavior modeling. However, it focuses on recommender systems, which is not the primary focus of your research interests. The use of contextual bandits is an interesting aspect, but it doesn't directly relate to query understanding, ranking models, or deep semantic understanding."
    },
    {
        "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts",
        "abstract": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans.",
        "url": "http://arxiv.org/abs/2509.06952v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06952v1",
        "arxiv_id": "2509.06952v1",
        "authors": [
            "Linlu Qiu",
            "Cedegao E. Zhang",
            "Joshua B. Tenenbaum",
            "Yoon Kim",
            "Roger P. Levy"
        ],
        "submitted": "2025-09-08 17:59:32",
        "source": "arxiv",
        "comment": "EMNLP 2025 (Main)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores pragmatic reasoning in language models, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on language comprehension and production, rather than search or ranking, limits its direct relevance to your core research interests. The study's findings on language understanding and social reasoning may be of interest, but it does not directly address your primary areas of focus."
    },
    {
        "title": "Outcome-based Exploration for LLM Reasoning",
        "abstract": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment.",
        "url": "http://arxiv.org/abs/2509.06941v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06941v1",
        "arxiv_id": "2509.06941v1",
        "authors": [
            "Yuda Song",
            "Julia Kempe",
            "Remi Munos"
        ],
        "submitted": "2025-09-08 17:52:56",
        "source": "arxiv",
        "comment": "26 pages, 11 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores reinforcement learning methods for improving large language model reasoning abilities, but it does not directly relate to information retrieval, search technologies, or user behavior modeling. While it touches on the importance of diversity in real-world performance, the focus is on language model reasoning rather than query understanding or ranking models."
    },
    {
        "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification",
        "abstract": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.",
        "url": "http://arxiv.org/abs/2509.06902v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06902v1",
        "arxiv_id": "2509.06902v1",
        "authors": [
            "Aivin V. Solatorio"
        ],
        "submitted": "2025-09-08 17:20:16",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a protocol for trustworthy numeric answers from Large Language Models (LLMs) through claim verification, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves NLP, the context is more about ensuring numeric fidelity rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet",
        "abstract": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge",
        "url": "http://arxiv.org/abs/2509.06861v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06861v1",
        "arxiv_id": "2509.06861v1",
        "authors": [
            "James Xu Zhao",
            "Bryan Hooi",
            "See-Kiong Ng"
        ],
        "submitted": "2025-09-08 16:28:25",
        "source": "arxiv",
        "comment": "20 pages, 4 figures, 6 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on test-time scaling in reasoning models, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on knowledge-intensive tasks, the primary focus is on reasoning models and their limitations, rather than search technologies or user behavior modeling."
    },
    {
        "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models",
        "abstract": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.",
        "url": "http://arxiv.org/abs/2509.06596v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06596v1",
        "arxiv_id": "2509.06596v1",
        "authors": [
            "Xin Tong",
            "Zhi Lin",
            "Jingya Wang",
            "Bo Jin"
        ],
        "submitted": "2025-09-08 12:06:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models and hallucination mitigation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific topic of hallucination mitigation in LLMs is not a central match for your research themes."
    },
    {
        "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing",
        "abstract": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.",
        "url": "http://arxiv.org/abs/2509.06552v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06552v1",
        "arxiv_id": "2509.06552v1",
        "authors": [
            "Zheqi Lv",
            "Wenqiao Zhang",
            "Kairui Fu",
            "Qi Tian",
            "Shengyu Zhang",
            "Jiajie Su",
            "Jingyuan Chen",
            "Kun Kuang",
            "Fei Wu"
        ],
        "submitted": "2025-09-08 11:06:50",
        "source": "arxiv",
        "comment": "Published on MM'25: Proceedings of the 33rd ACM International\n  Conference on Multimedia",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on addressing real-time data distribution shifts in on-device models using a novel personalized method called Persona. While it involves some aspects of model adaptation and optimization, it does not directly relate to the user's core research interests in Information Retrieval, query understanding, ranking models, or user behavior modeling. The paper's emphasis on on-device models and real-time adaptation does not align with the user's background in e-commerce or interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection",
        "abstract": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines.",
        "url": "http://arxiv.org/abs/2509.06524v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06524v1",
        "arxiv_id": "2509.06524v1",
        "authors": [
            "Jian Wu",
            "Hang Yu",
            "Bingchang Liu",
            "Wenjie Yang",
            "Peng Di",
            "Jianguo Li",
            "Yue Zhang"
        ],
        "submitted": "2025-09-08 10:30:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a novel approach for domain-specific data selection using pre-trained large language models, which is somewhat related to information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. While it involves deep semantic understanding, its focus on data selection and classification is distinct from the user's primary research themes."
    },
    {
        "title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models",
        "abstract": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy.",
        "url": "http://arxiv.org/abs/2509.06415v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06415v1",
        "arxiv_id": "2509.06415v1",
        "authors": [
            "Jaemin Son",
            "Sujin Choi",
            "Inyong Yun"
        ],
        "submitted": "2025-09-08 08:12:26",
        "source": "arxiv",
        "comment": "Submitted to ICASSP 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on vision-language models for document understanding, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves document understanding, the approach is primarily visual and does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "PL-CA: A Parametric Legal Case Augmentation Framework",
        "abstract": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix.",
        "url": "http://arxiv.org/abs/2509.06356v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06356v1",
        "arxiv_id": "2509.06356v1",
        "authors": [
            "Ao Chang",
            "Yubo Chen",
            "Jun Zhao"
        ],
        "submitted": "2025-09-08 06:08:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a legal case augmentation framework, which is not directly related to information retrieval, search technologies, or natural language processing. While it does involve ranking models and data augmentation, the context is specific to the judicial domain and does not align with the user's core research themes."
    },
    {
        "title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment",
        "abstract": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation.",
        "url": "http://arxiv.org/abs/2509.06200v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06200v1",
        "arxiv_id": "2509.06200v1",
        "authors": [
            "Omar Walid",
            "Mohamed T. Younes",
            "Khaled Shaban",
            "Mai Hassan",
            "Ali Hamdi"
        ],
        "submitted": "2025-09-07 20:27:58",
        "source": "arxiv",
        "comment": "Accepted in AICCSA 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recruitment automation and resume parsing, which is somewhat related to the e-commerce domain. However, the use of Large Language Models (LLMs) and ensemble finetuning is more aligned with NLP, but not directly related to query understanding, ranking models, or user behavior modeling in IR, which are the primary research interests."
    },
    {
        "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis",
        "abstract": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS.",
        "url": "http://arxiv.org/abs/2509.06074v1",
        "pdf_url": "http://arxiv.org/pdf/2509.06074v1",
        "arxiv_id": "2509.06074v1",
        "authors": [
            "Zhenqi Jia",
            "Rui Liu",
            "Berrak Sisman",
            "Haizhou Li"
        ],
        "submitted": "2025-09-07 14:32:29",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on Conversational Speech Synthesis and multimodal dialogue history, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling",
        "abstract": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths.",
        "url": "http://arxiv.org/abs/2509.05908v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05908v1",
        "arxiv_id": "2509.05908v1",
        "authors": [
            "Yue Gu",
            "Zhihao Du",
            "Ying Shi",
            "Shiliang Zhang",
            "Qian Chen",
            "Jiqing Han"
        ],
        "submitted": "2025-09-07 03:46:59",
        "source": "arxiv",
        "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:\n  10.1109/TASLPRO.2025.3606198",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the robustness of contextual ASR models, which is not directly related to information retrieval or search technologies. While it involves semantic correlation modeling, the context is speech recognition rather than query understanding or ranking models, making it less relevant to the user's core research themes."
    },
    {
        "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues",
        "abstract": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes.",
        "url": "http://arxiv.org/abs/2509.05882v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05882v1",
        "arxiv_id": "2509.05882v1",
        "authors": [
            "Abhijnan Nath",
            "Carine Graff",
            "Nikhil Krishnaswamy"
        ],
        "submitted": "2025-09-07 00:58:10",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it primarily focuses on the alignment of LLMs in collaborative dialogues, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it does involve some aspects of user behavior modeling, it is not a central match for your research interests."
    },
    {
        "title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing",
        "abstract": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains.",
        "url": "http://arxiv.org/abs/2509.05729v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05729v1",
        "arxiv_id": "2509.05729v1",
        "authors": [
            "Charles M. Varmantchaonala",
            "Niclas GÖtting",
            "Nils-Erik SchÜtte",
            "Jean Louis E. K. Fendji",
            "Christopher Gies"
        ],
        "submitted": "2025-09-06 14:25:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on Quantum Natural Language Processing (QNLP), which is not a core area of your research interests. While it does involve NLP, the application of quantum computation is not directly related to your areas of expertise in Information Retrieval, Search technologies, and user behavior modeling."
    },
    {
        "title": "Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis",
        "abstract": "Marine mammal vocalization analysis depends on interpreting bioacoustic\nspectrograms. Vision Language Models (VLMs) are not trained on these\ndomain-specific visualizations. We investigate whether VLMs can extract\nmeaningful patterns from spectrograms visually. Our framework integrates VLM\ninterpretation with LLM-based validation to build domain knowledge. This\nenables adaptation to acoustic data without manual annotation or model\nretraining.",
        "url": "http://arxiv.org/abs/2509.05703v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05703v1",
        "arxiv_id": "2509.05703v1",
        "authors": [
            "Ragib Amin Nihal",
            "Benjamin Yen",
            "Takeshi Ashizawa",
            "Kazuhiro Nakadai"
        ],
        "submitted": "2025-09-06 12:36:59",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on underwater bioacoustic spectrogram analysis, which is outside the user's core research themes in Information Retrieval and Search technologies. While it involves the use of Vision Language Models, the application is not related to query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR",
        "abstract": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.",
        "url": "http://arxiv.org/abs/2509.05609v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05609v1",
        "arxiv_id": "2509.05609v1",
        "authors": [
            "Xugang Lu",
            "Peng Shen",
            "Yu Tsao",
            "Hisashi Kawai"
        ],
        "submitted": "2025-09-06 05:58:52",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on automatic speech recognition (ASR) and knowledge transfer, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some aspects of representation alignment and matching, the context is specific to ASR and does not align with the user's core themes."
    },
    {
        "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents",
        "abstract": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.",
        "url": "http://arxiv.org/abs/2509.05607v1",
        "pdf_url": "http://arxiv.org/pdf/2509.05607v1",
        "arxiv_id": "2509.05607v1",
        "authors": [
            "Qiyuan Chen",
            "Jiahe Chen",
            "Hongsen Huang",
            "Qian Shao",
            "Jintai Chen",
            "Renjie Hua",
            "Hongxia Xu",
            "Ruijia Wu",
            "Ren Chuan",
            "Jian Wu"
        ],
        "submitted": "2025-09-06 05:46:38",
        "source": "arxiv",
        "comment": "Technical Report",
        "score": 2,
        "keyword_reasons": [
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns with your interests in Information Retrieval, particularly in the context of Generative Search Engines and real-time relevance optimization. The focus on content-centric agents and semantic impact also resonates with your background in NLP and deep semantic understanding. However, the e-commerce domain is not explicitly mentioned, which prevents a perfect match."
    }
]