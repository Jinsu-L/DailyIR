# Daily Papers Report - 2025-09-08

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Reasoning-enhanced Query Understanding through Decomposition and Interpretation

- **LLM Score**: 9
- **Keyword Score**: 18
- **Authors**: Yunfei Zhong, Jun Yang, Yixing Fan, Jiafeng Guo, Lixin Su, Maarten de Rijke, Ruqing Zhang, Dawei Yin, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2509.06544v1>
- **Submitted**: 2025-09-08 10:58:42
- **Topic Keywords**: dense retrieval, query, queries, ranking, rag, retrieval, rank, search
- **Reason**: This paper is extremely relevant to your research interests in Information Retrieval, particularly query understanding and ranking models. The proposed approach, ReDI, leverages large language models to decompose and interpret complex queries, which aligns with your focus on deep semantic understanding and real-time relevance optimization. The evaluation on BRIGHT and BEIR datasets further supports its relevance to your research areas.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Information Retrieval
- **Aim**: Improve information retrieval by leveraging reasoning capabilities of large language models (LLMs) to handle complex user queries.
- **Rationale**: Complex user queries often require nuanced understanding and reasoning, which traditional IR methods struggle with.
- **Ground**: The Coin dataset, a collection of 3,403 complex queries, was created to support the ReDI framework.
- **Experiment**: ReDI was evaluated on public benchmarks BRIGHT and BEIR, comparing its performance to existing query understanding (QU) baselines in both sparse and dense retrieval settings.
- **Takeaway**: ReDI significantly outperformed existing baselines, demonstrating the effectiveness of reasoning-enhanced decomposition for improving information retrieval accuracy, particularly for queries requiring abstract reasoning.

#### Abstract
> Accurate inference of user intent is crucial for enhancing document retrieval
in modern search engines. While large language models (LLMs) have made
significant strides in this area, their effectiveness has predominantly been
assessed with short, keyword-based queries. As AI-driven search evolves,
long-form queries with intricate intents are becoming more prevalent, yet they
remain underexplored in the context of LLM-based query understanding (QU). To
bridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query
understanding through Decomposition and Interpretation. ReDI leverages the
reasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)
it breaks down complex queries into targeted sub-queries to accurately capture
user intent; (ii) it enriches each sub-query with detailed semantic
interpretations to improve the query-document matching; and (iii) it
independently retrieves documents for each sub-query and employs a fusion
strategy to aggregate the results for the final ranking. We compiled a
large-scale dataset of real-world complex queries from a major search engine
and distilled the query understanding capabilities of teacher models into
smaller models for practical application. Experiments on BRIGHT and BEIR
demonstrate that ReDI consistently surpasses strong baselines in both sparse
and dense retrieval paradigms, affirming its effectiveness.

---

### 2. LESER: Learning to Expand via Search Engine-feedback Reinforcement in e-Commerce

- **LLM Score**: 8
- **Keyword Score**: 16
- **Authors**: Yipeng Zhang, Bowen Liu, Xiaoshuang Zhang, Aritra Mandal, Zhe Wu, Canran Xu
- **URL**: <http://arxiv.org/abs/2509.05570v1>
- **Submitted**: 2025-09-06 02:54:13
- **Topic Keywords**: query, queries, relevance, rag, retrieval, commerce, e-commerce, search
- **Reason**: This paper aligns well with your interests in Information Retrieval, particularly in query understanding and ranking models. The focus on e-commerce search and real-time relevance optimization also resonates with your background and expertise. However, the emphasis on Learning to Rank and user behavior modeling is not as prominent in this paper.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Query Expansion in E-commerce Search
- **Aim**: To develop a novel framework, LESER, that effectively expands vague and underspecified user queries in e-commerce search.
- **Rationale**: Vague user queries pose a challenge for e-commerce search, leading to incomplete retrieval and reduced user satisfaction.
- **Ground**: LESER leverages a context-aware LLM fine-tuned using real-time search engine feedback.
- **Experiment**: LESER employs a two-stage training pipeline: supervised fine-tuning on a dataset of reasoning examples and group-relative policy optimization using search engine rewards.
- **Takeaway**: LESER outperforms existing baselines in offline evaluations and demonstrates significant improvements in user engagement metrics and gross merchandise value in online A/B tests.

#### Abstract
> User queries in e-commerce search are often vague, short, and underspecified,
making it difficult for retrieval systems to match them accurately against
structured product catalogs. This challenge is amplified by the one-to-many
nature of user intent, where a single query can imply diverse and competing
needs. Existing methods, including neural query expansion and prompting-based
LLM approaches, fall short in real-world settings: they struggle to capture
nuanced user intent, often generate outputs that violate platform constraints,
and rely on workflows that are difficult to scale in production. We propose
Learning to Expand via Search Engine-feedback Reinforcement (LESER), a novel
framework that fine-tunes a context-aware LLM using real-time search engine
feedback as supervision. LESER formulates query expansion as a retrieval
optimization task and leverages Group Relative Policy Optimization to learn
directly from relevance and coverage metrics. LESER is trained to reason over
search results and produce high quality query expansions that align with
platform rules and retrieval objectives. We evaluate LESER on large-scale,
real-world e-commerce datasets, demonstrating substantial improvements in both
offline and online settings. Our results show that LESER not only enhances
semantic coverage and retrieval relevance but also delivers measurable gains in
user engagement, making it a practical and scalable solution for modern search
systems.

---

### 3. Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Firas Jarboui, Issa Memari
- **URL**: <http://arxiv.org/abs/2509.06185v1>
- **Submitted**: 2025-09-07 19:30:09
- **Topic Keywords**: retriever, query, queries, retrieval, recommend, commerce, e-commerce, rank
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the context of conversational recommender systems and large product catalogs. The use of entropy-driven dialogue policy and neural retrievers aligns with your focus on query understanding and ranking models. While the e-commerce domain is mentioned, the paper's emphasis on real-time relevance optimization and semantic understanding makes it a strong match for your research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Dialogue Policy for a Next-Generation Shopping Assistant
- **Aim**: Develop a dialogue policy that balances exploration and exploitation in a shopping assistant powered by large language models (LLMs).
- **Rationale**: To effectively assist users in navigating vast e-commerce catalogs, a shopping assistant needs to dynamically adapt its approach based on the user's query ambiguity.
- **Ground**: The research leverages entropy of product retrieval score distributions to gauge query ambiguity.
- **Experiment**: Experiments using recall analysis and A/B testing demonstrate the effectiveness of the entropy-driven dialogue policy in guiding user interactions and potentially increasing shopper engagement.
- **Takeaway**: An entropy-driven dialogue policy, combined with a multi-stage architecture, offers a promising approach for building sophisticated shopping assistants that provide personalized and effective product recommendations.

#### Abstract
> Conversational recommender systems promise rich interactions for e-commerce,
but balancing exploration (clarifying user needs) and exploitation (making
recommendations) remains challenging, especially when deploying large language
models (LLMs) with vast product catalogs. We address this challenge by modeling
the breadth of user interest via the entropy of retrieval score distributions.
Our method uses a neural retriever to fetch relevant items for a user query and
computes the entropy of the re-ranked scores to dynamically route the dialogue
policy: low-entropy (specific) queries trigger direct recommendations, whereas
high-entropy (ambiguous) queries prompt exploratory questions. This simple yet
effective strategy allows an LLM-driven agent to remain aware of an arbitrarily
large catalog in real-time without bloating its context window.

---

### 4. Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Haoxiang Jin, Ronghan Li, Qiguang Miao, Zixiang Lu
- **URL**: <http://arxiv.org/abs/2509.06472v1>
- **Submitted**: 2025-09-08 09:37:20
- **Topic Keywords**: queries, ranking, rerank, rag, retrieval, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The authors propose a novel approach to leveraging Large Language Models (LLMs) for post-retrieval confidence and dynamic retrieval, which aligns with your focus on deep semantic understanding and real-time relevance optimization. While the paper's primary focus is on LLMs and NLP, its contributions to retrieval and reranking make it a useful read for your IR research.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) Systems
- **Aim**: Enhance the accuracy and efficiency of RAG systems by focusing on post-retrieval knowledge filtering and the interaction between LLMs and Reranker models.
- **Rationale**: Current evaluation methods for retrieved context effectiveness are insufficient because they overlook the rich information present in LLMs' internal hidden states.
- **Ground**: LLMs' hidden states can provide insights into how retrieved contexts improve their confidence in answering a query.
- **Experiment**: Proposes Confidence-Based Dynamic Retrieval (CBDR) which adaptively triggers retrieval based on the LLM's initial confidence. Evaluates four Reranker models, including a fine-tuned version, on the NQ_Rerank dataset.
- **Takeaway**: Larger Reranker models and fine-tuning significantly improve accuracy. Dynamic retrieval strategies like CBDR reduce retrieval overhead while maintaining competitive accuracy. Further research is needed on preference alignment and efficient retrieval methods.

#### Abstract
> Large Language Models (LLMs) often generate inaccurate responses
(hallucinations) when faced with questions beyond their knowledge scope.
Retrieval-Augmented Generation (RAG) addresses this by leveraging external
knowledge, but a critical challenge remains: determining whether retrieved
contexts effectively enhance the model`s ability to answer specific queries.
This challenge underscores the importance of knowledge boundary awareness,
which current methods-relying on discrete labels or limited signals-fail to
address adequately, as they overlook the rich information in LLMs` continuous
internal hidden states. To tackle this, we propose a novel post-retrieval
knowledge filtering approach. First, we construct a confidence detection model
based on LLMs` internal hidden states to quantify how retrieved contexts
enhance the model`s confidence. Using this model, we build a preference dataset
(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts
preferred by the downstream LLM during reranking. Additionally, we introduce
Confidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval
based on the LLM`s initial confidence in the original question, reducing
knowledge conflicts and improving efficiency. Experimental results demonstrate
significant improvements in accuracy for context screening and end-to-end RAG
performance, along with a notable reduction in retrieval costs while
maintaining competitive accuracy.

---

### 5. AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Enrico Palumbo, Gustavo Penha, Alva Liu, Marcus Eltscheminov, Jefferson Carvalho dos Santos, Alice Wang, Hugues Bouchard, Humberto Jes√∫s Corona Pampin, Michelle Tran Luu
- **URL**: <http://arxiv.org/abs/2509.06452v1>
- **Submitted**: 2025-09-08 08:57:03
- **Comment**: EARL Workshop @ RecSys25
- **Topic Keywords**: query, queries, rag, click, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of synthetic query generation and Large Language Models to improve query formulation and retrieval aligns with your focus on deep semantic understanding and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Improving audiobook discoverability in Spotify search
- **Aim**: Address the 'cold-start' problem for new audiobooks by leveraging LLMs to generate synthetic queries and enhance search relevance.
- **Rationale**: New audiobooks lack user interaction data, hindering effective retrieval in search systems.
- **Ground**: Spotify's existing search function struggles to surface new audiobooks effectively.
- **Experiment**: AudioBoost system was developed and evaluated through offline quality assessment and an online A/B test.
- **Takeaway**: AudioBoost significantly increased impressions, clicks, coverage, and exploration of audiobooks, demonstrating the effectiveness of synthetic query generation for improving search visibility.

#### Abstract
> Spotify has recently introduced audiobooks as part of its catalog,
complementing its music and podcast offering. Search is often the first entry
point for users to access new items, and an important goal for Spotify is to
support users in the exploration of the audiobook catalog. More specifically,
we would like to enable users without a specific item in mind to broadly search
by topic, genre, story tropes, decade, and discover audiobooks, authors and
publishers they may like. To do this, we need to 1) inspire users to type more
exploratory queries for audiobooks and 2) augment our retrieval systems to
better deal with exploratory audiobook queries. This is challenging in a
cold-start scenario, where we have a retrievabiliy bias due to the little
amount of user interactions with audiobooks compared to previously available
items such as music and podcast content. To address this, we propose
AudioBoost, a system to boost audiobook retrievability in Spotify's Search via
synthetic query generation. AudioBoost leverages Large Language Models (LLMs)
to generate synthetic queries conditioned on audiobook metadata. The synthetic
queries are indexed both in the Query AutoComplete (QAC) and in the Search
Retrieval engine to improve query formulation and retrieval at the same time.
We show through offline evaluation that synthetic queries increase
retrievability and are of high quality. Moreover, results from an online A/B
test show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in
audiobook clicks, and +1.82% in audiobook exploratory query completions.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Jinrui Yang, Fan Jiang, Timothy Baldwin
- **URL**: <http://arxiv.org/abs/2509.06195v1>
- **Submitted**: 2025-09-07 20:10:49
- **Comment**: Accepted at EMNLP MRL 2024
- **Topic Keywords**: information retrieval, queries, ranking, retrieval, rank
- **Reason**: This paper is highly relevant to Information Retrieval, specifically addressing language bias in multilingual systems, which is a critical aspect of query understanding and ranking models. The use of neural rankers and a novel loss function to mitigate language biases aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the focus on multilingual systems and language fairness is somewhat specific, but still falls within your broader IR and NLP interests.

#### Abstract
> Language fairness in multilingual information retrieval (MLIR) systems is
crucial for ensuring equitable access to information across diverse languages.
This paper sheds light on the issue, based on the assumption that queries in
different languages, but with identical semantics, should yield equivalent
ranking lists when retrieving on the same multilingual documents. We evaluate
the degree of fairness using both traditional retrieval methods, and a DPR
neural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a
novel loss designed to mitigate language biases in neural MLIR approaches. Our
analysis exposes intrinsic language biases in current MLIR technologies, with
notable disparities across the retrieval methods, and the effectiveness of
LaKDA in enhancing language fairness.

### 7. UniSearch: Rethinking Search System with a Unified Generative Architecture

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Jiahui Chen, Xiaoze Jiang, Zhibo Wang, Quanzhi Zhu, Junyao Zhao, Feng Hu, Kang Pan, Ao Xie, Maohua Pei, Zhiheng Qin, Hongjing Zhang, Zhixin Zhai, Xiaobo Guo, Runbin Zhou, Kefeng Wang, Mingyang Geng, Cheng Chen, Jingshan Lv, Yupeng Huang, Xiao Liang, Han Li
- **URL**: <http://arxiv.org/abs/2509.06887v1>
- **Submitted**: 2025-09-08 17:08:26
- **Topic Keywords**: query, ranking, rag, recommend, rank, search
- **Reason**: This paper proposes a unified generative search framework, UniSearch, which integrates a search generator and a video encoder to improve representation quality and generation accuracy. While its focus is on video search, the use of a unified architecture and the introduction of Search Preference Optimization (SPO) aligns with the user's interests in information retrieval and ranking models. However, the specific application to video search and the use of a video encoder may limit its direct relevance to the user's core research themes.

#### Abstract
> Modern search systems play a crucial role in facilitating information
acquisition. Traditional search engines typically rely on a cascaded
architecture, where results are retrieved through recall, pre-ranking, and
ranking stages. The complexity of designing and maintaining multiple modules
makes it difficult to achieve holistic performance gains. Recent advances in
generative recommendation have motivated the exploration of unified generative
search as an alternative. However, existing approaches are not genuinely
end-to-end: they typically train an item encoder to tokenize candidates first
and then optimize a generator separately, leading to objective inconsistency
and limited generalization. To address these limitations, we propose UniSearch,
a unified generative search framework for Kuaishou Search. UniSearch replaces
the cascaded pipeline with an end-to-end architecture that integrates a Search
Generator and a Video Encoder. The Generator produces semantic identifiers of
relevant items given a user query, while the Video Encoder learns latent item
embeddings and provides their tokenized representations. A unified training
framework jointly optimizes both components, enabling mutual enhancement and
improving representation quality and generation accuracy. Furthermore, we
introduce Search Preference Optimization (SPO), which leverages a reward model
and real user feedback to better align generation with user preferences.
Extensive experiments on industrial-scale datasets, together with online A/B
testing in both short-video and live search scenarios, demonstrate the strong
effectiveness and deployment potential of UniSearch. Notably, its deployment in
live search yields the largest single-experiment improvement in recent years of
our product's history, highlighting its practical value for real-world
applications.

### 8. Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Hao Lin, Peitong Xie, Jingxue Chen, Jie Lin, Qingkun Tang, Qianchun Lu
- **URL**: <http://arxiv.org/abs/2509.06650v1>
- **Submitted**: 2025-09-08 13:04:07
- **Topic Keywords**: query, ranking, rag, retrieval, rank
- **Reason**: This paper proposes a novel method for optimizing Retrieval-Augmented Generation (RAG) systems, focusing on domain-aware retrieval and query enhancement. While it doesn't directly address ranking models or user behavior modeling, it's clearly related to information retrieval and query understanding, making it a useful contribution to the field.

#### Abstract
> Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval
stage, particularly the coarse-ranking process. Existing coarse-ranking
optimization approaches often struggle to balance domain-specific knowledge
learning with query enhencement, resulting in suboptimal retrieval performance.
To address this challenge, we propose MoLER, a domain-aware RAG method that
uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a
two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of
Losses (MoL) to balance domain-specific knowledge with general language
capabilities, and a reinforcement learning (RL) phase leveraging Group Relative
Policy Optimization (GRPO) to optimize query and passage generation for
maximizing document recall. A key innovation is our Multi-query Single-passage
Late Fusion (MSLF) strategy, which reduces computational overhead during RL
training while maintaining scalable inference via Multi-query Multi-passage
Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER
achieves state-of-the-art performance, significantly outperforming baseline
methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and
scalable retrieval in specialized domains.

### 9. Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents

- **LLM Score**: 8
- **Keyword Score**: 2
- **Authors**: Qiyuan Chen, Jiahe Chen, Hongsen Huang, Qian Shao, Jintai Chen, Renjie Hua, Hongxia Xu, Ruijia Wu, Ren Chuan, Jian Wu
- **URL**: <http://arxiv.org/abs/2509.05607v1>
- **Submitted**: 2025-09-06 05:46:38
- **Comment**: Technical Report
- **Topic Keywords**: rank, search
- **Reason**: This paper aligns with your interests in Information Retrieval, particularly in the context of Generative Search Engines and real-time relevance optimization. The focus on content-centric agents and semantic impact also resonates with your background in NLP and deep semantic understanding. However, the e-commerce domain is not explicitly mentioned, which prevents a perfect match.

#### Abstract
> The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

### 10. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents

- **LLM Score**: 7
- **Keyword Score**: 6
- **Authors**: Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen, Pengyu Zhao, Junxian He
- **URL**: <http://arxiv.org/abs/2509.06501v1>
- **Submitted**: 2025-09-08 10:07:03
- **Topic Keywords**: query, rag, search
- **Reason**: The paper explores web browsing capabilities and large language models for information seeking, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on web agents and long-horizon problem solving is not a central match to your primary research themes, but still relevant and potentially useful for your broader interests in NLP and data mining.

#### Abstract
> The paradigm of Large Language Models (LLMs) has increasingly shifted toward
agentic applications, where web browsing capabilities are fundamental for
retrieving information from diverse online sources. However, existing
open-source web agents either demonstrate limited information-seeking abilities
on complex tasks or lack transparent implementations. In this work, we identify
that the key challenge lies in the scarcity of challenging data for information
seeking. To address this limitation, we introduce WebExplorer: a systematic
data generation approach using model-based exploration and iterative,
long-to-short query evolution. This method creates challenging query-answer
pairs that require multi-step reasoning and complex web navigation. By
leveraging our curated high-quality dataset, we successfully develop advanced
web agent WebExplorer-8B through supervised fine-tuning followed by
reinforcement learning. Our model supports 128K context length and up to 100
tool calling turns, enabling long-horizon problem solving. Across diverse
information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art
performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able
to effectively search over an average of 16 turns after RL training, achieving
higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best
performance among models up to 100B parameters on WebWalkerQA and FRAMES.
Beyond these information-seeking tasks, our model also achieves strong
generalization on the HLE benchmark even though it is only trained on
knowledge-intensive QA data. These results highlight our approach as a
practical path toward long-horizon web agents.

### 11. A Survey of the State-of-the-Art in Conversational Question Answering Systems

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng
- **URL**: <http://arxiv.org/abs/2509.05716v1>
- **Submitted**: 2025-09-06 13:38:03
- **Comment**: 42 pages, 12 figures, 4 tables
- **Topic Keywords**: relevance, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and conversational systems, but it does not directly focus on information retrieval, query understanding, or ranking models. The paper's emphasis on conversational question answering systems and large language models may be of interest, but it is not a central match for your core research themes.

#### Abstract
> Conversational Question Answering (ConvQA) systems have emerged as a pivotal
area within Natural Language Processing (NLP) by driving advancements that
enable machines to engage in dynamic and context-aware conversations. These
capabilities are increasingly being applied across various domains, i.e.,
customer support, education, legal, and healthcare where maintaining a coherent
and relevant conversation is essential. Building on recent advancements, this
survey provides a comprehensive analysis of the state-of-the-art in ConvQA.
This survey begins by examining the core components of ConvQA systems, i.e.,
history selection, question understanding, and answer prediction, highlighting
their interplay in ensuring coherence and relevance in multi-turn
conversations. It further investigates the use of advanced machine learning
techniques, including but not limited to, reinforcement learning, contrastive
learning, and transfer learning to improve ConvQA accuracy and efficiency. The
pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,
Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact
through data scalability and architectural advancements. Additionally, this
survey presents a comprehensive analysis of key ConvQA datasets and concludes
by outlining open research directions. Overall, this work offers a
comprehensive overview of the ConvQA landscape and provides valuable insights
to guide future advancements in the field.

### 12. Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Mansi Garg, Lee-Chi Wang, Bhavesh Ghanchi, Sanjana Dumpala, Shreyash Kakde, Yen Chih Chen
- **URL**: <http://arxiv.org/abs/2509.05505v1>
- **Submitted**: 2025-09-05 21:29:52
- **Comment**: 10 pages, 6 figures, 3 tables
- **Topic Keywords**: queries, relevance, rag, retrieval, search
- **Reason**: The paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of biomedical literature and question answering. However, it focuses more on the biomedical domain and does not directly address your core areas of interest in e-commerce, query understanding, ranking models, and user behavior modeling.

#### Abstract
> This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

### 13. Beamforming-LLM: What, Where and When Did I Miss?

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Vishal Choudhari
- **URL**: <http://arxiv.org/abs/2509.06221v1>
- **Submitted**: 2025-09-07 21:52:26
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: The paper explores a novel application of spatial audio capture and retrieval-augmented generation in a multi-speaker environment. While it touches on natural language queries and summarization, its primary focus is on auditory memory systems, which is somewhat related to information retrieval but not a central match to your research interests.

#### Abstract
> We present Beamforming-LLM, a system that enables users to semantically
recall conversations they may have missed in multi-speaker environments. The
system combines spatial audio capture using a microphone array with
retrieval-augmented generation (RAG) to support natural language queries such
as, "What did I miss when I was following the conversation on dogs?"
Directional audio streams are separated using beamforming, transcribed with
Whisper, and embedded into a vector database using sentence encoders. Upon
receiving a user query, semantically relevant segments are retrieved,
temporally aligned with non-attended segments, and summarized using a
lightweight large language model (GPT-4o-mini). The result is a user-friendly
interface that provides contrastive summaries, spatial context, and timestamped
audio playback. This work lays the foundation for intelligent auditory memory
systems and has broad applications in assistive technology, meeting
summarization, and context-aware personal spatial computing.

### 14. LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li
- **URL**: <http://arxiv.org/abs/2509.05657v1>
- **Submitted**: 2025-09-06 09:26:39
- **Comment**: EMNLP2025
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: The paper explores Neural Architecture Search (NAS) using Large Language Models (LLMs), which is somewhat related to Information Retrieval, but the focus is on architecture search rather than query understanding or ranking models. While it involves deep semantic understanding, the application domain is different from e-commerce and information retrieval.

#### Abstract
> Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

### 15. Few-Shot Query Intent Detection via Relation-Aware Prompt Learning

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Liang Zhang, Yuan Li, Shijie Zhang, Zheng Zhang, Xitong Li
- **URL**: <http://arxiv.org/abs/2509.05635v1>
- **Submitted**: 2025-09-06 07:41:47
- **Topic Keywords**: query, rag
- **Reason**: This paper explores query intent detection in conversational systems, leveraging relation-aware prompt learning. While it touches on query understanding, it is more focused on intent detection and conversational systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the specific application and methodology differ from your primary focus on e-commerce and real-time relevance optimization.

#### Abstract
> Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

### 16. The Majority is not always right: RL training for solution aggregation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Wenting Zhao, Pranjal Aggarwal, Swarnadeep Saha, Asli Celikyilmaz, Jason Weston, Ilia Kulikov
- **URL**: <http://arxiv.org/abs/2509.06870v1>
- **Submitted**: 2025-09-08 16:39:38
- **Topic Keywords**: ranking, rank
- **Reason**: This paper explores solution aggregation using reinforcement learning, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on large language models and solution aggregation does not directly align with the user's primary interests in IR, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Scaling up test-time compute, by generating multiple independent solutions
and selecting or aggregating among them, has become a central paradigm for
improving large language models (LLMs) on challenging reasoning tasks. While
most prior work relies on simple majority voting or reward model ranking to
aggregate solutions, these approaches may only yield limited benefits. In this
work, we propose to learn aggregation as an explicit reasoning skill: given a
set of candidate solutions, we train an aggregator model to review, reconcile,
and synthesize a final, correct answer using reinforcement learning from
verifiable rewards. A key ingredient is careful balancing of easy and hard
training examples, allowing the model to learn both to recover
minority-but-correct answers as well as easy majority-correct answers.
Empirically, we find our method, AggLM, outperforms both strong rule-based and
reward-model baselines, across multiple benchmarks. Furthermore, it generalizes
effectively to solutions from differing models, including stronger ones than
contained in the training data, all while requiring substantially fewer tokens
than majority voting with larger numbers of solutions.

### 17. Guided Decoding and Its Critical Role in Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: √ñzg√ºr Uƒüur, Musa Yƒ±lmaz, Esra ≈ûavirdi, √ñzay Ezerceli, Mahmut El Huseyni, Selva Ta≈ü, Reyhan Bayraktar
- **URL**: <http://arxiv.org/abs/2509.06631v1>
- **Submitted**: 2025-09-08 12:51:40
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper explores Retrieval-Augmented Generation (RAG) systems, which is a related topic to Information Retrieval. However, the focus on Large Language Models and structured output generation is more aligned with NLP, making it somewhat relevant to your research interests.

#### Abstract
> The integration of Large Language Models (LLMs) into various applications has
driven the need for structured and reliable responses. A key challenge in
Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align
with expected formats while minimizing hallucinations. This study examines the
role of guided decoding in RAG systems, comparing three methods, Outlines,
XGrammar, and LM Format Enforcer, across different multi-turn prompting setups
(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,
and output quality, we provide insights into their performance and
applicability. Our findings reveal how multi-turn interactions influence guided
decoding, uncovering unexpected performance variations that can inform method
selection for specific use cases. This work advances the understanding of
structured output generation in RAG systems, offering both theoretical insights
and practical guidance for LLM deployment.

### 18. mmBERT: A Modern Multilingual Encoder with Annealed Language Learning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Marc Marone, Orion Weller, William Fleshman, Eugene Yang, Dawn Lawrie, Benjamin Van Durme
- **URL**: <http://arxiv.org/abs/2509.06888v1>
- **Submitted**: 2025-09-08 17:08:42
- **Topic Keywords**: retrieval, search
- **Reason**: This paper introduces a novel multilingual encoder model, mmBERT, which shows promise in classification and retrieval tasks. While it touches on retrieval, the focus is on classification and language learning, which is somewhat related to your interests in Information Retrieval and NLP. However, the emphasis on classification and multilingual models is not a central match for your research themes.

#### Abstract
> Encoder-only languages models are frequently used for a variety of standard
machine learning tasks, including classification and retrieval. However, there
has been a lack of recent research for encoder models, especially with respect
to multilingual models. We introduce mmBERT, an encoder-only language model
pretrained on 3T tokens of multilingual text in over 1800 languages. To build
mmBERT we introduce several novel elements, including an inverse mask ratio
schedule and an inverse temperature sampling ratio. We add over 1700
low-resource languages to the data mix only during the decay phase, showing
that it boosts performance dramatically and maximizes the gains from the
relatively small amount of training data. Despite only including these
low-resource languages in the short decay phase we achieve similar
classification performance to models like OpenAI's o3 and Google's Gemini 2.5
Pro. Overall, we show that mmBERT significantly outperforms the previous
generation of models on classification and retrieval tasks -- on both high and
low-resource languages.

### 19. Knowledge-Augmented Relation Learning for Complementary Recommendation with Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Chihiro Yamasaki, Kai Sugahara, Kazushi Okamoto
- **URL**: <http://arxiv.org/abs/2509.05564v1>
- **Submitted**: 2025-09-06 02:20:20
- **Topic Keywords**: recommend, commerce, e-commerce
- **Reason**: The paper focuses on recommender systems, specifically complementary recommendations, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the emphasis on large language models and knowledge augmentation is not a central match for the user's core research themes, which include query understanding, ranking models, and user behavior modeling.

#### Abstract
> Complementary recommendations play a crucial role in e-commerce by enhancing
user experience through suggestions of compatible items. Accurate
classification of complementary item relationships requires reliable labels,
but their creation presents a dilemma. Behavior-based labels are widely used
because they can be easily generated from interaction logs; however, they often
contain significant noise and lack reliability. While function-based labels
(FBLs) provide high-quality definitions of complementary relationships by
carefully articulating them based on item functions, their reliance on costly
manual annotation severely limits a model's ability to generalize to diverse
items. To resolve this trade-off, we propose Knowledge-Augmented Relation
Learning (KARL), a framework that strategically fuses active learning with
large language models (LLMs). KARL efficiently expands a high-quality FBL
dataset at a low cost by selectively sampling data points that the classifier
finds the most difficult and uses the label extension of the LLM. Our
experiments showed that in out-of-distribution (OOD) settings, an unexplored
item feature space, KARL improved the baseline accuracy by up to 37%. In
contrast, in in-distribution (ID) settings, the learned item feature space, the
improvement was less than 0.5%, with prolonged learning could degrade accuracy.
These contrasting results are due to the data diversity driven by KARL's
knowledge expansion, suggesting the need for a dynamic sampling strategy that
adjusts diversity based on the prediction context (ID or OOD).

### 20. Calibrated Recommendations with Contextual Bandits

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Diego Feijer, Himan Abdollahpouri, Sanket Gupta, Alexander Clare, Yuxiao Wen, Todd Wasson, Maria Dimakopoulou, Zahra Nazari, Kyle Kretschman, Mounia Lalmas
- **URL**: <http://arxiv.org/abs/2509.05460v1>
- **Submitted**: 2025-09-05 19:28:08
- **Comment**: Accepted at ACM RecSys '25, CONSEQUENCES workshop
- **Topic Keywords**: rag, recommend
- **Reason**: The paper explores contextual bandits for personalized content recommendations, which is somewhat related to information retrieval and user behavior modeling. However, it focuses on recommender systems, which is not the primary focus of your research interests. The use of contextual bandits is an interesting aspect, but it doesn't directly relate to query understanding, ranking models, or deep semantic understanding.

#### Abstract
> Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

### 21. On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Linlu Qiu, Cedegao E. Zhang, Joshua B. Tenenbaum, Yoon Kim, Roger P. Levy
- **URL**: <http://arxiv.org/abs/2509.06952v1>
- **Submitted**: 2025-09-08 17:59:32
- **Comment**: EMNLP 2025 (Main)
- **Topic Keywords**: rag
- **Reason**: This paper explores pragmatic reasoning in language models, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on language comprehension and production, rather than search or ranking, limits its direct relevance to your core research interests. The study's findings on language understanding and social reasoning may be of interest, but it does not directly address your primary areas of focus.

#### Abstract
> Language use is shaped by pragmatics -- i.e., reasoning about communicative
goals and norms in context. As language models (LMs) are increasingly used as
conversational agents, it becomes ever more important to understand their
pragmatic reasoning abilities. We propose an evaluation framework derived from
Wavelength, a popular communication game where a speaker and a listener
communicate about a broad range of concepts in a granular manner. We study a
range of LMs on both language comprehension and language production using
direct and Chain-of-Thought (CoT) prompting, and further explore a Rational
Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM
inference. We find that state-of-the-art LMs, but not smaller ones, achieve
strong performance on language comprehension, obtaining similar-to-human
accuracy and exhibiting high correlations with human judgments even without CoT
prompting or RSA. On language production, CoT can outperform direct prompting,
and using RSA provides significant improvements over both approaches. Our study
helps identify the strengths and limitations in LMs' pragmatic reasoning
abilities and demonstrates the potential for improving them with RSA, opening
up future avenues for understanding conceptual representation, language
understanding, and social reasoning in LMs and humans.

### 22. Outcome-based Exploration for LLM Reasoning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yuda Song, Julia Kempe, Remi Munos
- **URL**: <http://arxiv.org/abs/2509.06941v1>
- **Submitted**: 2025-09-08 17:52:56
- **Comment**: 26 pages, 11 figures
- **Topic Keywords**: rag
- **Reason**: The paper explores reinforcement learning methods for improving large language model reasoning abilities, but it does not directly relate to information retrieval, search technologies, or user behavior modeling. While it touches on the importance of diversity in real-world performance, the focus is on language model reasoning rather than query understanding or ranking models.

#### Abstract
> Reinforcement learning (RL) has emerged as a powerful method for improving
the reasoning abilities of large language models (LLMs). Outcome-based RL,
which rewards policies solely for the correctness of the final answer, yields
substantial accuracy gains but also induces a systematic loss in generation
diversity. This collapse undermines real-world performance, where diversity is
critical for test-time scaling. We analyze this phenomenon by viewing RL
post-training as a sampling process and show that, strikingly, RL can reduce
effective diversity even on the training set relative to the base model. Our
study highlights two central findings: (i) a transfer of diversity degradation,
where reduced diversity on solved problems propagates to unsolved ones, and
(ii) the tractability of the outcome space, since reasoning tasks admit only a
limited set of distinct answers. Motivated by these insights, we propose
outcome-based exploration, which assigns exploration bonuses according to final
outcomes. We introduce two complementary algorithms: historical exploration,
which encourages rarely observed answers via UCB-style bonuses, and batch
exploration, which penalizes within-batch repetition to promote test-time
diversity. Experiments on standard competition math with Llama and Qwen models
demonstrate that both methods improve accuracy while mitigating diversity
collapse. On the theoretical side, we formalize the benefit of outcome-based
exploration through a new model of outcome-based bandits. Together, these
contributions chart a practical path toward RL methods that enhance reasoning
without sacrificing the diversity essential for scalable deployment.

### 23. LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jian Wu, Hang Yu, Bingchang Liu, Wenjie Yang, Peng Di, Jianguo Li, Yue Zhang
- **URL**: <http://arxiv.org/abs/2509.06524v1>
- **Submitted**: 2025-09-08 10:30:58
- **Topic Keywords**: rag
- **Reason**: This paper introduces a novel approach for domain-specific data selection using pre-trained large language models, which is somewhat related to information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. While it involves deep semantic understanding, its focus on data selection and classification is distinct from the user's primary research themes.

#### Abstract
> Adapting large language models (LLMs) to specific domains often faces a
critical bottleneck: the scarcity of high-quality, human-curated data. While
large volumes of unchecked data are readily available, indiscriminately using
them for fine-tuning risks introducing noise and degrading performance.
Strategic data selection is thus crucial, requiring a method that is both
accurate and efficient. Existing approaches, categorized as similarity-based
and direct optimization methods, struggle to simultaneously achieve these
goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for
domain-specific DAta Selection), a novel approach that leverages the
pre-trained LLM itself as an implicit classifier, thereby bypassing explicit
feature engineering and computationally intensive optimization process. LAMDAS
reframes data selection as a one-class classification problem, identifying
candidate data that "belongs" to the target domain defined by a small reference
dataset. Extensive experimental results demonstrate that LAMDAS not only
exceeds the performance of full-data training using a fraction of the data but
also outperforms nine state-of-the-art (SOTA) baselines under various
scenarios. Furthermore, LAMDAS achieves the most compelling balance between
performance gains and computational efficiency compared to all evaluated
baselines.

### 24. MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Omar Walid, Mohamed T. Younes, Khaled Shaban, Mai Hassan, Ali Hamdi
- **URL**: <http://arxiv.org/abs/2509.06200v1>
- **Submitted**: 2025-09-07 20:27:58
- **Comment**: Accepted in AICCSA 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on recruitment automation and resume parsing, which is somewhat related to the e-commerce domain. However, the use of Large Language Models (LLMs) and ensemble finetuning is more aligned with NLP, but not directly related to query understanding, ranking models, or user behavior modeling in IR, which are the primary research interests.

#### Abstract
> This paper presents MSLEF, a multi-segment ensemble framework that employs
LLM fine-tuning to enhance resume parsing in recruitment automation. It
integrates fine-tuned Large Language Models (LLMs) using weighted voting, with
each model specializing in a specific resume segment to boost accuracy.
Building on MLAR , MSLEF introduces a segment-aware architecture that leverages
field-specific weighting tailored to each resume part, effectively overcoming
the limitations of single-model systems by adapting to diverse formats and
structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level
aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4
14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,
BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best
single model by up to +7% in RS. Its segment-aware design enhances
generalization across varied resume layouts, making it highly adaptable to
real-world hiring scenarios while ensuring precise and reliable candidate
representation.

### 25. Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Abhijnan Nath, Carine Graff, Nikhil Krishnaswamy
- **URL**: <http://arxiv.org/abs/2509.05882v1>
- **Submitted**: 2025-09-07 00:58:10
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it primarily focuses on the alignment of LLMs in collaborative dialogues, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it does involve some aspects of user behavior modeling, it is not a central match for your research interests.

#### Abstract
> As Large Language Models (LLMs) integrate into diverse workflows, they are
increasingly being considered "collaborators" with humans. If such AI
collaborators are to be reliable, their behavior over multiturn interactions
must be predictable, validated and verified before deployment. Common alignment
techniques are typically developed under simplified single-user settings and do
not account for the dynamics of long-horizon multiparty interactions. This
paper examines how different alignment methods affect LLM agents' effectiveness
as partners in multiturn, multiparty collaborations. We study this question
through the lens of friction agents that intervene in group dialogues to
encourage the collaborative group to slow down and reflect upon their reasoning
for deliberative decision-making. Using a roleplay methodology, we evaluate
interventions from differently-trained friction agents in collaborative task
conversations. We propose a novel counterfactual evaluation framework that
quantifies how friction interventions change the trajectory of group
collaboration and belief alignment. Our results show that a friction-aware
approach significantly outperforms common alignment baselines in helping both
convergence to a common ground, or agreed-upon task-relevant propositions, and
correctness of task outcomes.

### 26. Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Logan Lawrence, Ashton Williamson, Alexander Shelton
- **URL**: <http://arxiv.org/abs/2509.05440v1>
- **Submitted**: 2025-09-05 18:48:34
- **Comment**: 12 pages, 18 tables, 1 figure
- **Topic Keywords**: ranking, pairwise, rag, rank
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on Natural Language Generation (NLG) evaluation. While it involves large-language models, the context is evaluation rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's broader interests in NLP and data mining is limited.

#### Abstract
> As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

### 27. Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Yuze Liu, Zhaoyuan Zhang, Xiangsheng Zeng, Yihe Zhang, Leping Yu, Lejia Wang, Xi Yu
- **URL**: <http://arxiv.org/abs/2509.06093v1>
- **Submitted**: 2025-09-07 15:15:55
- **Topic Keywords**: rag, retrieval augmented generation, retrieval, search
- **Reason**: This paper appears to be primarily focused on a specific domain (chemical and materials research) and a novel database design, which doesn't align with your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does mention retrieval and LLMs, the context is quite different from your areas of focus.

#### Abstract
> Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

### 28. DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Philip Adams, Menghao Li, Shi Zhang, Li Tan, Qi Chen, Mingqin Li, Zengzhong Li, Knut Risvik, Harsha Vardhan Simhadri
- **URL**: <http://arxiv.org/abs/2509.06046v1>
- **Submitted**: 2025-09-07 13:13:02
- **Topic Keywords**: query, queries, search
- **Reason**: This paper focuses on distributed vector search and scalability, which is not directly related to the user's core research themes in Information Retrieval, query understanding, ranking models, and user behavior modeling. While it touches on search technologies, it's more about system architecture and scalability rather than semantic understanding or ranking models.

#### Abstract
> We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

### 29. From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Shay Dahary, Avi Edana, Alexander Apartsin, Yehudit Aperstein
- **URL**: <http://arxiv.org/abs/2509.05617v1>
- **Submitted**: 2025-09-06 06:28:28
- **Comment**: 5 pages, 2 figures
- **Topic Keywords**: information retrieval, rag, retrieval
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies, as it focuses on emotion estimation in song lyrics using Natural Language Processing techniques. While it touches on music information retrieval, it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

### 30. Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Jiacheng Miao, Joe R. Davis, Jonathan K. Pritchard, James Zou
- **URL**: <http://arxiv.org/abs/2509.06917v1>
- **Submitted**: 2025-09-08 17:28:42
- **Topic Keywords**: queries, rag, search
- **Reason**: This paper introduces a framework for converting research papers into AI agents, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves natural language and knowledge dissemination, the focus is on AI agents and knowledge sharing rather than query understanding, ranking models, or real-time relevance optimization.

#### Abstract
> We introduce Paper2Agent, an automated framework that converts research
papers into AI agents. Paper2Agent transforms research output from passive
artifacts into active systems that can accelerate downstream use, adoption, and
discovery. Conventional research papers require readers to invest substantial
effort to understand and adapt a paper's code, data, and methods to their own
work, creating barriers to dissemination and reuse. Paper2Agent addresses this
challenge by automatically converting a paper into an AI agent that acts as a
knowledgeable research assistant. It systematically analyzes the paper and the
associated codebase using multiple agents to construct a Model Context Protocol
(MCP) server, then iteratively generates and runs tests to refine and robustify
the resulting MCP. These paper MCPs can then be flexibly connected to a chat
agent (e.g. Claude Code) to carry out complex scientific queries through
natural language while invoking tools and workflows from the original paper. We
demonstrate Paper2Agent's effectiveness in creating reliable and capable paper
agents through in-depth case studies. Paper2Agent created an agent that
leverages AlphaGenome to interpret genomic variants and agents based on ScanPy
and TISSUE to carry out single-cell and spatial transcriptomics analyses. We
validate that these paper agents can reproduce the original paper's results and
can correctly carry out novel user queries. By turning static papers into
dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for
knowledge dissemination and a foundation for the collaborative ecosystem of AI
co-scientists.

### 31. Cross-Service Threat Intelligence in LLM Services using Privacy-Preserving Fingerprints

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Waris Gill, Natalie Isak, Matthew Dressman
- **URL**: <http://arxiv.org/abs/2509.05608v1>
- **Submitted**: 2025-09-06 05:57:20
- **Topic Keywords**: queries, rag, search
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. The focus on threat intelligence and security in LLM services is outside the user's primary areas of interest.

#### Abstract
> The widespread deployment of LLMs across enterprise services has created a
critical security blind spot. Organizations operate multiple LLM services
handling billions of queries daily, yet regulatory compliance boundaries
prevent these services from sharing threat intelligence about prompt injection
attacks, the top security risk for LLMs. When an attack is detected in one
service, the same threat may persist undetected in others for months, as
privacy regulations prohibit sharing user prompts across compliance boundaries.
  We present BinaryShield, the first privacy-preserving threat intelligence
system that enables secure sharing of attack fingerprints across compliance
boundaries. BinaryShield transforms suspicious prompts through a unique
pipeline combining PII redaction, semantic embedding, binary quantization, and
randomized response mechanism to potentially generate non-invertible
fingerprints that preserve attack patterns while providing privacy. Our
evaluations demonstrate that BinaryShield achieves an F1-score of 0.94,
significantly outperforming SimHash (0.77), the privacy-preserving baseline,
while achieving 64x storage reduction and 38x faster similarity search compared
to dense embeddings.

### 32. Modelling Intertextuality with N-gram Embeddings

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yi Xing
- **URL**: <http://arxiv.org/abs/2509.06637v1>
- **Submitted**: 2025-09-08 12:54:38
- **Topic Keywords**: pairwise, rag
- **Reason**: This paper focuses on literary studies and intertextuality, which is unrelated to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP techniques (n-gram embeddings), the context and application are not aligned with your interests.

#### Abstract
> Intertextuality is a central tenet in literary studies. It refers to the
intricate links between literary texts that are created by various types of
references. This paper proposes a new quantitative model of intertextuality to
enable scalable analysis and network-based insights: perform pairwise
comparisons of the embeddings of n-grams from two texts and average their
results as the overall intertextuality. Validation on four texts with known
degrees of intertextuality, alongside a scalability test on 267 diverse texts,
demonstrates the method's effectiveness and efficiency. Network analysis
further reveals centrality and community structures, affirming the approach's
success in capturing and quantifying intertextual relationships.

### 33. Compare: A Framework for Scientific Comparisons

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Moritz Staudinger, Wojciech Kusa, Matteo Cancellieri, David Pride, Petr Knoth, Allan Hanbury
- **URL**: <http://arxiv.org/abs/2509.06412v1>
- **Submitted**: 2025-09-08 08:05:26
- **Comment**: Accepted at CIKM 2025
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper focuses on a scientific comparison framework, which, although related to information retrieval, does not directly align with your core research themes of query understanding, ranking models, and user behavior modeling. The paper's emphasis on scientometrics and citation-supported comparisons also diverges from your primary focus on deep semantic understanding and real-time relevance optimization in the e-commerce domain.

#### Abstract
> Navigating the vast and rapidly increasing sea of academic publications to
identify institutional synergies, benchmark research contributions and pinpoint
key research contributions has become an increasingly daunting task, especially
with the current exponential increase in new publications. Existing tools
provide useful overviews or single-document insights, but none supports
structured, qualitative comparisons across institutions or publications.
  To address this, we demonstrate Compare, a novel framework that tackles this
challenge by enabling sophisticated long-context comparisons of scientific
contributions. Compare empowers users to explore and analyze research overlaps
and differences at both the institutional and publication granularity, all
driven by user-defined questions and automatic retrieval over online resources.
For this we leverage on Retrieval-Augmented Generation over evolving data
sources to foster long context knowledge synthesis. Unlike traditional
scientometric tools, Compare goes beyond quantitative indicators by providing
qualitative, citation-supported comparisons.

### 34. A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Kuan Zou, Aixin Sun
- **URL**: <http://arxiv.org/abs/2509.06002v1>
- **Submitted**: 2025-09-07 10:29:41
- **Comment**: Working paper
- **Topic Keywords**: relevance, recommend, search
- **Reason**: This paper is primarily focused on recommender systems, which is a related but secondary interest of yours. While it touches on real-world applications, it does not delve into information retrieval, query understanding, or ranking models, which are your core research themes.

#### Abstract
> Recommender systems have generated tremendous value for both users and
businesses, drawing significant attention from academia and industry alike.
However, due to practical constraints, academic research remains largely
confined to offline dataset optimizations, lacking access to real user data and
large-scale recommendation platforms. This limitation reduces practical
relevance, slows technological progress, and hampers a full understanding of
the key challenges in recommender systems. In this survey, we provide a
systematic review of industrial recommender systems and contrast them with
their academic counterparts. We highlight key differences in data scale,
real-time requirements, and evaluation methodologies, and we summarize major
real-world recommendation scenarios along with their associated challenges. We
then examine how industry practitioners address these challenges in
Transaction-Oriented Recommender Systems and Content-Oriented Recommender
Systems, a new classification grounded in item characteristics and
recommendation objectives. Finally, we outline promising research directions,
including the often-overlooked role of user decision-making, the integration of
economic and psychological theories, and concrete suggestions for advancing
academic research. Our goal is to enhance academia's understanding of practical
recommender systems, bridge the growing development gap, and foster stronger
collaboration between industry and academia.

### 35. ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: ZiXuan Zhang, Bowen Hao, Yingjie Li, Hongzhi Yin
- **URL**: <http://arxiv.org/abs/2509.05867v1>
- **Submitted**: 2025-09-06 23:48:46
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on a specific domain (Traditional Chinese Medicine) and proposes a framework for generating summaries and explanations of TCM formulas, which is not directly related to the user's core research themes in Information Retrieval and Search technologies, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> Traditional Chinese Medicine (TCM) formulas play a significant role in
treating epidemics and complex diseases. Existing models for TCM utilize
traditional algorithms or deep learning techniques to analyze formula
relationships, yet lack comprehensive results, such as complete formula
compositions and detailed explanations. Although recent efforts have used TCM
instruction datasets to fine-tune Large Language Models (LLMs) for explainable
formula generation, existing datasets lack sufficient details, such as the
roles of the formula's sovereign, minister, assistant, courier; efficacy;
contraindications; tongue and pulse diagnosis-limiting the depth of model
outputs. To address these challenges, we propose ZhiFangDanTai, a framework
combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM
fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured
TCM knowledge into concise summaries, while also constructing an enhanced
instruction dataset to improve LLMs' ability to integrate retrieved
information. Furthermore, we provide novel theoretical proofs demonstrating
that integrating GraphRAG with fine-tuning techniques can reduce generalization
error and hallucination rates in the TCM formula task. Experimental results on
both collected and clinical datasets demonstrate that ZhiFangDanTai achieves
significant improvements over state-of-the-art models. Our model is
open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

### 36. A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya
- **URL**: <http://arxiv.org/abs/2509.06813v1>
- **Submitted**: 2025-09-08 15:48:17
- **Comment**: Associated GitHub repository:
  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves Large Language Models, the focus is on a specific industrial application (wind turbine maintenance logs) and does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> Effective Operation and Maintenance (O&M) is critical to reducing the
Levelised Cost of Energy (LCOE) from wind power, yet the unstructured,
free-text nature of turbine maintenance logs presents a significant barrier to
automated analysis. Our paper addresses this by presenting a novel and
reproducible framework for benchmarking Large Language Models (LLMs) on the
task of classifying these complex industrial records. To promote transparency
and encourage further research, this framework has been made publicly available
as an open-source tool. We systematically evaluate a diverse suite of
state-of-the-art proprietary and open-source LLMs, providing a foundational
assessment of their trade-offs in reliability, operational efficiency, and
model calibration. Our results quantify a clear performance hierarchy,
identifying top models that exhibit high alignment with a benchmark standard
and trustworthy, well-calibrated confidence scores. We also demonstrate that
classification performance is highly dependent on the task's semantic
ambiguity, with all models showing higher consensus on objective component
identification than on interpretive maintenance actions. Given that no model
achieves perfect accuracy and that calibration varies dramatically, we conclude
that the most effective and responsible near-term application is a
Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate
and standardise data labelling for human experts, thereby enhancing O&M data
quality and downstream reliability analysis.

### 37. Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Valentin Quesnel, Damien Sileo
- **URL**: <http://arxiv.org/abs/2509.06809v1>
- **Submitted**: 2025-09-08 15:43:29
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on Large Language Models (LLMs) and automated theorem proving, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep semantic understanding, the context is specific to mathematical reasoning and theorem proving, making it somewhat tangential to your primary focus.

#### Abstract
> The scarcity of high-quality, logically sound data is a critical bottleneck
for advancing the mathematical reasoning of Large Language Models (LLMs). Our
work confronts this challenge by turning decades of automated theorem proving
research into a scalable data engine. Rather than relying on error-prone LLMs
or complex proof-assistant syntax like Lean and Isabelle, our framework
leverages E-prover's saturation capabilities on the vast TPTP axiom library to
derive a massive, guaranteed-valid corpus of theorems. Our pipeline is
principled and simple: saturate axioms, filter for "interesting" theorems, and
generate tasks. With no LLMs in the loop, we eliminate factual errors by
construction. This purely symbolic data is then transformed into three
difficulty-controlled challenges: entailment verification, premise selection,
and proof reconstruction. Our zero-shot experiments on frontier models reveal a
clear weakness: performance collapses on tasks requiring deep, structural
reasoning. Our framework provides both the diagnostic tool to measure this gap
and a scalable source of symbolic training data to address it. We make the code
and data publicly available.
  https://github.com/sileod/reasoning_core
https://hf.co/datasets/reasoning-core/rc1

### 38. MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke
- **URL**: <http://arxiv.org/abs/2509.06806v1>
- **Submitted**: 2025-09-08 15:38:31
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on continued pretraining of language models for in-context machine learning tasks, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the primary goal is to improve their ability to learn from in-context examples, which is more relevant to the NLP community than your specific focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) possess broad world knowledge and strong
general-purpose reasoning ability, yet they struggle to learn from many
in-context examples on standard machine learning (ML) tasks, that is, to
leverage many-shot demonstrations purely via in-context learning (ICL) without
gradient descent. We introduce MachineLearningLM, a portable
continued-pretraining framework that equips a general-purpose LLM with robust
in-context ML capability while preserving its general knowledge and reasoning
for broader chat workflows.
  Our pretraining procedure synthesizes ML tasks from millions of structural
causal models (SCMs), spanning shot counts up to 1,024. We begin with a
random-forest teacher, distilling tree-based decision strategies into the LLM
to strengthen robustness in numerical modeling. All tasks are serialized with a
token-efficient prompt, enabling 3x to 6x more examples per context window and
delivering up to 50x amortized throughput via batch inference.
  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),
MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an
average of about 15% on out-of-distribution tabular classification across
finance, physics, biology, and healthcare domains. It exhibits a striking
many-shot scaling law: accuracy increases monotonically as in-context
demonstrations grow from 8 to 1,024. Without any task-specific training, it
attains random-forest-level accuracy across hundreds of shots. General chat
capabilities, including knowledge and reasoning, are preserved: it achieves
75.4% on MMLU.

### 39. Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kefan Cao, Shuaicheng Wu
- **URL**: <http://arxiv.org/abs/2509.06100v1>
- **Submitted**: 2025-09-07 15:29:46
- **Comment**: 13 pages, 3 figures
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on continual learning of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves deep learning and optimization, the context is not aligned with the user's primary research interests.

#### Abstract
> Large language models (LLMs) are prone to catastrophic forgetting in
sequential multi-task settings. Parameter regularization methods such as O-LoRA
and N-LoRA alleviate task interference by enforcing low-rank subspace
orthogonality, but they overlook the fact that conventional additive
fine-tuning disrupts the intrinsic geometric structure of LLM parameters,
limiting performance. Our key insight is that the parameter space of LLMs
possesses a geometric structure, which must be preserved in addition to
enforcing orthogonality. Based on this, we propose Orthogonal Low-rank
Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM
fine-tuning: leveraging multiplicative updates to preserve parameter geometry
while applying orthogonality constraints to task subspaces. Experiments
demonstrate that OLieRA achieves state-of-the-art results on the Standard CL
benchmark and remains among the top-performing methods in the Large Number of
Tasks setting.

### 40. Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Aivin V. Solatorio
- **URL**: <http://arxiv.org/abs/2509.06902v1>
- **Submitted**: 2025-09-08 17:20:16
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on a protocol for trustworthy numeric answers from Large Language Models (LLMs) through claim verification, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves NLP, the context is more about ensuring numeric fidelity rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Large Language Models (LLMs) as stochastic systems may generate numbers that
deviate from available data, a failure known as \emph{numeric hallucination}.
Existing safeguards -- retrieval-augmented generation, citations, and
uncertainty estimation -- improve transparency but cannot guarantee fidelity:
fabricated or misquoted values may still be displayed as if correct. We propose
\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that
enforces numeric fidelity through mechanical verification. Under PCN, numeric
spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a
verifier checks each token under a declared policy (e.g., exact equality,
rounding, aliases, or tolerance with qualifiers). Crucially, PCN places
verification in the \emph{renderer}, not the model: only claim-checked numbers
are marked as verified, and all others default to unverified. This separation
prevents spoofing and guarantees fail-closed behavior. We formalize PCN and
prove soundness, completeness under honest tokens, fail-closed behavior, and
monotonicity under policy refinement. PCN is lightweight and model-agnostic,
integrates seamlessly into existing applications, and can be extended with
cryptographic commitments. By enforcing verification as a mandatory step before
display, PCN establishes a simple contract for numerically sensitive settings:
\emph{trust is earned only by proof}, while the absence of a mark communicates
uncertainty.

### 41. Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: James Xu Zhao, Bryan Hooi, See-Kiong Ng
- **URL**: <http://arxiv.org/abs/2509.06861v1>
- **Submitted**: 2025-09-08 16:28:25
- **Comment**: 20 pages, 4 figures, 6 tables
- **Topic Keywords**: rag
- **Reason**: This paper focuses on test-time scaling in reasoning models, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on knowledge-intensive tasks, the primary focus is on reasoning models and their limitations, rather than search technologies or user behavior modeling.

#### Abstract
> Test-time scaling increases inference-time computation by allowing models to
generate long reasoning chains, and has shown strong performance across many
domains. However, in this work, we show that this approach is not yet effective
for knowledge-intensive tasks, where high factual accuracy and low
hallucination rates are essential. We conduct a comprehensive evaluation of
test-time scaling using 12 reasoning models on two knowledge-intensive
benchmarks. Our results reveal that increasing test-time computation does not
consistently improve accuracy and, in many cases, it even leads to more
hallucinations. We then analyze how extended reasoning affects hallucination
behavior. We find that reduced hallucinations often result from the model
choosing to abstain after thinking more, rather than from improved factual
recall. Conversely, for some models, longer reasoning encourages attempts on
previously unanswered questions, many of which result in hallucinations. Case
studies show that extended reasoning can induce confirmation bias, leading to
overconfident hallucinations. Despite these limitations, we observe that
compared to non-thinking, enabling thinking remains beneficial. Code and data
are available at https://github.com/XuZhao0/tts-knowledge

### 42. HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xin Tong, Zhi Lin, Jingya Wang, Bo Jin
- **URL**: <http://arxiv.org/abs/2509.06596v1>
- **Submitted**: 2025-09-08 12:06:09
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on Large Language Models and hallucination mitigation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific topic of hallucination mitigation in LLMs is not a central match for your research themes.

#### Abstract
> Large Language Models (LLMs) often produce hallucinations in
retrieval-augmented or long-context generation, even when relevant evidence is
present. This stems from two issues: head importance is treated as
input-agnostic, and raw attention weights poorly reflect each token's true
contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a
parameter-free decoding framework that directly addresses both challenges. HAVE
introduces head-adaptive gating, which performs instance-level soft reweighing
of attention heads, and value calibration, which augments attention with the
magnitude of value vectors to approximate write-back contribution. Together,
these modules construct token-level evidence aligned with model updates and
fuse it with the LM distribution through a lightweight uncertainty-scaled
policy. HAVE requires no finetuning and operates in a single forward pass,
making it efficient and broadly applicable. Experiments across multiple QA
benchmarks and LLM families demonstrate that HAVE consistently reduces
hallucinations and outperforms strong baselines, including DAGCD, with modest
overhead. The framework is transparent, reproducible, and readily integrates
with off-the-shelf LLMs, advancing trustworthy generation in real-world
settings.

### 43. Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zheqi Lv, Wenqiao Zhang, Kairui Fu, Qi Tian, Shengyu Zhang, Jiajie Su, Jingyuan Chen, Kun Kuang, Fei Wu
- **URL**: <http://arxiv.org/abs/2509.06552v1>
- **Submitted**: 2025-09-08 11:06:50
- **Comment**: Published on MM'25: Proceedings of the 33rd ACM International
  Conference on Multimedia
- **Topic Keywords**: recommend, search
- **Reason**: This paper focuses on addressing real-time data distribution shifts in on-device models using a novel personalized method called Persona. While it involves some aspects of model adaptation and optimization, it does not directly relate to the user's core research interests in Information Retrieval, query understanding, ranking models, or user behavior modeling. The paper's emphasis on on-device models and real-time adaptation does not align with the user's background in e-commerce or interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> The on-device real-time data distribution shift on devices challenges the
generalization of lightweight on-device models. This critical issue is often
overlooked in current research, which predominantly relies on data-intensive
and computationally expensive fine-tuning approaches. To tackle this, we
introduce Persona, a novel personalized method using a prototype-based,
backpropagation-free parameter editing framework to enhance model
generalization without post-deployment retraining. Persona employs a neural
adapter in the cloud to generate a parameter editing matrix based on real-time
device data. This matrix adeptly adapts on-device models to the prevailing data
distributions, efficiently clustering them into prototype models. The
prototypes are dynamically refined via the parameter editing matrix,
facilitating efficient evolution. Furthermore, the integration of cross-layer
knowledge transfer ensures consistent and context-aware multi-layer parameter
changes and prototype assignment. Extensive experiments on vision task and
recommendation task on multiple datasets confirm Persona's effectiveness and
generality.

### 44. Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jaemin Son, Sujin Choi, Inyong Yun
- **URL**: <http://arxiv.org/abs/2509.06415v1>
- **Submitted**: 2025-09-08 08:12:26
- **Comment**: Submitted to ICASSP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on vision-language models for document understanding, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves document understanding, the approach is primarily visual and does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

### 45. PL-CA: A Parametric Legal Case Augmentation Framework

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ao Chang, Yubo Chen, Jun Zhao
- **URL**: <http://arxiv.org/abs/2509.06356v1>
- **Submitted**: 2025-09-08 06:08:06
- **Topic Keywords**: rag
- **Reason**: This paper focuses on a legal case augmentation framework, which is not directly related to information retrieval, search technologies, or natural language processing. While it does involve ranking models and data augmentation, the context is specific to the judicial domain and does not align with the user's core research themes.

#### Abstract
> Conventional RAG is considered one of the most effective methods for
addressing model knowledge insufficiency and hallucination, particularly in the
judicial domain that requires high levels of knowledge rigor, logical
consistency, and content integrity. However, the conventional RAG method only
injects retrieved documents directly into the model's context, which severely
constrains models due to their limited context windows and introduces
additional computational overhead through excessively long contexts, thereby
disrupting models' attention and degrading performance on downstream tasks.
Moreover, many existing benchmarks lack expert annotation and focus solely on
individual downstream tasks while real-world legal scenarios consist of
multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for
reflecting models' true capabilities. To address these limitations, we propose
PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data
augmentation on corpus knowledge and encode this legal knowledge into
parametric vectors, and then integrates this parametric knowledge into the
LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context
pressure. Additionally, we also construct a multi-task legal dataset comprising
more than 2000 training and test instances, which are all expert-annotated and
manually verified. We conduct our experiments on our dataset, and the
experimental results demonstrate that our method reduces the overhead
associated with excessively long contexts while maintaining competitive
performance on downstream tasks compared to conventional RAG. Our code and
dataset are provided in the appendix.

### 46. Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhenqi Jia, Rui Liu, Berrak Sisman, Haizhou Li
- **URL**: <http://arxiv.org/abs/2509.06074v1>
- **Submitted**: 2025-09-07 14:32:29
- **Comment**: Accepted by EMNLP 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on Conversational Speech Synthesis and multimodal dialogue history, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Conversational Speech Synthesis (CSS) aims to generate speech with natural
prosody by understanding the multimodal dialogue history (MDH). The latest work
predicts the accurate prosody expression of the target utterance by modeling
the utterance-level interaction characteristics of MDH and the target
utterance. However, MDH contains fine-grained semantic and prosody knowledge at
the word level. Existing methods overlook the fine-grained semantic and
prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a
novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our
approach constructs two specialized multimodal fine-grained dialogue
interaction graphs: a semantic interaction graph and a prosody interaction
graph. These two interaction graphs effectively encode interactions between
word-level semantics, prosody, and their influence on subsequent utterances in
MDH. The encoded interaction features are then leveraged to enhance synthesized
speech with natural conversational prosody. Experiments on the DailyTalk
dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of
prosodic expressiveness. Code and speech samples are available at
https://github.com/AI-S2-Lab/MFCIG-CSS.

### 47. Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yue Gu, Zhihao Du, Ying Shi, Shiliang Zhang, Qian Chen, Jiqing Han
- **URL**: <http://arxiv.org/abs/2509.05908v1>
- **Submitted**: 2025-09-07 03:46:59
- **Comment**: Accepted by IEEE Transactions on Audio, Speech and Language
  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:
  10.1109/TASLPRO.2025.3606198
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the robustness of contextual ASR models, which is not directly related to information retrieval or search technologies. While it involves semantic correlation modeling, the context is speech recognition rather than query understanding or ranking models, making it less relevant to the user's core research themes.

#### Abstract
> Recently, cross-attention-based contextual automatic speech recognition (ASR)
models have made notable advancements in recognizing personalized biasing
phrases. However, the effectiveness of cross-attention is affected by
variations in biasing information volume, especially when the length of the
biasing list increases significantly. We find that, regardless of the length of
the biasing list, only a limited amount of biasing information is most relevant
to a specific ASR intermediate representation. Therefore, by identifying and
integrating the most relevant biasing information rather than the entire
biasing list, we can alleviate the effects of variations in biasing information
volume for contextual ASR. To this end, we propose a purified semantic
correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and
calculate three semantic correlations between the ASR intermediate
representations and biasing information from coarse to fine: list-level,
phrase-level, and token-level. Then, the three correlations are jointly modeled
to produce their intersection, so that the most relevant biasing information
across various granularities is highlighted and integrated for contextual
recognition. In addition, to reduce the computational cost introduced by the
joint modeling of three semantic correlations, we also propose a purification
mechanism based on a grouped-and-competitive strategy to filter out irrelevant
biasing phrases. Compared with baselines, our PSC-Joint approach achieves
average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%
on KeSpeech, across biasing lists of varying lengths.

### 48. QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Charles M. Varmantchaonala, Niclas G√ñtting, Nils-Erik Sch√útte, Jean Louis E. K. Fendji, Christopher Gies
- **URL**: <http://arxiv.org/abs/2509.05729v1>
- **Submitted**: 2025-09-06 14:25:09
- **Topic Keywords**: rag
- **Reason**: This paper is primarily focused on Quantum Natural Language Processing (QNLP), which is not a core area of your research interests. While it does involve NLP, the application of quantum computation is not directly related to your areas of expertise in Information Retrieval, Search technologies, and user behavior modeling.

#### Abstract
> Quantum Natural Language Processing (QNLP) offers a novel approach to
encoding and understanding the complexity of natural languages through the
power of quantum computation. This paper presents a pretrained quantum
context-sensitive embedding model, called QCSE, that captures context-sensitive
word embeddings, leveraging the unique properties of quantum systems to learn
contextual relationships in languages. The model introduces quantum-native
context learning, enabling the utilization of quantum computers for linguistic
tasks. Central to the proposed approach are innovative context matrix
computation methods, designed to create unique, representations of words based
on their surrounding linguistic context. Five distinct methods are proposed and
tested for computing the context matrices, incorporating techniques such as
exponential decay, sinusoidal modulation, phase shifts, and hash-based
transformations. These methods ensure that the quantum embeddings retain
context sensitivity, thereby making them suitable for downstream language tasks
where the expressibility and properties of quantum systems are valuable
resources. To evaluate the effectiveness of the model and the associated
context matrix methods, evaluations are conducted on both a Fulani corpus, a
low-resource African language, dataset of small size and an English corpus of
slightly larger size. The results demonstrate that QCSE not only captures
context sensitivity but also leverages the expressibility of quantum systems
for representing rich, context-aware language information. The use of Fulani
further highlights the potential of QNLP to mitigate the problem of lack of
data for this category of languages. This work underscores the power of quantum
computation in natural language processing (NLP) and opens new avenues for
applying QNLP to real-world linguistic challenges across various tasks and
domains.

### 49. Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ragib Amin Nihal, Benjamin Yen, Takeshi Ashizawa, Kazuhiro Nakadai
- **URL**: <http://arxiv.org/abs/2509.05703v1>
- **Submitted**: 2025-09-06 12:36:59
- **Topic Keywords**: ctr
- **Reason**: This paper appears to be primarily focused on underwater bioacoustic spectrogram analysis, which is outside the user's core research themes in Information Retrieval and Search technologies. While it involves the use of Vision Language Models, the application is not related to query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Marine mammal vocalization analysis depends on interpreting bioacoustic
spectrograms. Vision Language Models (VLMs) are not trained on these
domain-specific visualizations. We investigate whether VLMs can extract
meaningful patterns from spectrograms visually. Our framework integrates VLM
interpretation with LLM-based validation to build domain knowledge. This
enables adaptation to acoustic data without manual annotation or model
retraining.

### 50. New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai
- **URL**: <http://arxiv.org/abs/2509.05609v1>
- **Submitted**: 2025-09-06 05:58:52
- **Topic Keywords**: rag
- **Reason**: This paper focuses on automatic speech recognition (ASR) and knowledge transfer, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some aspects of representation alignment and matching, the context is specific to ASR and does not align with the user's core themes.

#### Abstract
> Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

---

