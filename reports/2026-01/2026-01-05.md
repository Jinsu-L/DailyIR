# Daily Papers Report - 2026-01-05

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis

- **LLM Score**: 8
- **Keyword Score**: 19
- **Authors**: Samaneh Mohtadi, Gianluca Demartini
- **URL**: <http://arxiv.org/abs/2601.01751v1>
- **Submitted**: 2026-01-05 03:02:33
- **Comment**: Accepted for presentation at the ECIR 2026 Full Papers track
- **Topic Keywords**: information retrieval, query, queries, relevance, rag, retrieval, search, trec
- **Reason**: This paper is highly relevant to Information Retrieval, specifically focusing on the reliability of Large Language Models as relevance assessors. The proposed method for analyzing relevance label distributions and identifying patterns of disagreement between LLMs and humans is a key contribution to the field. While not directly related to query understanding or ranking models, the paper's focus on deep semantic understanding and real-time relevance optimization aligns with the user's broader research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Diagnosing Context‚ÄëDependent Biases in LLM‚ÄëBased Relevance Judgments
- **Aim**: Develop a context‚Äëaware diagnostic framework that embeds query‚Äìdocument pairs, clusters them semantically, and measures agreement with Gwet‚Äôs AC1 to identify bias‚Äëprone queries and judges.
- **Rationale**: Prior LLM bias studies identified generic error types but did not link them to semantic structure; class imbalance renders Cohen‚Äôs Œ∫ unreliable, while Gwet‚Äôs AC1 remains stable. Embedding‚Äëbased clustering can expose localized disagreement patterns.
- **Ground**: Data: TREC DL‚Äë2019 and DL‚Äë2020 collections with human qrels binarized to 0/1. Judges: Claude‚Äë3‚Äëhaiku, Gemini‚Äë1.5‚Äëflash‚Äë8b, GPT‚Äë4o, Llama‚Äë3.1. Encoder: INSTRUCTOR‚ÄëXL with prompt ‚ÄúJudge the document‚Äôs relevance to the query for ad‚Äëhoc retrieval.‚Äù Clustering: HDBSCAN on joint Q‚ÄìD embeddings. Metrics: Gwet‚Äôs AC1 per cluster, ŒîAC1, Bias‚ÄëSeverity Score (BSS).
- **Experiment**: 1) Compute global bias via Bland‚ÄìAltman plots comparing LLM vs. human judgments. 2) For each query, calculate cluster‚Äëlevel AC1, derive ŒîAC1, and apply bias‚Äëlocalization heuristics (thresholds, outlier detection, directional flip). 3) Flag bias‚Äëprone queries/judges and compute BSS. 4) Evaluate across all four LLMs on DL‚Äë2019 and DL‚Äë2020 to map bias patterns.
- **Takeaway**: LLM relevance‚Äëjudgment biases are structured and semantic‚Äëdependent, concentrated in specific clusters. The proposed framework, leveraging joint embeddings, HDBSCAN, and Gwet‚Äôs AC1, robustly localizes bias‚Äëprone queries and judges, revealing failure modes (definition‚Äëseeking, policy‚Äërelated, ambiguous contexts) that are invisible in corpus‚Äëlevel averages. This enables bias‚Äëaware IR evaluation and informs the design of more reliable synthetic test collections.

#### Abstract
> Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.

---

### 2. LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum

- **LLM Score**: 8
- **Keyword Score**: 19
- **Authors**: Zhichao Xu, Shengyao Zhuang, Crystina Zhang, Xueguang Ma, Yijun Tian, Maitrey Mehta, Jimmy Lin, Vivek Srikumar
- **URL**: <http://arxiv.org/abs/2601.01684v1>
- **Submitted**: 2026-01-04 22:42:20
- **Topic Keywords**: information retrieval, retriever, sparse retrieval, dense retrieval, ranking, retrieval, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of scalable and efficient search technologies. The focus on learned sparse retrieval and its comparison to dense models aligns with your interests in query understanding and ranking models. However, the specific application to commodity CPU hardware and real-world search applications is somewhat tangential to your primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.

---

### 3. Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Nuo Chen, Hanpei Fang, Piaohong Wang, Jiqun Liu, Tetsuya Sakai, Xiao-Ming Wu
- **URL**: <http://arxiv.org/abs/2601.01862v1>
- **Submitted**: 2026-01-05 07:46:29
- **Topic Keywords**: query, relevance, web search, search, trec
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the area of query understanding and ranking models. The study explores how simulated personalities influence relevance assessment and confidence calibration in large language models, which is a novel and interesting application of your core research themes.

#### Abstract
> Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

---

### 4. ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Qingqing Long, Haotian Chen, Chenyang Zhao, Xiaolei Du, Xuezhi Wang, Pengyao Wang, Chengzan Li, Yuanchun Zhou, Hengshu Zhu
- **URL**: <http://arxiv.org/abs/2601.01118v1>
- **Submitted**: 2026-01-03 08:42:53
- **Comment**: 12 pages, 9 figures
- **Topic Keywords**: queries, rag, retrieval, recommend, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The use of Large Language Models (LLMs) for deep semantic understanding and personalized recommendations aligns with your focus on real-time relevance optimization. However, the specific application to scientific dataset sharing services is somewhat niche, but still clearly related to your broader interests in IR and NLP.

#### Abstract
> The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.

---

### 5. SoulSeek: Exploring the Use of Social Cues in LLM-based Information Seeking

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Yubo Shu, Peng Zhang, Meng Wu, Yan Chen, Haoxuan Zhou, Guanming Liu, Yu Zhang, Liuxin Zhang, Qianying Wang, Tun Lu, Ning Gu
- **URL**: <http://arxiv.org/abs/2601.01094v1>
- **Submitted**: 2026-01-03 07:09:10
- **Topic Keywords**: relevance, search
- **Reason**: This paper explores the integration of social cues into LLM-based search systems, which aligns with your interests in query understanding and ranking models. The focus on social media platforms and user behavior modeling also resonates with your background in e-commerce and NLP. However, the specific application to social media platforms is somewhat narrow, preventing a higher score.

#### Abstract
> Social cues, which convey others' presence, behaviors, or identities, play a crucial role in human information seeking by helping individuals judge relevance and trustworthiness. However, existing LLM-based search systems primarily rely on semantic features, creating a misalignment with the socialized cognition underlying natural information seeking. To address this gap, we explore how the integration of social cues into LLM-based search influences users' perceptions, experiences, and behaviors. Focusing on social media platforms that are beginning to adopt LLM-based search, we integrate design workshops, the implementation of the prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to examine both outcome- and process-level findings. The workshop informs the prototype's cue-integrated design. The study shows that social cues improve perceived outcomes and experiences, promote reflective information behaviors, and reveal limits of current LLM-based search. We propose design implications emphasizing better social-knowledge understanding, personalized cue settings, and controllable interactions.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Ming Zhang, Kexin Tan, Yueyuan Huang, Yujiong Shen, Chunchun Ma, Li Ju, Xinran Zhang, Yuhui Wang, Wenqing Jing, Jingyi Deng, Huayu Sha, Binze Hu, Jingqi Tong, Changhao Jiang, Yage Geng, Yuankai Ying, Yue Zhang, Zhangyue Yin, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang
- **URL**: <http://arxiv.org/abs/2601.01576v1>
- **Submitted**: 2026-01-04 15:48:51
- **Topic Keywords**: semantic search, queries, retrieval, search, iclr
- **Reason**: The paper discusses an LLM-powered system for novelty assessment in peer review, which involves semantic search and full-text comparisons. While it touches on information retrieval aspects, its primary focus is on novelty assessment, which is somewhat related to query understanding and ranking models. However, the connection to user behavior modeling and e-commerce is not clear, limiting its relevance to your core research themes.

#### Abstract
> Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.

### 7. Tackling the Inherent Difficulty of Noise Filtering in RAG

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Jingyu Liu, Jiaen Lin, Yong Liu
- **URL**: <http://arxiv.org/abs/2601.01896v1>
- **Submitted**: 2026-01-05 08:40:37
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval, specifically in the context of Retrieval-Augmented Generation (RAG) and noise filtering. However, the focus is more on the robustness of Large Language Models (LLMs) rather than traditional IR techniques like query understanding or ranking models. The paper's emphasis on fine-tuning approaches and attention patterns also touches on related NLP topics.

#### Abstract
> Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.

### 8. HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery

- **LLM Score**: 4
- **Keyword Score**: 15
- **Authors**: Shiyuan Liu, Jianwei Wang, Xuemin Lin, Lu Qin, Wenjie Zhang, Ying Zhang
- **URL**: <http://arxiv.org/abs/2601.01015v1>
- **Submitted**: 2026-01-03 00:54:55
- **Topic Keywords**: query, ranking, rerank, pairwise, rag, rank
- **Reason**: This paper is somewhat related to information retrieval, but its focus on joinable table discovery and hypergraph link prediction is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it involves a large language model, the application is in data lake management rather than search technologies or e-commerce.

#### Abstract
> As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.

### 9. Curator: Efficient Vector Search with Low-Selectivity Filters

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Yicheng Jin, Yongji Wu, Wenjun Hu, Bruce M. Maggs, Jun Yang, Xiao Zhang, Danyang Zhuo
- **URL**: <http://arxiv.org/abs/2601.01291v1>
- **Submitted**: 2026-01-03 21:35:01
- **Comment**: Accepted at SIGMOD 2026
- **Topic Keywords**: dense retrieval, query, queries, rag, retrieval, search
- **Reason**: The paper discusses efficient vector search with low-selectivity filters, which is somewhat related to information retrieval and search technologies. However, the focus on indexing and query optimization is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to the user's broader interests in NLP and data mining is also limited.

#### Abstract
> Embedding-based dense retrieval has become the cornerstone of many critical applications, where approximate nearest neighbor search (ANNS) queries are often combined with filters on labels such as dates and price ranges. Graph-based indexes achieve state-of-the-art performance on unfiltered ANNS but encounter connectivity breakdown on low-selectivity filtered queries, where qualifying vectors become sparse and the graph structure among them fragments. Recent research proposes specialized graph indexes that address this issue by expanding graph degree, which incurs prohibitively high construction costs. Given these inherent limitations of graph-based methods, we argue for a dual-index architecture and present Curator, a partition-based index that complements existing graph-based approaches for low-selectivity filtered ANNS. Curator builds specialized indexes for different labels within a shared clustering tree, where each index adapts to the distribution of its qualifying vectors to ensure efficient search while sharing structure to minimize memory overhead. The system also supports incremental updates and handles arbitrary complex predicates beyond single-label filters by efficiently constructing temporary indexes on the fly. Our evaluation demonstrates that integrating Curator with state-of-the-art graph indexes reduces low-selectivity query latency by up to 20.9x compared to pre-filtering fallback, while increasing construction time and memory footprint by only 5.5% and 4.3%, respectively.

### 10. Entity-Aware and Secure Query Optimization in Database Using Named Entity Recognition

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Azrin Sultana, Hasibur Rashid Chayon
- **URL**: <http://arxiv.org/abs/2601.01254v1>
- **Submitted**: 2026-01-03 18:30:09
- **Comment**: 48 pages, 15 figures, 14 tables
- **Topic Keywords**: query, queries, rag, retrieval, rank, search
- **Reason**: The paper discusses query optimization in a database context using Named Entity Recognition (NER), which is somewhat related to query understanding in Information Retrieval. However, the focus on database security and encryption, while interesting, is not directly aligned with the user's primary research themes in IR and Search technologies.

#### Abstract
> Cloud storage has become the backbone of modern data infrastructure, yet privacy and efficient data retrieval remain significant challenges. Traditional privacy-preserving approaches primarily focus on enhancing database security but fail to address the automatic identification of sensitive information before encryption. This can dramatically reduce query processing time and mitigate errors during manual identification of sensitive information, thereby reducing potential privacy risks. To address this limitation, this research proposes an intelligent privacy-preserving query optimization framework that integrates Named Entity Recognition (NER) to detect sensitive information in queries, utilizing secure data encryption and query optimization techniques for both sensitive and non-sensitive data in parallel, thereby enabling efficient database optimization. Combined deep learning algorithms and transformer-based models to detect and classify sensitive entities with high precision, and the Advanced Encryption Standard (AES) algorithm to encrypt, with blind indexing to secure search functionality of the sensitive data, whereas non-sensitive data was divided into groups using the K-means algorithm, along with a rank search for optimization. Among all NER models, the Deep Belief Network combined with Long Short-Term Memory (DBN-LSTM) delivers the best performance, with an accuracy of 93% and precision (94%), recall, and F1 score of 93%, and 93%, respectively. Besides, encrypted search achieved considerably faster results with the help of blind indexing, and non-sensitive data fetching also outperformed traditional clustering-based searches. By integrating sensitive data detection, encryption, and query optimization, this work advances the state of privacy-preserving computation in modern cloud infrastructures.

### 11. SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Rajiv Chaitanya Muttur
- **URL**: <http://arxiv.org/abs/2601.01785v1>
- **Submitted**: 2026-01-05 04:39:31
- **Comment**: Presented at ICEdge 2025; nominated for Best Paper Award
- **Topic Keywords**: retriever, ltr, rag, retrieval
- **Reason**: The paper explores a novel approach to document selection for Retrieval-Augmented Generation (RAG) systems, leveraging reinforcement learning. While it touches on aspects of information retrieval, the focus is on edge-native deployment and latency-awareness, which is somewhat related to your interests in query understanding and ranking models, but not a central match.

#### Abstract
> Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.

### 12. Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Md. Asif Hossain, Nabil Subhan, Mantasha Rahman Mahi, Jannatul Ferdous Nabila
- **URL**: <http://arxiv.org/abs/2601.02065v1>
- **Submitted**: 2026-01-05 12:41:44
- **Comment**: 5 pages, 3 figures, 1 table
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of cross-lingual retrieval and query understanding. However, its focus on low-resource languages and agricultural advisory is not a central match to your primary interests in e-commerce and deep semantic understanding.

#### Abstract
> Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings

### 13. Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Praveenkumar Katwe, RakeshChandra Balabantaray, Kaliprasad Vittala
- **URL**: <http://arxiv.org/abs/2601.01543v1>
- **Submitted**: 2026-01-04 14:38:58
- **Comment**: Book chapter for River publications
- **Topic Keywords**: relevance, rag, search
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and data mining, but it is not directly focused on Information Retrieval (IR) or query understanding. The creation of a Hindi text summarization dataset is an interesting application of NLP, but it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.
  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.
  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.

### 14. MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Hamad Khan, Saddam Hussain Khan
- **URL**: <http://arxiv.org/abs/2601.01260v1>
- **Submitted**: 2026-01-03 19:01:33
- **Comment**: 28 Pages, Tables 12, Figure 09
- **Topic Keywords**: queries, ltr
- **Reason**: The paper proposes a novel Mixture-of-Experts framework for efficient medical question-answering, which involves routing decisions based on contextual complexity and domain-aware features. While it leverages large language models and transformer architectures, the focus is on clinical assistance and medical QA, which is somewhat related to information retrieval but not directly aligned with the user's core research themes.

#### Abstract
> The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.

### 15. Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Shivam Verma, Hannes Karlbom, Yu Zhao, Nick Topping, Vivian Chen, Kieran Stanley, Bharath Rengarajan
- **URL**: <http://arxiv.org/abs/2601.02306v1>
- **Submitted**: 2026-01-05 17:48:15
- **Comment**: Accepted at WSDM 2026
- **Topic Keywords**: rag, click, personalization
- **Reason**: This paper is somewhat related to information retrieval, but its focus on multi-objective modeling for podcast advertising and promotions is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it involves some aspects of personalization and cold-start initialization, the context is specific to the e-commerce domain and does not explore deep semantic understanding or real-time relevance optimization.

#### Abstract
> We present a unified multi-objective model for targeting both advertisements and promotions within the Spotify podcast ecosystem. Our approach addresses key challenges in personalization and cold-start initialization, particularly for new advertising objectives. By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks, including in-app promotions. This multi-objective design jointly optimizes podcast outcomes such as streams, clicks, and follows for both ads and promotions using a shared representation over user, content, context, and creative features, effectively supporting diverse business goals while improving user experience. Online A/B tests show up to a 22% reduction in effective Cost-Per-Stream (eCPS), particularly for less-streamed podcasts, and an 18-24% increase in podcast stream rates. Offline experiments and ablations highlight the contribution of ancillary objectives and feature groups to cold-start performance. Our experience shows that a unified modeling strategy improves maintainability, cold-start performance, and coverage, while breaking down historically siloed targeting pipelines. We discuss practical trade-offs of such joint models in a real-world advertising system.

### 16. pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Tobias Schimanski, Imene Kolli, Jingwei Ni, Yu Fan, Ario Saeid Vaghefi, Elliott Ash, Markus Leippold
- **URL**: <http://arxiv.org/abs/2601.02285v1>
- **Submitted**: 2026-01-05 17:15:26
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper presents a question answering dataset over PDFs, which is related to information retrieval and NLP. However, the focus is on question answering rather than search or ranking models, and the connection to user behavior modeling is not clear. While the paper touches on information retrieval, it is not a central match to the user's core research themes.

#### Abstract
> PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).

### 17. Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Amirali Ebrahimzadeh, Seyyed M. Salili
- **URL**: <http://arxiv.org/abs/2601.02023v1>
- **Submitted**: 2026-01-05 11:30:56
- **Comment**: 25 pages, 8 figures, 3 tables
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it does not directly address query understanding, ranking models, or user behavior modeling in the context of information retrieval. The study focuses on the performance of LLMs in extracting and inferring information from long contexts, which is a relevant topic but not a central match for the user's research interests.

#### Abstract
> Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

### 18. When Attention Becomes Exposure in Generative Search

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Shayan Alipour, Mehdi Kargar, Morteza Zihayat
- **URL**: <http://arxiv.org/abs/2601.01750v1>
- **Submitted**: 2026-01-05 03:01:46
- **Comment**: 8 pages, 2 figures
- **Topic Keywords**: queries, rank, search
- **Reason**: The paper explores the dynamics of generative search engines and their impact on exposure in Web3 platforms. While it touches on search technologies, its primary focus is on the social and economic aspects of Web3, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the paper's emphasis on exposure bias and viewpoint diversity does not directly align with the user's core research themes.

#### Abstract
> Generative search engines are reshaping information access by replacing traditional ranked lists with synthesized answers and references. In parallel, with the growth of Web3 platforms, incentive-driven creator ecosystems have become an essential part of how enterprises build visibility and community by rewarding creators for contributing to shared narratives. However, the extent to which exposure in generative search engine citations is shaped by external attention markets remains uncertain. In this study, we audit the exposure for 44 Web3 enterprises. First, we show that the creator community around each enterprise is persistent over time. Second, enterprise-specific queries reveal that more popular voices systematically receive greater citation exposure than others. Third, we find that larger follower bases and enterprises with more concentrated creator cores are associated with higher-ranked exposure. Together, these results show that generative search engine citations exhibit exposure bias toward already prominent voices, which risks entrenching incumbents and narrowing viewpoint diversity.

### 19. From Policy to Logic for Efficient and Interpretable Coverage Assessment

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Rhitabrat Pokharel, Hamid Hassanzadeh, Ameeta Agrawal
- **URL**: <http://arxiv.org/abs/2601.01266v1>
- **Submitted**: 2026-01-03 19:24:51
- **Comment**: Accepted at AIMedHealth @ AAAI 2026
- **Topic Keywords**: retriever, rag
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on policy interpretation and symbolic rule-based reasoning is not directly aligned with your primary interests in deep semantic understanding and real-time relevance optimization. The application to medical coverage policy review is also somewhat niche and not directly related to e-commerce or general search technologies.

#### Abstract
> Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.

### 20. EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Chuanrui Hu, Xingze Gao, Zuyi Zhou, Dannong Xu, Yi Bai, Xintong Li, Hui Zhang, Tong Li, Chong Zhang, Lidong Bing, Yafeng Deng
- **URL**: <http://arxiv.org/abs/2601.02163v1>
- **Submitted**: 2026-01-05 14:39:43
- **Comment**: 16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper introduces a self-organizing memory operating system for long-horizon reasoning, which is related to information retrieval and natural language processing. However, the focus on memory systems and episodic trace formation is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. While it touches on aspects of user behavior, it is more focused on memory-augmented reasoning tasks.

#### Abstract
> Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

### 21. A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Lilu Cheng, Jingjun Lu, Yi Xuan Chan, Quoc Khai Nguyen, John Bi, Sean Ho
- **URL**: <http://arxiv.org/abs/2601.01897v1>
- **Submitted**: 2026-01-05 08:40:44
- **Comment**: 19 pages, 3 figures, 3 tables
- **Topic Keywords**: rag, acl
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, particularly in the context of document understanding and field extraction. However, the specific focus on claims documents and healthcare operations is not directly aligned with the user's core research themes in e-commerce and real-time relevance optimization.

#### Abstract
> Claims documents are fundamental to healthcare and insurance operations, serving as the basis for reimbursement, auditing, and compliance. However, these documents are typically not born digital; they often exist as scanned PDFs or photographs captured under uncontrolled conditions. Consequently, they exhibit significant content heterogeneity, ranging from typed invoices to handwritten medical reports, as well as linguistic diversity. This challenge is exemplified by operations at Fullerton Health, which handles tens of millions of claims annually across nine markets, including Singapore, the Philippines, Indonesia, Malaysia, Mainland China, Hong Kong, Vietnam, Papua New Guinea, and Cambodia. Such variability, coupled with inconsistent image quality and diverse layouts, poses a significant obstacle to automated parsing and structured information extraction.
  This paper presents a robust multi-stage pipeline that integrates the multilingual optical character recognition (OCR) engine PaddleOCR, a traditional Logistic Regression classifier, and a compact Vision-Language Model (VLM), Qwen 2.5-VL-7B, to achieve efficient and accurate field extraction from large-scale claims data. The proposed system achieves a document-type classification accuracy of over 95 percent and a field-level extraction accuracy of approximately 87 percent, while maintaining an average processing latency of under 2 seconds per document. Compared to manual processing, which typically requires around 10 minutes per claim, our system delivers a 300x improvement in efficiency. These results demonstrate that combining traditional machine learning models with modern VLMs enables production-grade accuracy and speed for real-world automation. The solution has been successfully deployed in our mobile application and is currently processing tens of thousands of claims weekly from Vietnam and Singapore.

### 22. Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Md Abdullah Al Kafi, Raka Moni, Sumit Kumar Banshal
- **URL**: <http://arxiv.org/abs/2601.01341v1>
- **Submitted**: 2026-01-04 03:09:23
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, particularly in the context of query understanding and deep semantic understanding. However, the focus on mental health dialogue systems and Large Language Models is not a central match to the user's research themes, which are more broadly focused on e-commerce, search technologies, and recommender systems.

#### Abstract
> The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.

### 23. DeCode: Decoupling Content and Delivery for Medical QA

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Po-Jen Ko, Chen-Han Tsai, Yu-Shao Peng
- **URL**: <http://arxiv.org/abs/2601.02123v1>
- **Submitted**: 2026-01-05 13:54:38
- **Comment**: Preprint
- **Topic Keywords**: relevance
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and query understanding, as it involves adapting large language models for clinical question answering. However, it does not directly address your core focus on information retrieval, ranking models, or user behavior modeling. The paper's emphasis on clinical settings and medical knowledge also diverges from your e-commerce background.

#### Abstract
> Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\%$ to $49.8\%$, corresponding to a $75\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.

### 24. Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Dario Di Palma, Giovanni Maria Biancofiore, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia
- **URL**: <http://arxiv.org/abs/2601.01997v1>
- **Submitted**: 2026-01-05 10:56:01
- **Topic Keywords**: recommend, personalization, search
- **Reason**: The paper explores recommender systems, specifically ChatGPT's capabilities in providing diverse and novel recommendations. While it touches on aspects of user behavior modeling, it primarily focuses on recommender systems rather than information retrieval. The connection to search technologies is indirect, making it somewhat relevant but not a central match to the user's core research themes.

#### Abstract
> ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.
  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.

### 25. Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Danial Amin
- **URL**: <http://arxiv.org/abs/2601.01522v1>
- **Submitted**: 2026-01-04 13:19:27
- **Topic Keywords**: queries
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of query understanding and ranking models. However, the focus on Large Language Models (LLMs) and sequential decision-making is not a central match to your primary research themes. The paper's emphasis on cost-aware decision-making and fairness gains is an interesting aspect, but it does not directly align with your core interests in real-time relevance optimization and deep semantic understanding.

#### Abstract
> Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

### 26. Segmentation and Processing of German Court Decisions from Open Legal Data

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Harshil Darji, Martin Heckelmann, Christina Kratsch, Gerard de Melo
- **URL**: <http://arxiv.org/abs/2601.01449v1>
- **Submitted**: 2026-01-04 09:30:04
- **Comment**: Accepted and published as a research article in Legal Knowledge and Information Systems (JURIX 2025 proceedings, IOS Press). Pages 276--281
- **Topic Keywords**: retrieval, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and data mining, but it focuses on a specific domain (German court decisions) and task (segmentation and processing of legal data). While it involves text processing and retrieval, it does not directly relate to your core areas of interest in information retrieval, query understanding, and ranking models.

#### Abstract
> The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgr√ºnde (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.

### 27. Adaptive Diffusion-based Augmentation for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Na Li, Fanghui Sun, Yan Zou, Yangfu Zhu, Xiatian Zhu, Ying Ma
- **URL**: <http://arxiv.org/abs/2601.01448v1>
- **Submitted**: 2026-01-04 09:29:45
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommendation systems, which is related to the user's background in e-commerce and interests in recommender systems. However, it primarily deals with negative sampling and controllable negative sample generation, which is not directly aligned with the user's core research themes in Information Retrieval, query understanding, and ranking models.

#### Abstract
> Recommendation systems often rely on implicit feedback, where only positive user-item interactions can be observed. Negative sampling is therefore crucial to provide proper negative training signals. However, existing methods tend to mislabel potentially positive but unobserved items as negatives and lack precise control over negative sample selection. We aim to address these by generating controllable negative samples, rather than sampling from the existing item pool. In this context, we propose Adaptive Diffusion-based Augmentation for Recommendation (ADAR), a novel and model-agnostic module that leverages diffusion to synthesize informative negatives. Inspired by the progressive corruption process in diffusion, ADAR simulates a continuous transition from positive to negative, allowing for fine-grained control over sample hardness. To mine suitable negative samples, we theoretically identify the transition point at which a positive sample turns negative and derive a score-aware function to adaptively determine the optimal sampling timestep. By identifying this transition point, ADAR generates challenging negative samples that effectively refine the model's decision boundary. Experiments confirm that ADAR is broadly compatible and boosts the performance of existing recommendation models substantially, including collaborative filtering and sequential recommendation, without architectural modifications.

### 28. ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Tien-Huy Nguyen, Huu-Loc Tran, Thanh Duc Ngo
- **URL**: <http://arxiv.org/abs/2601.01024v1>
- **Submitted**: 2026-01-03 01:19:36
- **Comment**: Accepted at WACV Main Track 2026
- **Topic Keywords**: retrieval, search
- **Reason**: While the paper explores vision-language retrieval, which is a related area, it primarily focuses on visual understanding and alignment, rather than query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. The paper's emphasis on attention mechanisms and fine-grained alignment is somewhat relevant to your interests in deep semantic understanding, but it does not directly align with your primary focus on information retrieval in the e-commerce domain or other areas.

#### Abstract
> Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself

### 29. Beyond Homophily: Community Search on Heterophilic Graphs

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Qing Sima, Xiaoyang Wang, Wenjie Zhang
- **URL**: <http://arxiv.org/abs/2601.01703v1>
- **Submitted**: 2026-01-05 00:44:17
- **Topic Keywords**: query, rag, recommend, rank, search
- **Reason**: This paper focuses on community search on heterophilic graphs, which is not directly related to information retrieval, search technologies, or query understanding. While it involves graph-based methods, the context and application are distinct from the user's core research themes.

#### Abstract
> Community search aims to identify a refined set of nodes that are most relevant to a given query, supporting tasks ranging from fraud detection to recommendation. Unlike homophilic graphs, many real-world networks are heterophilic, where edges predominantly connect dissimilar nodes. Therefore, structural signals that once reflected smooth, low-frequency similarity now appear as sharp, high-frequency contrasts. However, both classical algorithms (e.g., k-core, k-truss) and recent ML-based models struggle to achieve effective community search on heterophilic graphs, where edge signs or semantics are generally unknown. Algorithm-based methods often return communities with mixed class labels, while GNNs, built on homophily, smooth away meaningful signals and blur community boundaries. Therefore, we propose Adaptive Community Search (AdaptCS), a unified framework featuring three key designs: (i) an AdaptCS Encoder that disentangles multi-hop and multi-frequency signals, enabling the model to capture both smooth (homophilic) and contrastive (heterophilic) relations; (ii) a memory-efficient low-rank optimization that removes the main computational bottleneck and ensures model scalability; and (iii) an Adaptive Community Score (ACS) that guides online search by balancing embedding similarity and topological relations. Extensive experiments on both heterophilic and homophilic benchmarks demonstrate that AdaptCS outperforms the best-performing baseline by an average of 11% in F1-score, retains robustness across heterophily levels, and achieves up to 2 orders of magnitude speedup.

### 30. MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Dongfang Zhao
- **URL**: <http://arxiv.org/abs/2601.01930v1>
- **Submitted**: 2026-01-05 09:23:48
- **Topic Keywords**: query, rag, search
- **Reason**: This paper focuses on graph-based Approximate Nearest Neighbor search, which is not directly related to Information Retrieval or Search technologies. While it involves indexing and search strategies, the context is more aligned with data mining and vector search, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\times$ higher throughput at 95\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\times$, while maintaining performance parity on standard lower-dimensional datasets.

### 31. ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Aniket Wattamwar, Sampson Akwafuo
- **URL**: <http://arxiv.org/abs/2601.01831v1>
- **Submitted**: 2026-01-05 06:50:40
- **Comment**: 6 pages, 14 figures, 1 table
- **Topic Keywords**: query, retrieval, search
- **Reason**: The paper's focus on epidemiological surveillance and outbreak monitoring, while utilizing AI and multi-agent frameworks, does not align with the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions querying and data synthesis, the context is specific to global health and not directly related to the user's core themes.

#### Abstract
> Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

### 32. From Failure to Mastery: Generating Hard Samples for Tool-use Agents

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Bingguang Hao, Zengzhuang Xu, Yuntao Wen, Xinyi Xu, Yang Liu, Tong Zhao, Maolin Wang, Long Chen, Dong Wang, Yicheng Chen, Cunyin Peng, Xiangyu Zhao, Chenyi Zhuang, Ji Zhang
- **URL**: <http://arxiv.org/abs/2601.01498v1>
- **Submitted**: 2026-01-04 11:56:33
- **Topic Keywords**: queries, rag, search
- **Reason**: This paper appears to be primarily focused on developing a method for generating hard samples for tool-use agents, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves the use of large language models, the context is more aligned with NLP and AI development rather than IR or search technologies.

#### Abstract
> The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.

### 33. Hidden State Poisoning Attacks against Mamba-based Language Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Alexandre Le Mercier, Chris Develder, Thomas Demeester
- **URL**: <http://arxiv.org/abs/2601.01972v1>
- **Submitted**: 2026-01-05 10:27:19
- **Comment**: 17 pages, 4 figures. Submitted to ACL 2026
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: This paper focuses on the security aspect of language models, specifically the vulnerability of state space models to hidden state poisoning attacks. While it touches on information retrieval capabilities, the primary focus is on the robustness of language models, which is not a central match to your research interests in IR, query understanding, and ranking models.

#### Abstract
> State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.

### 34. EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Houman Kazemzadeh, Nima Minaifar, Kamyar Naderi, Sho Tabibzadeh
- **URL**: <http://arxiv.org/abs/2601.01668v1>
- **Submitted**: 2026-01-04 21:10:42
- **Comment**: 19 pages
- **Topic Keywords**: rag, ctr, recommend
- **Reason**: This paper focuses on electronic health record summarization, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves data processing and summarization, the context and application are distinct from the user's areas of focus.

#### Abstract
> Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.

### 35. SongSage: A Large Musical Language Model with Lyric Generative Pre-training

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Jiani Guo, Jiajia Li, Jie Wu, Zuchao Li, Yujiu Yang, Ping Wang
- **URL**: <http://arxiv.org/abs/2601.01153v1>
- **Submitted**: 2026-01-03 10:54:37
- **Topic Keywords**: queries, recommend, search
- **Reason**: The paper focuses on developing a large musical language model for understanding lyric-centric knowledge, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models and query understanding, the context is specific to music and lyrics, and the paper's contributions do not seem to align with the user's primary focus on real-time relevance optimization and deep semantic understanding in e-commerce and other domains.

#### Abstract
> Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.

### 36. Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Antonio Colacicco, Vito Guida, Dario Di Palma, Fedelucio Narducci, Tommaso Di Noia
- **URL**: <http://arxiv.org/abs/2601.02002v1>
- **Submitted**: 2026-01-05 11:03:56
- **Topic Keywords**: retrieval, recommend, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. While it touches on Natural Language Processing, the focus is on Large Language Models and recommender systems, which is a tangential area of interest for you.

#### Abstract
> Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.

### 37. MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hyunsoo Kim, Jaewan Moon, Seongmin Park, Jongwuk Lee
- **URL**: <http://arxiv.org/abs/2601.01753v1>
- **Submitted**: 2026-01-05 03:14:23
- **Comment**: Accepted by KDD 2026
- **Topic Keywords**: rag, recommend, search
- **Reason**: This paper focuses on recommender systems, specifically proposing a new framework for cross-domain sequential recommendation. While it touches on model merging, it does not relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.

### 38. Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Yuxiang Mei, Dongxing Xu, Jiaen Liang, Yanhua Long
- **URL**: <http://arxiv.org/abs/2601.01461v1>
- **Submitted**: 2026-01-04 10:08:53
- **Comment**: 5 pages, 1 figure
- **Topic Keywords**: ranking, rank
- **Reason**: This paper is about speech language models and conversational ASR, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning and architecture design, the focus is on speech processing rather than text-based IR or NLP.

#### Abstract
> The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.

### 39. Simulated Reasoning is Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hendrik Kempt, Alon Lavie
- **URL**: <http://arxiv.org/abs/2601.02043v1>
- **Submitted**: 2026-01-05 12:00:04
- **Comment**: 21 pages
- **Topic Keywords**: relevance
- **Reason**: This paper appears to be primarily focused on the philosophical aspects of reasoning in artificial intelligence, particularly in the context of Foundational Models. While it touches on the brittleness of these models, it does not seem to be directly related to Information Retrieval, Search technologies, or any of the user's core research themes.

#### Abstract
> Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

### 40. CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Tran Sy Bao
- **URL**: <http://arxiv.org/abs/2601.01964v1>
- **Submitted**: 2026-01-05 10:15:35
- **Comment**: 9 pages, 8 tables, code available at https://github.com/transybao1393/csf-sign-language
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on sign language generation and translation, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language understanding and generation, the context and application are quite different from the user's core themes.

#### Abstract
> Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.

### 41. The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Matteo Esposito, Andrea Janes, Valentina Lenarduzzi, Davide Taibi
- **URL**: <http://arxiv.org/abs/2601.01944v1>
- **Submitted**: 2026-01-05 09:50:37
- **Comment**: ACCEPTED REGISTERED REPORT AT SANER (CORE A*) 2026
- **Topic Keywords**: relevance
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on AI, its focus is on the adoption and impact of AI libraries in Open Source Software projects, which is outside your primary areas of interest.

#### Abstract
> In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

### 42. HalluZig: Hallucination Detection using Zigzag Persistence

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shreyas N. Samaga, Gilberto Gonzalez Arroyo, Tamal K. Dey
- **URL**: <http://arxiv.org/abs/2601.01552v1>
- **Submitted**: 2026-01-04 14:55:43
- **Topic Keywords**: ltr
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, nor does it focus on query understanding, ranking models, or user behavior modeling. While it involves Natural Language Processing, the topic of hallucination detection in Large Language Models is not a central match for your research interests.

#### Abstract
> The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.

### 43. EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jicheng Ma, Guohua Wang, Xinhua Feng, Yiming Liu, Zhichao Hu, Yuhong Liu
- **URL**: <http://arxiv.org/abs/2601.01400v1>
- **Submitted**: 2026-01-04 06:40:25
- **Topic Keywords**: rag, search
- **Reason**: This paper is primarily focused on evaluating mathematical reasoning in large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the context is specific to mathematical reasoning and does not align with your interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.

### 44. FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Juan Junqueras, Florian Boudin, May-Myo Zin, Ha-Thanh Nguyen, Wachara Fungwacharakorn, Dami√°n Ariel Furman, Akiko Aizawa, Ken Satoh
- **URL**: <http://arxiv.org/abs/2601.01350v1>
- **Submitted**: 2026-01-04 03:38:46
- **Comment**: Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)
- **Topic Keywords**: retrieval, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves a retrieval system, its focus is on counterspeech and hate speech, which is not a central theme in your research. The paper's contribution is a new dataset, but it does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.

### 45. AppellateGen: A Benchmark for Appellate Legal Judgment Generation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hongkun Yang, Lionel Z. Wang, Wei Fan, Yiran Hu, Lixu Wang, Chenyu Liu, Shenghong Fu, Haoyang Li, Xin Xu, Jiexin Zheng, Wei Dong
- **URL**: <http://arxiv.org/abs/2601.01331v1>
- **Submitted**: 2026-01-04 02:15:17
- **Comment**: 15 pages, 4 figures, 3 tables
- **Topic Keywords**: retrieval, search
- **Reason**: This paper is not relevant to your research interests as it focuses on legal judgment generation and appellate review, which is outside your primary area of interest in Information Retrieval and Search technologies. Although it involves some aspects of modeling and generation, the context and application are not aligned with your core themes.

#### Abstract
> Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.

### 46. ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Anantha Sharma
- **URL**: <http://arxiv.org/abs/2601.01297v1>
- **Submitted**: 2026-01-03 22:39:20
- **Comment**: 26 pages
- **Topic Keywords**: pairwise
- **Reason**: This paper appears to be off-topic as it deals with distributional drift detection in high-dimensional data streams, which doesn't align with your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves some geometric concepts, the focus is on data stream analysis, not query understanding, ranking models, or user behavior modeling.

#### Abstract
> Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

### 47. Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sen Hu, Yuxiang Wei, Jiaxin Ran, Zhiyuan Yao, Lei Zou
- **URL**: <http://arxiv.org/abs/2601.01280v1>
- **Submitted**: 2026-01-03 20:39:39
- **Topic Keywords**: retrieval, search
- **Reason**: This paper appears to be primarily focused on dialog memory systems and their architectures, which is somewhat related to information retrieval, but does not directly address query understanding, ranking models, or user behavior modeling. The paper's focus on dialog memory systems and graph structures is more aligned with NLP and recommender systems, but the connection to IR is tenuous at best.

#### Abstract
> Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.

### 48. LLM Collusion

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shengyu Cao, Ming Hu
- **URL**: <http://arxiv.org/abs/2601.01279v1>
- **Submitted**: 2026-01-03 20:38:21
- **Comment**: 44 pages
- **Topic Keywords**: rag, recommend
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The topic of LLM collusion in a duopoly is more aligned with economics and game theory, and does not appear to involve query understanding, ranking models, or user behavior modeling.

#### Abstract
> We study how delegating pricing to large language models (LLMs) can facilitate collusion in a duopoly when both sellers rely on the same pre-trained model. The LLM is characterized by (i) a propensity parameter capturing its internal bias toward high-price recommendations and (ii) an output-fidelity parameter measuring how tightly outputs track that bias; the propensity evolves through retraining. We show that configuring LLMs for robustness and reproducibility can induce collusion via a phase transition: there exists a critical output-fidelity threshold that pins down long-run behavior. Below it, competitive pricing is the unique long-run outcome. Above it, the system is bistable, with competitive and collusive pricing both locally stable and the realized outcome determined by the model's initial preference. The collusive regime resembles tacit collusion: prices are elevated on average, yet occasional low-price recommendations provide plausible deniability. With perfect fidelity, full collusion emerges from any interior initial condition. For finite training batches of size $b$, infrequent retraining (driven by computational costs) further amplifies collusion: conditional on starting in the collusive basin, the probability of collusion approaches one as $b$ grows, since larger batches dampen stochastic fluctuations that might otherwise tip the system toward competition. The indeterminacy region shrinks at rate $O(1/\sqrt{b})$.

### 49. T3C: Test-Time Tensor Compression with Consistency Guarantees

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Ismail Lamaakal, Chaymae Yahyati, Yassine Maleh, Khalid El Makkaoui, Ibrahim Ouahbi
- **URL**: <http://arxiv.org/abs/2601.01299v1>
- **Submitted**: 2026-01-03 23:16:27
- **Topic Keywords**: ctr, rank
- **Reason**: This paper focuses on test-time tensor compression with consistency guarantees, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.

### 50. CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Yihao Liang, Ze Wang, Hao Chen, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Emad Barsoum, Zicheng Liu, Niraj K. Jha
- **URL**: <http://arxiv.org/abs/2601.02236v1>
- **Submitted**: 2026-01-05 16:09:22
- **Comment**: 33 pages, 7 figures
- **Topic Keywords**: rag
- **Reason**: The paper focuses on improving the efficiency of diffusion language models through a novel framework called CD4LM, which is primarily a contribution to the field of Natural Language Processing (NLP). While it involves some form of 'decoding', it is not directly related to query understanding, ranking models, or user behavior modeling in the context of Information Retrieval (IR).

#### Abstract
> Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive "long-jump" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM

---

