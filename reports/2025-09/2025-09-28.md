# Daily Papers Report - 2025-09-28

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: JiaYing Zheng, HaiNan Zhang, Liang Pang, YongXin Tong, ZhiMing Zheng
- **URL**: <http://arxiv.org/abs/2509.22325v1>
- **Submitted**: 2025-09-26 13:23:01
- **Comment**: 10 pages, 6 figures
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on synthetic query rewrites and their potential to capture user intent better than human annotators aligns with your interests in deep semantic understanding and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) Systems
- **Aim**: To enhance the performance of RAG systems by leveraging synthetically generated query rewrites.
- **Rationale**: Human-generated query rewrites often lack expressiveness and depth of understanding. Synthetic data can address this limitation.
- **Ground**: The paper introduces SynRewrite, a framework that utilizes ChatGPT-4o to generate synthetic query rewrites based on dialogue history, relevant documents, and gold-standard answers.
- **Experiment**: Experiments on TopiOCQA and QRECC datasets demonstrate that SynRewrite outperforms human-annotated rewrites and other query rewriting methods.
- **Takeaway**: Synthetic data-driven query rewriting is a promising approach for improving RAG system performance. The authors release their synthetic dataset to encourage further research.

#### Abstract
> Multi-turn RAG systems often face queries with colloquial omissions and
ambiguous references, posing significant challenges for effective retrieval and
generation. Traditional query rewriting relies on human annotators to clarify
queries, but due to limitations in annotators' expressive ability and depth of
understanding, manually rewritten queries often diverge from those needed in
real-world RAG systems, resulting in a gap between user intent and system
response. We observe that high-quality synthetic queries can better bridge this
gap, achieving superior performance in both retrieval and generation compared
to human rewrites. This raises an interesting question: Can rewriting models
trained on synthetic queries better capture user intent than human annotators?
In this paper, we propose SynRewrite, a synthetic data-driven query rewriting
model to generate high-quality synthetic rewrites more aligned with user
intent. To construct training data, we prompt GPT-4o with dialogue history,
current queries, positive documents, and answers to synthesize high-quality
rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue
history and queries to synthetic rewrites. Finally, we further enhance the
rewriter using the generator's feedback through the DPO algorithm to boost
end-task performance. Experiments on TopiOCQA and QRECC datasets show that
SynRewrite consistently outperforms human rewrites in both retrieval and
generation tasks. Our results demonstrate that synthetic rewrites can serve as
a scalable and effective alternative to human annotations.

---

### 2. Does Generative Retrieval Overcome the Limitations of Dense Retrieval?

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Yingchen Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2509.22116v1>
- **Submitted**: 2025-09-26 09:38:01
- **Topic Keywords**: dense retrieval, relevance, retrieval, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of neural retrieval models and their limitations. The authors investigate the theoretical and empirical differences between generative retrieval and dense retrieval, which aligns with your focus on query understanding and ranking models. However, the paper's primary focus on generative retrieval might not be directly related to your core research themes in query understanding and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Document Retrieval
- **Aim**: Investigate the performance and theoretical limitations of Dense Retrieval (DR) and Generative Retrieval (GR) models.
- **Rationale**: To compare the effectiveness and scalability of DR and GR models for document retrieval tasks.
- **Ground**: DR models utilize encoder-only architectures with locally normalized objectives and external embeddings, while GR models employ autoregressive architectures with both encoder and decoder components.
- **Experiment**: Experiments on benchmark datasets (Natural Questions and MS MARCO) evaluating DR and GR models with varying sizes and negative sampling strategies.
- **Takeaway**: GR models demonstrate theoretical advantages in calibration, representation rank, and scaling compared to DR. Empirical validation supports GR's potential to overcome DR's limitations, but further research is needed to bridge the gap between theoretical potential and real-world effectiveness.

#### Abstract
> Generative retrieval (GR) has emerged as a new paradigm in neural information
retrieval, offering an alternative to dense retrieval (DR) by directly
generating identifiers of relevant documents. In this paper, we theoretically
and empirically investigate how GR fundamentally diverges from DR in both
learning objectives and representational capacity. GR performs globally
normalized maximum-likelihood optimization and encodes corpus and relevance
information directly in the model parameters, whereas DR adopts locally
normalized objectives and represents the corpus with external embeddings before
computing similarity via a bilinear interaction. Our analysis suggests that,
under scaling, GR can overcome the inherent limitations of DR, yielding two
major benefits. First, with larger corpora, GR avoids the sharp performance
degradation caused by the optimization drift induced by DR's local
normalization. Second, with larger models, GR's representational capacity
scales with parameter size, unconstrained by the global low-rank structure that
limits DR. We validate these theoretical insights through controlled
experiments on the Natural Questions and MS MARCO datasets, across varying
negative sampling strategies, embedding dimensions, and model scales. But
despite its theoretical advantages, GR does not universally outperform DR in
practice. We outline directions to bridge the gap between GR's theoretical
potential and practical performance, providing guidance for future research in
scalable and robust generative retrieval.

---

### 3. SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation

- **LLM Score**: 7
- **Keyword Score**: 19
- **Authors**: Vianne R. Gao, Chen Xue, Marc Versage, Xie Zhou, Zhongruo Wang, Chao Li, Yeon Seonwoo, Nan Chen, Zhen Ge, Gourab Kundu, Weiqi Zhang, Tian Wang, Qingjun Cui, Trishul Chilimbi
- **URL**: <http://arxiv.org/abs/2509.21777v1>
- **Submitted**: 2025-09-26 02:27:04
- **Comment**: Generative Recommender, Recommendation System, Information Retrieval
- **Topic Keywords**: query, ranking, pointwise, pairwise, rag, retrieval, recommend, rank, search
- **Reason**: This paper introduces a novel generative recommender model, SynerGen, that bridges the gap between personalized search and recommendation. While it focuses on recommendation and search, it leverages deep semantic understanding and joint optimization, aligning with your interests in Information Retrieval and Search technologies. However, the primary focus on recommendation and the lack of explicit mention of query understanding or ranking models prevent it from being a central match.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Personalized Search and Recommendation
- **Aim**: To develop a unified generative model, SynerGen, that addresses the limitations of traditional retrieve-then-rank pipelines for personalized search and recommendation.
- **Rationale**: Traditional pipelines separate retrieval and ranking, leading to suboptimal performance and complex deployment. SynerGen aims to simplify this process by jointly optimizing both tasks within a single model.
- **Ground**: SynerGen leverages a Transformer-based decoder architecture and incorporates semantic signals, user behavioral history, and time-aware positional embeddings to capture relevant information for both search and recommendation.
- **Experiment**: SynerGen is trained using InfoNCE loss for retrieval and a hybrid pointwise-pairwise loss for ranking. It is evaluated on benchmark datasets (Book Review, eBook Search Sessions, and Session-US) and compared to existing generative recommender and joint search and recommendation models.
- **Takeaway**: SynerGen achieves superior performance compared to baselines in both recommendation-only and query-aware search settings, demonstrating the effectiveness of its unified approach and highlighting its potential for industrial-scale information access.

#### Abstract
> The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

---

### 4. GoalRank: Group-Relative Optimization for a Large Ranking Model

- **LLM Score**: 7
- **Keyword Score**: 7
- **Authors**: Kaike Zhang, Xiaobei Wang, Shuchang Liu, Hailan Yang, Xiang Li, Lantao Hu, Han Li, Qi Cao, Fei Sun, Kun Gai
- **URL**: <http://arxiv.org/abs/2509.22046v1>
- **Submitted**: 2025-09-26 08:32:16
- **Topic Keywords**: ranking, rag, recommend, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and ranking models, as it proposes a new framework for ranking called GoalRank. However, the focus on one-stage models and group-relative optimization is not a central match to your primary interests in query understanding, user behavior modeling, and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Recommender Systems
- **Aim**: To develop a novel generator-only ranking framework (GOALRANK) that outperforms existing methods in recommender systems.
- **Rationale**: A sufficiently large generator-only model can achieve a smaller approximation error compared to any finite set of generator-evaluator models.
- **Ground**: GOALRANK utilizes a generator model trained to minimize the cross-entropy loss between its output policy and a reference policy derived from a reward model and user feedback.
- **Experiment**: Extensive offline experiments and large-scale online A/B tests on four diverse datasets (ML-1M, Amazon-Book, Industry, and Industry-0.1B) were conducted to evaluate GOALRANK's performance.
- **Takeaway**: GOALRANK outperforms all baselines, exhibits strong scaling behavior, is robust to reward model bias, and demonstrates effectiveness in a real-world setting.

#### Abstract
> Mainstream ranking approaches typically follow a Generator-Evaluator
two-stage paradigm, where a generator produces candidate lists and an evaluator
selects the best one. Recent work has attempted to enhance performance by
expanding the number of candidate lists, for example, through multi-generator
settings. However, ranking involves selecting a recommendation list from a
combinatorially large space. Simply enlarging the candidate set remains
ineffective, and performance gains quickly saturate. At the same time, recent
advances in large recommendation models have shown that end-to-end one-stage
models can achieve promising performance with the expectation of scaling laws.
Motivated by this, we revisit ranking from a generator-only one-stage
perspective. We theoretically prove that, for any (finite
Multi-)Generator-Evaluator model, there always exists a generator-only model
that achieves strictly smaller approximation error to the optimal ranking
policy, while also enjoying scaling laws as its size increases. Building on
this result, we derive an evidence upper bound of the one-stage optimization
objective, from which we find that one can leverage a reward model trained on
real user feedback to construct a reference policy in a group-relative manner.
This reference policy serves as a practical surrogate of the optimal policy,
enabling effective training of a large generator-only ranker. Based on these
insights, we propose GoalRank, a generator-only ranking framework. Extensive
offline experiments on public benchmarks and large-scale online A/B tests
demonstrate that GoalRank consistently outperforms state-of-the-art methods.

---

### 5. Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong
- **URL**: <http://arxiv.org/abs/2509.22251v1>
- **Submitted**: 2025-09-26 12:14:01
- **Comment**: 11 pages, 5 figures
- **Topic Keywords**: retrieval, acl
- **Reason**: This paper explores the integration of structural and semantic information from Knowledge Graphs into Large Language Models to alleviate hallucination. While it touches on aspects of query understanding and deep semantic understanding, its primary focus is on addressing a specific issue in LLMs, which is somewhat related to your research interests in Information Retrieval and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Enhancing Large Language Models with Knowledge Graphs
- **Aim**: To develop a novel framework, SSKG-LLM, that significantly improves the performance of LLMs on knowledge-based tasks by effectively integrating both semantic and structural knowledge from KGs.
- **Rationale**: Existing methods either treat KGs as plain text or solely focus on structural information, limiting their effectiveness. SSKG-LLM addresses this by integrating both types of knowledge.
- **Ground**: SSKG-LLM leverages semantic grounding techniques for KG retrieval, GraphLM for encoding retrieved subgraphs, and a Knowledge Graph Adapter (KGA) to bridge the gap between KG and LLM embedding spaces.
- **Experiment**: Extensive experiments on benchmark datasets (Common-senseQA, SIQA, TruthfulQA) demonstrate SSKG-LLM's superior performance compared to baselines (LLMs with and without retrieval augmentation, KAPING, KG-Adapter). Ablation studies highlight the importance of each module, and analysis explores the impact of different graph traversal strategies and KG encoders.
- **Takeaway**: SSKG-LLM effectively integrates KGs with LLMs, leading to significant improvements in knowledge-based question answering. It demonstrates adaptability across different LLM architectures and datasets, showcasing its potential for broader applications.

#### Abstract
> Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Jiho Kim, Junseong Choi, Woosog Chay, Daeun Kyung, Yeonsu Kwon, Yohan Jo, Edward Choi
- **URL**: <http://arxiv.org/abs/2509.21730v1>
- **Submitted**: 2025-09-26 00:57:27
- **Topic Keywords**: retrieval, recommend, personalization
- **Reason**: The paper explores the development of proactive and personalized AI assistants, which is somewhat related to information retrieval and search technologies. However, the focus on user-assistant simulation and preference-aligned assistants is more aligned with recommender systems and NLP, rather than the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

### 7. Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Gaurav Bagwe, Saket S. Chaturvedi, Xiaolong Ma, Xiaoyong Yuan, Kuang-Ching Wang, Lan Zhang
- **URL**: <http://arxiv.org/abs/2509.22486v1>
- **Submitted**: 2025-09-26 15:33:36
- **Comment**: Accepted by EMNLP 2025
- **Topic Keywords**: query, relevance, rag, retrieval, search
- **Reason**: This paper is somewhat related to information retrieval, but its focus on fairness vulnerabilities in retrieval-augmented generation is not directly aligned with your primary research interests in query understanding, ranking models, and user behavior modeling. While it touches on the intersection of IR and NLP, the specific context of backdoor attacks and fairness in RAG is not a central match for your research themes.

#### Abstract
> Retrieval-augmented generation (RAG) enhances factual grounding by
integrating retrieval mechanisms with generative models but introduces new
attack surfaces, particularly through backdoor attacks. While prior research
has largely focused on disinformation threats, fairness vulnerabilities remain
underexplored. Unlike conventional backdoors that rely on direct
trigger-to-target mappings, fairness-driven attacks exploit the interaction
between retrieval and generation models, manipulating semantic relationships
between target groups and social biases to establish a persistent and covert
influence on content generation.
  This paper introduces BiasRAG, a systematic framework that exposes fairness
vulnerabilities in RAG through a two-phase backdoor attack. During the
pre-training phase, the query encoder is compromised to align the target group
with the intended social bias, ensuring long-term persistence. In the
post-deployment phase, adversarial documents are injected into knowledge bases
to reinforce the backdoor, subtly influencing retrieved content while remaining
undetectable under standard fairness evaluations. Together, BiasRAG ensures
precise target alignment over sensitive attributes, stealthy execution, and
resilience. Empirical evaluations demonstrate that BiasRAG achieves high attack
success rates while preserving contextual relevance and utility, establishing a
persistent and evolving threat to fairness in RAG.

### 8. Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Xiaojun Wu, Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Hui Xiong, Jia Li, Jian Guo
- **URL**: <http://arxiv.org/abs/2509.21710v1>
- **Submitted**: 2025-09-26 00:13:10
- **Comment**: 28 pages, 17 figures
- **Topic Keywords**: retriever, query, rag, retrieval
- **Reason**: This paper presents a novel framework for Large Language Models (LLMs) to reason on heterogeneous graphs, which is somewhat related to information retrieval and NLP. However, the focus is on enhancing LLMs with external knowledge, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. While the paper explores a relevant topic, it does not directly align with the user's primary research themes.

#### Abstract
> Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

### 9. GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Cehao Yang, Xiaojun Wu, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Yuanliang Sun, Jia Li, Hui Xiong, Jian Guo
- **URL**: <http://arxiv.org/abs/2509.22009v1>
- **Submitted**: 2025-09-26 07:45:56
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: The paper explores Graph Retrieval-Augmented Generation, which is a related topic to Information Retrieval, but it focuses on graph-based representations and generation, which is not a central match to your interests in query understanding, ranking models, and user behavior modeling. While it involves deep semantic understanding, the context is different from your primary focus on e-commerce and real-time relevance optimization.

#### Abstract
> Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

### 10. Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yash Saxena, Raviteja Bommireddy, Ankur Padia, Manas Gaur
- **URL**: <http://arxiv.org/abs/2509.21557v1>
- **Submitted**: 2025-09-25 20:39:26
- **Comment**: Accepted at NeurIPS 2025 LLM Evaluation Workshop
- **Topic Keywords**: rag, retrieval, recommend, search
- **Reason**: This paper explores Large Language Model (LLM) attribution, specifically focusing on citation generation and post-hoc citation methods. While it touches on retrieval-augmented methods, its primary focus is on citation correctness and coverage, which is somewhat related to our interests in information retrieval and query understanding. However, the paper's emphasis on citation and attribution is not directly aligned with our core research themes.

#### Abstract
> Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

### 11. Conversational Implicatures: Modelling Relevance Theory Probabilistically

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Christoph Unger, Hendrik Buschmeier
- **URL**: <http://arxiv.org/abs/2509.22354v1>
- **Submitted**: 2025-09-26 13:50:49
- **Topic Keywords**: relevance, rag
- **Reason**: This paper explores the application of Bayesian probability theory to pragmatics and semantics, specifically relevance-theoretic pragmatics. While it touches on the concept of relevance, which is relevant to information retrieval, the focus is on pragmatics and semantics rather than search technologies or ranking models. The connection to information retrieval is indirect and requires further context to fully understand its potential relevance.

#### Abstract
> Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

### 12. Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Lukas Twist, Jie M. Zhang, Mark Harman, Helen Yannakoudakis
- **URL**: <http://arxiv.org/abs/2509.22202v1>
- **Submitted**: 2025-09-26 11:14:38
- **Comment**: 23 pages, 5 tables
- **Topic Keywords**: queries, rag
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and query understanding, as it explores the limitations of Large Language Models (LLMs) in generating code. However, the focus on library hallucinations in LLMs and prompt engineering for mitigation is not directly aligned with your primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> Large language models (LLMs) are increasingly used to generate code, yet they
continue to hallucinate, often inventing non-existent libraries. Such library
hallucinations are not just benign errors: they can mislead developers, break
builds, and expose systems to supply chain threats such as slopsquatting.
Despite increasing awareness of these risks, little is known about how
real-world prompt variations affect hallucination rates. Therefore, we present
the first systematic study of how user-level prompt variations impact library
hallucinations in LLM-generated code. We evaluate six diverse LLMs across two
hallucination types: library name hallucinations (invalid imports) and library
member hallucinations (invalid calls from valid libraries). We investigate how
realistic user language extracted from developer forums and how user errors of
varying degrees (one- or multi-character misspellings and completely fake
names/members) affect LLM hallucination rates. Our findings reveal systemic
vulnerabilities: one-character misspellings in library names trigger
hallucinations in up to 26% of tasks, fake library names are accepted in up to
99% of tasks, and time-related prompts lead to hallucinations in up to 84% of
tasks. Prompt engineering shows promise for mitigating hallucinations, but
remains inconsistent and LLM-dependent. Our results underscore the fragility of
LLMs to natural prompt variation and highlight the urgent need for safeguards
against library-related hallucinations and their potential exploitation.

### 13. MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Xinping Lei, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
- **URL**: <http://arxiv.org/abs/2509.21978v1>
- **Submitted**: 2025-09-26 07:02:05
- **Comment**: EMNLP2025 Findings
- **Topic Keywords**: ranking, rank, iclr
- **Reason**: The paper explores the application of Large Language Models (LLMs) in ideation, which is somewhat related to Information Retrieval and Search technologies. However, the focus on LLM ideation and knowledge graphs is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While the paper touches on aspects of real-time relevance optimization, it is not a central match for the user's research interests.

#### Abstract
> Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

### 14. LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Lu Sun, Shihan Fu, Bingsheng Yao, Yuxuan Lu, Wenbo Li, Hansu Gu, Jiri Gesi, Jing Huang, Chen Luo, Dakuo Wang
- **URL**: <http://arxiv.org/abs/2509.21501v1>
- **Submitted**: 2025-09-25 19:58:02
- **Topic Keywords**: pairwise, shopping, search
- **Reason**: This paper is loosely relevant to your research interests in Information Retrieval, particularly in the context of e-commerce and user behavior modeling. However, the focus on simulating customers and evaluating agentic-AI-based shopping assistants is more aligned with recommender systems and human-computer interaction, which are not your primary areas of focus. The paper's use of LLM Agents and digital twins is an interesting application of NLP and data mining, but it does not directly address your core research themes.

#### Abstract
> Agentic AI is emerging, capable of executing tasks through natural language,
such as Copilot for coding or Amazon Rufus for shopping. Evaluating these
systems is challenging, as their rapid evolution outpaces traditional human
evaluation. Researchers have proposed LLM Agents to simulate participants as
digital twins, but it remains unclear to what extent a digital twin can
represent a specific customer in multi-turn interaction with an agentic AI
system. In this paper, we recruited 40 human participants to shop with Amazon
Rufus, collected their personas, interaction traces, and UX feedback, and then
created digital twins to repeat the task. Pairwise comparison of human and
digital-twin traces shows that while agents often explored more diverse
choices, their action patterns aligned with humans and yielded similar design
feedback. This study is the first to quantify how closely LLM agents can mirror
human multi-turn interaction with an agentic AI system, highlighting their
potential for scalable evaluation.

### 15. Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Wenyuan Chen, Fateme Nateghi Haredasht, Kameron C. Black, Francois Grolleau, Emily Alsentzer, Jonathan H. Chen, Stephen P. Ma
- **URL**: <http://arxiv.org/abs/2509.22565v1>
- **Submitted**: 2025-09-26 16:42:43
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to information retrieval, specifically in the context of retrieval-augmented evaluation and error detection in clinical messaging. However, it does not directly align with the user's core research themes in query understanding, ranking models, or user behavior modeling, and is more focused on a specific application in the healthcare domain.

#### Abstract
> Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

### 16. LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Min-Hsuan Yeh, Yixuan Li, Tanwi Mallick
- **URL**: <http://arxiv.org/abs/2509.21875v1>
- **Submitted**: 2025-09-26 04:57:46
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Retrieval-Augmented Generation (RAG) systems and hallucinations in large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus on hallucination detection and internal knowledge utilization is not directly aligned with the user's core research themes.

#### Abstract
> Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

### 17. KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Junhao Chen, Yu Huang, Siyuan Li, Rui Yao, Hanqian Li, Hanyu Zhang, Jungang Li, Jian Chen, Bowen Wang, Xuming Hu
- **URL**: <http://arxiv.org/abs/2509.21856v1>
- **Submitted**: 2025-09-26 04:32:29
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to your interests in Information Retrieval and Natural Language Processing, particularly in the context of knowledge-intensive question answering and conversational dialogue. However, it focuses more on Large Language Models and their evaluation in multi-turn dialogues, which is not a central match to your primary research themes.

#### Abstract
> Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

### 18. Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Qianen Zhang, Satoshi Nakamura
- **URL**: <http://arxiv.org/abs/2509.21801v1>
- **Submitted**: 2025-09-26 02:57:36
- **Topic Keywords**: rag, acl
- **Reason**: This paper focuses on Simultaneous Machine Translation, which is a specific application of Natural Language Processing. While it involves deep semantic understanding and real-time relevance optimization, it is not directly related to Information Retrieval or Search technologies. The paper's emphasis on machine interpretation and translation also diverges from the user's primary focus on information retrieval.

#### Abstract
> Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

### 19. IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Aayush Mishra, Daniel Khashabi, Anqi Liu
- **URL**: <http://arxiv.org/abs/2509.22621v1>
- **Submitted**: 2025-09-26 17:46:32
- **Topic Keywords**: queries
- **Reason**: The paper discusses a technique for improving supervised fine-tuning using in-context learning activations. While it touches on aspects of model adaptation and internal reasoning, which are related to query understanding and ranking models, it does not directly address information retrieval or search technologies. The connection to NLP is clear, but the focus on model adaptation and fine-tuning is somewhat tangential to the user's core research themes.

#### Abstract
> Supervised Fine-Tuning (SFT) is used to specialize model behavior by training
weights to produce intended target responses for queries. In contrast,
In-Context Learning (ICL) adapts models during inference with instructions or
demonstrations in the prompt. ICL can offer better generalizability and more
calibrated responses compared to SFT in data scarce settings, at the cost of
more inference compute. In this work, we ask the question: Can ICL's internal
computations be used to improve the qualities of SFT? We first show that ICL
and SFT produce distinct activation patterns, indicating that the two methods
achieve adaptation through different functional mechanisms. Motivated by this
observation and to use ICL's rich functionality, we introduce ICL Activation
Alignment (IA2), a self-distillation technique which aims to replicate ICL's
activation patterns in SFT models and incentivizes ICL-like internal reasoning.
Performing IA2 as a priming step before SFT significantly improves the accuracy
and calibration of model outputs, as shown by our extensive empirical results
on 12 popular benchmarks and 2 model families. This finding is not only
practically useful, but also offers a conceptual window into the inner
mechanics of model adaptation.

### 20. You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Bochuan Cao, Changjiang Li, Yuanpu Cao, Yameng Ge, Ting Wang, Jinghui Chen
- **URL**: <http://arxiv.org/abs/2509.21884v1>
- **Submitted**: 2025-09-26 05:17:38
- **Comment**: 29 pages, 10 tables, 6figures, accepted by CCS 25
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on mitigating prompt leakages in Large Language Models (LLMs), which is a topic related to Natural Language Processing (NLP). Although it touches on the security aspect of LLMs, it doesn't directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> Large language models (LLMs) have been widely adopted across various
applications, leveraging customized system prompts for diverse tasks. Facing
potential system prompt leakage risks, model developers have implemented
strategies to prevent leakage, primarily by disabling LLMs from repeating their
context when encountering known attack patterns. However, it remains vulnerable
to new and unforeseen prompt-leaking techniques. In this paper, we first
introduce a simple yet effective prompt leaking attack to reveal such risks.
Our attack is capable of extracting system prompts from various LLM-based
application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our
findings further inspire us to search for a fundamental solution to the
problems by having no system prompt in the context. To this end, we propose
SysVec, a novel method that encodes system prompts as internal representation
vectors rather than raw text. By doing so, SysVec minimizes the risk of
unauthorized disclosure while preserving the LLM's core language capabilities.
Remarkably, this approach not only enhances security but also improves the
model's general instruction-following abilities. Experimental results
demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves
the LLM's functional integrity, and helps alleviate the forgetting issue in
long-context scenarios.

### 21. "Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Madison Van Doren, Cory Holland
- **URL**: <http://arxiv.org/abs/2509.21577v1>
- **Submitted**: 2025-09-25 20:55:36
- **Topic Keywords**: commerce, e-commerce, search
- **Reason**: This paper explores the localisation capabilities of multilingual AI models, focusing on cultural appropriateness and overall localisation quality. While it touches on real-world applications like e-commerce, its primary focus is on machine translation, which is related to but distinct from information retrieval. The paper's emphasis on cultural nuances and figurative language translation is somewhat relevant to query understanding and user behavior modeling, but it does not directly align with the user's core research themes.

#### Abstract
> This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

### 22. A State-of-the-Art SQL Reasoning Model using RLVR

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Alnur Ali, Ashutosh Baheti, Jonathan Chang, Ta-Chung Chi, Brandon Cui, Andrew Drozdov, Jonathan Frankle, Abhay Gupta, Pallavi Koppol, Sean Kulinski, Jonathan Li, Dipendra Misra, Krista Opsahl-Ong, Jose Javier Gonzalez Ortiz, Matei Zaharia, Yue Zhang
- **URL**: <http://arxiv.org/abs/2509.21459v1>
- **Submitted**: 2025-09-25 19:27:35
- **Topic Keywords**: query
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. However, it focuses on SQL reasoning and reinforcement learning, which is not a central match to your primary focus on deep semantic understanding and real-time relevance optimization in IR. The application to the e-commerce domain is also not explicitly mentioned, making it less relevant to your background.

#### Abstract
> Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

### 23. CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin
- **URL**: <http://arxiv.org/abs/2509.22647v1>
- **Submitted**: 2025-09-26 17:59:55
- **Comment**: Code is available at https://github.com/InternLM/CapRL
- **Topic Keywords**: rag
- **Reason**: This paper focuses on image captioning using reinforcement learning, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the topic is more aligned with Natural Language Processing and computer vision, and the paper's emphasis on deep semantic understanding is not directly applicable to the user's primary research interests in IR and search technologies.

#### Abstract
> Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.

### 24. Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch
- **URL**: <http://arxiv.org/abs/2509.22646v1>
- **Submitted**: 2025-09-26 17:59:54
- **Comment**: Project Page: https://deeptracereward.github.io/
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and multimodal models, but it does not directly align with the user's primary focus on Information Retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization. The paper's focus on video generation and deepfake detection is not a central match for the user's research themes.

#### Abstract
> Can humans identify AI-generated (fake) videos and provide grounded reasons?
While video generation models have advanced rapidly, a critical dimension --
whether humans can detect deepfake traces within a generated video, i.e.,
spatiotemporal grounded visual artifacts that reveal a video as machine
generated -- has been largely overlooked. We introduce DeeptraceReward, the
first fine-grained, spatially- and temporally- aware benchmark that annotates
human-perceived fake traces for video generation reward. The dataset comprises
4.3K detailed annotations across 3.3K high-quality generated videos. Each
annotation provides a natural-language explanation, pinpoints a bounding-box
region containing the perceived trace, and marks precise onset and offset
timestamps. We consolidate these annotations into 9 major categories of
deepfake traces that lead humans to identify a video as AI-generated, and train
multimodal language models (LMs) as reward models to mimic human judgments and
localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by
34.7% on average across fake clue identification, grounding, and explanation.
Interestingly, we observe a consistent difficulty gradient: binary fake v.s.
real classification is substantially easier than fine-grained deepfake trace
detection; within the latter, performance degrades from natural language
explanations (easiest), to spatial grounding, to temporal labeling (hardest).
By foregrounding human-perceived deepfake traces, DeeptraceReward provides a
rigorous testbed and training signal for socially aware and trustworthy video
generation.

### 25. Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Arkadiy Saakyan, Najoung Kim, Smaranda Muresan, Tuhin Chakrabarty
- **URL**: <http://arxiv.org/abs/2509.22641v1>
- **Submitted**: 2025-09-26 17:59:05
- **Comment**: 26 pages, 10 figures, under review
- **Topic Keywords**: rag
- **Reason**: This paper explores the relationship between n-gram novelty and textual creativity, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and text generation is not a central match to your primary research interests in IR and Search technologies.

#### Abstract
> N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

### 26. We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Gautam Siddharth Kashyap, Mark Dras, Usman Naseem
- **URL**: <http://arxiv.org/abs/2509.22510v1>
- **Submitted**: 2025-09-26 15:52:21
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Large Language Model alignment, which is related to query understanding and ranking models in Information Retrieval. However, the specific context of language model alignment and its objectives (helpfulness, harmlessness, and honesty) is not directly aligned with the user's primary research interests in IR and Search technologies. The paper's relevance is somewhat tangential, but the techniques and methods discussed may have indirect applications in the user's areas of interest.

#### Abstract
> Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

### 27. Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve G√ºrel
- **URL**: <http://arxiv.org/abs/2509.22472v1>
- **Submitted**: 2025-09-26 15:19:12
- **Comment**: 39 pages, 36 figures. Code and evaluation pipeline available at
  https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs
- **Topic Keywords**: rag
- **Reason**: This paper evaluates the performance of Large Language Models in multilingual legal reasoning, which is somewhat related to information retrieval and search technologies. However, the focus on legal applications and language models is not a central match to your primary research interests in IR, query understanding, and ranking models.

#### Abstract
> In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

### 28. From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Jonne S√§lev√§, Duygu Ataman, Constantine Lignos
- **URL**: <http://arxiv.org/abs/2509.22612v1>
- **Submitted**: 2025-09-26 17:37:55
- **Comment**: Paper currently under review at ACL Rolling Review
- **Topic Keywords**: ranking, pairwise, rag, rank
- **Reason**: This paper is primarily focused on NLP evaluation benchmarks and statistical variability, which is somewhat related to the user's interests in NLP and data mining. However, it does not directly address the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

### 29. S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Shaoning Sun, Jiachen Yu, Zongqi Wang, Xuewei Yang, Tianle Gu, Yujiu Yang
- **URL**: <http://arxiv.org/abs/2509.22099v1>
- **Submitted**: 2025-09-26 09:21:17
- **Topic Keywords**: queries, rag
- **Reason**: This paper focuses on Generative Reward Models and their evaluation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on aspects of model optimization and performance, it does not align with your specific areas of focus, such as query understanding, ranking models, or user behavior modeling.

#### Abstract
> With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

### 30. Taxonomy of Comprehensive Safety for Clinical Agents

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Jean Seo, Hyunkyung Lee, Gibaeg Kim, Wooseok Han, Jaehyo Yoo, Seungseop Lim, Kihun Shin, Eunho Yang
- **URL**: <http://arxiv.org/abs/2509.22041v1>
- **Submitted**: 2025-09-26 08:22:59
- **Comment**: EMNLP 2025 Industry
- **Topic Keywords**: queries, ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on clinical chatbot safety and taxonomy, which is outside your primary areas of interest.

#### Abstract
> Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

### 31. ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim
- **URL**: <http://arxiv.org/abs/2509.21991v1>
- **Submitted**: 2025-09-26 07:15:19
- **Topic Keywords**: query, rag
- **Reason**: This paper focuses on vision-language models and efficient processing of high-resolution images, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves multimodal context and perception, the application domain and techniques used are not aligned with your primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.

### 32. UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen
- **URL**: <http://arxiv.org/abs/2509.21766v1>
- **Submitted**: 2025-09-26 02:04:00
- **Topic Keywords**: ltr, rag
- **Reason**: This paper is not relevant to your research interests as it focuses on benchmarking agent capabilities in ultra-long-horizon scenarios, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's emphasis on autonomous agents and their limitations in long-horizon tasks does not align with your primary research themes.

#### Abstract
> Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

### 33. Agribot: agriculture-specific question answer system

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Naman Jain, Pranjali Jain, Pratik Kayal, Jayakrishna Sahit, Soham Pachpande, Jayesh Choudhari
- **URL**: <http://arxiv.org/abs/2509.21535v1>
- **Submitted**: 2025-09-25 20:22:09
- **Topic Keywords**: queries, ctr
- **Reason**: This paper is not relevant to your research interests as it focuses on an agriculture-specific question-answering system, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves a chatbot and sentence embedding model, the context and application are not aligned with your core research themes.

#### Abstract
> India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

### 34. Why Chain of Thought Fails in Clinical Text Understanding

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jiageng Wu, Kevin Xie, Bowen Gu, Nils Kr√ºger, Kueiyu Joshua Lin, Jie Yang
- **URL**: <http://arxiv.org/abs/2509.21933v1>
- **Submitted**: 2025-09-26 06:18:15
- **Topic Keywords**: rag, ctr
- **Reason**: This paper is primarily focused on the evaluation of Chain-of-Thought prompting in clinical text understanding, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the topic of language models, the context is clinical and does not align with your e-commerce background or interests in real-time relevance optimization.

#### Abstract
> Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

### 35. Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Mst Eshita Khatun, Halima Akter, Tasnimul Rehan, Toufiq Ahmed
- **URL**: <http://arxiv.org/abs/2509.21579v1>
- **Submitted**: 2025-09-25 20:56:13
- **Comment**: Accepted & presented at THE 16th INTERNATIONAL IEEE CONFERENCE ON
  COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT) 2025
- **Topic Keywords**: rag, shopping, search
- **Reason**: This paper focuses on spam detection in Amazon reviews using big data analytics and machine learning, which is somewhat related to information retrieval, but primarily deals with a specific application in e-commerce, lacking a strong connection to query understanding, ranking models, or user behavior modeling.

#### Abstract
> In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

### 36. Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Gen Li, Yuling Yan
- **URL**: <http://arxiv.org/abs/2509.22633v1>
- **Submitted**: 2025-09-26 17:57:17
- **Topic Keywords**: queries
- **Reason**: This paper focuses on reinforcement learning with human feedback, which is not directly related to your core research interests in Information Retrieval and Search technologies. While it touches on the topic of human feedback, it does not explore query understanding, ranking models, or user behavior modeling, making it somewhat tangential to your research areas.

#### Abstract
> Reinforcement learning with human feedback (RLHF), which learns a reward
model from human preference data and then optimizes a policy to favor preferred
responses, has emerged as a central paradigm for aligning large language models
(LLMs) with human preferences. In this paper, we investigate exploration
principles for online RLHF, where one seeks to adaptively collect new
preference data to refine both the reward model and the policy in a
data-efficient manner. By examining existing optimism-based exploration
algorithms, we identify a drawback in their sampling protocol: they tend to
gather comparisons that fail to reduce the most informative uncertainties in
reward differences, and we prove lower bounds showing that such methods can
incur linear regret over exponentially long horizons. Motivated by this
insight, we propose a new exploration scheme that directs preference queries
toward reducing uncertainty in reward differences most relevant to policy
improvement. Under a multi-armed bandit model of RLHF, we establish regret
bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta>0$ is a hyperparameter
that balances reward maximization against mitigating distribution shift. To our
knowledge, this is the first online RLHF algorithm with regret scaling
polynomially in all model parameters.

### 37. LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru, Vipin Chaudhary
- **URL**: <http://arxiv.org/abs/2509.22631v1>
- **Submitted**: 2025-09-26 17:55:26
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on data curation in computer vision, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of data processing, the context and techniques used are specific to computer vision and do not align with your primary areas of focus.

#### Abstract
> Curating high-quality, domain-specific datasets is a major bottleneck for
deploying robust vision systems, requiring complex trade-offs between data
quality, diversity, and cost when researching vast, unlabeled data lakes. We
introduce Labeling Copilot, the first data curation deep research agent for
computer vision. A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting. Our large-scale validation proves the effectiveness of
Labeling Copilot's components. The Consensus Annotation module excels at object
discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per
image-nearly double the 7.4 ground-truth objects-achieving a final annotation
mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a
10-million sample scale, features an active learning strategy that is up to 40x
more computationally efficient than alternatives with equivalent sample
efficiency. These experiments validate that an agentic workflow with optimized,
scalable tools provides a robust foundation for curating industrial-scale
datasets.

### 38. PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hieu Tran, Zonghai Yao, Nguyen Luong Tran, Zhichao Yang, Feiyun Ouyang, Shuo Han, Razieh Rahimi, Hong Yu
- **URL**: <http://arxiv.org/abs/2509.22315v1>
- **Submitted**: 2025-09-26 13:16:36
- **Comment**: 8 pages
- **Topic Keywords**: retrieval, search
- **Reason**: This paper appears to be primarily focused on multi-agent reasoning and large language models, which doesn't align closely with your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve some form of retrieval, it's not directly related to query understanding, ranking models, or user behavior modeling.

#### Abstract
> Inspired by the dual-process theory of human cognition from \textit{Thinking,
Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated
Memory for Enhanced Reasoning), a multi-agent reasoning framework that
dynamically integrates \textbf{System 1} (fast, intuitive thinking) and
\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick
Thinking Agent (System 1) to generate a rapid answer; if uncertainty is
detected, it then triggers a structured System 2 reasoning pipeline composed of
specialized agents for \textit{planning}, \textit{hypothesis generation},
\textit{retrieval}, \textit{information integration}, and
\textit{decision-making}. This multi-agent design faithfully mimics human
cognitive processes and enhances both efficiency and accuracy. Experimental
results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to
perform competitively with state-of-the-art closed-source models like GPT-4 and
GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This
research establishes PRIME as a scalable solution for improving LLMs in domains
requiring complex, knowledge-intensive reasoning.

### 39. Multilingual Dialogue Generation and Localization with Dialogue Act Scripting

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Justin Vasselli, Eunike Andriani Kardinata, Yusuke Sakai, Taro Watanabe
- **URL**: <http://arxiv.org/abs/2509.22086v1>
- **Submitted**: 2025-09-26 09:09:08
- **Comment**: 16 pages, 10 tables, 2 figures, Accepted at EMNLP Main 2025
- **Topic Keywords**: relevance
- **Reason**: This paper focuses on multilingual dialogue generation and localization, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it involves natural language processing, the primary goal is dialogue generation rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

### 40. COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dmitriy Shopkhoev, Denis Makhov, Magauiya Zhussip, Ammar Ali, Stamatios Lefkimmiatis
- **URL**: <http://arxiv.org/abs/2509.22075v1>
- **Submitted**: 2025-09-26 08:55:09
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on post-training compression of large language models (LLMs), which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep learning and NLP, the primary topic is model compression, which is not a core area of interest for the user.

#### Abstract
> Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

### 41. RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yudong Li, Yufei Sun, Yuhan Yao, Peiru Yang, Wanyue Li, Jiajun Zou, Yongfeng Huang, Linlin Shen
- **URL**: <http://arxiv.org/abs/2509.22055v1>
- **Submitted**: 2025-09-26 08:36:45
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on a dataset for AI-generated text analysis in social media, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the specific context and application are quite different from your areas of focus.

#### Abstract
> The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

### 42. OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram Murugesan, Tejaswini Pedapati, Pin-Yu Chen
- **URL**: <http://arxiv.org/abs/2509.21623v1>
- **Submitted**: 2025-09-25 21:42:27
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on key-value cache compression for large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP concepts, the primary goal is to optimize memory usage, which is not a central theme in your research.

#### Abstract
> The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

### 43. Learning GUI Grounding with Spatial Reasoning from Visual Feedback

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yu Zhao, Wei-Ning Chen, Huseyin Atahan Inan, Samuel Kessler, Lu Wang, Lukas Wutschitz, Fangkai Yang, Chaoyun Zhang, Pasquale Minervini, Saravan Rajmohan, Robert Sim
- **URL**: <http://arxiv.org/abs/2509.21552v1>
- **Submitted**: 2025-09-25 20:38:01
- **Topic Keywords**: click, search
- **Reason**: This paper focuses on GUI grounding with spatial reasoning, which is not directly related to information retrieval, search technologies, or query understanding. While it involves visual feedback and interactive processes, the context is more aligned with computer vision and natural language processing, rather than the user's core research themes.

#### Abstract
> Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.

### 44. WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li
- **URL**: <http://arxiv.org/abs/2509.22644v1>
- **Submitted**: 2025-09-26 17:59:51
- **Topic Keywords**: rag
- **Reason**: This paper focuses on website generation using multi-level feedback and reinforcement learning, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing and large language models, the context is not aligned with the user's core research themes.

#### Abstract
> Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

### 45. EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xu Wujiang, Wentian Zhao, Zhenting Wang, Li Yu-Jhe, Jin Can, Jin Mingyu, Mei Kai, Wan Kun, Metaxas Dimitris
- **URL**: <http://arxiv.org/abs/2509.22576v1>
- **Submitted**: 2025-09-26 16:51:44
- **Topic Keywords**: rag
- **Reason**: This paper focuses on reinforcement learning for LLM agents, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on entropy regularization, the context is specific to reinforcement learning and does not align with your primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Training LLM agents in multi-turn environments with sparse rewards, where
completing a single task requires 30+ turns of interaction within an episode,
presents a fundamental challenge for reinforcement learning. We identify a
critical failure mode unique to this setting: the exploration-exploitation
cascade failure. This cascade begins with early-stage policy premature
convergence, where sparse feedback causes agents to commit to flawed,
low-entropy strategies. Subsequently, agents enter late-stage policy collapse,
where conventional entropy regularization becomes counterproductive, promoting
chaotic exploration that destabilizes training. We propose Entropy-regularized
Policy Optimization (EPO), a general framework that breaks this failure cycle
through three synergistic mechanisms: (1) adopting entropy regularization in
multi-turn settings to enhance exploration, (2) an entropy smoothing
regularizer that bounds policy entropy within historical averages to prevent
abrupt fluctuations, and (3) adaptive phase-based weighting that balances
exploration and exploitation across training. Our analysis justifies that EPO
guarantees monotonically decreasing entropy variance while maintaining
convergence. EPO achieves up to 152% performance improvement on ScienceWorld
and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn
sparse-reward settings require fundamentally different entropy control than
traditional RL, with broad implications for LLM agent training.

### 46. Does AI Coaching Prepare us for Workplace Negotiations?

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Veda Duddu, Jash Rajesh Parekh, Andy Mao, Hanyi Min, Ziang Xiao, Vedant Das Swain, Koustuv Saha
- **URL**: <http://arxiv.org/abs/2509.22545v1>
- **Submitted**: 2025-09-26 16:21:24
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The study focuses on AI coaching for workplace negotiations, which is outside your primary areas of interest.

#### Abstract
> Workplace negotiations are undermined by psychological barriers, which can
even derail well-prepared tactics. AI offers personalized and always --
available negotiation coaching, yet its effectiveness for negotiation
preparedness remains unclear. We built Trucey, a prototype AI coach grounded in
Brett's negotiation model. We conducted a between-subjects experiment (N=267),
comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by
in-depth interviews (N=15). While Trucey showed the strongest reductions in
fear relative to both comparison conditions, the Handbook outperformed both AIs
in usability and psychological empowerment. Interviews revealed that the
Handbook's comprehensive, reviewable content was crucial for participants'
confidence and preparedness. In contrast, although participants valued AI's
rehearsal capability, its guidance often felt verbose and fragmented --
delivered in bits and pieces that required additional effort -- leaving them
uncertain or overwhelmed. These findings challenge assumptions of AI
superiority and motivate hybrid designs that integrate structured,
theory-driven content with targeted rehearsal, clear boundaries, and adaptive
scaffolds to address psychological barriers and support negotiation
preparedness.

### 47. Ontological foundations for contrastive explanatory narration of robot plans

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Alberto Olivares-Alarcos, Sergi Foix, J√∫lia Borr√†s, Gerard Canal, Guillem Aleny√†
- **URL**: <http://arxiv.org/abs/2509.22493v1>
- **Submitted**: 2025-09-26 15:37:47
- **Comment**: This version was submitted to the journal Information Sciences and is
  under review since October 2024
- **Topic Keywords**: rag
- **Reason**: This paper focuses on ontological foundations for human-robot interaction, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of decision-making and explanation, the context is specific to robot plans and human-robot interaction, making it somewhat off-topic for the user's research interests.

#### Abstract
> Mutual understanding of artificial agents' decisions is key to ensuring a
trustworthy and successful human-robot interaction. Hence, robots are expected
to make reasonable decisions and communicate them to humans when needed. In
this article, the focus is on an approach to modeling and reasoning about the
comparison of two competing plans, so that robots can later explain the
divergent result. First, a novel ontological model is proposed to formalize and
reason about the differences between competing plans, enabling the
classification of the most appropriate one (e.g., the shortest, the safest, the
closest to human preferences, etc.). This work also investigates the
limitations of a baseline algorithm for ontology-based explanatory narration.
To address these limitations, a novel algorithm is presented, leveraging
divergent knowledge between plans and facilitating the construction of
contrastive narratives. Through empirical evaluation, it is observed that the
explanations excel beyond the baseline method.

### 48. JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hossain Shaikh Saadi, Minh Duc Bui, Mario Sanz-Guerrero, Katharina von der Wense
- **URL**: <http://arxiv.org/abs/2509.22490v1>
- **Submitted**: 2025-09-26 15:35:38
- **Comment**: WMT 25 Shared Task LLMs with Limited Resources for Slavic Languages:
  MT and QA
- **Topic Keywords**: retrieval
- **Reason**: This paper is primarily focused on machine translation and question answering for Slavic languages, using large language models and techniques such as retrieval-augmented generation. While it involves some aspects of natural language processing, it does not appear to be directly related to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

### 49. IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xinyu Liu, Bei Li, Jiahao Liu, Junhao Ruan, Kechen Jiao, Hongyin Tang, Jingang Wang, Xiao Tong, Jingbo Zhu
- **URL**: <http://arxiv.org/abs/2509.22463v1>
- **Submitted**: 2025-09-26 15:14:03
- **Topic Keywords**: rag
- **Reason**: The paper focuses on optimizing Transformer architectures for NLP and CV tasks, but does not relate to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling.

#### Abstract
> High-order numerical methods enhance Transformer performance in tasks like
NLP and CV, but introduce a performance-efficiency trade-off due to increased
computational overhead. Our analysis reveals that conventional efficiency
techniques, such as distillation, can be detrimental to the performance of
these models, exemplified by PCformer. To explore more optimizable ODE-based
Transformer architectures, we propose the \textbf{I}terative \textbf{I}mplicit
\textbf{E}uler \textbf{T}ransformer \textbf{(IIET)}, which simplifies
high-order methods using an iterative implicit Euler approach. This
simplification not only leads to superior performance but also facilitates
model compression compared to PCformer. To enhance inference efficiency, we
introduce \textbf{I}teration \textbf{I}nfluence-\textbf{A}ware
\textbf{D}istillation \textbf{(IIAD)}. Through a flexible threshold, IIAD
allows users to effectively balance the performance-efficiency trade-off. On
lm-evaluation-harness, IIET boosts average accuracy by 2.65\% over vanilla
Transformers and 0.8\% over PCformer. Its efficient variant, E-IIET,
significantly cuts inference overhead by 55\% while retaining 99.4\% of the
original task accuracy. Moreover, the most efficient IIET variant achieves an
average performance gain exceeding 1.6\% over vanilla Transformer with
comparable speed.

### 50. SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models

- **LLM Score**: 0
- **Keyword Score**: 5
- **Authors**: Jingkai Guo, Chaitali Chakrabarti, Deliang Fan
- **URL**: <http://arxiv.org/abs/2509.21843v1>
- **Submitted**: 2025-09-26 04:03:53
- **Comment**: 10 pages, 4 figures, 5 tables, 2 equations. Topics: Bit-flip attacks,
  adversarial attacks, large language models (LLMs)
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper is unrelated to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. It focuses on security concerns in Large Language Models, which is outside the user's primary research interests.

#### Abstract
> Model integrity of Large language models (LLMs) has become a pressing
security concern with their massive online deployment. Prior Bit-Flip Attacks
(BFAs) -- a class of popular AI weight memory fault-injection techniques -- can
severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips
can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs
and reveal that, despite the intuition of better robustness from modularity and
redundancy, only a handful of adversarial bit flips can also cause LLMs'
catastrophic accuracy degradation. However, existing BFA methods typically
focus on either integer or floating-point models separately, limiting attack
flexibility. Moreover, in floating-point models, random bit flips often cause
perturbed parameters to extreme values (e.g., flipping in exponent bit), making
it not stealthy and leading to numerical runtime error (e.g., invalid tensor
values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky
Bit-Flip Attack), which collapses LLM performance with only one single bit flip
while keeping perturbed values within benign layer-wise weight distribution. It
is achieved through iterative searching and ranking through our defined
parameter sensitivity metric, ImpactScore, which combines gradient sensitivity
and perturbation range constrained by the benign layer-wise weight
distribution. A novel lightweight SKIP searching algorithm is also proposed to
greatly reduce searching complexity, which leads to successful SBFA searching
taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma
models, with only one single bit flip, SBFA successfully degrades accuracy to
below random levels on MMLU and SST-2 in both BF16 and INT8 data formats.
Remarkably, flipping a single bit out of billions of parameters reveals a
severe security concern of SOTA LLM models.

---

