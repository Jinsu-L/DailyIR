[
    {
        "title": "Comparing Approaches to Automatic Summarization in Less-Resourced Languages",
        "abstract": "Automatic text summarization has achieved high performance in high-resourced languages like English, but comparatively less attention has been given to summarization in less-resourced languages. This work compares a variety of different approaches to summarization from zero-shot prompting of LLMs large and small to fine-tuning smaller models like mT5 with and without three data augmentation approaches and multilingual transfer. We also explore an LLM translation pipeline approach, translating from the source language to English, summarizing and translating back. Evaluating with five different metrics, we find that there is variation across LLMs in their performance across similar parameter sizes, that our multilingual fine-tuned mT5 baseline outperforms most other approaches including zero-shot LLM performance for most metrics, and that LLM as judge may be less reliable on less-resourced languages.",
        "url": "http://arxiv.org/abs/2512.24410v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24410v1",
        "arxiv_id": "2512.24410v1",
        "authors": [
            "Chester Palen-Michel",
            "Constantine Lignos"
        ],
        "submitted": "2025-12-30 18:45:00",
        "source": "arxiv",
        "comment": "Under review"
    },
    {
        "title": "Skim-Aware Contrastive Learning for Efficient Document Representation",
        "abstract": "Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.",
        "url": "http://arxiv.org/abs/2512.24373v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24373v1",
        "arxiv_id": "2512.24373v1",
        "authors": [
            "Waheed Ahmed Abro",
            "Zied Bouraoui"
        ],
        "submitted": "2025-12-30 17:33:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Factual Consistency of Text-based Explainable Recommendation Models",
        "abstract": "Text-based explainable recommendation aims to generate natural-language explanations that justify item recommendations, to improve user trust and system transparency. Although recent advances leverage LLMs to produce fluent outputs, a critical question remains underexplored: are these explanations factually consistent with the available evidence? We introduce a comprehensive framework for evaluating the factual consistency of text-based explainable recommenders. We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a ground truth that isolates and focuses on their factual content. Applying this pipeline to five categories from the Amazon Reviews dataset, we create augmented benchmarks for fine-grained evaluation of explanation quality. We further propose statement-level alignment metrics that combine LLM- and NLI-based approaches to assess both factual consistency and relevance of generated explanations. Across extensive experiments on six state-of-the-art explainable recommendation models, we uncover a critical gap: while models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), all our factuality metrics reveal alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%). These findings underscore the need for factuality-aware evaluation in explainable recommendation and provide a foundation for developing more trustworthy explanation systems.",
        "url": "http://arxiv.org/abs/2512.24366v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24366v1",
        "arxiv_id": "2512.24366v1",
        "authors": [
            "Ben Kabongo",
            "Vincent Guigue"
        ],
        "submitted": "2025-12-30 17:25:15",
        "source": "arxiv",
        "comment": "13 pages, 2 figures, 4 tables"
    },
    {
        "title": "DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images",
        "abstract": "Recent advances in dermatological image analysis have been driven by large-scale annotated datasets; however, most existing benchmarks focus on dermatoscopic images and lack patient-authored queries and clinical context, limiting their applicability to patient-centered care. To address this gap, we introduce DermaVQA-DAS, an extension of the DermaVQA dataset that supports two complementary tasks: closed-ended question answering (QA) and dermatological lesion segmentation. Central to this work is the Dermatology Assessment Schema (DAS), a novel expert-developed framework that systematically captures clinically meaningful dermatological features in a structured and standardized form. DAS comprises 36 high-level and 27 fine-grained assessment questions, with multiple-choice options in English and Chinese. Leveraging DAS, we provide expert-annotated datasets for both closed QA and segmentation and benchmark state-of-the-art multimodal models. For segmentation, we evaluate multiple prompting strategies and show that prompt design impacts performance: the default prompt achieves the best results under Mean-of-Max and Mean-of-Mean evaluation aggregation schemes, while an augmented prompt incorporating both patient query title and content yields the highest performance under majority-vote-based microscore evaluation, achieving a Jaccard index of 0.395 and a Dice score of 0.566 with BiomedParse. For closed-ended QA, overall performance is strong across models, with average accuracies ranging from 0.729 to 0.798; o3 achieves the best overall accuracy (0.798), closely followed by GPT-4.1 (0.796), while Gemini-1.5-Pro shows competitive performance within the Gemini family (0.783). We publicly release DermaVQA-DAS, the DAS schema, and evaluation protocols to support and accelerate future research in patient-centered dermatological vision-language modeling (https://osf.io/72rp3).",
        "url": "http://arxiv.org/abs/2512.24340v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24340v1",
        "arxiv_id": "2512.24340v1",
        "authors": [
            "Wen-wai Yim",
            "Yujuan Fu",
            "Asma Ben Abacha",
            "Meliha Yetisgen",
            "Noel Codella",
            "Roberto Andres Novoa",
            "Josep Malvehy"
        ],
        "submitted": "2025-12-30 16:48:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "World model inspired sarcasm reasoning with large language model agents",
        "abstract": "Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker's intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.",
        "url": "http://arxiv.org/abs/2512.24329v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24329v1",
        "arxiv_id": "2512.24329v1",
        "authors": [
            "Keito Inoshita",
            "Shinnosuke Mizuno"
        ],
        "submitted": "2025-12-30 16:31:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MaRCA: Multi-Agent Reinforcement Learning for Dynamic Computation Allocation in Large-Scale Recommender Systems",
        "abstract": "Modern recommender systems face significant computational challenges due to growing model complexity and traffic scale, making efficient computation allocation critical for maximizing business revenue. Existing approaches typically simplify multi-stage computation resource allocation, neglecting inter-stage dependencies, thus limiting global optimality. In this paper, we propose MaRCA, a multi-agent reinforcement learning framework for end-to-end computation resource allocation in large-scale recommender systems. MaRCA models the stages of a recommender system as cooperative agents, using Centralized Training with Decentralized Execution (CTDE) to optimize revenue under computation resource constraints. We introduce an AutoBucket TestBench for accurate computation cost estimation, and a Model Predictive Control (MPC)-based Revenue-Cost Balancer to proactively forecast traffic loads and adjust the revenue-cost trade-off accordingly. Since its end-to-end deployment in the advertising pipeline of a leading global e-commerce platform in November 2024, MaRCA has consistently handled hundreds of billions of ad requests per day and has delivered a 16.67% revenue uplift using existing computation resources.",
        "url": "http://arxiv.org/abs/2512.24325v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24325v1",
        "arxiv_id": "2512.24325v1",
        "authors": [
            "Wan Jiang",
            "Xinyi Zang",
            "Yudong Zhao",
            "Yusi Zou",
            "Yunfei Lu",
            "Junbo Tong",
            "Yang Liu",
            "Ming Li",
            "Jiani Shi",
            "Xin Yang"
        ],
        "submitted": "2025-12-30 16:27:41",
        "source": "arxiv",
        "comment": "12 pages, 5 figures"
    },
    {
        "title": "QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs",
        "abstract": "Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.\n  Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.",
        "url": "http://arxiv.org/abs/2512.24314v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24314v1",
        "arxiv_id": "2512.24314v1",
        "authors": [
            "Shupeng Li",
            "Weipeng Lu",
            "Linyun Liu",
            "Chen Lin",
            "Shaofei Li",
            "Zhendong Tan",
            "Hanjun Zhong",
            "Yucheng Zeng",
            "Chenghao Zhu",
            "Mengyue Liu",
            "Daxiang Dong",
            "Jianmin Wu",
            "Yunting Xiao",
            "Annan Li",
            "Danyu Liu",
            "Jingnan Zhang",
            "Licen Liu",
            "Dawei Yin",
            "Dou Shen"
        ],
        "submitted": "2025-12-30 16:10:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking",
        "abstract": "Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.",
        "url": "http://arxiv.org/abs/2512.24297v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24297v1",
        "arxiv_id": "2512.24297v1",
        "authors": [
            "Meiqi Chen",
            "Fandong Meng",
            "Jie Zhou"
        ],
        "submitted": "2025-12-30 15:39:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs",
        "abstract": "The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.",
        "url": "http://arxiv.org/abs/2512.24289v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24289v1",
        "arxiv_id": "2512.24289v1",
        "authors": [
            "Jonathan Schmoll",
            "Adam Jatowt"
        ],
        "submitted": "2025-12-30 15:28:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines corpus poisoning where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMask. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.",
        "url": "http://arxiv.org/abs/2512.24268v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24268v1",
        "arxiv_id": "2512.24268v1",
        "authors": [
            "Pankayaraj Pathmanathan",
            "Michael-Andrei Panaitescu-Liess",
            "Cho-Yu Jason Chiang",
            "Furong Huang"
        ],
        "submitted": "2025-12-30 14:43:57",
        "source": "arxiv",
        "comment": "Published at AAAI 2026 Workshop on New Frontiers in Information Retrieval [Oral]"
    },
    {
        "title": "Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning",
        "abstract": "A fine-grained data recipe is crucial for pre-training large language models, as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in embeddings, which can be roughly categorized into quality and diversity metrics. Due to the high computational cost when applied to trillion-scale token pre-training datasets such as FineWeb and DCLM, these two or more types of metrics are rarely considered jointly in a single selection process. However, in our empirical study, selecting samples based on quality metrics exhibit severe diminishing returns during long-term pre-training, while selecting on diversity metrics removes too many valuable high-quality samples, both of which limit pre-trained LLMs' capabilities. Therefore, we introduce DATAMASK, a novel and efficient joint learning framework designed for large-scale pre-training data selection that can simultaneously optimize multiple types of metrics in a unified process, with this study focusing specifically on quality and diversity metrics. DATAMASK approaches the selection process as a mask learning problem, involving iterative sampling of data masks, computation of policy gradients based on predefined objectives with sampled masks, and updating of mask sampling logits. Through policy gradient-based optimization and various acceleration enhancements, it significantly reduces selection time by 98.9% compared to greedy algorithm, enabling our study to explore joint learning within trillion-scale tokens. With DATAMASK, we select a subset of about 10% from the 15 trillion-token FineWeb dataset, termed FineWeb-Mask. Evaluated across 12 diverse tasks, we achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model.",
        "url": "http://arxiv.org/abs/2512.24265v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24265v1",
        "arxiv_id": "2512.24265v1",
        "authors": [
            "Ziqing Fan",
            "Yuqiao Xian",
            "Yan Sun",
            "Li Shen"
        ],
        "submitted": "2025-12-30 14:38:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tracing the Flow of Knowledge From Science to Technology Using Deep Learning",
        "abstract": "We develop a language similarity model suitable for working with patents and scientific publications at the same time. In a horse race-style evaluation, we subject eight language (similarity) models to predict credible Patent-Paper Citations. We find that our Pat-SPECTER model performs best, which is the SPECTER2 model fine-tuned on patents. In two real-world scenarios (separating patent-paper-pairs and predicting patent-paper-pairs) we demonstrate the capabilities of the Pat-SPECTER. We finally test the hypothesis that US patents cite papers that are semantically less similar than in other large jurisdictions, which we posit is because of the duty of candor. The model is open for the academic community and practitioners alike.",
        "url": "http://arxiv.org/abs/2512.24259v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24259v1",
        "arxiv_id": "2512.24259v1",
        "authors": [
            "Michael E. Rose",
            "Mainak Ghosh",
            "Sebastian Erhardt",
            "Cheng Li",
            "Erik Buunk",
            "Dietmar Harhoff"
        ],
        "submitted": "2025-12-30 14:36:17",
        "source": "arxiv",
        "comment": "4 tables, 7 figures"
    },
    {
        "title": "Time-Aware Adaptive Side Information Fusion for Sequential Recommendation",
        "abstract": "Incorporating item-side information, such as category and brand, into sequential recommendation is a well-established and effective approach for improving performance. However, despite significant advancements, current models are generally limited by three key challenges: they often overlook the fine-grained temporal dynamics inherent in timestamps, exhibit vulnerability to noise in user interaction sequences, and rely on computationally expensive fusion architectures. To systematically address these challenges, we propose the Time-Aware Adaptive Side Information Fusion (TASIF) framework. TASIF integrates three synergistic components: (1) a simple, plug-and-play time span partitioning mechanism to capture global temporal patterns; (2) an adaptive frequency filter that leverages a learnable gate to denoise feature sequences adaptively, thereby providing higher-quality inputs for subsequent fusion modules; and (3) an efficient adaptive side information fusion layer, this layer employs a \"guide-not-mix\" architecture, where attributes guide the attention mechanism without being mixed into the content-representing item embeddings, ensuring deep interaction while ensuring computational efficiency. Extensive experiments on four public datasets demonstrate that TASIF significantly outperforms state-of-the-art baselines while maintaining excellent efficiency in training. Our source code is available at https://github.com/jluo00/TASIF.",
        "url": "http://arxiv.org/abs/2512.24246v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24246v1",
        "arxiv_id": "2512.24246v1",
        "authors": [
            "Jie Luo",
            "Wenyu Zhang",
            "Xinming Zhang",
            "Yuan Fang"
        ],
        "submitted": "2025-12-30 14:15:06",
        "source": "arxiv",
        "comment": "10 pages. Accepted by WSDM'26"
    },
    {
        "title": "LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring",
        "abstract": "Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.",
        "url": "http://arxiv.org/abs/2512.24235v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24235v1",
        "arxiv_id": "2512.24235v1",
        "authors": [
            "May Bashendy",
            "Walid Massoud",
            "Sohaila Eltanbouly",
            "Salam Albatarni",
            "Marwan Sayed",
            "Abrar Abir",
            "Houda Bouamor",
            "Tamer Elsayed"
        ],
        "submitted": "2025-12-30 13:49:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring",
        "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.",
        "url": "http://arxiv.org/abs/2512.24181v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24181v1",
        "arxiv_id": "2512.24181v1",
        "authors": [
            "Qipeng Wang",
            "Rui Sheng",
            "Yafei Li",
            "Huamin Qu",
            "Yushi Sun",
            "Min Zhu"
        ],
        "submitted": "2025-12-30 12:31:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Training Report of TeleChat3-MoE",
        "abstract": "TeleChat3-MoE is the latest series of TeleChat large language models, featuring a Mixture-of-Experts (MoE) architecture with parameter counts ranging from 105 billion to over one trillion,trained end-to-end on Ascend NPU cluster. This technical report mainly presents the underlying training infrastructure that enables reliable and efficient scaling to frontier model sizes. We detail systematic methodologies for operator-level and end-to-end numerical accuracy verification, ensuring consistency across hardware platforms and distributed parallelism strategies. Furthermore, we introduce a suite of performance optimizations, including interleaved pipeline scheduling, attention-aware data scheduling for long-sequence training,hierarchical and overlapped communication for expert parallelism, and DVM-based operator fusion. A systematic parallelization framework, leveraging analytical estimation and integer linear programming, is also proposed to optimize multi-dimensional parallelism configurations. Additionally, we present methodological approaches to cluster-level optimizations, addressing host- and device-bound bottlenecks during large-scale training tasks. These infrastructure advancements yield significant throughput improvements and near-linear scaling on clusters comprising thousands of devices, providing a robust foundation for large-scale language model development on hardware ecosystems.",
        "url": "http://arxiv.org/abs/2512.24157v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24157v1",
        "arxiv_id": "2512.24157v1",
        "authors": [
            "Xinzhang Liu",
            "Chao Wang",
            "Zhihao Yang",
            "Zhuo Jiang",
            "Xuncheng Zhao",
            "Haoran Wang",
            "Lei Li",
            "Dongdong He",
            "Luobin Liu",
            "Kaizhe Yuan",
            "Han Gao",
            "Zihan Wang",
            "Yitong Yao",
            "Sishi Xiong",
            "Wenmin Deng",
            "Haowei He",
            "Kaidong Yu",
            "Yu Zhao",
            "Ruiyu Fang",
            "Yuhao Jiang",
            "Yingyan Li",
            "Xiaohui Hu",
            "Xi Yu",
            "Jingqi Li",
            "Yanwei Liu",
            "Qingli Li",
            "Xinyu Shi",
            "Junhao Niu",
            "Chengnuo Huang",
            "Yao Xiao",
            "Ruiwen Wang",
            "Fengkai Li",
            "Luwen Pu",
            "Kaipeng Jia",
            "Fubei Yao",
            "Yuyao Huang",
            "Xuewei He",
            "Zhuoru Jiang",
            "Ruiting Song",
            "Rui Xue",
            "Qiyi Xie",
            "Jie Zhang",
            "Zilu Huang",
            "Zhaoxi Zhang",
            "Zhilong Lu",
            "Yanhan Zhang",
            "Yin Zhang",
            "Yanlei Xue",
            "Zhu Yuan",
            "Teng Su",
            "Xin Jiang",
            "Shuangyong Song",
            "Yongxiang Li",
            "Xuelong Li"
        ],
        "submitted": "2025-12-30 11:42:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Emotional World Model",
        "abstract": "World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.",
        "url": "http://arxiv.org/abs/2512.24149v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24149v1",
        "arxiv_id": "2512.24149v1",
        "authors": [
            "Changhao Song",
            "Yazhou Zhang",
            "Hui Gao",
            "Chang Yang",
            "Peng Zhang"
        ],
        "submitted": "2025-12-30 11:26:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Activation Steering for Masked Diffusion Language Models",
        "abstract": "Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steering in MDLMs remain largely unexplored. We present an activation-steering framework for MDLMs that computes layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. These directions are applied at every reverse-diffusion step, yielding an efficient inference-time control mechanism. Experiments on LLaDA-8B-Instruct demonstrate reliable modulation of high-level attributes, with ablations examining the effects of steering across transformer sub-modules and token scope (prompt vs.\\ response).",
        "url": "http://arxiv.org/abs/2512.24143v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24143v1",
        "arxiv_id": "2512.24143v1",
        "authors": [
            "Adi Shnaidman",
            "Erin Feiglin",
            "Osher Yaari",
            "Efrat Mentel",
            "Amit Levi",
            "Raz Lapid"
        ],
        "submitted": "2025-12-30 11:10:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization",
        "abstract": "The presence of outliers in Large Language Models (LLMs) weights and activations makes them difficult to quantize. Recent work has leveraged rotations to mitigate these outliers. In this work, we propose methods that learn fusible rotations by minimizing principled and cheap proxy objectives to the weight quantization error. We primarily focus on GPTQ as the quantization method. Our main method is OptRot, which reduces weight outliers simply by minimizing the element-wise fourth power of the rotated weights. We show that OptRot outperforms both Hadamard rotations and more expensive, data-dependent methods like SpinQuant and OSTQuant for weight quantization. It also improves activation quantization in the W4A8 setting. We also propose a data-dependent method, OptRot$^{+}$, that further improves performance by incorporating information on the activation covariance. In the W4A4 setting, we see that both OptRot and OptRot$^{+}$ perform worse, highlighting a trade-off between weight and activation quantization.",
        "url": "http://arxiv.org/abs/2512.24124v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24124v1",
        "arxiv_id": "2512.24124v1",
        "authors": [
            "Advait Gadhikar",
            "Riccardo Grazzi",
            "James Hensman"
        ],
        "submitted": "2025-12-30 10:13:50",
        "source": "arxiv",
        "comment": "25 pages, 10 figures"
    },
    {
        "title": "CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation",
        "abstract": "Large Language Models (LLMs) have demonstrated a remarkable capacity in understanding user preferences for recommendation systems. However, they are constrained by several critical challenges, including their inherent \"Black-Box\" characteristics, susceptibility to knowledge hallucination, and limited online learning capacity. These factors compromise their trustworthiness and adaptability. Conversely, cognitive architectures such as Soar offer structured and interpretable reasoning processes, yet their knowledge acquisition is notoriously laborious. To address these complementary challenges, we propose a novel cognitive recommender agent called CogRec which synergizes the strengths of LLMs with the Soar cognitive architecture. CogRec leverages Soar as its core symbolic reasoning engine and leverages an LLM for knowledge initialization to populate its working memory with production rules. The agent operates on a Perception-Cognition-Action(PCA) cycle. Upon encountering an impasse, it dynamically queries the LLM to obtain a reasoned solution. This solution is subsequently transformed into a new symbolic production rule via Soar's chunking mechanism, thereby enabling robust online learning. This learning paradigm allows the agent to continuously evolve its knowledge base and furnish highly interpretable rationales for its recommendations. Extensive evaluations conducted on three public datasets demonstrate that CogRec demonstrates significant advantages in recommendation accuracy, explainability, and its efficacy in addressing the long-tail problem.",
        "url": "http://arxiv.org/abs/2512.24113v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24113v1",
        "arxiv_id": "2512.24113v1",
        "authors": [
            "Jiaxin Hu",
            "Tao Wang",
            "Bingsan Yang",
            "Hongrun Wang"
        ],
        "submitted": "2025-12-30 09:50:50",
        "source": "arxiv",
        "comment": "9 pages, 6 figures"
    },
    {
        "title": "Training a Huggingface Model on AWS Sagemaker (Without Tears)",
        "abstract": "The development of Large Language Models (LLMs) has primarily been driven by resource-rich research groups and industry partners. Due to the lack of on-premise computing resources required for increasingly complex models, many researchers are turning to cloud services like AWS SageMaker to train Hugging Face models. However, the steep learning curve of cloud platforms often presents a barrier for researchers accustomed to local environments. Existing documentation frequently leaves knowledge gaps, forcing users to seek fragmented information across the web. This demo paper aims to democratize cloud adoption by centralizing the essential information required for researchers to successfully train their first Hugging Face model on AWS SageMaker from scratch.",
        "url": "http://arxiv.org/abs/2512.24098v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24098v1",
        "arxiv_id": "2512.24098v1",
        "authors": [
            "Liling Tan"
        ],
        "submitted": "2025-12-30 09:14:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Factorized Learning for Temporally Grounded Video-Language Models",
        "abstract": "Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a \"grounding then answering with evidence referencing\" paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at https://github.com/nusnlp/d2vlm.",
        "url": "http://arxiv.org/abs/2512.24097v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24097v1",
        "arxiv_id": "2512.24097v1",
        "authors": [
            "Wenzheng Zeng",
            "Difei Gao",
            "Mike Zheng Shou",
            "Hwee Tou Ng"
        ],
        "submitted": "2025-12-30 09:13:20",
        "source": "arxiv",
        "comment": "ICCV 2025 paper. This arXiv version updates Figure 1 to include the concurrent work Qwen2.5-VL to ensure consistency with Table 1"
    },
    {
        "title": "HY-MT1.5 Technical Report",
        "abstract": "In this report, we introduce our latest translation models, HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of machine translation models developed through a holistic training framework tailored for high-performance translation. Our methodology orchestrates a multi-stage pipeline that integrates general and MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. HY-MT1.5-1.8B, the 1.8B-parameter model demonstrates remarkable parameter efficiency, comprehensively outperforming significantly larger open-source baselines (e.g., Tower-Plus-72B, Qwen3-32B) and mainstream commercial APIs (e.g., Microsoft Translator, Doubao Translator) in standard Chinese-foreign and English-foreign tasks. It achieves approximately 90% of the performance of ultra-large proprietary models such as Gemini-3.0-Pro, while marginally trailing Gemini-3.0-Pro on WMT25 and Mandarin-minority language benchmarks, it maintains a substantial lead over other competing models. Furthermore, HY-MT1.5-7B establishes a new state-of-the-art for its size class, achieving 95% of Gemini-3.0-Pro's performance on Flores-200 and surpassing it on the challenging WMT25 and Mandarin-minority language test sets. Beyond standard translation, the HY-MT1.5 series supports advanced constraints, including terminology intervention, context-aware translation, and format preservation. Extensive empirical evaluations confirm that both models offer highly competitive, robust solutions for general and specialized translation tasks within their respective parameter scales.",
        "url": "http://arxiv.org/abs/2512.24092v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24092v1",
        "arxiv_id": "2512.24092v1",
        "authors": [
            "Mao Zheng",
            "Zheng Li",
            "Tao Chen",
            "Mingyang Song",
            "Di Wang"
        ],
        "submitted": "2025-12-30 09:06:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "High-dimensional Regret Minimization",
        "abstract": "Multi-criteria decision making in large databases is very important in real world applications. Recently, an interactive query has been studied extensively in the database literature with the advantage of both the top-k query (with limited output size) and the skyline query (which does not require users to explicitly specify their preference function). This approach iteratively asks the user to select the one preferred within a set of options. Based on rounds of feedback, the query learns the implicit preference and returns the most favorable as a recommendation.\n  However, many modern applications in areas like housing or financial product markets feature datasets with hundreds of attributes. Existing interactive algorithms either fail to scale or require excessive user interactions (often exceeding 1000 rounds). Motivated by this, we propose FHDR (Fast High-Dimensional Reduction), a novel framework that takes less than 0.01s with fewer than 30 rounds of interaction. It is considered a breakthrough in the field of interactive queries since most, if not all, existing studies are not scalable to high-dimensional datasets.\n  Extensive experiments demonstrate that FHDR outperforms the best-known algorithms by at least an order of magnitude in execution time and up to several orders of magnitude in terms of the number of interactions required, establishing a new state of the art for scalable interactive regret minimization.",
        "url": "http://arxiv.org/abs/2512.24078v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24078v1",
        "arxiv_id": "2512.24078v1",
        "authors": [
            "Junyu Liao",
            "Ashwin Lall",
            "Mitsunori Ogihara",
            "Raymond Wong"
        ],
        "submitted": "2025-12-30 08:40:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models",
        "abstract": "Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are increasingly used in decision-critical domains such as healthcare, law, and finance, yet their reliability remains uncertain. They often make overconfident errors, degrade under input shifts, and lack clear uncertainty estimates. Existing evaluations are fragmented, addressing only isolated aspects. We introduce the Composite Reliability Score (CRS), a unified framework that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric. Through experiments on ten leading open-source LLMs across five QA datasets, we assess performance under baselines, perturbations, and calibration methods. CRS delivers stable model rankings, uncovers hidden failure modes missed by single metrics, and highlights that the most dependable systems balance accuracy, robustness, and calibrated uncertainty.",
        "url": "http://arxiv.org/abs/2512.24058v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24058v1",
        "arxiv_id": "2512.24058v1",
        "authors": [
            "Rohit Kumar Salla",
            "Manoj Saravanan",
            "Shrikar Reddy Kota"
        ],
        "submitted": "2025-12-30 08:07:28",
        "source": "arxiv",
        "comment": "5 pages, 4 tables, accepted at AAAI 2026"
    },
    {
        "title": "AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives",
        "abstract": "Although Large Audio-Language Models (LALMs) deliver state-of-the-art (SOTA) performance, they frequently suffer from hallucinations, e.g. generating text not grounded in the audio input. We analyze these grounding failures and identify a distinct taxonomy: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. To address this, we introduce the AHA (Audio Hallucination Alignment) framework. By leveraging counterfactual hard negative mining, our pipeline constructs a high-quality preference dataset that forces models to distinguish strict acoustic evidence from linguistically plausible fabrications. Additionally, we establish AHA-Eval, a diagnostic benchmark designed to rigorously test these fine-grained temporal reasoning capabilities. We apply this data to align Qwen2.5-Omni. The resulting model, Qwen-Audio-AHA, achieves a 13.7% improvement on AHA-Eval. Crucially, this benefit generalizes beyond our diagnostic set. Our model shows substantial gains on public benchmarks, including 1.3% on MMAU-Test and 1.6% on MMAR, outperforming latest SOTA methods.",
        "url": "http://arxiv.org/abs/2512.24052v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24052v1",
        "arxiv_id": "2512.24052v1",
        "authors": [
            "Yanxi Chen",
            "Wenhui Zhu",
            "Xiwen Chen",
            "Zhipeng Wang",
            "Xin Li",
            "Peijie Qiu",
            "Hao Wang",
            "Xuanzhao Dong",
            "Yujian Xiong",
            "Anderson Schneider",
            "Yuriy Nevmyvaka",
            "Yalin Wang"
        ],
        "submitted": "2025-12-30 07:52:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?",
        "abstract": "As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.",
        "url": "http://arxiv.org/abs/2512.24044v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24044v1",
        "arxiv_id": "2512.24044v1",
        "authors": [
            "Yuan Xin",
            "Dingfan Chen",
            "Linyi Yang",
            "Michael Backes",
            "Xiao Zhang"
        ],
        "submitted": "2025-12-30 07:36:19",
        "source": "arxiv",
        "comment": "26 pages,11 tables, 7 figures"
    },
    {
        "title": "iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning",
        "abstract": "Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.",
        "url": "http://arxiv.org/abs/2512.24014v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24014v1",
        "arxiv_id": "2512.24014v1",
        "authors": [
            "Sijia Chen",
            "Di Niu"
        ],
        "submitted": "2025-12-30 06:19:04",
        "source": "arxiv",
        "comment": "9 pages, 6 figures. The source code is publicly available at https://github.com/AgenticFinLab/latent-planning"
    },
    {
        "title": "WISE: Web Information Satire and Fakeness Evaluation",
        "abstract": "Distinguishing fake or untrue news from satire or humor poses a unique challenge due to their overlapping linguistic features and divergent intent. This study develops WISE (Web Information Satire and Fakeness Evaluation) framework which benchmarks eight lightweight transformer models alongside two baseline models on a balanced dataset of 20,000 samples from Fakeddit, annotated as either fake news or satire. Using stratified 5-fold cross-validation, we evaluate models across comprehensive metrics including accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC, MCC, Brier score, and Expected Calibration Error. Our evaluation reveals that MiniLM, a lightweight model, achieves the highest accuracy (87.58%) among all models, while RoBERTa-base achieves the highest ROC-AUC (95.42%) and strong accuracy (87.36%). DistilBERT offers an excellent efficiency-accuracy trade-off with 86.28\\% accuracy and 93.90\\% ROC-AUC. Statistical tests confirm significant performance differences between models, with paired t-tests and McNemar tests providing rigorous comparisons. Our findings highlight that lightweight models can match or exceed baseline performance, offering actionable insights for deploying misinformation detection systems in real-world, resource-constrained settings.",
        "url": "http://arxiv.org/abs/2512.24000v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24000v1",
        "arxiv_id": "2512.24000v1",
        "authors": [
            "Gaurab Chhetri",
            "Subasish Das",
            "Tausif Islam Chowdhury"
        ],
        "submitted": "2025-12-30 05:44:32",
        "source": "arxiv",
        "comment": "This is the author's preprint. Accepted to WEB&GRAPH 2026 (co-located with WSDM 2026), Boise, Idaho, USA, Feb 26, 2026. Final version will appear in WSDM 2026 Companion Proceedings. Conf: https://wsdm-conference.org/2026/ Workshop: https://aiimlab.org/events/WSDM_2026_WEB_and_GRAPH_2026_Workshop_on_Web_and_Graphs_Responsible_Intelligence_and_Social_Media.html"
    },
    {
        "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
        "abstract": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.",
        "url": "http://arxiv.org/abs/2512.23988v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23988v1",
        "arxiv_id": "2512.23988v1",
        "authors": [
            "Zhenyu Zhang",
            "Shujian Zhang",
            "John Lambert",
            "Wenxuan Zhou",
            "Zhangyang Wang",
            "Mingqing Chen",
            "Andrew Hard",
            "Rajiv Mathews",
            "Lun Wang"
        ],
        "submitted": "2025-12-30 05:09:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards",
        "abstract": "Large-scale Chinese spelling correction (CSC) remains critical for real-world text processing, yet existing LLMs and supervised methods lack robustness to novel errors and rely on costly annotations. We introduce CEC-Zero, a zero-supervision reinforcement learning framework that addresses this by enabling LLMs to correct their own mistakes. CEC-Zero synthesizes errorful inputs from clean text, computes cluster-consensus rewards via semantic similarity and candidate agreement, and optimizes the policy with PPO. It outperforms supervised baselines by 10--13 F$_1$ points and strong LLM fine-tunes by 5--8 points across 9 benchmarks, with theoretical guarantees of unbiased rewards and convergence. CEC-Zero establishes a label-free paradigm for robust, scalable CSC, unlocking LLM potential in noisy text pipelines.",
        "url": "http://arxiv.org/abs/2512.23971v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23971v1",
        "arxiv_id": "2512.23971v1",
        "authors": [
            "Zhiming Lin",
            "Kai Zhao",
            "Sophie Zhang",
            "Peilai Yu",
            "Canran Xiao"
        ],
        "submitted": "2025-12-30 03:58:38",
        "source": "arxiv",
        "comment": "AAAI'26 poster"
    },
    {
        "title": "Efficient Context Scaling with LongCat ZigZag Attention",
        "abstract": "We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.",
        "url": "http://arxiv.org/abs/2512.23966v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23966v1",
        "arxiv_id": "2512.23966v1",
        "authors": [
            "Chen Zhang",
            "Yang Bai",
            "Jiahuan Li",
            "Anchun Gui",
            "Keheng Wang",
            "Feifan Liu",
            "Guanyu Wu",
            "Yuwei Jiang",
            "Defei Bu",
            "Li Wei",
            "Haihang Jing",
            "Hongyin Tang",
            "Xin Chen",
            "Xiangzhou Huang",
            "Fengcun Li",
            "Rongxiang Weng",
            "Yulei Qian",
            "Yifan Lu",
            "Yerui Sun",
            "Jingang Wang",
            "Yuchen Xie",
            "Xunliang Cai"
        ],
        "submitted": "2025-12-30 03:39:04",
        "source": "arxiv",
        "comment": "10 pages, 3 figures, 3 tables"
    },
    {
        "title": "An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System",
        "abstract": "This research presents a cutting-edge recommendation system utilizing agentic AI for KYC (Know Your Customer in the financial domain), and its evaluation across five distinct content verticals: Advertising (Ad), News, Gossip, Sharing (User-Generated Content), and Technology (Tech). The study compares the performance of four experimental groups, grouping by the intense usage of KYC, benchmarking them against the Normalized Discounted Cumulative Gain (nDCG) metric at truncation levels of $k=1$, $k=3$, and $k=5$. By synthesizing experimental data with theoretical frameworks and industry benchmarks from platforms such as Baidu and Xiaohongshu, this research provides insight by showing experimental results for engineering a large-scale agentic recommendation system.",
        "url": "http://arxiv.org/abs/2512.23961v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23961v1",
        "arxiv_id": "2512.23961v1",
        "authors": [
            "Junjie H. Xu"
        ],
        "submitted": "2025-12-30 03:25:49",
        "source": "arxiv",
        "comment": "5 pages, 1 figure"
    },
    {
        "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
        "abstract": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.",
        "url": "http://arxiv.org/abs/2512.23959v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23959v1",
        "arxiv_id": "2512.23959v1",
        "authors": [
            "Chulun Zhou",
            "Chunkang Zhang",
            "Guoxin Yu",
            "Fandong Meng",
            "Jie Zhou",
            "Wai Lam",
            "Mo Yu"
        ],
        "submitted": "2025-12-30 03:13:10",
        "source": "arxiv",
        "comment": "21 pages"
    },
    {
        "title": "Disentangling Learning from Judgment: Representation Learning for Open Response Analytics",
        "abstract": "Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and derive text representations from sentence embeddings, incorporating centering and residualization to mitigate prompt and teacher confounds. Temporally-validated linear models quantify the contributions of each signal, and a projection surfaces model disagreements for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the residual content representation, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.",
        "url": "http://arxiv.org/abs/2512.23941v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23941v1",
        "arxiv_id": "2512.23941v1",
        "authors": [
            "Conrad Borchers",
            "Manit Patel",
            "Seiyon M. Lee",
            "Anthony F. Botelho"
        ],
        "submitted": "2025-12-30 02:06:28",
        "source": "arxiv",
        "comment": "Short research paper accepted at Learning Analytics and Knowledge (LAK '26)"
    },
    {
        "title": "Deletion Considered Harmful",
        "abstract": "In a world of information overload, understanding how we can most effectively manage information is crucial to success. We set out to understand how people view deletion, the removal of material no longer needed: does it help by reducing clutter and improving the signal to noise ratio, or does the effort required to decide to delete something make it not worthwhile? How does deletion relate to other strategies like filing; do people who spend extensive time in filing also prune their materials too? We studied the behaviour of 51 knowledge workers though a series of questionnaires and interviews to evaluate a range of tactics they used aimed at organizing, filing, and retrieving digital resources. Our study reveals that deletion is consistently under-adopted compared to other tactics such as Filing, Coverage, Ontology, and Timeliness. Moreover, the empirical data indicate that deletion is actually detrimental to retrieval success and satisfaction. In this paper, we examine the practice of deletion, review the related literature, and present detailed statistical results and clustering outcomes that underscore its adverse effects.",
        "url": "http://arxiv.org/abs/2512.23907v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23907v1",
        "arxiv_id": "2512.23907v1",
        "authors": [
            "Paul Englefield",
            "Russell Beale"
        ],
        "submitted": "2025-12-30 00:08:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining",
        "abstract": "This study investigates small-scale pretraining for Small Language Models (SLMs) to enable efficient use of limited data and compute, improve accessibility in low-resource settings and reduce costs. To enhance long-context extrapolation in compact models, we focus on Infini-attention, which builds a compressed memory from past segments while preserving local attention. In our work, we conduct an empirical study using 300M-parameter LLaMA models pretrained with Infini-attention. The model demonstrates training stability and outperforms the baseline in long-context retrieval. We identify the balance factor as a key part of the model performance, and we found that retrieval accuracy drops with repeated memory compressions over long sequences. Even so, Infini-attention still effectively compensates for the SLM's limited parameters. Particularly, despite performance degradation at a 16,384-token context, the Infini-attention model achieves up to 31% higher accuracy than the baseline. Our findings suggest that achieving robust long-context capability in SLMs benefits from architectural memory like Infini-attention.",
        "url": "http://arxiv.org/abs/2512.23862v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23862v1",
        "arxiv_id": "2512.23862v1",
        "authors": [
            "Ruizhe Huang",
            "Kexuan Zhang",
            "Yihao Fang",
            "Baifeng Yu"
        ],
        "submitted": "2025-12-29 21:02:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Trellis: Learning to Compress Key-Value Memory in Attention Models",
        "abstract": "Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.",
        "url": "http://arxiv.org/abs/2512.23852v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23852v1",
        "arxiv_id": "2512.23852v1",
        "authors": [
            "Mahdi Karami",
            "Ali Behrouz",
            "Praneeth Kacham",
            "Vahab Mirrokni"
        ],
        "submitted": "2025-12-29 20:32:10",
        "source": "arxiv",
        "comment": "In Second Conference on Language Modeling (COLM) (2025)"
    },
    {
        "title": "The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models",
        "abstract": "Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model's ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.",
        "url": "http://arxiv.org/abs/2512.23850v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23850v1",
        "arxiv_id": "2512.23850v1",
        "authors": [
            "Rahul Baxi"
        ],
        "submitted": "2025-12-29 20:29:09",
        "source": "arxiv",
        "comment": "Currently under review at TMLR"
    },
    {
        "title": "Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs",
        "abstract": "This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (>7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.",
        "url": "http://arxiv.org/abs/2512.23848v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23848v1",
        "arxiv_id": "2512.23848v1",
        "authors": [
            "Yukun Zhang",
            "Stefan Elbl Droguett",
            "Samyak Jain"
        ],
        "submitted": "2025-12-29 20:24:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation",
        "abstract": "Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model's own generation process. We evaluate whether tokens extracted from intermediate layers can serve as effective adversarial perturbations for downstream evaluation tasks. We conduct experiments on argument quality assessment using the ArgQuality dataset, with LLaMA-3.1-Instruct-8B serving as both the generator and evaluator. Our results show that attention-based adversarial examples lead to measurable drops in evaluation performance while remaining semantically similar to the original inputs. However, we also observe that substitutions drawn from certain layers and token positions can introduce grammatical degradation, limiting their practical effectiveness. Overall, our findings highlight both the promise and current limitations of using intermediate-layer representations as a principled source of adversarial examples for stress-testing LLM-based evaluation pipelines.",
        "url": "http://arxiv.org/abs/2512.23837v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23837v1",
        "arxiv_id": "2512.23837v1",
        "authors": [
            "Kaustubh Dhole"
        ],
        "submitted": "2025-12-29 19:59:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?",
        "abstract": "The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model's generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs' ability to effectively decline requests when faced with inadequate information.",
        "url": "http://arxiv.org/abs/2512.23836v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23836v1",
        "arxiv_id": "2512.23836v1",
        "authors": [
            "Dingmin Wang",
            "Ji Ma",
            "Shankar Kumar"
        ],
        "submitted": "2025-12-29 19:59:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms",
        "abstract": "Automated bias detection in news text is heavily used to support journalistic analysis and media accountability, yet little is known about how bias detection models arrive at their decisions or why they fail. In this work, we present a comparative interpretability study of two transformer-based bias detection models: a bias detector fine-tuned on the BABE dataset and a domain-adapted pre-trained RoBERTa model fine-tuned on the BABE dataset, using SHAP-based explanations. We analyze word-level attributions across correct and incorrect predictions to characterize how different model architectures operationalize linguistic bias. Our results show that although both models attend to similar categories of evaluative language, they differ substantially in how these signals are integrated into predictions. The bias detector model assigns stronger internal evidence to false positives than to true positives, indicating a misalignment between attribution strength and prediction correctness and contributing to systematic over-flagging of neutral journalistic content. In contrast, the domain-adaptive model exhibits attribution patterns that better align with prediction outcomes and produces 63\\% fewer false positives. We further demonstrate that model errors arise from distinct linguistic mechanisms, with false positives driven by discourse-level ambiguity rather than explicit bias cues. These findings highlight the importance of interpretability-aware evaluation for bias detection systems and suggest that architectural and training choices critically affect both model reliability and deployment suitability in journalistic contexts.",
        "url": "http://arxiv.org/abs/2512.23835v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23835v1",
        "arxiv_id": "2512.23835v1",
        "authors": [
            "Himel Ghosh"
        ],
        "submitted": "2025-12-29 19:58:11",
        "source": "arxiv",
        "comment": "10 pages, 8 figures"
    },
    {
        "title": "StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection",
        "abstract": "The prevalence of chronic stress represents a significant public health concern, with social media platforms like Twitter serving as important venues for individuals to share their experiences. This paper introduces StressRoBERTa, a cross-condition transfer learning approach for automatic detection of self-reported chronic stress in English tweets. The investigation examines whether continual training on clinically related conditions (depression, anxiety, PTSD), disorders with high comorbidity with chronic stress, improves stress detection compared to general language models and broad mental health models. RoBERTa is continually trained on the Stress-SMHD corpus (108M words from users with self-reported diagnoses of depression, anxiety, and PTSD) and fine-tuned on the SMM4H 2022 Task 8 dataset. StressRoBERTa achieves 82% F1-score, outperforming the best shared task system (79% F1) by 3 percentage points. The results demonstrate that focused cross-condition transfer from stress-related disorders (+1% F1 over vanilla RoBERTa) provides stronger representations than general mental health training. Evaluation on Dreaddit (81% F1) further demonstrates transfer from clinical mental health contexts to situational stress discussions.",
        "url": "http://arxiv.org/abs/2512.23813v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23813v1",
        "arxiv_id": "2512.23813v1",
        "authors": [
            "Amal Alqahtani",
            "Efsun Kayi",
            "Mona Diab"
        ],
        "submitted": "2025-12-29 19:16:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MiMo-Audio: Audio Language Models are Few-Shot Learners",
        "abstract": "Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio's pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-Audio.",
        "url": "http://arxiv.org/abs/2512.23808v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23808v1",
        "arxiv_id": "2512.23808v1",
        "authors": [
            "Xiaomi LLM-Core Team",
            ":",
            "Dong Zhang",
            "Gang Wang",
            "Jinlong Xue",
            "Kai Fang",
            "Liang Zhao",
            "Rui Ma",
            "Shuhuai Ren",
            "Shuo Liu",
            "Tao Guo",
            "Weiji Zhuang",
            "Xin Zhang",
            "Xingchen Song",
            "Yihan Yan",
            "Yongzhe He",
            "Cici",
            "Bowen Shen",
            "Chengxuan Zhu",
            "Chong Ma",
            "Chun Chen",
            "Heyu Chen",
            "Jiawei Li",
            "Lei Li",
            "Menghang Zhu",
            "Peidian Li",
            "Qiying Wang",
            "Sirui Deng",
            "Weimin Xiong",
            "Wenshan Huang",
            "Wenyu Yang",
            "Yilin Jiang",
            "Yixin Yang",
            "Yuanyuan Tian",
            "Yue Ma",
            "Yue Yu",
            "Zihan Zhang",
            "Zihao Yue",
            "Bangjun Xiao",
            "Bingquan Xia",
            "Bofei Gao",
            "Bowen Ye",
            "Can Cai",
            "Chang Liu",
            "Chenhong He",
            "Chunan Li",
            "Dawei Zhu",
            "Duo Zhang",
            "Fengyuan Shi",
            "Guoan Wang",
            "Hailin Zhang",
            "Hanglong Lv",
            "Hanyu Li",
            "Hao Tian",
            "Heng Qu",
            "Hongshen Xu",
            "Houbin Zhang",
            "Huaqiu Liu",
            "Jiangshan Duo",
            "Jianguang Zuo",
            "Jianyu Wei",
            "Jiebao Xiao",
            "Jinhao Dong",
            "Jun Shi",
            "Junhao Hu",
            "Kainan Bao",
            "Kang Zhou",
            "Linghao Zhang",
            "Meng Chen",
            "Nuo Chen",
            "Peng Zhang",
            "Qianli Chen",
            "Qiantong Wang",
            "Rang Li",
            "Shaohui Liu",
            "Shengfan Wang",
            "Shicheng Li",
            "Shihua Yu",
            "Shijie Cao",
            "Shimao Chen",
            "Shuhao Gu",
            "Weikun Wang",
            "Wenhan Ma",
            "Xiangwei Deng",
            "Xing Yong",
            "Xing Zhang",
            "Xu Wang",
            "Yifan Song",
            "Yihao Zhao",
            "Yingbo Zhao",
            "Yizhao Gao",
            "Yu Cheng",
            "Yu Tu",
            "Yudong Wang",
            "Zhaojun Huang",
            "Zhengju Tang",
            "Zhenru Lin",
            "Zhichao Song",
            "Zhipeng Xu",
            "Zhixian Zheng",
            "Zihan Jiang"
        ],
        "submitted": "2025-12-29 19:06:05",
        "source": "arxiv",
        "comment": null
    }
]