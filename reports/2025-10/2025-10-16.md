# Daily Papers Report - 2025-10-16

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Jiale Han, Austin Cheung, Yubai Wei, Zheng Yu, Xusheng Wang, Bing Zhu, Yi Yang
- **URL**: <http://arxiv.org/abs/2510.13590v1>
- **Submitted**: 2025-10-15 14:21:08
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The focus on temporal knowledge and incremental updates aligns with your interest in real-time relevance optimization, and the use of temporal graphs and multi-granularity temporal summaries is an innovative approach to deep semantic understanding.

#### Abstract
> Knowledge is inherently time-sensitive and continuously evolves over time.
Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with
external knowledge, they largely ignore this temporal nature. This raises two
challenges for RAG. First, current RAG methods lack effective time-aware
representations. Same facts of different time are difficult to distinguish with
vector embeddings or conventional knowledge graphs. Second, most RAG
evaluations assume a static corpus, leaving a blind spot regarding update costs
and retrieval stability as knowledge evolves. To make RAG time-aware, we
propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level
temporal graph consisting of a temporal knowledge graph with timestamped
relations and a hierarchical time graph. Multi-granularity temporal summaries
are generated for each time node to capture both key events and broader trends
at that time. The design supports incremental updates by extracting new
temporal facts from the incoming corpus and merging them into the existing
graph. The temporal graph explicitly represents identical facts at different
times as distinct edges to avoid ambiguity, and the time hierarchy graph allows
only generating reports for new leaf time nodes and their ancestors, ensuring
effective and efficient updates. During inference, TG-RAG dynamically retrieves
a subgraph within the temporal and semantic scope of the query, enabling
precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive
question-answering dataset featuring both specific and abstract queries, along
with a comprehensive evaluation protocol designed to assess incremental update
capabilities of RAG systems. Extensive experiments show that TG-RAG
significantly outperforms existing baselines, demonstrating the effectiveness
of our method in handling temporal knowledge and incremental updates.

---

### 2. BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Jia-Chen Gu, Junyi Zhang, Di Wu, Yuankai Li, Kai-Wei Chang, Nanyun Peng
- **URL**: <http://arxiv.org/abs/2510.13799v1>
- **Submitted**: 2025-10-15 17:57:45
- **Comment**: Code and data: https://github.com/JasonForJoy/BRIEF
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the context of query understanding and ranking models. The paper discusses a novel approach to compressing retrieved documents for faster and more accurate multi-hop reasoning, which aligns with your focus on real-time relevance optimization. However, the primary application of this work is in natural language generation, which is a related but distinct area of research.

#### Abstract
> As retrieval-augmented generation (RAG) tackles complex tasks, increasingly
expanded contexts offer richer information, but at the cost of higher latency
and increased cognitive load on the model. To mitigate this bottleneck,
especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a
universal, lightweight compressor that distills relevant evidence for a given
query from retrieved documents into a concise summary for seamless integration
into in-context RAG. Using seed data consisting of relatively short contexts
(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression
of extended contexts exceeding 10k words across a wide range of scenarios.
Furthermore, BRIEF-Pro offers flexible user control over summary length by
allowing users to specify the desired number of sentences. Experiments on four
open-domain multi-hop question-answering datasets show that BRIEF-Pro generates
more concise and relevant summaries, enhancing performance across small, large,
and proprietary language models. With the 70B reader model, 32x compression by
BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,
while requiring only 23% of its computational overhead.

---

### 3. Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Zhiqi Huang, Vivek Datla, Chenyang Zhu, Alfy Samuel, Daben Liu, Anoop Kumar, Ritesh Soni
- **URL**: <http://arxiv.org/abs/2510.13750v2>
- **Submitted**: 2025-10-15 16:55:56
- **Comment**: UncertaiNLP at EMNLP 2025
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is highly relevant to the field of Information Retrieval, particularly in the context of retrieval-augmented generation (RAG) systems and large language models (LLMs). While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it explores confidence estimation and uncertainty quantification in LLMs, which is a related and important aspect of IR. The paper's application in a real-world financial industry setting also aligns with the user's e-commerce background.

#### Abstract
> We propose a method for confidence estimation in retrieval-augmented
generation (RAG) systems that aligns closely with the correctness of large
language model (LLM) outputs. Confidence estimation is especially critical in
high-stakes domains such as finance and healthcare, where the cost of an
incorrect answer outweighs that of not answering the question. Our approach
extends prior uncertainty quantification methods by leveraging raw feed-forward
network (FFN) activations as auto-regressive signals, avoiding the information
loss inherent in token logits and probabilities after projection and softmax
normalization. We model confidence prediction as a sequence classification
task, and regularize training with a Huber loss term to improve robustness
against noisy supervision. Applied in a real-world financial industry
customer-support setting with complex knowledge bases, our method outperforms
strong baselines and maintains high accuracy under strict latency constraints.
Experiments on Llama 3.1 8B model show that using activations from only the
16th layer preserves accuracy while reducing response latency. Our results
demonstrate that activation-based confidence modeling offers a scalable,
architecture-aware path toward trustworthy RAG deployment.

---

### 4. Assessing Web Search Credibility and Response Groundedness in Chat Assistants

- **LLM Score**: 8
- **Keyword Score**: 2
- **Authors**: Ivan Vykopal, Mat√∫≈° Pikuliak, Simon Ostermann, Mari√°n ≈†imko
- **URL**: <http://arxiv.org/abs/2510.13749v1>
- **Submitted**: 2025-10-15 16:55:47
- **Topic Keywords**: web search, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of web search and credibility assessment. The study focuses on evaluating chat assistants' web search behavior, which aligns with your interest in query understanding and ranking models. Although the paper's primary focus is on NLP and chat assistants, its implications for information retrieval and credibility assessment make it a useful contribution to your field.

#### Abstract
> Chat assistants increasingly integrate web search functionality, enabling
them to retrieve and cite external sources. While this promises more reliable
answers, it also raises the risk of amplifying misinformation from
low-credibility sources. In this paper, we introduce a novel methodology for
evaluating assistants' web search behavior, focusing on source credibility and
the groundedness of responses with respect to cited sources. Using 100 claims
across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,
and Qwen Chat. Our findings reveal differences between the assistants, with
Perplexity achieving the highest source credibility, whereas GPT-4o exhibits
elevated citation of non-credibility sources on sensitive topics. This work
provides the first systematic comparison of commonly used chat assistants for
fact-checking behavior, offering a foundation for evaluating AI systems in
high-stakes information environments.

---

### 5. MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts

- **LLM Score**: 7
- **Keyword Score**: 7
- **Authors**: Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li
- **URL**: <http://arxiv.org/abs/2510.13500v1>
- **Submitted**: 2025-10-15 12:50:33
- **Comment**: Preprint, work in progress
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of medical applications and retrieval-based editing. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it explores a novel approach to editing medical Large Language Models (LLMs) with a focus on real-time relevance optimization, which aligns with your broader interests in IR and NLP.

#### Abstract
> LLMs hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation overlap within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
LLMs. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Jingyi Zhou, Cheng Chen, Kai Zuo, Manjie Xu, Zhendong Fu, Yibo Chen, Xu Tang, Yao Hu
- **URL**: <http://arxiv.org/abs/2510.13738v1>
- **Submitted**: 2025-10-15 16:45:59
- **Topic Keywords**: queries, rag, user behavior, recommend
- **Reason**: The paper focuses on sequential recommendation using large language models, which is somewhat related to the user's interests in information retrieval and search technologies. However, the primary focus on recommender systems and sequential recommendation is not a central match for the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on deep semantic understanding and real-time relevance optimization is also not a primary focus.

#### Abstract
> Large language models (LLMs) have recently demonstrated strong potential for
sequential recommendation. However, current LLM-based approaches face critical
limitations in modeling users' long-term and diverse interests. First, due to
inference latency and feature fetching bandwidth constraints, existing methods
typically truncate user behavior sequences to include only the most recent
interactions, resulting in the loss of valuable long-range preference signals.
Second, most current methods rely on next-item prediction with a single
predicted embedding, overlooking the multifaceted nature of user interests and
limiting recommendation diversity. To address these challenges, we propose
HyMiRec, a hybrid multi-interest sequential recommendation framework, which
leverages a lightweight recommender to extracts coarse interest embeddings from
long user sequences and an LLM-based recommender to captures refined interest
embeddings. To alleviate the overhead of fetching features, we introduce a
residual codebook based on cosine similarity, enabling efficient compression
and reuse of user history embeddings. To model the diverse preferences of
users, we design a disentangled multi-interest learning module, which leverages
multiple interest queries to learn disentangles multiple interest signals
adaptively, allowing the model to capture different facets of user intent.
Extensive experiments are conducted on both benchmark datasets and a collected
industrial dataset, demonstrating our effectiveness over existing
state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec
brings consistent improvements in real-world recommendation systems.

### 7. NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Run Luo, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua
- **URL**: <http://arxiv.org/abs/2510.13721v2>
- **Submitted**: 2025-10-15 16:25:18
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper focuses on multimodal foundation models, which is somewhat related to information retrieval, particularly in areas that require deep semantic understanding. However, the primary focus on multimodal generation and interaction, rather than search or ranking, limits its direct relevance to the user's core research themes.

#### Abstract
> Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal retrieval.
In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model
that achieves unified modeling through discrete flow paradigms. By leveraging
metric-induced probability paths and kinetic optimal velocities, NExT-OMNI
natively supports any-to-any understanding and generation with enhanced
response efficiency, while enabling broader application scenarios through
concise unified representations rather than task-decoupled designs. Trained on
large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers
competitive performance on multimodal generation and understanding benchmarks,
while outperforming prior unified models in multi-turn multimodal interaction
and cross-modal retrieval, highlighting its architectural advantages as a
next-generation multimodal foundation model. To advance further research, we
release training details, data protocols, and open-source both the code and
model checkpoints.

### 8. LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Tommaso Bonomo, Luca Gioffr√©, Roberto Navigli
- **URL**: <http://arxiv.org/abs/2510.13494v1>
- **Submitted**: 2025-10-15 12:43:59
- **Comment**: Accepted to EMNLP 2025 Main Conference. 22 pages
- **Topic Keywords**: ranking, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the area of question answering and evaluation metrics. However, it focuses on narrative text and literary works, which is a specific domain that may not be central to your interests. The use of deep learning models and evaluation metrics is relevant, but the paper's scope is more aligned with NLP and text analysis rather than search technologies or user behavior modeling.

#### Abstract
> Question Answering (QA) on narrative text poses a unique challenge to current
systems, requiring a deep understanding of long, complex documents. However,
the reliability of NarrativeQA, the most widely used benchmark in this domain,
is hindered by noisy documents and flawed QA pairs. In this work, we introduce
LiteraryQA, a high-quality subset of NarrativeQA focused on literary works.
Using a human- and LLM-validated pipeline, we identify and correct low-quality
QA samples while removing extraneous text from source documents. We then carry
out a meta-evaluation of automatic metrics to clarify how systems should be
evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics
have a low system-level correlation to human judgment, while LLM-as-a-Judge
evaluations, even with small open-weight models, can strongly agree with the
ranking identified by humans. Finally, we benchmark a set of long-context LLMs
on LiteraryQA. We release our code and data at
https://github.com/SapienzaNLP/LiteraryQA.

### 9. MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang
- **URL**: <http://arxiv.org/abs/2510.13614v1>
- **Submitted**: 2025-10-15 14:43:31
- **Topic Keywords**: retrieval
- **Reason**: The paper explores temporal knowledge graph enhanced large language model reasoning, which is somewhat related to query understanding and ranking models in the context of information retrieval. However, the focus on temporal understanding and reasoning is not directly aligned with the user's primary research interests in IR and NLP.

#### Abstract
> Large Language Models (LLMs) have achieved impressive reasoning abilities,
but struggle with temporal understanding, especially when questions involve
multiple entities, compound operators, and evolving event sequences. Temporal
Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a
structured format, offer a reliable source for temporal reasoning. However,
existing TKG-based LLM reasoning methods still struggle with four major
challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving
multi-entity temporal synchronization, adapting retrieval to diverse temporal
operators, and reusing prior reasoning experience for stability and efficiency.
To address these issues, we propose MemoTime, a memory-augmented temporal
knowledge graph framework that enhances LLM reasoning through structured
grounding, recursive reasoning, and continual experience learning. MemoTime
decomposes complex temporal questions into a hierarchical Tree of Time,
enabling operator-aware reasoning that enforces monotonic timestamps and
co-constrains multiple entities under unified temporal bounds. A dynamic
evidence retrieval layer adaptively selects operator-specific retrieval
strategies, while a self-evolving experience memory stores verified reasoning
traces, toolkit decisions, and sub-question embeddings for cross-type reuse.
Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime
achieves overall state-of-the-art results, outperforming the strong baseline by
up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to
achieve reasoning performance comparable to that of GPT-4-Turbo.

### 10. Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yang Li, Zhichen Dong, Yuhan Sun, Weixun Wang, Shaopan Xiong, Yijia Luo, Jiashun Liu, Han Lu, Jiamang Wang, Wenbo Su, Bo Zheng, Junchi Yan
- **URL**: <http://arxiv.org/abs/2510.13554v1>
- **Submitted**: 2025-10-15 13:49:51
- **Comment**: 23 pages, 8 figures, 5 tables
- **Topic Keywords**: rag
- **Reason**: This paper explores the reasoning patterns of Large Language Models (LLMs) using attention mechanisms, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and reinforcement learning is not directly aligned with the user's primary research interests in IR and search technologies.

#### Abstract
> The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

### 11. NOSA: Native and Offloadable Sparse Attention

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu
- **URL**: <http://arxiv.org/abs/2510.13602v1>
- **Submitted**: 2025-10-15 14:33:16
- **Comment**: Preprint
- **Topic Keywords**: query
- **Reason**: This paper focuses on improving the efficiency of Large Language Models (LLMs) through sparse attention, which is not directly related to information retrieval, query understanding, or user behavior modeling. While it involves ranking models, the context is not relevant to the user's primary research interests in IR and NLP.

#### Abstract
> Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).

### 12. Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Pasin Buakhaw, Kun Kerdthaisong, Phuree Phenhiran, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot
- **URL**: <http://arxiv.org/abs/2510.13586v1>
- **Submitted**: 2025-10-15 14:17:23
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on using large language models for game dialogue, which is a specific application of NLP. While it involves some aspects of query understanding and ranking models, it is not directly related to the user's core research themes in Information Retrieval and Search technologies. The paper's emphasis on task-oriented dialogue and persona-consistent dialogue generation does not align with the user's primary focus on real-time relevance optimization and deep semantic understanding in IR.

#### Abstract
> The emergence of large language models (LLMs) has opened new opportunities
for cre- ating dynamic non-player characters (NPCs) in gaming environments,
enabling both func- tional task execution and persona-consistent dialogue
generation. In this paper, we (Tu_Character_lab) report our participation in
the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which
eval- uates agents across three tracks: task-oriented dialogue, context-aware
dialogue, and their integration. Our approach combines two complementary
strategies: (i) lightweight prompting techniques in the API track, including a
Deflanderization prompting method to suppress excessive role-play and improve
task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging
Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our
best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on
Task 3 (GPU track).

### 13. K-Merge: Online Continual Merging of Adapters for On-device Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Donald Shenaj, Ondrej Bohdal, Taha Ceritli, Mete Ozay, Pietro Zanuttigh, Umberto Michieli
- **URL**: <http://arxiv.org/abs/2510.13537v1>
- **Submitted**: 2025-10-15 13:32:25
- **Comment**: 15 pages, 8 figures
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on on-device deployment of Large Language Models and model merging techniques, which is somewhat related to information retrieval and search technologies. However, the primary focus is on model adaptation and merging, rather than query understanding, ranking models, or user behavior modeling, making it less relevant to your core research interests.

#### Abstract
> On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

### 14. Generative Universal Verifier as Multimodal Meta-Reasoner

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xinchen Zhang, Xiaoying Zhang, Youbin Wu, Yanbin Cao, Renrui Zhang, Ruihang Chu, Ling Yang, Yujiu Yang
- **URL**: <http://arxiv.org/abs/2510.13804v1>
- **Submitted**: 2025-10-15 17:59:24
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multimodal reasoning and visual verification in vision-language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of 'verification', it's more aligned with computer vision and multimodal understanding rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

### 15. Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Giovanni Monea, Yair Feldman, Shankar Padmanabhan, Kiant√© Brantley, Yoav Artzi
- **URL**: <http://arxiv.org/abs/2510.13797v1>
- **Submitted**: 2025-10-15 17:57:21
- **Topic Keywords**: rag
- **Reason**: This paper appears to be related to Natural Language Processing (NLP) and large language models, but it focuses on memory-efficient reasoning and compression techniques. While it touches on scalability and computational costs, it does not seem to directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research. Therefore, the paper is only loosely relevant to your research interests.

#### Abstract
> The scalability of large language models for long-context reasoning is
severely constrained by the linear growth of their Transformer key-value cache,
which incurs significant memory and computational costs. We posit that as a
model generates reasoning tokens, the informational value of past generated
tokens diminishes, creating an opportunity for compression. In this work, we
propose to periodically compress the generation KV cache with a learned,
special-purpose token and evict compressed entries. We train the model to
perform this compression via a modified joint distillation and reinforcement
learning (RL) framework. Our training method minimizes overhead over the
conventional RL process, as it leverages RL outputs for distillation.
Empirically, our method achieves a superior memory-accuracy Pareto frontier
compared to both the model without cache compression and training-free
compression techniques.

### 16. Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Stefan Lenz, Lakisha Ortiz Rosario, Georg Vollmar, Arsenij Ustjanzew, Fatma Alickovic, Thomas Kindler, Torsten Panholzer
- **URL**: <http://arxiv.org/abs/2510.13624v1>
- **Submitted**: 2025-10-15 14:51:28
- **Comment**: 19 pages, 4 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are the core areas of your research interests. While it involves Natural Language Processing (NLP) and fine-tuning of Large Language Models (LLMs), the focus is on medical documentation tasks, specifically ICD coding of tumor diagnoses, which is not a central match to your research themes.

#### Abstract
> Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential
for structured cancer documentation in Germany. Smaller open-weight LLMs are
appealing for privacy-preserving automation but often struggle with coding
accuracy in German-language contexts. This study investigates whether
instruction-based fine-tuning on public datasets improves the coding accuracy
of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded
diagnoses from the local tumor documentation system as test data. In a
systematic data quality assessment, the upper limit for ICD-10 coding
performance was estimated at 60-79% for exact and 81-94% for partial
(three-character codes only) derivation. As training data, over 500,000
question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS
catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families
(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to
41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3
topography coding also improved but started and remained considerably lower
with an exact accuracy of 22-40% and a partial accuracy of 56-67% after
fine-tuning. Malformed code outputs dropped to 0% for all models.
Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with
model size, but gaps between small and large models narrowed after fine-tuning.
The reasoning mode in Qwen3 generally yielded a lower performance than
fine-tuning and was over 100 times slower. Our findings highlight the potential
of leveraging public catalogues to build instruction datasets that improve LLMs
in medical documentation tasks. The complete training dataset and the
best-performing checkpoints of the fine-tuned models are available from
https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.

### 17. GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Xiuyuan Chen, Tao Sun, Dexin Su, Ailing Yu, Junwei Liu, Zhe Chen, Gangzeng Jin, Xin Wang, Jingnan Liu, Hansong Xiao, Hualei Zhou, Dongjie Tao, Chunxiao Guo, Minghui Yang, Yuan Xia, Jing Zhao, Qianrui Fan, Yanyun Wang, Shuai Zhen, Kezhong Chen, Jun Wang, Zewen Sun, Heng Zhao, Tian Guan, Shaodong Wang, Geyun Chang, Jiaming Deng, Hongchengcheng Chen, Kexin Feng, Ruzhen Li, Jiayi Geng, Changtai Zhao, Jun Wang, Guihu Lin, Peihao Li, Liqi Liu, Peng Wei, Jian Wang, Jinjie Gu, Ping Wang, Fan Yang
- **URL**: <http://arxiv.org/abs/2510.13734v1>
- **Submitted**: 2025-10-15 16:40:28
- **Topic Keywords**: search
- **Reason**: This paper appears to be focused on AI clinician systems and their evaluation, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper does involve some aspects of deep semantic understanding and model evaluation, it is not directly relevant to the user's core research themes.

#### Abstract
> Current benchmarks for AI clinician systems, often based on multiple-choice
exams or manual rubrics, fail to capture the depth, robustness, and safety
required for real-world clinical practice. To address this, we introduce the
GAPS framework, a multidimensional paradigm for evaluating \textbf{G}rounding
(cognitive depth), \textbf{A}dequacy (answer completeness),
\textbf{P}erturbation (robustness), and \textbf{S}afety. Critically, we
developed a fully automated, guideline-anchored pipeline to construct a
GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity
limitations of prior work. Our pipeline assembles an evidence neighborhood,
creates dual graph and tree representations, and automatically generates
questions across G-levels. Rubrics are synthesized by a DeepResearch agent that
mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring
is performed by an ensemble of large language model (LLM) judges. Validation
confirmed our automated questions are high-quality and align with clinician
judgment. Evaluating state-of-the-art models on the benchmark revealed key
failure modes: performance degrades sharply with increased reasoning depth
(G-axis), models struggle with answer completeness (A-axis), and they are
highly vulnerable to adversarial perturbations (P-axis) as well as certain
safety issues (S-axis). This automated, clinically-grounded approach provides a
reproducible and scalable method for rigorously evaluating AI clinician systems
and guiding their development toward safer, more reliable clinical practice.

### 18. How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Matthieu Dubois, Fran√ßois Yvon, Pablo Piantanida
- **URL**: <http://arxiv.org/abs/2510.13681v1>
- **Submitted**: 2025-10-15 15:36:45
- **Comment**: EMNLP 2025 Findings
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. While it touches on Natural Language Processing, its focus on detecting machine-written texts and the impact of sampling on detection accuracy does not align with your primary research themes.

#### Abstract
> As texts generated by Large Language Models (LLMs) are ever more common and
often indistinguishable from human-written content, research on automatic text
detection has attracted growing attention. Many recent detectors report
near-perfect accuracy, often boasting AUROC scores above 99\%. However, these
claims typically assume fixed generation settings, leaving open the question of
how robust such systems are to changes in decoding strategies. In this work, we
systematically examine how sampling-based decoding impacts detectability, with
a focus on how subtle variations in a model's (sub)word-level distribution
affect detection performance. We find that even minor adjustments to decoding
parameters - such as temperature, top-p, or nucleus sampling - can severely
impair detector accuracy, with AUROC dropping from near-perfect levels to 1\%
in some settings. Our findings expose critical blind spots in current detection
methods and emphasize the need for more comprehensive evaluation protocols. To
facilitate future research, we release a large-scale dataset encompassing 37
decoding configurations, along with our code and evaluation framework
https://github.com/BaggerOfWords/Sampling-and-Detection

### 19. Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Daniil Gurgurov, Josef van Genabith, Simon Ostermann
- **URL**: <http://arxiv.org/abs/2510.13580v1>
- **Submitted**: 2025-10-15 14:14:49
- **Comment**: preprint
- **Topic Keywords**: search
- **Reason**: This paper focuses on enhancing language models for underrepresented languages, which is outside the primary scope of information retrieval and search technologies. Although it involves fine-tuning and adaptation of large language models, the context and goals are distinct from query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models exhibit uneven performance across languages, with
substantial gaps between high- and low-resource languages. We present a
framework for enhancing monolingual capabilities of LLMs in underrepresented
languages while preserving their general-purpose performance through targeted
fine-tuning of language-specific subnetworks. Our approach identifies
language-specific neurons using Language Activation Probability Entropy and
fine-tunes only the weights associated with these neurons, a dedicated
subnetwork, on target-language data. Experiments on Llama-3.1-8B and
Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our
method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA
adaptation, and random subset fine-tuning baselines while efficiently updating
only up to 1% of model parameters. Beyond performance improvements, we observe
enhanced favorable training dynamics, cross-lingual representational alignment,
and systematic weight update changes. To facilitate future research, we release
language-specific neuron identifications for over 100 languages as well as our
adaptation pipeline, offering a cost-effective pathway for adapting
state-of-the-art models to underrepresented languages.

---

