# Daily Papers Report - 2025-10-08

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Scalable In-context Ranking with Generative Models

- **LLM Score**: 9
- **Keyword Score**: 20
- **Authors**: Nilesh Gupta, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Inderjit Dhillon, Felix Yu
- **URL**: <http://arxiv.org/abs/2510.05396v2>
- **Submitted**: 2025-10-06 21:41:58
- **Topic Keywords**: information retrieval, query, ranking, listwise, relevance, rag, retrieval, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The paper introduces a novel method, BlockRank, which leverages contextual understanding of LLMs to improve retrieval efficiency and effectiveness. The focus on scalability and real-time relevance optimization aligns with your interests.

#### Abstract
> In-context Ranking (ICR) is an emerging paradigm for Information Retrieval
(IR), which leverages contextual understanding of LLMs by directly
incorporating the task description, candidate documents, and the query into the
model's input prompt and tasking the LLM to identify relevant document(s).
While it is effective, efficiency is a significant challenge in this paradigm,
especially as the candidate list grows due to quadratic/super-linear scaling of
attention operation with context length. To this end, this paper first
identifies inherent and exploitable structures in the attention of LLMs
finetuned for ICR: (1) inter-document block sparsity: attention is dense within
each document block but sparse across different documents in the context; and
(2) query-document block relevance: the attention scores from certain query
tokens to a document block in middle layers strongly correlate with that
document's actual relevance. Motivated by these observations, we introduce
BlockRank (Blockwise In-context Ranking), a novel method that adapts the
attention operation in an LLM by (a) architecturally enforcing the observed
inter-document block sparsity, reducing attention complexity from quadratic to
linear without loss in performance, and (b) optimizing query-document block
relevance for true relevant documents during fine-tuning using an auxiliary
contrastive training objective, improving retrieval in attention. Experiments
on BEIR, MSMarco and NQ with Mistral-7B demonstrate that BlockRank Mistral
matches or outperforms existing SOTA listwise rankers and controlled fine-tuned
baseline while being significantly more efficient at inference (4.7x for 100
MSMarco documents in context) and scaling gracefully to long-context
shortlists, around 500 documents in-context (approximately 100K context length)
within a second, presenting a scalable and effective solution for ICR.

---

### 2. Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Dong Yan, Gaochen Wu, Bowen Zhou
- **URL**: <http://arxiv.org/abs/2510.05577v1>
- **Submitted**: 2025-10-07 04:46:58
- **Topic Keywords**: information retrieval, retrieval, search
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the context of query understanding and ranking models. The focus on multi-hop reasoning tasks and the use of dynamic and adaptive strategies for information exploration aligns with your research themes. However, the primary focus on language agents and LLMs is somewhat outside your core e-commerce domain expertise.

#### Abstract
> Recent advancements in language agents have led to significant improvements
in multi-hop reasoning tasks. However, existing approaches often struggle with
handling open-domain problems, which require massive information retrieval due
to their reliance on a fixed sequence of actions. To address this, we propose
Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework
tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive
strategies for information exploration in open-domain multi-hop reasoning
tasks. Our approach begins by identifying key entities relevant to the problem,
which serve as the initial nodes in the reasoning process. From these initial
nodes, we then generate reasoning child nodes with the process being refined
through a combination of historical error analysis and real-time feedback,
which allows the framework to dynamically adjust and optimize its reasoning
strategies. By integrating depth-first search with an innovative node
generation technique, our framework adapts based on both prior error paths and
concurrently generated nodes at the same hierarchical level. This dynamic
strategy effectively expands the search space while ensuring the reasoning
process systematically converges toward accurate solutions. Experimental
results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset
and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and
7.25% respectively, highlighting its versatility and potential to enhance
language agents in multi-hop reasoning tasks.

---

### 3. DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Yongqi Leng, Yikun Lei, Xikai Liu, Meizhi Zhong, Bojian Xiong, Yurong Zhang, Yan Gao, Yi Wu, Yao Hu, Deyi Xiong
- **URL**: <http://arxiv.org/abs/2510.05691v1>
- **Submitted**: 2025-10-07 08:49:22
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper aligns well with your interests in Information Retrieval, particularly in the context of retrieval-augmented generation and process supervision. The use of reinforcement learning and Markov Decision Processes (MDPs) for optimizing retrieval and generation processes is also relevant to your focus on query understanding and ranking models. However, the specific domain of large language models and process supervision is somewhat specialized, preventing a perfect match.

#### Abstract
> Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.

---

### 4. Context Length Alone Hurts LLM Performance Despite Perfect Retrieval

- **LLM Score**: 8
- **Keyword Score**: 2
- **Authors**: Yufeng Du, Minyang Tian, Srikanth Ronanki, Subendhu Rongali, Sravan Bodapati, Aram Galstyan, Azton Wells, Roy Schwartz, Eliu A Huerta, Hao Peng
- **URL**: <http://arxiv.org/abs/2510.05381v1>
- **Submitted**: 2025-10-06 21:17:13
- **Comment**: 18 pages (9 pages of main content), 5 figures, accepted at the
  Findings of EMNLP 2025
- **Topic Keywords**: retrieval
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the context of large language models (LLMs) and their limitations in handling long-context tasks. The paper's focus on retrieval performance and its surprising findings about the impact of input length on LLM performance aligns with your research themes in query understanding and ranking models.

#### Abstract
> Large language models (LLMs) often fail to scale their performance on
long-context tasks performance in line with the context lengths they support.
This gap is commonly attributed to retrieval failures -- the models' inability
to identify relevant information in the long inputs. Accordingly, recent
efforts often focus on evaluating and improving LLMs' retrieval performance: if
retrieval is perfect, a model should, in principle, perform just as well on a
long input as it does on a short one -- or should it? This paper presents
findings that the answer to this question may be negative. Our systematic
experiments across 5 open- and closed-source LLMs on math, question answering,
and coding tasks reveal that, even when models can perfectly retrieve all
relevant information, their performance still degrades substantially
(13.9%--85%) as input length increases but remains well within the models'
claimed lengths. This failure occurs even when the irrelevant tokens are
replaced with minimally distracting whitespace, and, more surprisingly, when
they are all masked and the models are forced to attend only to the relevant
tokens. A similar performance drop is observed when all relevant evidence is
placed immediately before the question. Our findings reveal a
previously-unrealized limitation: the sheer length of the input alone can hurt
LLM performance, independent of retrieval quality and without any distraction.
They motivate our simple, model-agnostic mitigation strategy that transforms a
long-context task into a short-context one by prompting the model to recite the
retrieved evidence before attempting to solve the problem. On RULER, we observe
a consistent improvement of GPT-4o up to 4% on an already strong baseline.

---

### 5. Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents

- **LLM Score**: 8
- **Keyword Score**: 1
- **Authors**: Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia
- **URL**: <http://arxiv.org/abs/2510.06214v1>
- **Submitted**: 2025-10-07 17:59:13
- **Topic Keywords**: search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval and Search technologies, particularly in the context of Large Language Model (LLM) search agents and reinforcement learning. The proposed Stratified GRPO method addresses a key challenge in training LLM search agents, which is structurally heterogeneous search trajectories. While the focus is on LLM search agents, the underlying concepts of query understanding, ranking models, and user behavior modeling are relevant to your broader research interests.

#### Abstract
> Large language model (LLM) agents increasingly rely on external tools such as
search engines to solve complex, multi-step problems, and reinforcement
learning (RL) has become a key paradigm for training them. However, the
trajectories of search agents are structurally heterogeneous, where variations
in the number, placement, and outcomes of search calls lead to fundamentally
different answer directions and reward distributions. Standard policy gradient
methods, which use a single global baseline, suffer from what we identify and
formalize as cross-stratum bias-an "apples-to-oranges" comparison of
heterogeneous trajectories. This cross-stratum bias distorts credit assignment
and hinders exploration of complex, multi-step search strategies. To address
this, we propose Stratified GRPO, whose central component, Stratified Advantage
Normalization (SAN), partitions trajectories into homogeneous strata based on
their structural properties and computes advantages locally within each
stratum. This ensures that trajectories are evaluated only against their true
peers. Our analysis proves that SAN eliminates cross-stratum bias, yields
conditionally unbiased unit-variance estimates inside each stratum, and retains
the global unbiasedness and unit-variance properties enjoyed by standard
normalization, resulting in a more pure and scale-stable learning signal. To
improve practical stability under finite-sample regimes, we further linearly
blend SAN with the global estimator. Extensive experiments on diverse
single-hop and multi-hop question-answering benchmarks demonstrate that
Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3
points, achieving higher training rewards, greater training stability, and more
effective search policies. These results establish stratification as a
principled remedy for structural heterogeneity in RL for LLM search agents.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input

- **LLM Score**: 7
- **Keyword Score**: 2
- **Authors**: Faeze Ghorbanpour, Alexander Fraser
- **URL**: <http://arxiv.org/abs/2510.05864v1>
- **Submitted**: 2025-10-07 12:33:21
- **Topic Keywords**: retrieval
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of query understanding and ranking models. The focus on Large Language Models (LLMs) and their sensitivity to harmful content is relevant to the user's background in NLP and e-commerce. However, the paper's primary focus on safety-critical scenarios and LLMs' behavior in long contexts is not a central match for the user's core research themes.

#### Abstract
> Large language models (LLMs) increasingly support applications that rely on
extended context, from document processing to retrieval-augmented generation.
While their long-context capabilities are well studied for reasoning and
retrieval, little is known about their behavior in safety-critical scenarios.
We evaluate LLMs' sensitivity to harmful content under extended context,
varying type (explicit vs. implicit), position (beginning, middle, end),
prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).
Across harmful content categories such as toxic, offensive, and hate speech,
with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance
peaks at moderate harmful prevalence (0.25) but declines when content is very
sparse or dominant; recall decreases with increasing context length; harmful
sentences at the beginning are generally detected more reliably; and explicit
content is more consistently recognized than implicit. These findings provide
the first systematic view of how LLMs prioritize and calibrate harmful content
in long contexts, highlighting both their emerging strengths and the challenges
that remain for safety-critical use.

### 7. Automated Research Article Classification and Recommendation Using NLP and ML

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Shadikur Rahman, Hasibul Karim Shanto, Umme Ayman Koana, Syed Muhammad Danish
- **URL**: <http://arxiv.org/abs/2510.05495v1>
- **Submitted**: 2025-10-07 01:24:35
- **Comment**: 8 pages, 4 figures, Accepted in Foundation and Large Language Models
  (FLLM2025)
- **Topic Keywords**: rag, retrieval, recommend, search
- **Reason**: The paper explores an automated framework for research article classification and recommendation using NLP and ML, which is somewhat related to your interests in Information Retrieval and NLP. However, the focus is on classification and recommendation rather than query understanding, ranking models, or user behavior modeling, limiting its relevance to your core research themes.

#### Abstract
> In the digital era, the exponential growth of scientific publications has
made it increasingly difficult for researchers to efficiently identify and
access relevant work. This paper presents an automated framework for research
article classification and recommendation that leverages Natural Language
Processing (NLP) techniques and machine learning. Using a large-scale arXiv.org
dataset spanning more than three decades, we evaluate multiple feature
extraction approaches (TF--IDF, Count Vectorizer, Sentence-BERT, USE,
Mirror-BERT) in combination with diverse machine learning classifiers (Logistic
Regression, SVM, Na\"ive Bayes, Random Forest, Gradient Boosted Trees, and
k-Nearest Neighbour). Our experiments show that Logistic Regression with
TF--IDF consistently yields the best classification performance, achieving an
accuracy of 69\%. To complement classification, we incorporate a recommendation
module based on the cosine similarity of vectorized articles, enabling
efficient retrieval of related research papers. The proposed system directly
addresses the challenge of information overload in digital libraries and
demonstrates a scalable, data-driven solution to support literature discovery.

### 8. The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Matthieu Bou, Nyal Patel, Arjun Jagota, Satyapriya Krishna, Sonali Parbhoo
- **URL**: <http://arxiv.org/abs/2510.06096v2>
- **Submitted**: 2025-10-07 16:25:14
- **Comment**: Preprint
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and AI safety, but it primarily focuses on auditing and aligning Large Language Models (LLMs), which is a specific area within NLP. While it touches on topics like reward inference and uncertainty-aware diagnostics, it does not directly address your core areas of interest in Information Retrieval (IR) and Search technologies.

#### Abstract
> The objectives that Large Language Models (LLMs) implicitly optimize remain
dangerously opaque, making trustworthy alignment and auditing a grand
challenge. While Inverse Reinforcement Learning (IRL) can infer reward
functions from behaviour, existing approaches either produce a single,
overconfident reward estimate or fail to address the fundamental ambiguity of
the task (non-identifiability). This paper introduces a principled auditing
framework that re-frames reward inference from a simple estimation task to a
comprehensive process for verification. Our framework leverages Bayesian IRL to
not only recover a distribution over objectives but to enable three critical
audit capabilities: (i) Quantifying and systematically reducing
non-identifiability by demonstrating posterior contraction over sequential
rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics
that expose spurious shortcuts and identify out-of-distribution prompts where
the inferred objective cannot be trusted; and (iii) Validating policy-level
utility by showing that the refined, low-uncertainty reward can be used
directly in RLHF to achieve training dynamics and toxicity reductions
comparable to the ground-truth alignment process. Empirically, our framework
successfully audits a detoxified LLM, yielding a well-calibrated and
interpretable objective that strengthens alignment guarantees. Overall, this
work provides a practical toolkit for auditors, safety teams, and regulators to
verify what LLMs are truly trying to achieve, moving us toward more trustworthy
and accountable AI.

### 9. AgentDR Dynamic Recommendation with Implicit Item-Item Relations via LLM-based Agents

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Mingdai Yang, Nurendra Choudhary, Jiangshu Du, Edward W. Huang, Philip S. Yu, Karthik Subbian, Danai Kourta
- **URL**: <http://arxiv.org/abs/2510.05598v1>
- **Submitted**: 2025-10-07 05:48:05
- **Topic Keywords**: ranking, relevance, rag, user behavior, recommend, rank
- **Reason**: The paper explores a novel recommendation framework using LLMs, which is somewhat related to information retrieval and search technologies. However, the focus is on recommender systems rather than query understanding, ranking models, or user behavior modeling. The use of LLMs for relational reasoning is an interesting aspect, but it doesn't directly align with the user's core research themes.

#### Abstract
> Recent agent-based recommendation frameworks aim to simulate user behaviors
by incorporating memory mechanisms and prompting strategies, but they struggle
with hallucinating non-existent items and full-catalog ranking. Besides, a
largely underexplored opportunity lies in leveraging LLMs'commonsense reasoning
to capture user intent through substitute and complement relationships between
items, which are usually implicit in datasets and difficult for traditional
ID-based recommenders to capture. In this work, we propose a novel LLM-agent
framework, AgenDR, which bridges LLM reasoning with scalable recommendation
tools. Our approach delegates full-ranking tasks to traditional models while
utilizing LLMs to (i) integrate multiple recommendation outputs based on
personalized tool suitability and (ii) reason over substitute and complement
relationships grounded in user history. This design mitigates hallucination,
scales to large catalogs, and enhances recommendation relevance through
relational reasoning. Through extensive experiments on three public grocery
datasets, we show that our framework achieves superior full-ranking
performance, yielding on average a twofold improvement over its underlying
tools. We also introduce a new LLM-based evaluation metric that jointly
measures semantic alignment and ranking correctness.

### 10. WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Yongan Yu, Xianda Du, Qingchen Hu, Jiahao Liang, Jingwei Ni, Dan Qiang, Kaiyu Huang, Grant McKenzie, Renee Sieber, Fengran Mo
- **URL**: <http://arxiv.org/abs/2510.05336v1>
- **Submitted**: 2025-10-06 19:58:42
- **Topic Keywords**: retriever, ranking, rag, retrieval, rank, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of retrieval-augmented generation (RAG) systems. However, the focus on historical weather archives and climate research is not directly aligned with your core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

### 11. Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Hudson de Martim
- **URL**: <http://arxiv.org/abs/2510.06002v1>
- **Submitted**: 2025-10-07 15:04:23
- **Topic Keywords**: query, queries, rag, retrieval, search
- **Reason**: The paper focuses on legal retrieval and introduces a query execution layer for a structured knowledge graph, which is somewhat related to information retrieval and query understanding. However, the specific application domain (legal) and the focus on deterministic properties and explainability are not central to the user's core research themes.

#### Abstract
> The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.

### 12. Limitations of Current Evaluation Practices for Conversational Recommender Systems and the Potential of User Simulation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Nolwenn Bernard, Krisztian Balog
- **URL**: <http://arxiv.org/abs/2510.05624v1>
- **Submitted**: 2025-10-07 07:12:47
- **Comment**: Proceedings of the 2025 Annual International ACM SIGIR Conference on
  Research and Development in Information Retrieval in the Asia Pacific Region
  (SIGIR-AP 2025), December 7--10, 2025, Xi'an, China
- **Topic Keywords**: ranking, recommend, rank, search
- **Reason**: The paper explores conversational recommender systems, which is a related topic to information retrieval, but it focuses more on recommender systems and user simulation. While it touches on evaluation methodologies, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat related but not central to the user's research themes.

#### Abstract
> Research and development on conversational recommender systems (CRSs)
critically depends on sound and reliable evaluation methodologies. However, the
interactive nature of these systems poses significant challenges for automatic
evaluation. This paper critically examines current evaluation practices and
identifies two key limitations: the over-reliance on static test collections
and the inadequacy of existing evaluation metrics. To substantiate this
critique, we analyze real user interactions with nine existing CRSs and
demonstrate a striking disconnect between self-reported user satisfaction and
performance scores reported in prior literature. To address these limitations,
this work explores the potential of user simulation to generate dynamic
interaction data, offering a departure from static datasets. Furthermore, we
propose novel evaluation metrics, based on a general reward/cost framework,
designed to better align with real user satisfaction. Our analysis of different
simulation approaches provides valuable insights into their effectiveness and
reveals promising initial results, showing improved correlation with system
rankings compared to human evaluation. While these findings indicate a
significant step forward in CRS evaluation, we also identify areas for future
research and refinement in both simulation techniques and evaluation metrics.

### 13. Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Yoav Gur-Arieh, Mor Geva, Atticus Geiger
- **URL**: <http://arxiv.org/abs/2510.06182v1>
- **Submitted**: 2025-10-07 17:44:30
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper explores how language models bind and retrieve entities in-context, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and entity binding is not directly aligned with the user's primary research interests in IR and search technologies. The connection to deep semantic understanding and real-time relevance optimization is also not explicitly mentioned.

#### Abstract
> A key component of in-context reasoning is the ability of language models
(LMs) to bind entities for later retrieval. For example, an LM might represent
"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"
when asked "Who loves pie?" Prior research on short lists of bound entities
found strong evidence that LMs implement such retrieval via a positional
mechanism, where "Ann" is retrieved based on its position in context. In this
work, we find that this mechanism generalizes poorly to more complex settings;
as the number of bound entities in context increases, the positional mechanism
becomes noisy and unreliable in middle positions. To compensate for this, we
find that LMs supplement the positional mechanism with a lexical mechanism
(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism
(retrieving "Ann" through a direct pointer). Through extensive experiments on
nine models and ten binding tasks, we uncover a consistent pattern in how LMs
mix these mechanisms to drive model behavior. We leverage these insights to
develop a causal model combining all three mechanisms that estimates next token
distributions with 95% agreement. Finally, we show that our model generalizes
to substantially longer inputs of open-ended text interleaved with entity
groups, further demonstrating the robustness of our findings in more natural
settings. Overall, our study establishes a more complete picture of how LMs
bind and retrieve entities in-context.

### 14. AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zheyuan Zhang, Kaiwen Shi, Zhengqing Yuan, Zehong Wang, Tianyi Ma, Keerthiram Murugesan, Vincent Galassi, Chuxu Zhang, Yanfang Ye
- **URL**: <http://arxiv.org/abs/2510.05445v1>
- **Submitted**: 2025-10-06 23:20:49
- **Topic Keywords**: queries, rag
- **Reason**: This paper explores a novel approach to multi-agent question answering using knowledge graphs and graph neural networks. While it touches on aspects of query understanding and ranking models, its primary focus is on collaborative multi-agent systems, which is somewhat related to your interests in information retrieval and search technologies. However, the paper's emphasis on knowledge graphs and multi-agent systems is not a central match for your core research themes.

#### Abstract
> Large language models (LLMs) and agent-based frameworks have advanced
rapidly, enabling diverse applications. Yet, with the proliferation of models
and agentic strategies, practitioners face substantial uncertainty in selecting
the best configuration for a downstream task. Prior studies show that different
agents and backbones exhibit complementary strengths, and that larger models
are not always superior, underscoring the need for adaptive routing mechanisms.
Existing approaches to agent routing, however, often emphasize cost efficiency
while overlooking the fine-grained contextual and relational structure inherent
in QA tasks. In this paper, we propose tAgentRouter, a framework that
formulates multi-agent QA as a knowledge-graph-guided routing problem
supervised by empirical performance signals. Specifically, we convert QA
instance into a knowledge graph that jointly encodes queries, contextual
entities, and agents, and then train a heterogeneous graph neural network (GNN)
to propagate information across node types and produce task-aware routing
distributions over agents. By leveraging soft supervision and weighted
aggregation of agent outputs, AgentRouter learns principled collaboration
schemes that capture the complementary strengths of diverse agents. Extensive
experiments demonstrate that our framework consistently outperforms
single-agent and ensemble baselines, while generalizing across benchmarks and
LLM backbones. These results highlight the effectiveness and robustness of
graph-supervised multi-agent routing for question answering.

### 15. Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Taylor Sorensen, Benjamin Newman, Jared Moore, Chan Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang, Yejin Choi
- **URL**: <http://arxiv.org/abs/2510.06084v1>
- **Submitted**: 2025-10-07 16:10:26
- **Topic Keywords**: rag, ctr
- **Reason**: This paper explores post-training techniques for language models, focusing on in-context steerability and distributional coverage. While it touches on aspects of model behavior, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Language model post-training has enhanced instruction-following and
performance on many downstream tasks, but also comes with an often-overlooked
cost on tasks with many possible valid answers. We characterize three
desiderata for conditional distributional modeling: in-context steerability,
valid output space coverage, and distributional alignment, and document across
three model families how current post-training can reduce these properties. In
particular, we disambiguate between two kinds of in-context learning: ICL for
eliciting existing underlying knowledge or capabilities, and in-context
steerability, where a model must use in-context information to override its
priors and steer to a novel data generating distribution. To better evaluate
and improve these desiderata, we introduce Spectrum Suite, a large-scale
resource compiled from >40 data sources and spanning >90 tasks requiring models
to steer to and match diverse distributions ranging from varied human
preferences to numerical distributions and more. We find that while current
post-training techniques help elicit underlying capabilities and knowledge,
they hurt models' ability to flexibly steer in-context. To mitigate these
issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite
to improve steerability and distributional coverage. We find that Spectrum
Tuning often improves over pretrained models and their instruction-tuned
counterparts, enhancing steerability, spanning more of the output space, and
improving distributional alignment on held-out datasets.

### 16. KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Kuangshi Ai, Jonathan A. Karr Jr, Meng Jiang, Nitesh V. Chawla, Chaoli Wang
- **URL**: <http://arxiv.org/abs/2510.05524v1>
- **Submitted**: 2025-10-07 02:29:13
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses a knowledge extraction framework using large language models and knowledge graphs, which is somewhat related to the user's interests in Information Retrieval and Natural Language Processing. However, the focus on safety-critical aviation maintenance and QA benchmarking in a specific domain is not directly aligned with the user's core research themes, particularly in e-commerce or real-time relevance optimization.

#### Abstract
> We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge
extraction and reasoning framework with large language models (LLMs) in
safety-critical contexts. Using the Operations and Maintenance Intelligence
(OMIn) dataset, we construct a QA benchmark spanning global sensemaking and
actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and
integrates it into a retrieval-augmented generation (RAG) pipeline, enabling
more coherent, dataset-wide reasoning than traditional text-chunk RAG. We
evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ
stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO
markedly improves global sensemaking by revealing patterns and system-level
insights, while text-chunk RAG remains effective for fine-grained procedural
tasks requiring localized retrieval. These findings underscore the promise of
KG-augmented LLMs for secure, domain-specific QA and their potential in
high-stakes reasoning.

### 17. Prompt reinforcing for long-term planning of large language models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Hsien-Chin Lin, Benjamin Matthias Ruppik, Carel van Niekerk, Chia-Hao Shen, Michael Heck, Nurul Lubis, Renato Vukovic, Shutong Feng, Milica Ga≈°iƒá
- **URL**: <http://arxiv.org/abs/2510.05921v1>
- **Submitted**: 2025-10-07 13:30:18
- **Topic Keywords**: rag, search
- **Reason**: This paper explores the application of reinforcement learning to improve the performance of large language models in multi-turn interactions, which is somewhat related to the user's interests in query understanding and ranking models. However, the focus on dialogue systems and text-to-SQL tasks is not directly aligned with the user's primary research themes in information retrieval and search technologies. The connection to NLP is relevant, but the paper's scope is more narrow and specialized.

#### Abstract
> Large language models (LLMs) have achieved remarkable success in a wide range
of natural language processing tasks and can be adapted through prompting.
However, they remain suboptimal in multi-turn interactions, often relying on
incorrect early assumptions and failing to track user goals over time, which
makes such tasks particularly challenging. Prior works in dialogue systems have
shown that long-term planning is essential for handling interactive tasks. In
this work, we propose a prompt optimisation framework inspired by reinforcement
learning, which enables such planning to take place by only modifying the task
instruction prompt of the LLM-based agent. By generating turn-by-turn feedback
and leveraging experience replay for prompt rewriting, our proposed method
shows significant improvement in multi-turn tasks such as text-to-SQL and
task-oriented dialogue. Moreover, it generalises across different LLM-based
agents and can leverage diverse LLMs as meta-prompting agents. This warrants
future research in reinforcement learning-inspired parameter-free optimisation
methods.

### 18. CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Rui Li, Zeyu Zhang, Xiaohe Bo, Zihang Tian, Xu Chen, Quanyu Dai, Zhenhua Dong, Ruiming Tang
- **URL**: <http://arxiv.org/abs/2510.05520v1>
- **Submitted**: 2025-10-07 02:16:30
- **Comment**: Accepted by NeurIPS 2025
- **Topic Keywords**: query
- **Reason**: This paper focuses on Large Language Models (LLMs) and reading comprehension, which is somewhat related to your interests in Information Retrieval and NLP. However, the specific application and theoretical framework (Constructivist Theory) are not directly aligned with your core research themes.

#### Abstract
> Current Large Language Models (LLMs) are confronted with overwhelming
information volume when comprehending long-form documents. This challenge
raises the imperative of a cohesive memory module, which can elevate vanilla
LLMs into autonomous reading agents. Despite the emergence of some heuristic
approaches, a systematic design principle remains absent. To fill this void, we
draw inspiration from Jean Piaget's Constructivist Theory, illuminating three
traits of the agentic memory -- structured schemata, flexible assimilation, and
dynamic accommodation. This blueprint forges a clear path toward a more robust
and efficient memory system for LLM-based reading comprehension. To this end,
we develop CAM, a prototype implementation of Constructivist Agentic Memory
that simultaneously embodies the structurality, flexibility, and dynamicity. At
its core, CAM is endowed with an incremental overlapping clustering algorithm
for structured memory development, supporting both coherent hierarchical
summarization and online batch integration. During inference, CAM adaptively
explores the memory structure to activate query-relevant information for
contextual response, akin to the human associative process. Compared to
existing approaches, our design demonstrates dual advantages in both
performance and efficiency across diverse long-text reading comprehension
tasks, including question answering, query-based summarization, and claim
verification.

### 19. Adversarial Reinforcement Learning for Large Language Model Agent Safety

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zizhao Wang, Dingcheng Li, Vaishakh Keshava, Phillip Wallis, Ananth Balashankar, Peter Stone, Lukas Rutishauser
- **URL**: <http://arxiv.org/abs/2510.05442v1>
- **Submitted**: 2025-10-06 23:09:18
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on Large Language Model (LLM) agent safety, which is a specific application of NLP. While it touches on the idea of 'tool usage' and 'prompt injections', it doesn't directly relate to information retrieval, search technologies, or query understanding. The use of reinforcement learning and adversarial techniques is somewhat relevant to the broader field of AI and NLP, but it doesn't align with the user's core research themes.

#### Abstract
> Large Language Model (LLM) agents can leverage tools such as Google Search to
complete complex tasks. However, this tool usage introduces the risk of
indirect prompt injections, where malicious instructions hidden in tool outputs
can manipulate the agent, posing security risks like data leakage. Current
defense strategies typically rely on fine-tuning LLM agents on datasets of
known attacks. However, the generation of these datasets relies on manually
crafted attack patterns, which limits their diversity and leaves agents
vulnerable to novel prompt injections. To address this limitation, we propose
Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework
that leverages adversarial reinforcement learning (RL) by formulating the
problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker
that learns to autonomously generate diverse prompt injections and an agent
that learns to defend against them while completing its assigned tasks. To
ensure robustness against a wide range of attacks and to prevent cyclic
learning, we employ a population-based learning framework that trains the agent
to defend against all previous attacker checkpoints. Evaluated on BrowserGym
and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower
attack success rate than the original model while also improving their task
success rate. Our analysis further confirms that the adversarial process
generates a diverse and challenging set of attacks, leading to a more robust
agent compared to the base model.

### 20. TokenChain: A Discrete Speech Chain via Semantic Token Modeling

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mingxuan Wang, Satoshi Nakamura
- **URL**: <http://arxiv.org/abs/2510.06201v1>
- **Submitted**: 2025-10-07 17:54:12
- **Comment**: 5 pages, 3 figures. Submitted to IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP) 2026
- **Topic Keywords**: rag
- **Reason**: The paper focuses on speech processing, specifically a discrete speech chain model, which is not directly related to information retrieval or search technologies. However, it does involve natural language processing and deep semantic understanding, which are tangentially relevant to your research interests.

#### Abstract
> Machine Speech Chain, simulating the human perception-production loop, proves
effective in jointly improving ASR and TTS. We propose TokenChain, a fully
discrete speech chain coupling semantic-token ASR with a two-stage TTS: an
autoregressive text-to-semantic model co-trained with ASR and a
masked-generative semantic-to-acoustic model for synthesis only. End-to-end
feedback across the text interface is enabled with straight-through
argmax/Gumbel-Softmax and balanced with supervised ASR via dynamic weight
averaging. Ablations examine optimal temperature schedules for in- and
cross-domain transfer. Evaluation reveals TokenChain surpasses baseline
accuracy 2-6 epochs earlier and yields 5-13% lower equal-epoch error with
stable T2S on LibriSpeech, and reduces relative ASR WER by 56% and T2S WER by
31% on TED-LIUM with minimal forgetting, showing that chain learning remains
effective with token interfaces and models.

### 21. Taxonomy of User Needs and Actions

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Renee Shelby, Fernando Diaz, Vinodkumar Prabhakaran
- **URL**: <http://arxiv.org/abs/2510.06124v1>
- **Submitted**: 2025-10-07 17:04:42
- **Topic Keywords**: user action
- **Reason**: This paper introduces a taxonomy of user needs and actions in conversational AI, which is somewhat related to information retrieval, particularly in the context of query understanding and user behavior modeling. However, the focus is more on the user's needs and actions rather than the technical aspects of search or ranking models. While it may have some implications for search technologies, it is not directly aligned with the user's core research themes.

#### Abstract
> The growing ubiquity of conversational AI highlights the need for frameworks
that capture not only users' instrumental goals but also the situated,
adaptive, and social practices through which they achieve them. Existing
taxonomies of conversational behavior either overgeneralize, remain
domain-specific, or reduce interactions to narrow dialogue functions. To
address this gap, we introduce the Taxonomy of User Needs and Actions (TUNA),
an empirically grounded framework developed through iterative qualitative
analysis of 1193 human-AI conversations, supplemented by theoretical review and
validation across diverse contexts. TUNA organizes user actions into a
three-level hierarchy encompassing behaviors associated with information
seeking, synthesis, procedural guidance, content creation, social interaction,
and meta-conversation. By centering user agency and appropriation practices,
TUNA enables multi-scale evaluation, supports policy harmonization across
products, and provides a backbone for layering domain-specific taxonomies. This
work contributes a systematic vocabulary for describing AI use, advancing both
scholarly understanding and practical design of safer, more responsive, and
more accountable conversational systems.

### 22. MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Wei-Chieh Huang, Cornelia Caragea
- **URL**: <http://arxiv.org/abs/2510.05611v1>
- **Submitted**: 2025-10-07 06:27:42
- **Topic Keywords**: commerce, e-commerce
- **Reason**: While the paper touches on e-commerce and multimodal data, it primarily focuses on implicit attribute value extraction, which is not a central match to your research interests in information retrieval, query understanding, and ranking models. However, the use of multimodal large language models and debate strategies may be tangentially related to your work in NLP and related topics.

#### Abstract
> Implicit Attribute Value Extraction (AVE) is essential for accurately
representing products in e-commerce, as it infers lantent attributes from
multimodal data. Despite advances in multimodal large language models (MLLMs),
implicit AVE remains challenging due to the complexity of multidimensional data
and gaps in vision-text understanding. In this work, we introduce
\textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM
agents to iteratively refine inferences. Through a series of debate rounds,
agents verify and update each other's responses, thereby improving inference
performance and robustness. Experiments on the ImplicitAVE dataset demonstrate
that even a few rounds of debate significantly boost accuracy, especially for
attributes with initially low performance. We systematically evaluate various
debate configurations, including identical or different MLLM agents, and
analyze how debate rounds affect convergence dynamics. Our findings highlight
the potential of multi-agent debate strategies to address the limitations of
single-agent approaches and offer a scalable solution for implicit AVE in
multimodal e-commerce.

### 23. Domain-Shift-Aware Conformal Prediction for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Zhexiao Lin, Yuanyuan Li, Neeraj Sarna, Yuanyuan Gao, Michael von Gablenz
- **URL**: <http://arxiv.org/abs/2510.05566v1>
- **Submitted**: 2025-10-07 04:22:06
- **Comment**: 26 pages
- **Topic Keywords**: rag
- **Reason**: While the paper explores a relevant topic in Natural Language Processing (NLP) and large language models, its focus on conformal prediction and domain shift is not directly related to the user's core research themes in Information Retrieval, query understanding, and ranking models. However, the paper's emphasis on trustworthy uncertainty quantification and real-world deployment may have some tangential relevance to the user's interests in real-time relevance optimization.

#### Abstract
> Large language models have achieved impressive performance across diverse
tasks. However, their tendency to produce overconfident and factually incorrect
outputs, known as hallucinations, poses risks in real world applications.
Conformal prediction provides finite-sample, distribution-free coverage
guarantees, but standard conformal prediction breaks down under domain shift,
often leading to under-coverage and unreliable prediction sets. We propose a
new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our
framework adapts conformal prediction to large language models under domain
shift, by systematically reweighting calibration samples based on their
proximity to the test prompt, thereby preserving validity while enhancing
adaptivity. Our theoretical analysis and experiments on the MMLU benchmark
demonstrate that the proposed method delivers more reliable coverage than
standard conformal prediction, especially under substantial distribution
shifts, while maintaining efficiency. This provides a practical step toward
trustworthy uncertainty quantification for large language models in real-world
deployment.

### 24. Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yoo Yongmin, Zhang Xu, Cao Longbing
- **URL**: <http://arxiv.org/abs/2510.05431v1>
- **Submitted**: 2025-10-06 22:50:01
- **Topic Keywords**: rag
- **Reason**: This paper explores the use of trust indicators generated by Large Language Models (LLMs) for reliable patent classification. While it touches on aspects of query understanding and ranking models, its primary focus is on patent classification and trust indicators, which is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, but not a central match.

#### Abstract
> Large language models (LLMs) increasingly generate natural language
rationales to enhance interpretability, but these often contain logical errors,
label mismatches, and domain-specific misalignments. Directly using such
rationales as supervision risks propagating noise and undermining training
stability. To address this challenge, we introduce Self-Filtered Distillation,
a framework specifically tailored for patent classification, which treats
LLM-generated rationales as trust signals rather than ground-truth supervision.
The framework employs selective distillation guided by three unsupervised trust
metrics: (1) Self-Consistency, which measures the stability of LLM-generated
rationales across multiple generations; (2) Class Entailment Alignment, which
assesses semantic coherence with patent-specific class definitions; and (3) LLM
Agreement Scoring, which validates rationale-label plausibility. These metrics
are integrated into a unified trust score that primarily weights training
samples while optionally filtering out extremely low-trust cases, enabling
reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used
benchmark for patent classification, show that our method outperforms
label-based learning and conventional distillation in accuracy, stability, and
interpretability, establishing a reliable paradigm for leveraging
reasoning-aware trust indicators in patent analytics.

### 25. Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Radha Gulhane, Sathish Reddy Indurthi
- **URL**: <http://arxiv.org/abs/2510.05283v1>
- **Submitted**: 2025-10-06 18:53:23
- **Topic Keywords**: rag
- **Reason**: The paper discusses a novel approach to aligning multimodal large language models with human preferences using a hybrid reward modeling framework. While it touches on aspects of model-based rewards and reinforcement learning, which are related to information retrieval and search technologies, the primary focus is on NLP and MLLM alignment, which is somewhat relevant to the user's interests but not a central match.

#### Abstract
> Aligning multimodal large language models (MLLMs) with human preferences
often relies on single-signal, model-based reward methods. Such monolithic
rewards often lack confidence calibration across domain-specific tasks, fail to
capture diverse aspects of human preferences, and require extensive data
annotation and reward model training. In this work, we propose a hybrid reward
modeling framework that integrates complementary reward paradigms: (i)
model-based rewards, where a learned reward model predicts scalar or vector
scores from synthetic and human feedback, and (ii) rule-based rewards, where
domain-specific heuristics provide explicit correctness signals with
confidence. Beyond accuracy, we further incorporate multi-aspect rewards to
enforce instruction adherence and introduce a generalized length-penalty reward
to stabilize training and improve performance. The proposed framework provides
a flexible and effective approach to aligning MLLMs through reinforcement
learning policy optimization. Our experiments show consistent improvements
across different multimodal benchmarks when applying hybrid and multi-aspect
reward modeling. Our best performing model in the 3B family achieves an overall
average improvement of ~9.5% across general and math reasoning tasks. Focusing
specifically on mathematical benchmarks, the model achieves a significant
average improvement of ~16%, highlighting its effectiveness in mathematical
reasoning and problem solving.

### 26. RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Yining She, Daniel W. Peterson, Marianne Menglin Liu, Vikas Upadhyay, Mohammad Hossein Chaghazardi, Eunsuk Kang, Dan Roth
- **URL**: <http://arxiv.org/abs/2510.05310v1>
- **Submitted**: 2025-10-06 19:20:43
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper focuses on the robustness of guardrails in large language models, which is a topic related to NLP, but it does not directly align with your core research interests in Information Retrieval, Search technologies, and query understanding. The paper's context-robustness gap in guardrails is not directly applicable to your areas of focus.

#### Abstract
> With the increasing adoption of large language models (LLMs), ensuring the
safety of LLM systems has become a pressing concern. External LLM-based
guardrail models have emerged as a popular solution to screen unsafe inputs and
outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are
vulnerable to data distribution shifts. In this paper, taking Retrieval
Augmentation Generation (RAG) as a case study, we investigated how robust
LLM-based guardrails are against additional information embedded in the
context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss
models, we confirmed that inserting benign documents into the guardrail context
alters the judgments of input and output guardrails in around 11% and 8% of
cases, making them unreliable. We separately analyzed the effect of each
component in the augmented context: retrieved documents, user query, and
LLM-generated response. The two mitigation methods we tested only bring minor
improvements. These results expose a context-robustness gap in current
guardrails and motivate training and evaluation protocols that are robust to
retrieval and query composition.

### 27. VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dingyu Yao, Chenxu Yang, Zhengyang Tong, Zheng Lin, Wei Liu, Jian Luan, Weiping Wang
- **URL**: <http://arxiv.org/abs/2510.06175v1>
- **Submitted**: 2025-10-07 17:35:28
- **Topic Keywords**: ltr
- **Reason**: This paper focuses on efficient inference methods for large language models, specifically addressing memory overhead and performance degradation. While it touches on vector quantization and caching, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.

### 28. MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Qin Dong, Yuntian Tang, Heming Jia, Yunhang Shen, Bohan Jia, Wenxuan Huang, Lianyue Zhang, Jiao Xie, Shaohui Lin
- **URL**: <http://arxiv.org/abs/2510.06005v1>
- **Submitted**: 2025-10-07 15:06:46
- **Comment**: 14 pages, 5 figures
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on improving the performance of large language models through a novel architecture called MASA, which is primarily concerned with parameter-efficient fine-tuning. While it touches on the concept of adaptation, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Low-Rank Adaptation (LoRA) has emerged as a dominant method in
Parameter-Efficient Fine-Tuning (PEFT) for large language models, which
augments the transformer layer with one down-projection $A$ and one
up-projection $B$. However, LoRA's reliance on a single down-projection matrix
($A$) creates a representational bottleneck, as this solitary feature extractor
is inherently insufficient for capturing the diverse signals required by
complex tasks. This motivates our architectural shift to focus on enriching the
feature adaptation to improve the downstream task adaptation ability. We
propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a
multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is
asymmetrically shared across layers to ensure parameter efficiency. In MASA,
these specialized experts capture diverse features, which are then integrated
by a single, layer-specific $B$-matrix. The effectiveness and versatility of
our method are validated through a comprehensive suite of experiments spanning
multi-domain generalization, single-domain specialization, and multi-task
reasoning. For example, on the MMLU benchmark, MASA achieves an average
accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative
improvement of 1.84%) with comparable learnable parameters of 0.52%.

### 29. The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sheriff Issaka, Keyi Wang, Yinka Ajibola, Oluwatumininu Samuel-Ipaye, Zhaoyi Zhang, Nicte Aguillon Jimenez, Evans Kofi Agyei, Abraham Lin, Rohan Ramachandran, Sadick Abdul Mumin, Faith Nchifor, Mohammed Shuraim, Lieqi Liu, Erick Rosas Gonzalez, Sylvester Kpei, Jemimah Osei, Carlene Ajeneza, Persis Boateng, Prisca Adwoa Dufie Yeboah, Saadia Gabriel
- **URL**: <http://arxiv.org/abs/2510.05644v1>
- **Submitted**: 2025-10-07 07:42:52
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on advancing NLP technologies for African languages, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and query understanding. While it involves NLP, the specific context and goals are quite different from the user's areas of focus.

#### Abstract
> Despite representing nearly one-third of the world's languages, African
languages remain critically underserved by modern NLP technologies, with 88\%
classified as severely underrepresented or completely ignored in computational
linguistics. We present the African Languages Lab (All Lab), a comprehensive
research initiative that addresses this technological gap through systematic
data collection, model development, and capacity building. Our contributions
include: (1) a quality-controlled data collection pipeline, yielding the
largest validated African multi-modal speech and text dataset spanning 40
languages with 19 billion tokens of monolingual text and 12,628 hours of
aligned speech data; (2) extensive experimental validation demonstrating that
our dataset, combined with fine-tuning, achieves substantial improvements over
baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points
across 31 evaluated languages; and (3) a structured research program that has
successfully mentored fifteen early-career researchers, establishing
sustainable local capacity. Our comparative evaluation against Google Translate
reveals competitive performance in several languages while identifying areas
that require continued development.

### 30. In-the-Flow Agentic System Optimization for Effective Planning and Tool Use

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, Pan Lu
- **URL**: <http://arxiv.org/abs/2510.05592v1>
- **Submitted**: 2025-10-07 05:32:44
- **Comment**: 45 pages, 12 figures. Project website:
  https://agentflow.stanford.edu/
- **Topic Keywords**: rag, search
- **Reason**: This paper appears to be primarily focused on agentic systems and reinforcement learning, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the paper's focus on large language models and tool use optimization does not seem to align closely with your core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

### 31. AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yurun Song, Zhuoyi Yang, Ian G. Harris, Sangeetha Abdu Jyothi
- **URL**: <http://arxiv.org/abs/2510.05468v1>
- **Submitted**: 2025-10-07 00:05:16
- **Comment**: 14 pages
- **Topic Keywords**: ltr
- **Reason**: This paper focuses on collaborative training and parameter-efficient fine-tuning for Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some optimization techniques, the context and application are quite different from your areas of focus.

#### Abstract
> Large Language Models (LLMs) are scaling rapidly, creating significant
challenges for collaborative server client distributed training, particularly
in terms of communication efficiency and computational overheads. To address
these challenges, we implement Parameter-efficient Split Learning, which
effectively balances efficiency and performance for collaborative training on
low-resource devices.
  To reduce communication overhead in collaborative training, we introduce
Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that
progressively compresses activations and gradients from high precision (6 to 8
bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively
allocating bit budgets across channels based on feature wise and layer wise
importance using bit regularization.
  Under the same bit budgets, AMAQ outperforms fixed-precision approaches,
delivering about 2.5% higher generation accuracy and about 1.3% better
classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition,
it significantly enhances training stability and reducing ultra-low bit
representation collapse during the training.
  Experiments demonstrate that AMAQ integrates effectively into practical
multi-machine collaborative training setups, offering superior inference
accuracy with only a modest communication overhead for bits adaptation during
training. This trade off makes AMAQ a practical and effective solution for
collaborative training with minimal communication cost.

### 32. TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiaru Zou, Soumya Roy, Vinay Kumar Verma, Ziyi Wang, David Wipf, Pan Lu, Sumit Negi, James Zou, Jingrui He
- **URL**: <http://arxiv.org/abs/2510.06217v1>
- **Submitted**: 2025-10-07 17:59:41
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on Process Reward Models for tabular reasoning, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it involves reasoning and reward modeling, the context is specific to tabular data and does not align with the user's core research themes.

#### Abstract
> Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.

### 33. BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jakir Hasan, Shubhashis Roy Dipta
- **URL**: <http://arxiv.org/abs/2510.06188v1>
- **Submitted**: 2025-10-07 17:47:39
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speech assistance for Bengali regional dialects, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves speech recognition, the context and application are quite different from your areas of focus.

#### Abstract
> Real-time speech assistants are becoming increasingly popular for ensuring
improved accessibility to information. Bengali, being a low-resource language
with a high regional dialectal diversity, has seen limited progress in
developing such systems. Existing systems are not optimized for real-time use
and focus only on standard Bengali. In this work, we present BanglaTalk, the
first real-time speech assistance system for Bengali regional dialects.
BanglaTalk follows the client-server architecture and uses the Real-time
Transport Protocol (RTP) to ensure low-latency communication. To address
dialectal variation, we introduce a dialect-aware ASR system, BRDialect,
developed by fine-tuning the IndicWav2Vec model in ten Bengali regional
dialects. It outperforms the baseline ASR models by 12.41-33.98% on the
RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of
24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low
bandwidth usage and minimal end-to-end delay make the system both
cost-effective and interactive for real-time use cases, enabling inclusive and
accessible speech technology for the diverse community of Bengali speakers.

### 34. CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kangyu Wang, Zhiyun Jiang, Haibo Feng, Weijia Zhao, Lin Liu, Jianguo Li, Zhenzhong Lan, Weiyao Lin
- **URL**: <http://arxiv.org/abs/2510.06133v1>
- **Submitted**: 2025-10-07 17:08:33
- **Comment**: 18 pages,8 figures,4 tables
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus is on accelerating parallel decoding in Diffusion Large Language Models, which is outside your primary areas of interest.

#### Abstract
> Diffusion large language models (dLLMs) generate text through iterative
denoising steps, achieving parallel decoding by denoising only high-confidence
positions at each step. However, existing approaches often repetitively remask
tokens due to initially low confidence scores, leading to redundant iterations
and limiting overall acceleration. Through the analysis of dLLM decoding
traces, we observe that the model often determines the final prediction for a
token several steps before the decoding step. To leverage this historical
information and avoid redundant steps, we introduce the concept of Trace
Credit, which quantifies each token's convergence potential by accumulating
historical logits. Furthermore, we propose CreditDecoding, a training-free
parallel decoding algorithm that accelerates the confidence convergence of
correct but underconfident tokens by fusing current logits with Trace Credit.
This process significantly reduces redundant iterations and enhances decoding
robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup
and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times
speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.
Importantly, CreditDecoding scales effectively to long sequences and is
orthogonal to mainstream inference optimizations, making it a readily
integrable and versatile solution.

### 35. MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dayy√°n O'Brien, Barry Haddow, Emily Allaway, Pinzhen Chen
- **URL**: <http://arxiv.org/abs/2510.05962v1>
- **Submitted**: 2025-10-07 14:19:21
- **Topic Keywords**: rag
- **Reason**: This paper focuses on generating dynamic mathematics benchmarks to evaluate mathematical capabilities, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Conducting contamination-free evaluation of mathematical capabilities can be
difficult for two reasons: models may memorize a test set once it is made
public, and current mathematical benchmarks are prone to overfitting due to
having limited diversity of symbols and rules, coupled with closed-ended
answers. This paper proposes a method to leverage these shortcomings as useful
features to a construct dynamic, counterfactual benchmark, which can be used to
both reveal overfitting and measure true reasoning. We demonstrate this via
MatheMagic, which generates math test instances with the interpretations of
numbers and operators altered, yet has automatically verifiable answers. Test
instances are randomly seeded and constructed at test time to evaluate a
model's induction or deduction capability, offering stability, extensibility,
comparability, and robustness to overfitting. Our experiments find that models
solve deduction more easily than induction, but they revert to standard math.
Further analysis reveals that math-adapted models fail to exhibit a general
"skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.

### 36. How public datasets constrain the development of diversity-aware news recommender systems, and what law could do about it

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Max van Drunen, Sanne Vrijenhoek
- **URL**: <http://arxiv.org/abs/2510.05952v1>
- **Submitted**: 2025-10-07 14:08:38
- **Topic Keywords**: recommend, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on recommender systems, its focus is on the legal and societal aspects of news recommendation, which is not a central match to your areas of expertise.

#### Abstract
> News recommender systems increasingly determine what news individuals see
online. Over the past decade, researchers have extensively critiqued
recommender systems that prioritise news based on user engagement. To offer an
alternative, researchers have analysed how recommender systems could support
the media's ability to fulfil its role in democratic society by recommending
news based on editorial values, particularly diversity. However, there
continues to be a large gap between normative theory on how news recommender
systems should incorporate diversity, and technical literature that designs
such systems. We argue that to realise diversity-aware recommender systems in
practice, it is crucial to pay attention to the datasets that are needed to
train modern news recommenders. We aim to make two main contributions. First,
we identify the information a dataset must include to enable the development of
the diversity-aware news recommender systems proposed in normative literature.
Based on this analysis, we assess the limitations of currently available public
datasets, and show what potential they do have to expand research into
diversity-aware recommender systems. Second, we analyse why and how European
law and policy can be used to provide researchers with structural access to the
data they need to develop diversity-aware news recommender systems.

### 37. EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hadi Mohammadi, Anastasia Giachanou, Ayoub Bagheri
- **URL**: <http://arxiv.org/abs/2510.05942v2>
- **Submitted**: 2025-10-07 13:52:16
- **Topic Keywords**: rag
- **Reason**: This paper appears to be unrelated to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on moral alignment in Large Language Models and cultural bias does not align with your areas of expertise.

#### Abstract
> We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that
uses two scoring methods (log-probabilities and direct ratings) plus a
model-as-judge peer review to evaluate moral alignment in 20 large language
models. We assess models on the World Values Survey (55 countries, 19 topics)
and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,
top models align closely with survey responses (Pearson's r approximately 0.90
on WVS). Yet we find a clear regional difference: Western regions average
r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),
indicating consistent regional bias. Our framework adds three parts: (1) two
scoring methods for all models to enable fair comparison, (2) a structured
chain-of-thought protocol with self-consistency checks, and (3) a
model-as-judge peer review that flags 348 conflicts using a data-driven
threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,
both p<.001), supporting automated quality checks. These results show real
progress toward culture-aware AI while highlighting open challenges for use
across regions.

### 38. The fragility of "cultural tendencies" in LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kun Sun, Rong Wang
- **URL**: <http://arxiv.org/abs/2510.05869v1>
- **Submitted**: 2025-10-07 12:37:06
- **Topic Keywords**: rag
- **Reason**: This paper focuses on the cultural tendencies of large language models, which is a topic in Natural Language Processing (NLP). However, it does not directly relate to your core research interests in Information Retrieval, Search technologies, or query understanding, ranking models, and user behavior modeling.

#### Abstract
> In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large
language models (LLMs), when prompted in different languages, display
culturally specific tendencies. They report that the two models (i.e., GPT and
ERNIE) respond in more interdependent and holistic ways when prompted in
Chinese, and more independent and analytic ways when prompted in English. LSZ
attribute these differences to deep-seated cultural patterns in the models,
claiming that prompt language alone can induce substantial cultural shifts.
While we acknowledge the empirical patterns they observed, we find their
experiments, methods, and interpretations problematic. In this paper, we
critically re-evaluate the methodology, theoretical framing, and conclusions of
LSZ. We argue that the reported "cultural tendencies" are not stable traits but
fragile artifacts of specific models and task design. To test this, we
conducted targeted replications using a broader set of LLMs and a larger number
of test items. Our results show that prompt language has minimal effect on
outputs, challenging LSZ's claim that these models encode grounded cultural
beliefs.

### 39. EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Liang Chen, Xueting Han, Qizhou Wang, Bo Han, Jing Bai, Hinrich Schutze, Kam-Fai Wong
- **URL**: <http://arxiv.org/abs/2510.05837v1>
- **Submitted**: 2025-10-07 12:02:03
- **Topic Keywords**: rag
- **Reason**: The paper focuses on reinforcement learning with verifiable rewards, which is not directly related to information retrieval or search technologies. Although it involves large language models, the primary goal is to improve exploration in reinforcement learning, which is not a core aspect of the user's research interests.

#### Abstract
> Balancing exploration and exploitation remains a central challenge in
reinforcement learning with verifiable rewards (RLVR) for large language models
(LLMs). Current RLVR methods often overemphasize exploitation, leading to
entropy collapse, diminished exploratory capacity, and ultimately limited
performance gains. Although techniques that increase policy stochasticity can
promote exploration, they frequently fail to escape dominant behavioral modes.
This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant
modes-that further erodes exploration. We introduce Exploration-Enhanced Policy
Optimization (EEPO), a framework that promotes exploration via two-stage
rollouts with adaptive unlearning. In the first stage, the model generates half
of the trajectories; it then undergoes a lightweight unlearning step to
temporarily suppress these sampled responses, forcing the second stage to
explore different regions of the output space. This sample-then-forget
mechanism disrupts the self-reinforcing loop and promotes wider exploration
during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,
achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on
Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.

### 40. Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh
- **URL**: <http://arxiv.org/abs/2510.05678v1>
- **Submitted**: 2025-10-07 08:35:42
- **Topic Keywords**: rag
- **Reason**: This paper focuses on cross-lingual transfer of large language models, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it touches on language understanding, the context is more aligned with NLP and multilingual systems, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> While large language models (LLMs) exhibit strong multilingual abilities,
their reliance on English as latent representations creates a translation
barrier, where reasoning implicitly depends on internal translation into
English. When this process fails, performance in non-English languages
deteriorates sharply, limiting the inclusiveness of LLM-based applications.
Existing cross-lingual in-context learning (X-ICL) methods primarily leverage
monolingual demonstrations, often failing to mitigate this barrier and instead
reinforcing it. In this work, we introduce code-switching in-context learning
(CSICL), a simple yet effective prompting strategy that progressively
transitions from a target language to English within demonstrations and
instruction to facilitate their latent reasoning in English. By explicitly
scaffolding the reasoning process through controlled code-switching, CSICL acts
as an implicit linguistic bridge that enhances cross-lingual alignment and
reduces reliance on the translation barrier. We conduct extensive experiments
across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive
and reasoning-oriented domains. Our results demonstrate that CSICL consistently
outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target
and unseen languages, respectively. The improvement is even more pronounced in
low-resource settings, with gains of 14.7% in target and 5.3% in unseen
languages. These findings establish code-switching as a principled and robust
approach for overcoming the translation barrier during inference, moving LLMs
toward more equitable and effective multilingual systems.

### 41. Improving Chain-of-Thought Efficiency for Autoregressive Image Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, Jiawei Zhou, Abe Davis, Jialiang Wang
- **URL**: <http://arxiv.org/abs/2510.05593v1>
- **Submitted**: 2025-10-07 05:40:43
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on autoregressive image generation, chain-of-thought reasoning, and multimodal large language models, which are outside your primary areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Autoregressive multimodal large language models have recently gained
popularity for image generation, driven by advances in foundation models. To
enhance alignment and detail, newer approaches employ chain-of-thought (CoT)
reasoning, expanding user inputs into elaborated prompts prior to image
synthesis. However, this strategy can introduce unnecessary redundancy -- a
phenomenon we call visual overthinking -- which increases computational costs
and can introduce details that contradict the original prompt. In this work, we
explore how to generate more concise CoT sequences for more efficient image
generation. We introduce ShortCoTI, a lightweight optimization framework that
encourages more concise CoT while preserving output image quality. ShortCoTI
rewards more concise prompts with an adaptive function that scales according to
an estimated difficulty for each task. Incorporating this reward into a
reinforcement learning paradigm reduces prompt reasoning length by 54% while
maintaining or slightly improving quality metrics across multiple benchmarks
(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates
verbose explanations and repetitive refinements, producing reasoning prompts
that are both concise and semantically rich. As a result, ShortCoTI improves
computational efficiency without compromising the fidelity or visual appeal of
generated images.

### 42. Sci-Phi: A Large Language Model Spatial Audio Descriptor

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xilin Jiang, Hannes Gamper, Sebastian Braun
- **URL**: <http://arxiv.org/abs/2510.05542v1>
- **Submitted**: 2025-10-07 03:06:02
- **Topic Keywords**: ctr
- **Reason**: This paper focuses on spatial audio descriptor using a large language model, which is not directly related to information retrieval, query understanding, or ranking models. While it involves machine learning and NLP, the application domain is quite different from the user's core research interests.

#### Abstract
> Acoustic scene perception involves describing the type of sounds, their
timing, their direction and distance, as well as their loudness and
reverberation. While audio language models excel in sound recognition,
single-channel input fundamentally limits spatial understanding. This work
presents Sci-Phi, a spatial audio large language model with dual spatial and
spectral encoders that estimates a complete parameter set for all sound sources
and the surrounding environment. Learning from over 4,000 hours of synthetic
first-order Ambisonics recordings including metadata, Sci-Phi enumerates and
describes up to four directional sound sources in one pass, alongside
non-directional background sounds and room characteristics. We evaluate the
model with a permutation-invariant protocol and 15 metrics covering content,
location, timing, loudness, and reverberation, and analyze its robustness
across source counts, signal-to-noise ratios, reverberation levels, and
challenging mixtures of acoustically, spatially, or temporally similar sources.
Notably, Sci-Phi generalizes to real room impulse responses with only minor
performance degradation. Overall, this work establishes the first audio LLM
capable of full spatial-scene description, with strong potential for real-world
deployment. Demo: https://sci-phi-audio.github.io/demo

### 43. Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Junyi Fan, Li Sun, Negin Ashrafi, Kamiar Alaei, Maryam Pishgar
- **URL**: <http://arxiv.org/abs/2510.05410v1>
- **Submitted**: 2025-10-06 22:04:37
- **Topic Keywords**: ctr
- **Reason**: This paper focuses on adapting language models for clinical documentation in ICUs, which is not directly related to your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context and application are quite different from your areas of focus.

#### Abstract
> Nursing documentation in intensive care units (ICUs) provides essential
clinical intelligence but often suffers from inconsistent terminology, informal
styles, and lack of standardization, challenges that are particularly critical
in heart failure care. This study applies Direct Preference Optimization (DPO)
to adapt Mistral-7B, a locally deployable language model, using 8,838 heart
failure nursing notes from the MIMIC-III database and 21,210 preference pairs
derived from expert-verified GPT outputs, model generations, and original
notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert
qualitative assessments demonstrates that DPO markedly enhances documentation
quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore
improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy
(+14.4 points), completeness (+14.5 points), logical consistency (+14.1
points), readability (+11.1 points), and structural clarity (+6.0 points).
These results indicate that DPO can align lightweight clinical language models
with expert standards, supporting privacy-preserving, AI-assisted documentation
within electronic health record systems to reduce administrative burden and
improve ICU patient safety.

### 44. Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ananth Kandala, Ratna Kandala, Akshata Kishore Moharir, Niva Manchanda, Sunaina Singh
- **URL**: <http://arxiv.org/abs/2510.05387v1>
- **Submitted**: 2025-10-06 21:27:37
- **Topic Keywords**: rag
- **Reason**: This paper focuses on cross-lingual mental health ontologies and NLP for mental health care in multilingual contexts, which is somewhat related to your interests in NLP and IR. However, it does not align with your core research themes in query understanding, ranking models, or user behavior modeling, and is more specific to the healthcare domain.

#### Abstract
> Mental health communication in India is linguistically fragmented, culturally
diverse, and often underrepresented in clinical NLP. Current health ontologies
and mental health resources are dominated by diagnostic frameworks centered on
English or Western culture, leaving a gap in representing patient distress
expressions in Indian languages. We propose cross-linguistic graphs of patient
stress expressions (CL-PDE), a framework for building cross-lingual mental
health ontologies through graph-based methods that capture culturally embedded
expressions of distress, align them across languages, and link them with
clinical terminology. Our approach addresses critical gaps in healthcare
communication by grounding AI systems in culturally valid representations,
allowing more inclusive and patient-centric NLP tools for mental health care in
multilingual contexts.

### 45. Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Paloma Garc√≠a-de-Herreros, Philipp Slusallek, Dietrich Klakow, Vagrant Gautam
- **URL**: <http://arxiv.org/abs/2510.05278v1>
- **Submitted**: 2025-10-06 18:46:50
- **Topic Keywords**: ctr
- **Reason**: This paper focuses on cross-modal adaptation of decoder-only models to partial differential equations (PDEs), which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Large language models have shown great success on natural language tasks in
recent years, but they have also shown great promise when adapted to new
modalities, e.g., for scientific machine learning tasks. Even though
decoder-only models are more popular within NLP and scale exceedingly well at
generating natural language, most proposed approaches for cross-modal
adaptation focus on encoder-only models, raising the question of how model
architecture affects these approaches. In this paper, we therefore perform a
series of ablation studies to answer this question, systematically comparing
encoder-only and decoder-only models on cross-modal adaptation for
time-dependent simulation tasks based on partial differential equations (PDEs).
We find that decoder-only models are far worse than encoder-only models, when
existing approaches are applied unmodified. In contrast to several other
domains, scaling decoder-only models also does not help. To harness the
potential of decoder-only models in this context, we introduce two novel
approaches, Parallel Flipping and Sequence Doubling, attempting to mimic
bidirectionality in autoregressive models. Both our methods improve overall
performance using decoder-only models for all tasks and all cross-model
adaptation methods, closing the gap to encoder-only model performance. We hope
that our findings broaden the spectrum of models used on cross-modal adaptation
tasks to further scientific ML.

### 46. Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chenghao Yang, Lin Gui, Chenxiao Yang, Victor Veitch, Lizhu Zhang, Zhuokai Zhao
- **URL**: <http://arxiv.org/abs/2510.05251v1>
- **Submitted**: 2025-10-06 18:15:43
- **Comment**: Codebase: https://github.com/yangalan123/EAD-RLVR
- **Topic Keywords**: rag
- **Reason**: This paper focuses on verifiable reinforcement learning for large language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves sequential generation, the context is not aligned with the user's interests in deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm
for enhancing the reasoning capabilities of large language models (LLMs), yet
its success hinges on effective exploration. An ideal exploration strategy must
navigate two fundamental challenges: it must preserve sample quality while also
ensuring training stability. While standard fixed-temperature sampling is
simple, it struggles to balance these competing demands, as high temperatures
degrade sample quality and low temperatures limit discovery. In this work, we
propose a simpler and more effective strategy, Exploratory Annealed Decoding
(EAD), grounded in the insight that exploration is most impactful on early
tokens which define a sequence's semantic direction. EAD implements an
intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by
annealing the sampling temperature from high to low during generation. This
dynamic schedule encourages meaningful, high-level diversity at the start, then
gradually lowers the temperature to preserve sample quality and keep the
sampling distribution close to the target policy, which is essential for stable
training. We demonstrate that EAD is a lightweight, plug-and-play method that
significantly improves sample efficiency, consistently outperforming
fixed-temperature sampling across various RLVR algorithms and model sizes. Our
work suggests that aligning exploration with the natural dynamics of sequential
generation offers a robust path to improving LLM reasoning.

### 47. Latent Speech-Text Transformer

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yen-Ju Lu, Yashesh Gaur, Wei Zhou, Benjamin Muller, Jesus Villalba, Najim Dehak, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Srinivasan Iyer, Duc Le
- **URL**: <http://arxiv.org/abs/2510.06195v1>
- **Submitted**: 2025-10-07 17:52:08
- **Comment**: 16 pages, 13 figures
- **Topic Keywords**: search
- **Reason**: This paper focuses on speech-text transformer models, which is not directly related to your primary research interests in Information Retrieval and Search technologies. While it does involve deep semantic understanding, the context is speech-to-text and text-to-text generation, which is not a central match for your research themes.

#### Abstract
> Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.

### 48. RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, Yifan Li, Wooseong Yang, Bowei He, Xinni Zhang, Dianzhi Yu, Hanchen Yang, Hoang H Nguyen, Yue Zhou, Jie Yang, Jizhou Guo, Wenzhe Fan, Chin-Yuan Yeh, Panpan Meng, Liancheng Fang, Jinhu Qi, Wei-Chieh Huang, Zhengyao Gu, Yuwei Han, Langzhou He, Yuyao Yang, Xue Liu, Irwin King, Philip S. Yu
- **URL**: <http://arxiv.org/abs/2510.06186v1>
- **Submitted**: 2025-10-07 17:45:35
- **Comment**: Code and dataset are available at github.com/ChunyuMiao98/RECODE
- **Topic Keywords**: search
- **Reason**: This paper focuses on developing a benchmark for large language models to generate correct and executable code in scientific research, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on the application of LLMs, it does not explore query understanding, ranking models, or click models, making it only loosely relevant to your research interests.

#### Abstract
> Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation

### 49. Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Peter Ochieng
- **URL**: <http://arxiv.org/abs/2510.05767v1>
- **Submitted**: 2025-10-07 10:35:58
- **Topic Keywords**: ctr, rank
- **Reason**: This paper appears to be unrelated to Information Retrieval, Search technologies, or Natural Language Processing. It focuses on contrastive learning and gradient magnitudes, which are topics more relevant to computer vision and deep learning.

#### Abstract
> We derive non-asymptotic spectral bands that bound the squared InfoNCE
gradient norm via alignment, temperature, and batch spectrum, recovering the
\(1/\tau^{2}\) law and closely tracking batch-mean gradients on synthetic data
and ImageNet. Using effective rank \(R_{\mathrm{eff}}\) as an anisotropy proxy,
we design spectrum-aware batch selection, including a fast greedy builder. On
ImageNet-100, Greedy-64 cuts time-to-67.5\% top-1 by 15\% vs.\ random (24\%
vs.\ Pool--P3) at equal accuracy; CIFAR-10 shows similar gains. In-batch
whitening promotes isotropy and reduces 50-step gradient variance by
\(1.37\times\), matching our theoretical upper bound.

### 50. NorMuon: Making Muon more efficient and scalable

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Zichong Li, Liming Liu, Chen Liang, Weizhu Chen, Tuo Zhao
- **URL**: <http://arxiv.org/abs/2510.05491v1>
- **Submitted**: 2025-10-07 01:13:41
- **Topic Keywords**: rag
- **Reason**: This paper focuses on optimizer design for large language models, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it involves deep learning, the context and application are not relevant to the user's core themes.

#### Abstract
> The choice of optimizer significantly impacts the training efficiency and
computational costs of large language models (LLMs). Recently, the Muon
optimizer has demonstrated promising results by orthogonalizing parameter
updates, improving optimization geometry through better conditioning. Despite
Muon's emergence as a candidate successor to Adam, the potential for jointly
leveraging their strengths has not been systematically explored. In this work,
we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an
optimizer that synergistically combines orthogonalization with neuron-level
adaptive learning rates. Our analysis reveals that while Muon effectively
reduces condition numbers, the resulting updates exhibit highly non-uniform
neuron norms, causing certain neurons to dominate the optimization process.
NorMuon addresses this imbalance by maintaining second-order momentum
statistics for each neuron and applying row-wise normalization after
orthogonalization, ensuring balanced parameter utilization while preserving
Muon's conditioning benefits. To enable practical deployment at scale, we
develop an efficient distributed implementation under the FSDP2 framework that
strategically distributes orthogonalization computations across devices.
Experiments across multiple model scales demonstrate that NorMuon consistently
outperforms both Adam and Muon, achieving 21.74% better training efficiency
than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while
maintaining a comparable memory footprint to Muon. Our findings suggest that
orthogonalization and adaptive learning rates are complementary rather than
competing approaches, opening new avenues for optimizer design in large-scale
deep learning.

---

