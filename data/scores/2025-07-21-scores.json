[
    {
        "title": "FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval",
        "abstract": "Patent examiners and inventors face significant pressure to verify the\noriginality and non-obviousness of inventions, and the intricate nature of\npatent data intensifies the challenges of patent retrieval. Therefore, there is\na pressing need to devise cutting-edge retrieval strategies that can reliably\nachieve the desired recall. This study introduces FullRecall, a novel patent\nretrieval approach that effectively manages the complexity of patent data while\nmaintaining the reliability of relevance matching and maximising recall. It\nleverages IPC-guided knowledge to generate informative phrases, which are\nprocessed to extract key information in the form of noun phrases characterising\nthe query patent under observation. From these, the top k keyphrases are\nselected to construct a query for retrieving a focused subset of the dataset.\nThis initial retrieval step achieves complete recall, successfully capturing\nall relevant documents. To further refine the results, a ranking scheme is\napplied to the retrieved subset, reducing its size while maintaining 100%\nrecall. This multi-phase process demonstrates an effective strategy for\nbalancing precision and recall in patent retrieval tasks. Comprehensive\nexperiments were conducted, and the results were compared with baseline\nstudies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded\nsuperior results, achieving 100% recall in all five test cases. However,\nHRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and\n14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the\nsecond test case, and 0% for the third, fourth, and fifth test cases. The 100%\nrecall ensures that no relevant prior art is overlooked, thereby strengthening\nthe patent pre-filing and examination processes, hence reducing potential legal\nrisks.",
        "url": "http://arxiv.org/abs/2507.14946v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14946v1",
        "arxiv_id": "2507.14946v1",
        "authors": [
            "Amna Ali",
            "Liyanage C. De Silva",
            "Pg Emeroylariffion Abas"
        ],
        "submitted": "2025-07-20 12:52:58",
        "source": "arxiv",
        "comment": null,
        "score": 18,
        "keyword_reasons": [
            "Found 'semantic search' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper's focus on patent retrieval and its emphasis on recall are somewhat related to my interests in Information Retrieval and Search technologies. However, the specific domain of patent retrieval and the use of IPC-guided knowledge are not directly aligned with my research themes. The paper's approach, while innovative, does not seem to involve query understanding, ranking models, or user behavior modeling, which are key areas of interest for me."
    },
    {
        "title": "GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou",
        "abstract": "Currently, short video platforms have become the primary place for\nindividuals to share experiences and obtain information. To better meet users'\nneeds for acquiring information while browsing short videos, some apps have\nintroduced a search entry at the bottom of videos, accompanied with recommended\nrelevant queries. This scenario is known as query recommendation in\nvideo-related search, where core task is item-to-query (I2Q) recommendation. As\nthis scenario has only emerged in recent years, there is a notable scarcity of\nacademic research and publicly available datasets in this domain. To address\nthis gap, we systematically examine the challenges associated with this\nscenario for the first time. Subsequently, we release a large-scale dataset\nderived from real-world data pertaining to the query recommendation in\nvideo-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the\n\\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods\nrely on embeddings to calculate similarity for matching short videos with\nqueries, lacking deep interaction between the semantic content and the query.\nIn this paper, we introduce a novel LLM-based framework named \\textbf{GREAT},\nwhich \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y\ng\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to\naddress I2Q recommendation in related search. Specifically, we initially gather\nhigh-quality queries with high exposure and click-through rate to construct a\nquery-based trie. During training, we enhance the LLM's capability to generate\nhigh-quality queries using the query-based trie. In the inference phase, the\nquery-based trie serves as a guide for the token generation. Finally, we\nfurther refine the relevance and literal quality between items and queries via\na post-processing module. Extensive offline and online experiments demonstrate\nthe effectiveness of our proposed method.",
        "url": "http://arxiv.org/abs/2507.15267v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15267v1",
        "arxiv_id": "2507.15267v1",
        "authors": [
            "Ninglu Shao",
            "Jinshan Wang",
            "Chenxu Wang",
            "Qingbiao Li",
            "Xiaoxue Zang",
            "Han Li"
        ],
        "submitted": "2025-07-21 06:10:30",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on query recommendation in video-related search, which is somewhat related to information retrieval and search technologies. However, the specific domain and approach (trie-based query generation) are not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection",
        "abstract": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy.",
        "url": "http://arxiv.org/abs/2507.15042v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15042v1",
        "arxiv_id": "2507.15042v1",
        "authors": [
            "Jerry Wang",
            "Fang Yu"
        ],
        "submitted": "2025-07-20 16:48:20",
        "source": "arxiv",
        "comment": "Accepted by KDD Workshop on Prompt Optimization 2025",
        "score": 11,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores adversarial attacks on Retrieval-Augmented Generation (RAG) systems, which is related to query understanding and ranking models in Information Retrieval. The focus on RAG-based question answering and the use of Differential Evolution to optimize adversarial prompts are novel and interesting. However, the paper's primary focus is on attacking RAG systems rather than improving them, which is not directly aligned with the user's interests in developing more effective search technologies."
    },
    {
        "title": "LOVO: Efficient Complex Object Query in Large-Scale Video Datasets",
        "abstract": "The widespread deployment of cameras has led to an exponential increase in\nvideo data, creating vast opportunities for applications such as traffic\nmanagement and crime surveillance. However, querying specific objects from\nlarge-scale video datasets presents challenges, including (1) processing\nmassive and continuously growing data volumes, (2) supporting complex query\nrequirements, and (3) ensuring low-latency execution. Existing video analysis\nmethods struggle with either limited adaptability to unseen object classes or\nsuffer from high query latency. In this paper, we present LOVO, a novel system\ndesigned to efficiently handle comp$\\underline{L}$ex $\\underline{O}$bject\nqueries in large-scale $\\underline{V}$ide$\\underline{O}$ datasets. Agnostic to\nuser queries, LOVO performs one-time feature extraction using pre-trained\nvisual encoders, generating compact visual embeddings for key frames to build\nan efficient index. These visual embeddings, along with associated bounding\nboxes, are organized in an inverted multi-index structure within a vector\ndatabase, which supports queries for any objects. During the query phase, LOVO\ntransforms object queries to query embeddings and conducts fast approximate\nnearest-neighbor searches on the visual embeddings. Finally, a cross-modal\nrerank is performed to refine the results by fusing visual features with\ndetailed textual features. Evaluation on real-world video datasets demonstrates\nthat LOVO outperforms existing methods in handling complex queries, with\nnear-optimal query accuracy and up to 85x lower search latency, while\nsignificantly reducing index construction costs. This system redefines the\nstate-of-the-art object query approaches in video analysis, setting a new\nbenchmark for complex object queries with a novel, scalable, and efficient\napproach that excels in dynamic environments.",
        "url": "http://arxiv.org/abs/2507.14301v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14301v1",
        "arxiv_id": "2507.14301v1",
        "authors": [
            "Yuxin Liu",
            "Yuezhang Peng",
            "Hefeng Zhou",
            "Hongze Liu",
            "Xinyu Lu",
            "Jiong Lou",
            "Chentao Wu",
            "Wei Zhao",
            "Jie Li"
        ],
        "submitted": "2025-07-18 18:21:43",
        "source": "arxiv",
        "comment": "@inproceedings{liu2025lovo,title={LOVO: Efficient Complex Object\n  Query in Large-Scale Video Datasets},author={Liu, Yuxin and Peng, Yuezhang\n  and Zhou, Hefeng and Liu, Hongze and Lu, Xinyu and Lou, Jiong and Wu, Chentao\n  and Zhao, Wei and Li, Jie},booktitle={2025 IEEE 41st International Conference\n  on Data Engineering (ICDE)},pages={1938--1951},year={2025},organization={IEEE\n  Computer Society}}",
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on video analysis and object query in large-scale video datasets, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions query embeddings and nearest-neighbor searches, the context is different from the user's background in e-commerce and query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care",
        "abstract": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.",
        "url": "http://arxiv.org/abs/2507.14681v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14681v1",
        "arxiv_id": "2507.14681v1",
        "authors": [
            "Vinicius Anjos de Almeida",
            "Vinicius de Camargo",
            "Raquel Gómez-Bravo",
            "Egbert van der Haring",
            "Kees van Boven",
            "Marcelo Finger",
            "Luis Fernandez Lopez"
        ],
        "submitted": "2025-07-19 16:11:10",
        "source": "arxiv",
        "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper",
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'semantic search' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. Although it uses large language models, the focus is on medical coding and does not involve query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is limited."
    },
    {
        "title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation",
        "abstract": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks.",
        "url": "http://arxiv.org/abs/2507.15826v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15826v1",
        "arxiv_id": "2507.15826v1",
        "authors": [
            "Alessandro B. Melchiorre",
            "Elena V. Epure",
            "Shahed Masoudian",
            "Gustavo Escobedo",
            "Anna Hausberger",
            "Manuel Moussallam",
            "Markus Schedl"
        ],
        "submitted": "2025-07-21 17:36:03",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper presents a natural language music recommendation system, which is related to information retrieval and search technologies. However, the focus is on music recommendation rather than general query understanding and ranking models, which are core areas of interest. The paper's use of multimodal item features and knowledge graph embedding methods is somewhat relevant to my background in NLP and data mining."
    },
    {
        "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs",
        "abstract": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.",
        "url": "http://arxiv.org/abs/2507.15715v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15715v1",
        "arxiv_id": "2507.15715v1",
        "authors": [
            "Alina Hyk",
            "Kiera McCormick",
            "Mian Zhong",
            "Ioana Ciucă",
            "Sanjib Sharma",
            "John F Wu",
            "J. E. G. Peek",
            "Kartheik G. Iyer",
            "Ziang Xiao",
            "Anjalie Field"
        ],
        "submitted": "2025-07-21 15:26:58",
        "source": "arxiv",
        "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures",
        "score": 9,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to my research interests in Information Retrieval and Search technologies, as it explores the evaluation of Large Language Models (LLMs) in a specific domain (astronomy). However, the focus on LLMs and their evaluation in a scientific research context is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search",
        "abstract": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR",
        "url": "http://arxiv.org/abs/2507.15245v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15245v1",
        "arxiv_id": "2507.15245v1",
        "authors": [
            "Xiaofeng Shi",
            "Yuduo Li",
            "Qian Kou",
            "Longbin Yu",
            "Jinxin Xie",
            "Hua Zhou"
        ],
        "submitted": "2025-07-21 05:06:53",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper presents a novel framework for academic literature retrieval, leveraging large language models and multi-agent architecture. While it's not directly focused on query understanding, ranking models, or user behavior modeling, it explores advanced search technologies and relevance optimization, which aligns with your interests in Information Retrieval. However, the paper's primary focus is on scholarly retrieval, which is a specific domain, and may not be directly applicable to your e-commerce background."
    },
    {
        "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
        "abstract": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%.",
        "url": "http://arxiv.org/abs/2507.15551v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15551v1",
        "arxiv_id": "2507.15551v1",
        "authors": [
            "Jie Zhu",
            "Zhifang Fan",
            "Xiaoxie Zhu",
            "Yuchen Jiang",
            "Hangyu Wang",
            "Xintian Han",
            "Haoran Ding",
            "Xinmin Wang",
            "Wenlin Zhao",
            "Zhen Gong",
            "Huizhi Yang",
            "Zheng Chai",
            "Zhe Chen",
            "Yuchao Zheng",
            "Qiwei Chen",
            "Feng Zhang",
            "Xun Zhou",
            "Peng Xu",
            "Xiao Yang",
            "Di Wu",
            "Zuotao Liu"
        ],
        "submitted": "2025-07-21 12:28:55",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on scaling up ranking models in industrial recommenders, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and the lack of deep semantic understanding and real-time relevance optimization make it less relevant to my primary research focus."
    },
    {
        "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning",
        "abstract": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.",
        "url": "http://arxiv.org/abs/2507.15714v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15714v1",
        "arxiv_id": "2507.15714v1",
        "authors": [
            "Tian Li",
            "Yujian Sun",
            "Huizhi Liang"
        ],
        "submitted": "2025-07-21 15:25:47",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on emotion perception using contrastive learning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on emotion detection and language models is not aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations",
        "abstract": "User journeys in e-commerce routinely violate the one-to-one assumption that\na clicked item on an advertising platform is the same item later purchased on\nthe merchant's website/app. For a significant number of converting sessions on\nour platform, users click product A but buy product B -- the Click A, Buy B\n(CABB) phenomenon. Training recommendation models on raw click-conversion pairs\ntherefore rewards items that merely correlate with purchases, leading to biased\nlearning and sub-optimal conversion rates. We reframe conversion prediction as\na multi-task problem with separate heads for Click A Buy A (CABA) and Click A\nBuy B (CABB). To isolate informative CABB conversions from unrelated CABB\nconversions, we introduce a taxonomy-aware collaborative filtering weighting\nscheme where each product is first mapped to a leaf node in a product taxonomy,\nand a category-to-category similarity matrix is learned from large-scale\nco-engagement logs. This weighting amplifies pairs that reflect genuine\nsubstitutable or complementary relations while down-weighting coincidental\ncross-category purchases. Offline evaluation on e-commerce sessions reduces\nnormalized entropy by 13.9% versus a last-click attribution baseline. An online\nA/B test on live traffic shows +0.25% gains in the primary business metric.",
        "url": "http://arxiv.org/abs/2507.15113v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15113v1",
        "arxiv_id": "2507.15113v1",
        "authors": [
            "Xiangyu Zeng",
            "Amit Jaspal",
            "Bin Liu",
            "Goutham Panneeru",
            "Kevin Huang",
            "Nicolas Bievre",
            "Mohit Jaggi",
            "Prathap Maniraju",
            "Ankur Jain"
        ],
        "submitted": "2025-07-20 20:25:20",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'conversion rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores a specific problem in e-commerce recommendations, focusing on conversion attribution and rethinking the traditional click-purchase assumption. While it touches on user behavior modeling and click models, the primary focus is on recommender systems rather than information retrieval. The paper's relevance to the user's interests lies in its application to e-commerce, but the topics and techniques used are not directly related to the user's core research themes."
    },
    {
        "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership",
        "abstract": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.",
        "url": "http://arxiv.org/abs/2507.15759v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15759v1",
        "arxiv_id": "2507.15759v1",
        "authors": [
            "Lyumanshan Ye",
            "Xiaojie Cai",
            "Xinkai Wang",
            "Junfei Wang",
            "Xiangkun Hu",
            "Jiadi Su",
            "Yang Nan",
            "Sihan Wang",
            "Bohan Zhang",
            "Xiaoze Fan",
            "Jinbin Luo",
            "Yuxiang Zheng",
            "Tianze Xu",
            "Dayuan Fu",
            "Yunze Wu",
            "Pengrui Lu",
            "Zengzhi Wang",
            "Yiwei Qin",
            "Zhen Huang",
            "Yan Ma",
            "Zhulin Hu",
            "Haoyang Zou",
            "Tiantian Mi",
            "Yixin Ye",
            "Ethan Chern",
            "Pengfei Liu"
        ],
        "submitted": "2025-07-21 16:15:18",
        "source": "arxiv",
        "comment": "30 pages, 10 figures",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'user behavior' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the concept of 'Interaction as Intelligence' in deep research tasks, proposing a new paradigm for human-AI collaboration. While it touches on some aspects of information retrieval, such as query refinement and fine-grained dialogue, the primary focus is on the human-AI partnership and the cognitive oversight model. The paper's relevance to my research interests is somewhat related, but not a central match, as it does not specifically address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs",
        "abstract": "Universal multimodal retrieval (UMR), which aims to address complex retrieval\ntasks where both queries and candidates span diverse modalities, has been\nsignificantly advanced by the emergence of MLLMs. While state-of-the-art\nMLLM-based methods in the literature predominantly adopt contrastive learning\nprinciples, they often differ in their specific training recipes. Despite their\nsuccess, the mechanisms underlying their retrieval capabilities remain largely\nunexplored, potentially resulting in suboptimal performance and limited\ngeneralization ability. To address these issues, we present a comprehensive\nstudy aimed at uncovering the key factors that drive effective embedding\nlearning for UMR using MLLMs. We begin by implementing a general MLLM-based\nembedding learning pipeline, and systematically analyze the primary\ncontributors to high-performing universal retrieval systems. Based on this, we\nexplore various aspects of the details in embedding generation and training\nstrategies, including progressive transition, hard negative mining and\nre-ranker distillation. Notably, our findings reveal that often-overlooked\nfactors can have a substantial impact on model performance. Building on these\ndiscoveries, we introduce a unified framework termed U-MARVEL\n(\\textbf{U}niversal \\textbf{M}ultimod\\textbf{A}l \\textbf{R}etrie\\textbf{V}al\nvia \\textbf{E}mbedding \\textbf{L}earning), which outperforms state-of-the-art\ncompetitors on the M-BEIR benchmark by a large margin in supervised settings,\nand also exihibits strong zero-shot performance on several tasks such as\ncomposed image retrieval and text-to-video retrieval. These results underscore\nthe generalization potential of our framework across various embedding-based\nretrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL",
        "url": "http://arxiv.org/abs/2507.14902v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14902v1",
        "arxiv_id": "2507.14902v1",
        "authors": [
            "Xiaojie Li",
            "Chu Li",
            "Shi-Zhe Chen",
            "Xi Chen"
        ],
        "submitted": "2025-07-20 10:27:34",
        "source": "arxiv",
        "comment": "Technical Report (in progress)",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on universal multimodal retrieval, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. While the paper mentions MLLMs, which are relevant to the user's background in NLP, the specific application and techniques used are not directly applicable to the user's areas of interest."
    },
    {
        "title": "Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining",
        "abstract": "Large Language Models (LLMs) face significant challenges in specialized\ndomains like law, where precision and domain-specific knowledge are critical.\nThis paper presents a streamlined two-stage framework consisting of Retrieval\nand Re-ranking to enhance legal document retrieval efficiency and accuracy. Our\napproach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,\nfollowed by a Cross-Encoder for precise re-ranking, both optimized through\nstrategic negative example mining. Key innovations include the introduction of\nthe Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard\nnegatives to mitigate training bias, which significantly improved re-ranking\nperformance. Evaluated on the SoICT Hackathon 2024 for Legal Document\nRetrieval, our team, 4Huiter, achieved a top-three position. While\ntop-performing teams employed ensemble models and iterative self-training on\nlarge bge-m3 architectures, our lightweight, single-pass approach offered a\ncompetitive alternative with far fewer parameters. The framework demonstrates\nthat optimized data processing, tailored loss functions, and balanced negative\nsampling are pivotal for building robust retrieval-augmented systems in legal\ncontexts.",
        "url": "http://arxiv.org/abs/2507.14619v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14619v1",
        "arxiv_id": "2507.14619v1",
        "authors": [
            "Van-Hoang Le",
            "Duc-Vu Nguyen",
            "Kiet Van Nguyen",
            "Ngan Luu-Thuy Nguyen"
        ],
        "submitted": "2025-07-19 13:30:14",
        "source": "arxiv",
        "comment": "Accepted at ICCCI 2025",
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on legal document retrieval in Vietnamese, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper employs some relevant techniques like fine-tuned Bi-Encoder and Cross-Encoder, the domain and specific challenges are not aligned with the user's background and interests."
    },
    {
        "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper",
        "abstract": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.",
        "url": "http://arxiv.org/abs/2507.14615v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14615v1",
        "arxiv_id": "2507.14615v1",
        "authors": [
            "Fred Mutisya",
            "Shikoh Gitau",
            "Christine Syovata",
            "Diana Oigara",
            "Ibrahim Matende",
            "Muna Aden",
            "Munira Ali",
            "Ryan Nyotu",
            "Diana Marion",
            "Job Nyangena",
            "Nasubo Ongoma",
            "Keith Mbae",
            "Elizabeth Wamicha",
            "Eric Mibuari",
            "Jean Philbert Nsengemana",
            "Talkmore Chidede"
        ],
        "submitted": "2025-07-19 13:25:26",
        "source": "arxiv",
        "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming",
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on clinical benchmarking and healthcare access in low-resource settings, using large language models for generating clinical scenarios and questions. The paper does not address query understanding, ranking models, or user behavior modeling, which are core areas of your research focus."
    },
    {
        "title": "Text-to-SQL for Enterprise Data Analytics",
        "abstract": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.",
        "url": "http://arxiv.org/abs/2507.14372v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14372v1",
        "arxiv_id": "2507.14372v1",
        "authors": [
            "Albert Chen",
            "Manas Bundele",
            "Gaurav Ahlawat",
            "Patrick Stetz",
            "Zhitao Wang",
            "Qiang Fei",
            "Donghoon Jung",
            "Audrey Chu",
            "Bharadwaj Jayaraman",
            "Ayushi Panth",
            "Yatin Arora",
            "Sourav Jain",
            "Renjith Varma",
            "Alexey Ilin",
            "Iuliia Melnychuk",
            "Chelsea Chueh",
            "Joyan Sil",
            "Xiaofeng Wang"
        ],
        "submitted": "2025-07-18 21:39:17",
        "source": "arxiv",
        "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Text-to-SQL for enterprise data analytics, which is not directly related to my primary research interests in Information Retrieval and Search technologies. While it mentions query understanding and ranking models, the context is different and not as relevant to my work. The paper's emphasis on building a chatbot and knowledge graph is also not a central match for my research themes."
    },
    {
        "title": "P3: Prompts Promote Prompting",
        "abstract": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.",
        "url": "http://arxiv.org/abs/2507.15675v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15675v1",
        "arxiv_id": "2507.15675v1",
        "authors": [
            "Xinyu Zhang",
            "Yuanquan Hu",
            "Fangchao Liu",
            "Zhicheng Dou"
        ],
        "submitted": "2025-07-21 14:37:46",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 findings",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing prompts for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization strategies, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation",
        "abstract": "In real-world recommendation scenarios, users typically engage with platforms\nthrough multiple types of behavioral interactions. Multi-behavior\nrecommendation algorithms aim to leverage various auxiliary user behaviors to\nenhance prediction for target behaviors of primary interest (e.g., buy),\nthereby overcoming performance limitations caused by data sparsity in target\nbehavior records. Current state-of-the-art approaches typically employ\nhierarchical design following either cascading (e.g.,\nview$\\rightarrow$cart$\\rightarrow$buy) or parallel\n(unified$\\rightarrow$behavior$\\rightarrow$specific components) paradigms, to\ncapture behavioral relationships. However, these methods still face two\ncritical challenges: (1) severe distribution disparities across behaviors, and\n(2) negative transfer effects caused by noise in auxiliary behaviors. In this\npaper, we propose a novel model-agnostic Hierarchical Graph Information\nBottleneck (HGIB) framework for multi-behavior recommendation to effectively\naddress these challenges. Following information bottleneck principles, our\nframework optimizes the learning of compact yet sufficient representations that\npreserve essential information for target behavior prediction while eliminating\ntask-irrelevant redundancies. To further mitigate interaction noise, we\nintroduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant\nedges through learnable edge dropout mechanisms. We conduct comprehensive\nexperiments on three real-world public datasets, which demonstrate the superior\neffectiveness of our framework. Beyond these widely used datasets in the\nacademic community, we further expand our evaluation on several real industrial\nscenarios and conduct an online A/B testing, showing again a significant\nimprovement in multi-behavior recommendations. The source code of our proposed\nHGIB is available at https://github.com/zhy99426/HGIB.",
        "url": "http://arxiv.org/abs/2507.15395v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15395v1",
        "arxiv_id": "2507.15395v1",
        "authors": [
            "Hengyu Zhang",
            "Chunxu Shen",
            "Xiangguo Sun",
            "Jie Tan",
            "Yanchao Tan",
            "Yu Rong",
            "Hong Cheng",
            "Lingling Yi"
        ],
        "submitted": "2025-07-21 08:53:49",
        "source": "arxiv",
        "comment": "Accepted by RecSys2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multi-behavior recommendation, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and behavioral interactions is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of graph-based methods and information bottleneck principles is an interesting aspect, but it does not seem to have a direct connection to my research areas."
    },
    {
        "title": "User Invariant Preference Learning for Multi-Behavior Recommendation",
        "abstract": "In multi-behavior recommendation scenarios, analyzing users' diverse\nbehaviors, such as click, purchase, and rating, enables a more comprehensive\nunderstanding of their interests, facilitating personalized and accurate\nrecommendations. A fundamental assumption of multi-behavior recommendation\nmethods is the existence of shared user preferences across behaviors,\nrepresenting users' intrinsic interests. Based on this assumption, existing\napproaches aim to integrate information from various behaviors to enrich user\nrepresentations. However, they often overlook the presence of both\ncommonalities and individualities in users' multi-behavior preferences. These\nindividualities reflect distinct aspects of preferences captured by different\nbehaviors, where certain auxiliary behaviors may introduce noise, hindering the\nprediction of the target behavior. To address this issue, we propose a user\ninvariant preference learning for multi-behavior recommendation (UIPL for\nshort), aiming to capture users' intrinsic interests (referred to as invariant\npreferences) from multi-behavior interactions to mitigate the introduction of\nnoise. Specifically, UIPL leverages the paradigm of invariant risk minimization\nto learn invariant preferences. To implement this, we employ a variational\nautoencoder (VAE) to extract users' invariant preferences, replacing the\nstandard reconstruction loss with an invariant risk minimization constraint.\nAdditionally, we construct distinct environments by combining multi-behavior\ndata to enhance robustness in learning these preferences. Finally, the learned\ninvariant preferences are used to provide recommendations for the target\nbehavior. Extensive experiments on four real-world datasets demonstrate that\nUIPL significantly outperforms current state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2507.14925v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14925v1",
        "arxiv_id": "2507.14925v1",
        "authors": [
            "Mingshi Yan",
            "Zhiyong Cheng",
            "Fan Liu",
            "Yingda Lyu",
            "Yahong Han"
        ],
        "submitted": "2025-07-20 11:47:36",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multi-behavior recommendation, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and the lack of deep semantic understanding and real-time relevance optimization in the paper make it less relevant to my core research themes."
    },
    {
        "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents",
        "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.",
        "url": "http://arxiv.org/abs/2507.14819v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14819v1",
        "arxiv_id": "2507.14819v1",
        "authors": [
            "Akriti Jain",
            "Pritika Ramu",
            "Aparna Garimella",
            "Apoorv Saxena"
        ],
        "submitted": "2025-07-20 04:34:59",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel task of intent-driven chart generation from documents, which is related to information retrieval and search technologies. However, the focus is on natural language processing and data visualization, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user."
    },
    {
        "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization",
        "abstract": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.",
        "url": "http://arxiv.org/abs/2507.14758v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14758v1",
        "arxiv_id": "2507.14758v1",
        "authors": [
            "Luyi Ma",
            "Wanjia Zhang",
            "Kai Zhao",
            "Abhishek Kulkarni",
            "Lalitesh Morishetti",
            "Anjana Ganesh",
            "Ashish Ranjan",
            "Aashika Padmanabhan",
            "Jianpeng Xu",
            "Jason Cho",
            "Praveen Kanumala",
            "Kaushiki Nag",
            "Sumit Dutta",
            "Kamiya Motwani",
            "Malay Patel",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "submitted": "2025-07-19 21:23:23",
        "source": "arxiv",
        "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a generative framework for multi-behavior sequential recommendation, which is not directly related to information retrieval or search technologies. While it uses tokenization and attention mechanisms, the focus is on recommender systems rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is limited, but it may be of interest to those with a broader background in NLP and data mining."
    },
    {
        "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding",
        "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.",
        "url": "http://arxiv.org/abs/2507.15846v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15846v1",
        "arxiv_id": "2507.15846v1",
        "authors": [
            "Fei Tang",
            "Zhangxuan Gu",
            "Zhengxi Lu",
            "Xuyang Liu",
            "Shuheng Shen",
            "Changhua Meng",
            "Wen Wang",
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "submitted": "2025-07-21 17:53:42",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on GUI grounding, which is not directly related to information retrieval or search technologies. While it uses reinforcement learning, the application is in GUI interaction tasks, which is not a core area of interest. The paper does not mention query understanding, ranking models, or user behavior modeling, which are key aspects of the user's research interests."
    },
    {
        "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.",
        "url": "http://arxiv.org/abs/2507.15586v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15586v1",
        "arxiv_id": "2507.15586v1",
        "authors": [
            "Xinping Zhao",
            "Shouzheng Huang",
            "Yan Zhong",
            "Xinshuo Hu",
            "Baotian Hu",
            "Min Zhang"
        ],
        "submitted": "2025-07-21 13:03:55",
        "source": "arxiv",
        "comment": "16 pages, 7 Figures, 10 Tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores Retrieval-Augmented Generation, which is related to information retrieval, but the focus is on language models and generation rather than search technologies or query understanding. While the paper mentions retrieval, it is not directly applicable to the user's interests in ranking models or user behavior modeling."
    },
    {
        "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding",
        "abstract": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.",
        "url": "http://arxiv.org/abs/2507.14849v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14849v1",
        "arxiv_id": "2507.14849v1",
        "authors": [
            "Yifei Wang"
        ],
        "submitted": "2025-07-20 07:43:16",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the intersection of natural language processing and information retrieval, specifically focusing on long-context understanding and reasoning. While it doesn't directly address query understanding, ranking models, or user behavior modeling, it does touch on the importance of contextual information in retrieval-augmented generation systems, which is relevant to the broader field of information retrieval. However, the paper's primary focus on long-context understanding and reasoning distillation doesn't directly align with the user's specific research interests."
    },
    {
        "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
        "abstract": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot",
        "url": "http://arxiv.org/abs/2507.14675v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14675v1",
        "arxiv_id": "2507.14675v1",
        "authors": [
            "Yuchen Duan",
            "Zhe Chen",
            "Yusong Hu",
            "Weiyun Wang",
            "Shenglong Ye",
            "Botian Shi",
            "Lewei Lu",
            "Qibin Hou",
            "Tong Lu",
            "Hongsheng Li",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "submitted": "2025-07-19 16:03:34",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on multimodal document comprehension and presents a new dataset and model for document-level understanding. While it touches on information retrieval and multimodal processing, the primary focus is on document comprehension rather than query understanding or ranking models, which are key areas of interest for you. The paper's relevance is somewhat related to your interests in IR and NLP, but it does not directly address your core research themes."
    },
    {
        "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification",
        "abstract": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.",
        "url": "http://arxiv.org/abs/2507.14578v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14578v1",
        "arxiv_id": "2507.14578v1",
        "authors": [
            "Sachin Yadav",
            "Dominik Schlechtweg"
        ],
        "submitted": "2025-07-19 11:40:37",
        "source": "arxiv",
        "comment": "8 pages",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a finetuned Sentence Transformer model for ordinal Word-in-Context classification, which is a specific NLP task. While it touches on ranking and optimization, the focus is on classification rather than query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval. The paper's relevance to IR is limited, but it may be of interest to those with a broader NLP focus."
    },
    {
        "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display",
        "abstract": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.",
        "url": "http://arxiv.org/abs/2507.14430v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14430v1",
        "arxiv_id": "2507.14430v1",
        "authors": [
            "Xiaolin Yan",
            "Yangxing Liu",
            "Jiazhang Zheng",
            "Chi Liu",
            "Mingyu Du",
            "Caisheng Chen",
            "Haoyang Liu",
            "Ming Ding",
            "Yuan Li",
            "Qiuping Liao",
            "Linfeng Li",
            "Zhili Mei",
            "Siyu Wan",
            "Li Li",
            "Ruyi Zhong",
            "Jiangling Yu",
            "Xule Liu",
            "Huihui Hu",
            "Jiameng Yue",
            "Ruohui Cheng",
            "Qi Yang",
            "Liangqing Wu",
            "Ke Zhu",
            "Chi Zhang",
            "Chufei Jing",
            "Yifan Zhou",
            "Yan Liang",
            "Dongdong Li",
            "Zhaohui Wang",
            "Bin Zhao",
            "Mingzhou Wu",
            "Mingzhong Zhou",
            "Peng Du",
            "Zuomin Liao",
            "Chao Dai",
            "Pengfei Liang",
            "Xiaoguang Zhu",
            "Yu Zhang",
            "Yu Gu",
            "Kun Pan",
            "Yuan Wu",
            "Yanqing Guan",
            "Shaojing Wu",
            "Zikang Feng",
            "Xianze Ma",
            "Peishan Cheng",
            "Wenjuan Jiang",
            "Jing Ba",
            "Huihao Yu",
            "Zeping Hu",
            "Yuan Xu",
            "Zhiwei Liu",
            "He Wang",
            "Zhenguo Lin",
            "Ming Liu",
            "Yanhong Meng"
        ],
        "submitted": "2025-07-19 01:20:39",
        "source": "arxiv",
        "comment": "Technical Report",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language models for the semiconductor display industry, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on domain-specific training and expertise also does not align with the user's interests in generalizable models and real-time relevance optimization."
    },
    {
        "title": "A Reproducibility Study of Product-side Fairness in Bundle Recommendation",
        "abstract": "Recommender systems are known to exhibit fairness issues, particularly on the\nproduct side, where products and their associated suppliers receive unequal\nexposure in recommended results. While this problem has been widely studied in\ntraditional recommendation settings, its implications for bundle recommendation\n(BR) remain largely unexplored. This emerging task introduces additional\ncomplexity: recommendations are generated at the bundle level, yet user\nsatisfaction and product (or supplier) exposure depend on both the bundle and\nthe individual items it contains. Existing fairness frameworks and metrics\ndesigned for traditional recommender systems may not directly translate to this\nmulti-layered setting. In this paper, we conduct a comprehensive\nreproducibility study of product-side fairness in BR across three real-world\ndatasets using four state-of-the-art BR methods. We analyze exposure\ndisparities at both the bundle and item levels using multiple fairness metrics,\nuncovering important patterns. Our results show that exposure patterns differ\nnotably between bundles and items, revealing the need for fairness\ninterventions that go beyond bundle-level assumptions. We also find that\nfairness assessments vary considerably depending on the metric used,\nreinforcing the need for multi-faceted evaluation. Furthermore, user behavior\nplays a critical role: when users interact more frequently with bundles than\nwith individual items, BR systems tend to yield fairer exposure distributions\nacross both levels. Overall, our findings offer actionable insights for\nbuilding fairer bundle recommender systems and establish a vital foundation for\nfuture research in this emerging domain.",
        "url": "http://arxiv.org/abs/2507.14352v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14352v1",
        "arxiv_id": "2507.14352v1",
        "authors": [
            "Huy-Son Nguyen",
            "Yuanna Liu",
            "Masoud Mansoury",
            "Mohammad Alian Nejadi",
            "Alan Hanjalic",
            "Maarten de Rijke"
        ],
        "submitted": "2025-07-18 20:06:39",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores fairness issues in bundle recommendation, which is a related topic to information retrieval and search technologies. However, the focus on recommender systems and fairness metrics is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat related but not a central match."
    },
    {
        "title": "Supernova: Achieving More with Less in Transformer Architectures",
        "abstract": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 53% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.",
        "url": "http://arxiv.org/abs/2507.15773v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15773v1",
        "arxiv_id": "2507.15773v1",
        "authors": [
            "Andrei-Valentin Tanase",
            "Elena Pelican"
        ],
        "submitted": "2025-07-21 16:27:48",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on transformer architectures and tokenization innovations, which are not directly related to information retrieval, search technologies, or query understanding. While it explores efficiency and compression, the topics are not aligned with the user's primary research interests."
    },
    {
        "title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates",
        "abstract": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements.",
        "url": "http://arxiv.org/abs/2507.15641v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15641v1",
        "arxiv_id": "2507.15641v1",
        "authors": [
            "Alessio Pittiglio"
        ],
        "submitted": "2025-07-21 14:03:08",
        "source": "arxiv",
        "comment": "12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multimodal argument mining and fallacy classification in political debates, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's context and multimodal approach are not directly applicable to the user's areas of focus, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution",
        "abstract": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.",
        "url": "http://arxiv.org/abs/2507.15501v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15501v1",
        "arxiv_id": "2507.15501v1",
        "authors": [
            "Alexandru Coca",
            "Mark Gaynor",
            "Zhenxing Zhang",
            "Jianpeng Cheng",
            "Bo-Hsiang Tseng",
            "Pete Boothroyd",
            "Héctor Martinez Alonso",
            "Diarmuid Ó Séaghdha",
            "Anders Johannsen"
        ],
        "submitted": "2025-07-21 11:07:05",
        "source": "arxiv",
        "comment": "37 pages, 22 figures. To appear at ACL 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the application of large language models in digital assistants for complex action execution, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on NLP, it is more focused on programming knowledge and task generation, which is not a central match for your research interests."
    },
    {
        "title": "A Novel Self-Evolution Framework for Large Language Models",
        "abstract": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs.",
        "url": "http://arxiv.org/abs/2507.15281v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15281v1",
        "arxiv_id": "2507.15281v1",
        "authors": [
            "Haoran Sun",
            "Zekun Zhang",
            "Shaoning Zeng"
        ],
        "submitted": "2025-07-21 06:30:39",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a framework for optimizing large language models, focusing on user preference adaptation and domain-specific competence. While it touches on topics related to information retrieval, such as user behavior modeling, the primary focus is on language models and NLP, which is not directly aligned with the user's core research themes in IR and search technologies."
    },
    {
        "title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling",
        "abstract": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability.",
        "url": "http://arxiv.org/abs/2507.15275v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15275v1",
        "arxiv_id": "2507.15275v1",
        "authors": [
            "Yuanhe Tian",
            "Junjie Liu",
            "Zhizhou Kou",
            "Yuxiang Li",
            "Yan Song"
        ],
        "submitted": "2025-07-21 06:23:16",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on building a large Chinese medical dataset for language modeling, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions pre-training and fine-tuning, the context is specific to language models and does not align with the user's interests in ranking models, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations",
        "abstract": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$.",
        "url": "http://arxiv.org/abs/2507.15092v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15092v1",
        "arxiv_id": "2507.15092v1",
        "authors": [
            "Vijeta Deshpande",
            "Ishita Dasgupta",
            "Uttaran Bhattacharya",
            "Somdeb Sarkhel",
            "Saayan Mitra",
            "Anna Rumshisky"
        ],
        "submitted": "2025-07-20 19:14:43",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on measuring lexical diversity in synthetic texts generated by Large Language Models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest."
    },
    {
        "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs",
        "abstract": "Persona-driven LLMs have emerged as powerful tools in computational social\nscience, yet existing approaches fall at opposite extremes, either relying on\ncostly human-curated data or producing synthetic personas that lack consistency\nand realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from\n10,000 real social media users from BlueSky open platform across three time\nwindows, bridging this spectrum by grounding synthetic generation in authentic\nuser activity. Our evaluation demonstrates that SYNTHIA achieves competitive\nperformance with state-of-the-art methods in demographic diversity and social\nsurvey alignment while significantly outperforming them in narrative\nconsistency. Uniquely, SYNTHIA incorporates temporal dimensionality and\nprovides rich social interaction metadata from the underlying network, enabling\nnew research directions in computational social science and persona-driven\nlanguage modeling.",
        "url": "http://arxiv.org/abs/2507.14922v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14922v1",
        "arxiv_id": "2507.14922v1",
        "authors": [
            "Vahid Rahimzadeh",
            "Erfan Moosavi Monazzah",
            "Mohammad Taher Pilehvar",
            "Yadollah Yaghoobzadeh"
        ],
        "submitted": "2025-07-20 11:37:07",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on persona-driven language models and synthetic data generation, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language modeling, the primary focus is on computational social science and persona-driven language models, which is not a central match for the user's research interests."
    },
    {
        "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization",
        "abstract": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.",
        "url": "http://arxiv.org/abs/2507.14683v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14683v1",
        "arxiv_id": "2507.14683v1",
        "authors": [
            "Xingxuan Li",
            "Yao Xiao",
            "Dianwen Ng",
            "Hai Ye",
            "Yue Deng",
            "Xiang Lin",
            "Bin Wang",
            "Zhanfeng Mo",
            "Chong Zhang",
            "Yueyi Zhang",
            "Zonglin Yang",
            "Ruilin Li",
            "Lei Lei",
            "Shihao Xu",
            "Han Zhao",
            "Weiling Chen",
            "Feng Ji",
            "Lidong Bing"
        ],
        "submitted": "2025-07-19 16:21:23",
        "source": "arxiv",
        "comment": "Technical report",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on mathematical reasoning and large language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on transparency and reproducibility in RLM development is also not a central match for the user's interests."
    },
    {
        "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems",
        "abstract": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.",
        "url": "http://arxiv.org/abs/2507.14660v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14660v1",
        "arxiv_id": "2507.14660v1",
        "authors": [
            "Qibing Ren",
            "Sitao Xie",
            "Longxuan Wei",
            "Zhenfei Yin",
            "Junchi Yan",
            "Lizhuang Ma",
            "Jing Shao"
        ],
        "submitted": "2025-07-19 15:17:30",
        "source": "arxiv",
        "comment": "Code is available at https://github.com/renqibing/RogueAgent",
        "score": 3,
        "keyword_reasons": [
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper's focus on autonomous AI systems, multi-agent collusion, and malicious actions is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, user behavior modeling, or deep semantic understanding, which are key areas of interest for the user."
    },
    {
        "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models",
        "abstract": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.",
        "url": "http://arxiv.org/abs/2507.14579v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14579v1",
        "arxiv_id": "2507.14579v1",
        "authors": [
            "Kester Wong",
            "Sahan Bulathwela",
            "Mutlu Cukurova"
        ],
        "submitted": "2025-07-19 11:47:08",
        "source": "arxiv",
        "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper explores the application of BERT models in a specific domain (CPS diagnosis in education) with a focus on multimodal features and human-AI complementarity. While it touches on machine learning and AI, it does not align with the user's primary research interests in Information Retrieval, Search technologies, and query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers",
        "abstract": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge.",
        "url": "http://arxiv.org/abs/2507.14353v1",
        "pdf_url": "http://arxiv.org/pdf/2507.14353v1",
        "arxiv_id": "2507.14353v1",
        "authors": [
            "Harsh Nilesh Pathak",
            "Randy Paffenroth"
        ],
        "submitted": "2025-07-18 20:11:50",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on fine-tuning transformer models for language generation tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions transformer models, the paper's primary concern is adapting language models for new tasks, which is not a key area of interest for the user."
    },
    {
        "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
        "abstract": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.",
        "url": "http://arxiv.org/abs/2507.15849v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15849v1",
        "arxiv_id": "2507.15849v1",
        "authors": [
            "Yihao Li",
            "Jiayi Xin",
            "Miranda Muqing Miao",
            "Qi Long",
            "Lyle Ungar"
        ],
        "submitted": "2025-07-21 17:56:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus on bilingual large language models and language mixing in reasoning is outside the user's primary areas of interest."
    },
    {
        "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
        "abstract": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.",
        "url": "http://arxiv.org/abs/2507.15844v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15844v1",
        "arxiv_id": "2507.15844v1",
        "authors": [
            "Shangke Lyu",
            "Linjuan Wu",
            "Yuchen Yan",
            "Xingyu Wu",
            "Hao Li",
            "Yongliang Shen",
            "Peisheng Jiang",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "submitted": "2025-07-21 17:52:34",
        "source": "arxiv",
        "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing the efficiency of reasoning models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of adapting to problem complexity, the approach is not applicable to the user's areas of interest, such as ranking models or user behavior modeling."
    },
    {
        "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.",
        "url": "http://arxiv.org/abs/2507.15778v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15778v1",
        "arxiv_id": "2507.15778v1",
        "authors": [
            "Jiakang Wang",
            "Runze Liu",
            "Fuzheng Zhang",
            "Xiu Li",
            "Guorui Zhou"
        ],
        "submitted": "2025-07-21 16:34:01",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Reinforcement Learning with Verifiable Rewards for improving the reasoning abilities of Large Language Models, which is outside your primary focus area."
    },
    {
        "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization",
        "abstract": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
        "url": "http://arxiv.org/abs/2507.15758v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15758v1",
        "arxiv_id": "2507.15758v1",
        "authors": [
            "Xingyu Wu",
            "Yuchen Yan",
            "Shangke Lyu",
            "Linjuan Wu",
            "Yiwen Qiu",
            "Yongliang Shen",
            "Weiming Lu",
            "Jian Shao",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "submitted": "2025-07-21 16:14:41",
        "source": "arxiv",
        "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing the length of reasoning sequences in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the concept of efficient reasoning, the context is different from the user's primary research interests."
    },
    {
        "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme",
        "abstract": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.",
        "url": "http://arxiv.org/abs/2507.15742v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15742v1",
        "arxiv_id": "2507.15742v1",
        "authors": [
            "Paul Sheridan",
            "Zeyad Ahmed",
            "Aitazaz A. Farooque"
        ],
        "submitted": "2025-07-21 15:54:23",
        "source": "arxiv",
        "comment": "23 pages, 4 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically discussing the TF-IDF term-weighting scheme, which is a fundamental concept in IR. However, the focus is on justifying the scheme from a statistical perspective, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Understanding Large Language Models' Ability on Interdisciplinary Research",
        "abstract": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.",
        "url": "http://arxiv.org/abs/2507.15736v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15736v1",
        "arxiv_id": "2507.15736v1",
        "authors": [
            "Yuanhao Shen",
            "Daniel Xavier de Sousa",
            "Ricardo Marçal",
            "Ali Asad",
            "Hongyu Guo",
            "Xiaodan Zhu"
        ],
        "submitted": "2025-07-21 15:43:05",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the capabilities of Large Language Models in Interdisciplinary Research, which is a related topic to Information Retrieval and Search technologies. However, the focus on LLMs and their limitations in proposing research ideas is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
        "abstract": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
        "url": "http://arxiv.org/abs/2507.15698v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15698v1",
        "arxiv_id": "2507.15698v1",
        "authors": [
            "Congmin Zheng",
            "Jiachen Zhu",
            "Jianghao Lin",
            "Xinyi Dai",
            "Yong Yu",
            "Weinan Zhang",
            "Mengyue Yang"
        ],
        "submitted": "2025-07-21 15:07:59",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Process Reward Models and length debiasing in large language models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the topic is more specific to language models and reward prediction, and does not align with the user's primary research interests."
    },
    {
        "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming",
        "abstract": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com",
        "url": "http://arxiv.org/abs/2507.15378v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15378v1",
        "arxiv_id": "2507.15378v1",
        "authors": [
            "Jierui Li",
            "Raymond Mooney"
        ],
        "submitted": "2025-07-21 08:34:20",
        "source": "arxiv",
        "comment": "19 pages, pre-print only",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on competitive programming and algorithmic similarity detection, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions LLMs and retrieval methods, the context is different and the techniques are not applicable to the user's areas of interest."
    },
    {
        "title": "A2TTS: TTS for Low Resource Indian Languages",
        "abstract": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil.",
        "url": "http://arxiv.org/abs/2507.15272v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15272v1",
        "arxiv_id": "2507.15272v1",
        "authors": [
            "Ayush Singh Bhadoriya",
            "Abhishek Nikunj Shinde",
            "Isha Pandey",
            "Ganesh Ramakrishnan"
        ],
        "submitted": "2025-07-21 06:20:27",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on Text-to-Speech (TTS) systems for low-resource Indian languages, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper does not address query understanding, ranking models, or user behavior modeling, which are your primary areas of interest."
    },
    {
        "title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest",
        "abstract": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance.",
        "url": "http://arxiv.org/abs/2507.15236v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15236v1",
        "arxiv_id": "2507.15236v1",
        "authors": [
            "Shayan Vassef",
            "Amirhossein Dabiriaghdam",
            "Mohammadreza Bakhtiari",
            "Yadollah Yaghoobzadeh"
        ],
        "submitted": "2025-07-21 04:43:21",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the training dynamics of language models, introducing a novel categorization framework called Subsets of Interest (SOI). While it explores multi-task, multi-lingual, and multi-source learning approaches, the paper's primary focus is on language models, which is not directly related to information retrieval, search technologies, or query understanding. The paper's relevance to the user's research interests is limited."
    },
    {
        "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?",
        "abstract": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences.",
        "url": "http://arxiv.org/abs/2507.15100v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15100v1",
        "arxiv_id": "2507.15100v1",
        "authors": [
            "Chathuri Jayaweera",
            "Brianna Yanqui",
            "Bonnie Dorr"
        ],
        "submitted": "2025-07-20 19:42:45",
        "source": "arxiv",
        "comment": "9 pages, 8 figures and 5 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of Large Language Models as commonsense knowledge generators for Natural Language Inference, which is a topic in NLP. While it touches on the theme of query understanding and ranking models, it is not directly related to information retrieval or search technologies, which are the user's primary research interests."
    }
]