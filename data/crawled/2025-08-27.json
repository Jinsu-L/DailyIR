[
    {
        "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
        "abstract": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.",
        "url": "http://arxiv.org/abs/2508.19229v2",
        "pdf_url": "http://arxiv.org/pdf/2508.19229v2",
        "arxiv_id": "2508.19229v2",
        "authors": [
            "Wei Xiong",
            "Wenting Zhao",
            "Weizhe Yuan",
            "Olga Golovneva",
            "Tong Zhang",
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "submitted": "2025-08-26 17:45:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Generative Interfaces for Language Models",
        "abstract": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.",
        "url": "http://arxiv.org/abs/2508.19227v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19227v1",
        "arxiv_id": "2508.19227v1",
        "authors": [
            "Jiaqi Chen",
            "Yanzhe Zhang",
            "Yutong Zhang",
            "Yijia Shao",
            "Diyi Yang"
        ],
        "submitted": "2025-08-26 17:43:20",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?",
        "abstract": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.",
        "url": "http://arxiv.org/abs/2508.19221v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19221v1",
        "arxiv_id": "2508.19221v1",
        "authors": [
            "Isabel Cachola",
            "Daniel Khashabi",
            "Mark Dredze"
        ],
        "submitted": "2025-08-26 17:38:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VibeVoice Technical Report",
        "abstract": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.",
        "url": "http://arxiv.org/abs/2508.19205v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19205v1",
        "arxiv_id": "2508.19205v1",
        "authors": [
            "Zhiliang Peng",
            "Jianwei Yu",
            "Wenhui Wang",
            "Yaoyao Chang",
            "Yutao Sun",
            "Li Dong",
            "Yi Zhu",
            "Weijiang Xu",
            "Hangbo Bao",
            "Zehua Wang",
            "Shaohan Huang",
            "Yan Xia",
            "Furu Wei"
        ],
        "submitted": "2025-08-26 17:09:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning",
        "abstract": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.",
        "url": "http://arxiv.org/abs/2508.19202v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19202v1",
        "arxiv_id": "2508.19202v1",
        "authors": [
            "Alan Li",
            "Yixin Liu",
            "Arpan Sarkar",
            "Doug Downey",
            "Arman Cohan"
        ],
        "submitted": "2025-08-26 17:04:23",
        "source": "arxiv",
        "comment": "28 pages, 16 figures"
    },
    {
        "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
        "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
        "url": "http://arxiv.org/abs/2508.19200v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19200v1",
        "arxiv_id": "2508.19200v1",
        "authors": [
            "Xinran Zhao",
            "Boyuan Zheng",
            "Chenglei Si",
            "Haofei Yu",
            "Ken Liu",
            "Runlong Zhou",
            "Ruochen Li",
            "Tong Chen",
            "Xiang Li",
            "Yiming Zhang",
            "Tongshuang Wu"
        ],
        "submitted": "2025-08-26 17:03:43",
        "source": "arxiv",
        "comment": "21 pages, 3 figures"
    },
    {
        "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs",
        "abstract": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.",
        "url": "http://arxiv.org/abs/2508.19111v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19111v1",
        "arxiv_id": "2508.19111v1",
        "authors": [
            "Zhikai Ding",
            "Shiyu Ni",
            "Keping Bi"
        ],
        "submitted": "2025-08-26 15:14:19",
        "source": "arxiv",
        "comment": "EMNLP2025 Findings"
    },
    {
        "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic",
        "abstract": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.",
        "url": "http://arxiv.org/abs/2508.19099v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19099v1",
        "arxiv_id": "2508.19099v1",
        "authors": [
            "Thomas Compton"
        ],
        "submitted": "2025-08-26 15:00:04",
        "source": "arxiv",
        "comment": "5 pages conference paper, 4 tables"
    },
    {
        "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index",
        "abstract": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.",
        "url": "http://arxiv.org/abs/2508.19093v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19093v1",
        "arxiv_id": "2508.19093v1",
        "authors": [
            "Mathew Henrickson"
        ],
        "submitted": "2025-08-26 14:58:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs",
        "abstract": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.",
        "url": "http://arxiv.org/abs/2508.19089v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19089v1",
        "arxiv_id": "2508.19089v1",
        "authors": [
            "Yue Li",
            "Zhixue Zhao",
            "Carolina Scarton"
        ],
        "submitted": "2025-08-26 14:51:10",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025"
    },
    {
        "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues",
        "abstract": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification.",
        "url": "http://arxiv.org/abs/2508.19077v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19077v1",
        "arxiv_id": "2508.19077v1",
        "authors": [
            "Tom Röhr",
            "Soumyadeep Roy",
            "Fares Al Mohamad",
            "Jens-Michalis Papaioannou",
            "Wolfgang Nejdl",
            "Felix Gers",
            "Alexander Löser"
        ],
        "submitted": "2025-08-26 14:38:17",
        "source": "arxiv",
        "comment": "Accepted at ECAI 2025"
    },
    {
        "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance",
        "abstract": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.",
        "url": "http://arxiv.org/abs/2508.19076v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19076v1",
        "arxiv_id": "2508.19076v1",
        "authors": [
            "Ziyue Li",
            "Yuan Chang",
            "Gaihong Yu",
            "Xiaoqiu Le"
        ],
        "submitted": "2025-08-26 14:37:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "An Investigation on Group Query Hallucination Attacks",
        "abstract": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models.",
        "url": "http://arxiv.org/abs/2508.19321v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19321v1",
        "arxiv_id": "2508.19321v1",
        "authors": [
            "Kehao Miao",
            "Xiaolong Jin"
        ],
        "submitted": "2025-08-26 14:30:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MovieCORE: COgnitive REasoning in Movies",
        "abstract": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
        "url": "http://arxiv.org/abs/2508.19026v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19026v1",
        "arxiv_id": "2508.19026v1",
        "authors": [
            "Gueter Josmy Faure",
            "Min-Hung Chen",
            "Jia-Fong Yeh",
            "Ying Cheng",
            "Hung-Ting Su",
            "Yung-Hao Tang",
            "Shang-Hong Lai",
            "Winston H. Hsu"
        ],
        "submitted": "2025-08-26 13:43:45",
        "source": "arxiv",
        "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html"
    },
    {
        "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark",
        "abstract": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.",
        "url": "http://arxiv.org/abs/2508.19005v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19005v1",
        "arxiv_id": "2508.19005v1",
        "authors": [
            "Yuxuan Cai",
            "Yipeng Hao",
            "Jie Zhou",
            "Hang Yan",
            "Zhikai Lei",
            "Rui Zhen",
            "Zhenhua Han",
            "Yutao Yang",
            "Junsong Li",
            "Qianjun Pan",
            "Tianyu Huai",
            "Qin Chen",
            "Xin Li",
            "Kai Chen",
            "Bo Zhang",
            "Xipeng Qiu",
            "Liang He"
        ],
        "submitted": "2025-08-26 13:04:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Automatic Prompt Optimization with Prompt Distillation",
        "abstract": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.",
        "url": "http://arxiv.org/abs/2508.18992v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18992v1",
        "arxiv_id": "2508.18992v1",
        "authors": [
            "Viktor N. Zhuravlev",
            "Artur R. Khairullin",
            "Ernest A. Dyagin",
            "Alena N. Sitkina",
            "Nikita I. Kulin"
        ],
        "submitted": "2025-08-26 12:46:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models",
        "abstract": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.",
        "url": "http://arxiv.org/abs/2508.18988v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18988v1",
        "arxiv_id": "2508.18988v1",
        "authors": [
            "Hung Ming Liu"
        ],
        "submitted": "2025-08-26 12:40:21",
        "source": "arxiv",
        "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html"
    },
    {
        "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization",
        "abstract": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.",
        "url": "http://arxiv.org/abs/2508.18976v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18976v1",
        "arxiv_id": "2508.18976v1",
        "authors": [
            "Stephen Meisenbacher",
            "Alexandra Klymenko",
            "Andreea-Elena Bodea",
            "Florian Matthes"
        ],
        "submitted": "2025-08-26 12:22:45",
        "source": "arxiv",
        "comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025"
    },
    {
        "title": "Sycophancy as compositions of Atomic Psychometric Traits",
        "abstract": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.",
        "url": "http://arxiv.org/abs/2508.19316v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19316v1",
        "arxiv_id": "2508.19316v1",
        "authors": [
            "Shreyans Jain",
            "Alexandra Yost",
            "Amirali Abdullah"
        ],
        "submitted": "2025-08-26 11:21:27",
        "source": "arxiv",
        "comment": "8 pages, 4 figures"
    },
    {
        "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework",
        "abstract": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.",
        "url": "http://arxiv.org/abs/2508.18929v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18929v1",
        "arxiv_id": "2508.18929v1",
        "authors": [
            "Ilias Driouich",
            "Hongliu Cao",
            "Eoin Thomas"
        ],
        "submitted": "2025-08-26 11:16:14",
        "source": "arxiv",
        "comment": "ECAI 2025 TRUST AI workshop"
    },
    {
        "title": "Affective Polarization across European Parliaments",
        "abstract": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.",
        "url": "http://arxiv.org/abs/2508.18916v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18916v1",
        "arxiv_id": "2508.18916v1",
        "authors": [
            "Bojan Evkoski",
            "Igor Mozetič",
            "Nikola Ljubešić",
            "Petra Kralj Novak"
        ],
        "submitted": "2025-08-26 10:39:56",
        "source": "arxiv",
        "comment": "6 pages, 4 figures"
    },
    {
        "title": "Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search",
        "abstract": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval.",
        "url": "http://arxiv.org/abs/2508.18877v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18877v1",
        "arxiv_id": "2508.18877v1",
        "authors": [
            "Kushagra Agrawal",
            "Nisharg Nargund",
            "Oishani Banerjee"
        ],
        "submitted": "2025-08-26 09:51:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis",
        "abstract": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.",
        "url": "http://arxiv.org/abs/2508.18872v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18872v1",
        "arxiv_id": "2508.18872v1",
        "authors": [
            "Laurie Gale",
            "Sebastian Mateos Nicolajsen"
        ],
        "submitted": "2025-08-26 09:46:59",
        "source": "arxiv",
        "comment": "7 pages, 2 figures"
    },
    {
        "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms",
        "abstract": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.",
        "url": "http://arxiv.org/abs/2508.18870v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18870v1",
        "arxiv_id": "2508.18870v1",
        "authors": [
            "Viktor N. Zhuravlev",
            "Artur R. Khairullin",
            "Ernest A. Dyagin",
            "Alena N. Sitkina",
            "Nikita I. Kulin"
        ],
        "submitted": "2025-08-26 09:46:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.",
        "url": "http://arxiv.org/abs/2508.18847v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18847v1",
        "arxiv_id": "2508.18847v1",
        "authors": [
            "Yibo Li",
            "Miao Xiong",
            "Jiaying Wu",
            "Bryan Hooi"
        ],
        "submitted": "2025-08-26 09:25:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness",
        "abstract": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.",
        "url": "http://arxiv.org/abs/2508.18824v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18824v1",
        "arxiv_id": "2508.18824v1",
        "authors": [
            "Sirui Chen",
            "Changxin Tian",
            "Binbin Hu",
            "Kunlong Chen",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "submitted": "2025-08-26 09:01:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection",
        "abstract": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.",
        "url": "http://arxiv.org/abs/2508.18819v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18819v1",
        "arxiv_id": "2508.18819v1",
        "authors": [
            "Shubham Gupta",
            "Shraban Kumar Chatterjee",
            "Suman Kundu"
        ],
        "submitted": "2025-08-26 08:58:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination",
        "abstract": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.",
        "url": "http://arxiv.org/abs/2508.18791v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18791v1",
        "arxiv_id": "2508.18791v1",
        "authors": [
            "Ziming Zhu",
            "Chenglong Wang",
            "Shunjie Xing",
            "Yifu Huo",
            "Fengning Tian",
            "Quan Du",
            "Di Yang",
            "Chunliang Zhang",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "submitted": "2025-08-26 08:17:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Controllable Conversational Theme Detection Track at DSTC 12",
        "abstract": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.",
        "url": "http://arxiv.org/abs/2508.18783v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18783v1",
        "arxiv_id": "2508.18783v1",
        "authors": [
            "Igor Shalyminov",
            "Hang Su",
            "Jake Vincent",
            "Siffi Singh",
            "Jason Cai",
            "James Gung",
            "Raphael Shu",
            "Saab Mansour"
        ],
        "submitted": "2025-08-26 08:10:01",
        "source": "arxiv",
        "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection"
    },
    {
        "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction",
        "abstract": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.",
        "url": "http://arxiv.org/abs/2508.18780v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18780v1",
        "arxiv_id": "2508.18780v1",
        "authors": [
            "Yilin Li",
            "Xunjian Yin",
            "Yilin Chen",
            "Xiaojun Wan"
        ],
        "submitted": "2025-08-26 08:04:04",
        "source": "arxiv",
        "comment": "Code will be released upon publication"
    },
    {
        "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models",
        "abstract": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.",
        "url": "http://arxiv.org/abs/2508.18773v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18773v1",
        "arxiv_id": "2508.18773v1",
        "authors": [
            "Qianyu He",
            "Siyu Yuan",
            "Xuefeng Li",
            "Mingxuan Wang",
            "Jiangjie Chen"
        ],
        "submitted": "2025-08-26 07:57:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs",
        "abstract": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels.",
        "url": "http://arxiv.org/abs/2508.18772v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18772v1",
        "arxiv_id": "2508.18772v1",
        "authors": [
            "Wanqiang Wang",
            "Longzhu He",
            "Wei Zheng"
        ],
        "submitted": "2025-08-26 07:55:46",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models",
        "abstract": "Large reasoning models (LRMs) have shown remarkable progress on complex\nreasoning tasks. However, some questions posed to LRMs are inherently\nunanswerable, such as math problems lacking sufficient conditions. We find that\nLRMs continually fail to provide appropriate abstentions when confronted with\nthese unanswerable questions. In this paper, we systematically analyze,\ninvestigate, and resolve this issue for trustworthy AI. We first conduct a\ndetailed analysis of the distinct response behaviors of LRMs when facing\nunanswerable questions. Then, we show that LRMs possess sufficient cognitive\ncapabilities to recognize the flaws in these questions. However, they fail to\nexhibit appropriate abstention behavior, revealing a misalignment between their\ninternal cognition and external response. Finally, to resolve this issue, we\npropose a lightweight, two-stage method that combines cognitive monitoring with\ninference-time intervention. Experimental results demonstrate that our method\nsignificantly improves the abstention rate while maintaining the overall\nreasoning performance.",
        "url": "http://arxiv.org/abs/2508.18760v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18760v1",
        "arxiv_id": "2508.18760v1",
        "authors": [
            "Yi Liu",
            "Xiangyu Liu",
            "Zequn Sun",
            "Wei Hu"
        ],
        "submitted": "2025-08-26 07:37:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Text to Query Plans for Question Answering on Large Tables",
        "abstract": "Efficient querying and analysis of large tabular datasets remain significant\nchallenges, especially for users without expertise in programming languages\nlike SQL. Text-to-SQL approaches have shown promising performance on benchmark\ndata; however, they inherit SQL's drawbacks, including inefficiency with large\ndatasets and limited support for complex data analyses beyond basic querying.\nWe propose a novel framework that transforms natural language queries into\nquery plans. Our solution is implemented outside traditional databases,\nallowing us to support classical SQL commands while avoiding SQL's inherent\nlimitations. Additionally, we enable complex analytical functions, such as\nprincipal component analysis and anomaly detection, providing greater\nflexibility and extensibility than traditional SQL capabilities. We leverage\nLLMs to iteratively interpret queries and construct operation sequences,\naddressing computational complexity by incrementally building solutions. By\nexecuting operations directly on the data, we overcome context length\nlimitations without requiring the entire dataset to be processed by the model.\nWe validate our framework through experiments on both standard databases and\nlarge scientific tables, demonstrating its effectiveness in handling extensive\ndatasets and performing sophisticated data analyses.",
        "url": "http://arxiv.org/abs/2508.18758v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18758v1",
        "arxiv_id": "2508.18758v1",
        "authors": [
            "Yipeng Zhang",
            "Chen Wang",
            "Yuzhe Zhang",
            "Jacky Jiang"
        ],
        "submitted": "2025-08-26 07:35:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering",
        "abstract": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.",
        "url": "http://arxiv.org/abs/2508.18748v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18748v1",
        "arxiv_id": "2508.18748v1",
        "authors": [
            "Byeongjeong Kim",
            "Jeonghyun Park",
            "Joonho Yang",
            "Hwanhee Lee"
        ],
        "submitted": "2025-08-26 07:23:23",
        "source": "arxiv",
        "comment": "7 pages, 3 figures"
    },
    {
        "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks",
        "abstract": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves\napproximately 85% on GSM8K and approximately 40% on GPQA (System-2) while\nretaining approximately 90% on S1-Bench (System-1). Its reasoning traces\naverage approximately 300 tokens(ART), about one-third the length of baseline\ntraces, delivering higher efficiency without loss of accuracy.",
        "url": "http://arxiv.org/abs/2508.18743v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18743v1",
        "arxiv_id": "2508.18743v1",
        "authors": [
            "Sunguk Choi",
            "Yonghoon Kwon",
            "Heondeuk Lee"
        ],
        "submitted": "2025-08-26 07:17:21",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 findings"
    },
    {
        "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations",
        "abstract": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.",
        "url": "http://arxiv.org/abs/2508.18740v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18740v1",
        "arxiv_id": "2508.18740v1",
        "authors": [
            "Qiao Liang",
            "Ying Shen",
            "Tiantian Chen",
            "Lin Zhang"
        ],
        "submitted": "2025-08-26 07:14:27",
        "source": "arxiv",
        "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025"
    },
    {
        "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models",
        "abstract": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.",
        "url": "http://arxiv.org/abs/2508.18739v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18739v1",
        "arxiv_id": "2508.18739v1",
        "authors": [
            "Chang Wang",
            "Siyu Yan",
            "Depeng Yuan",
            "Yuqi Chen",
            "Yanhua Huang",
            "Yuanhang Zheng",
            "Shuhao Li",
            "Yinqi Zhang",
            "Kedi Chen",
            "Mingrui Zhu",
            "Ruiwen Xu"
        ],
        "submitted": "2025-08-26 07:11:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval",
        "abstract": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy.",
        "url": "http://arxiv.org/abs/2508.18724v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18724v1",
        "arxiv_id": "2508.18724v1",
        "authors": [
            "Karanbir Singh",
            "Deepak Muppiri",
            "William Ngu"
        ],
        "submitted": "2025-08-26 06:44:04",
        "source": "arxiv",
        "comment": "Accepted at KDD'2025 Agent4IR workshop"
    },
    {
        "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues",
        "abstract": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.",
        "url": "http://arxiv.org/abs/2508.18715v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18715v1",
        "arxiv_id": "2508.18715v1",
        "authors": [
            "Angela Yifei Yuan",
            "Haoyi Li",
            "Soyeon Caren Han",
            "Christopher Leckie"
        ],
        "submitted": "2025-08-26 06:27:10",
        "source": "arxiv",
        "comment": "15 pages"
    },
    {
        "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs",
        "abstract": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.",
        "url": "http://arxiv.org/abs/2508.18709v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18709v1",
        "arxiv_id": "2508.18709v1",
        "authors": [
            "Duy Le",
            "Kent Ziti",
            "Evan Girard-Sun",
            "Sean O'Brien",
            "Vasu Sharma",
            "Kevin Zhu"
        ],
        "submitted": "2025-08-26 06:21:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System",
        "abstract": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.",
        "url": "http://arxiv.org/abs/2508.18701v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18701v1",
        "arxiv_id": "2508.18701v1",
        "authors": [
            "Yanfan Du",
            "Jun Zhang",
            "Bin Wang",
            "Jin Qiu",
            "Lu Huang",
            "Yuan Ge",
            "Xiaoqian Liu",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "submitted": "2025-08-26 06:08:17",
        "source": "arxiv",
        "comment": "9 pages, 4 figures, 5 tables"
    },
    {
        "title": "Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training",
        "abstract": "ID-based embeddings are widely used in web-scale online recommendation\nsystems. However, their susceptibility to overfitting, particularly due to the\nlong-tail nature of data distributions, often limits training to a single\nepoch, a phenomenon known as the \"one-epoch problem.\" This challenge has driven\nresearch efforts to optimize performance within the first epoch by enhancing\nconvergence speed or feature sparsity. In this study, we introduce a novel\ntwo-stage training strategy that incorporates a pre-training phase using a\nminimal model with contrastive loss, enabling broader data coverage for the\nembedding system. Our offline experiments demonstrate that multi-epoch training\nduring the pre-training phase does not lead to overfitting, and the resulting\nembeddings improve online generalization when fine-tuned for more complex\ndownstream recommendation tasks. We deployed the proposed system in live\ntraffic at Pinterest, achieving significant site-wide engagement gains.",
        "url": "http://arxiv.org/abs/2508.18700v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18700v1",
        "arxiv_id": "2508.18700v1",
        "authors": [
            "Yi-Ping Hsu",
            "Po-Wei Wang",
            "Chantat Eksombatchai",
            "Jiajing Xu"
        ],
        "submitted": "2025-08-26 06:06:21",
        "source": "arxiv",
        "comment": "Published at RecSys'24, see\n  https://dl.acm.org/doi/10.1145/3640457.3688053"
    },
    {
        "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning",
        "abstract": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.",
        "url": "http://arxiv.org/abs/2508.18687v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18687v1",
        "arxiv_id": "2508.18687v1",
        "authors": [
            "Songtao Jiang",
            "Yuxi Chen",
            "Sibo Song",
            "Yan Zhang",
            "Yeying Jin",
            "Yang Feng",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "submitted": "2025-08-26 05:21:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation",
        "abstract": "Signature-based Intrusion Detection Systems (IDS) detect malicious activities\nby matching network or host activity against predefined rules. These rules are\nderived from extensive Cyber Threat Intelligence (CTI), which includes attack\nsignatures and behavioral patterns obtained through automated tools and manual\nthreat analysis, such as sandboxing. The CTI is then transformed into\nactionable rules for the IDS engine, enabling real-time detection and\nprevention. However, the constant evolution of cyber threats necessitates\nfrequent rule updates, which delay deployment time and weaken overall security\nreadiness. Recent advancements in agentic systems powered by Large Language\nModels (LLMs) offer the potential for autonomous IDS rule generation with\ninternal evaluation. We introduce FALCON, an autonomous agentic framework that\ngenerates deployable IDS rules from CTI data in real-time and evaluates them\nusing built-in multi-phased validators. To demonstrate versatility, we target\nboth network (Snort) and host-based (YARA) mediums and construct a\ncomprehensive dataset of IDS rules with their corresponding CTIs. Our\nevaluations indicate FALCON excels in automatic rule generation, with an\naverage of 95% accuracy validated by qualitative evaluation with 84%\ninter-rater agreement among multiple cybersecurity analysts across all metrics.\nThese results underscore the feasibility and effectiveness of LLM-driven data\nmining for real-time cyber threat mitigation.",
        "url": "http://arxiv.org/abs/2508.18684v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18684v1",
        "arxiv_id": "2508.18684v1",
        "authors": [
            "Shaswata Mitra",
            "Azim Bazarov",
            "Martin Duclos",
            "Sudip Mittal",
            "Aritran Piplai",
            "Md Rayhanur Rahman",
            "Edward Zieglar",
            "Shahram Rahimi"
        ],
        "submitted": "2025-08-26 05:08:53",
        "source": "arxiv",
        "comment": "11 pages, 5 figures, 4 tables"
    },
    {
        "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum",
        "abstract": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.",
        "url": "http://arxiv.org/abs/2508.18673v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18673v1",
        "arxiv_id": "2508.18673v1",
        "authors": [
            "Xinglong Yang",
            "Quan Feng",
            "Zhongying Pan",
            "Xiang Chen",
            "Yu Tian",
            "Wentong Li",
            "Shuofei Qiao",
            "Yuxia Geng",
            "Xingyu Zhao",
            "Sheng-Jun Huang"
        ],
        "submitted": "2025-08-26 04:32:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks",
        "abstract": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization and reasoning. We train families of\nMoE Transformers that systematically vary total parameters, active parameters,\nand top-$k$ routing while holding the compute budget fixed. For every model we\nrecord pre-training loss, downstream task loss, and task accuracy, allowing us\nto separate the train-test generalization gap from the loss-accuracy gap.\nMemorization benchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast, reasoning performance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-$k$ alone has little effect when active parameters are constant,\nand classic hyperparameters such as learning rate and initialization modulate\nthe generalization gap in the same direction as sparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues the reasoning\ndeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity.",
        "url": "http://arxiv.org/abs/2508.18672v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18672v1",
        "arxiv_id": "2508.18672v1",
        "authors": [
            "Taishi Nakamura",
            "Satoki Ishikawa",
            "Masaki Kawamura",
            "Takumi Okamoto",
            "Daisuke Nohara",
            "Jun Suzuki",
            "Rio Yokota"
        ],
        "submitted": "2025-08-26 04:31:28",
        "source": "arxiv",
        "comment": "Presented at the Second AI for Math Workshop at ICML"
    },
    {
        "title": "Membership Inference Attacks on LLM-based Recommender Systems",
        "abstract": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots.",
        "url": "http://arxiv.org/abs/2508.18665v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18665v1",
        "arxiv_id": "2508.18665v1",
        "authors": [
            "Jiajie He",
            "Yuechun Gu",
            "Min-Chun Chen",
            "Keke Chen"
        ],
        "submitted": "2025-08-26 04:14:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Extracting Information from Scientific Literature via Visual Table Question Answering Models",
        "abstract": "This study explores three approaches to processing table data in scientific\npapers to enhance extractive question answering and develop a software tool for\nthe systematic review process. The methods evaluated include: (1) Optical\nCharacter Recognition (OCR) for extracting information from documents, (2)\nPre-trained models for document visual question answering, and (3) Table\ndetection and structure recognition to extract and merge key information from\ntables with textual content to answer extractive questions. In exploratory\nexperiments, we augmented ten sample test documents containing tables and\nrelevant content against RF- EMF-related scientific papers with seven\npredefined extractive question-answer pairs. The results indicate that\napproaches preserving table structure outperform the others, particularly in\nrepresenting and organizing table content. Accurately recognizing specific\nnotations and symbols within the documents emerged as a critical factor for\nimproved results. Our study concludes that preserving the structural integrity\nof tables is essential for enhancing the accuracy and reliability of extractive\nquestion answering in scientific documents.",
        "url": "http://arxiv.org/abs/2508.18661v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18661v1",
        "arxiv_id": "2508.18661v1",
        "authors": [
            "Dongyoun Kim",
            "Hyung-do Choi",
            "Youngsun Jang",
            "John Kim"
        ],
        "submitted": "2025-08-26 04:08:16",
        "source": "arxiv",
        "comment": "Accepted at ACM International Conference on Research in Adaptive and\n  Convergent Systems, November 5-8, 2024, Pompei, Italy"
    },
    {
        "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models",
        "abstract": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/",
        "url": "http://arxiv.org/abs/2508.18655v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18655v1",
        "arxiv_id": "2508.18655v1",
        "authors": [
            "Haoyu Wang",
            "Guangyan Zhang",
            "Jiale Chen",
            "Jingyu Li",
            "Yuehai Wang",
            "Yiwen Guo"
        ],
        "submitted": "2025-08-26 03:54:39",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, submitted to ICASSP 2026"
    },
    {
        "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems.",
        "url": "http://arxiv.org/abs/2508.18652v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18652v1",
        "arxiv_id": "2508.18652v1",
        "authors": [
            "Runpeng Geng",
            "Yanting Wang",
            "Ying Chen",
            "Jinyuan Jia"
        ],
        "submitted": "2025-08-26 03:50:52",
        "source": "arxiv",
        "comment": "21 pages, 4 figures"
    },
    {
        "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models",
        "abstract": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.",
        "url": "http://arxiv.org/abs/2508.18651v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18651v1",
        "arxiv_id": "2508.18651v1",
        "authors": [
            "Chenxu Yang",
            "Qingyi Si",
            "Zheng Lin"
        ],
        "submitted": "2025-08-26 03:48:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach",
        "abstract": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS",
        "url": "http://arxiv.org/abs/2508.18648v2",
        "pdf_url": "http://arxiv.org/pdf/2508.18648v2",
        "arxiv_id": "2508.18648v2",
        "authors": [
            "Cong Liu",
            "Wenchang Chai",
            "Hejun Wu",
            "Yan Pan",
            "Pengxu Wei",
            "Liang Lin"
        ],
        "submitted": "2025-08-26 03:43:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap",
        "abstract": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval.",
        "url": "http://arxiv.org/abs/2508.18646v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18646v1",
        "arxiv_id": "2508.18646v1",
        "authors": [
            "Jun Wang",
            "Ninglun Gu",
            "Kailai Zhang",
            "Zijiao Zhang",
            "Yelun Bao",
            "Jin Yang",
            "Xu Yin",
            "Liwei Liu",
            "Yihuan Liu",
            "Pengyong Li",
            "Gary G. Yen",
            "Junchi Yan"
        ],
        "submitted": "2025-08-26 03:43:05",
        "source": "arxiv",
        "comment": "Preprint. Under review"
    },
    {
        "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing",
        "abstract": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization.",
        "url": "http://arxiv.org/abs/2508.18642v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18642v1",
        "arxiv_id": "2508.18642v1",
        "authors": [
            "Jianxing Liao",
            "Tian Zhang",
            "Xiao Feng",
            "Yusong Zhang",
            "Rui Yang",
            "Haorui Wang",
            "Bosi Wen",
            "Ziying Wang",
            "Runzhi Shi"
        ],
        "submitted": "2025-08-26 03:40:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models",
        "abstract": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.",
        "url": "http://arxiv.org/abs/2508.18609v2",
        "pdf_url": "http://arxiv.org/pdf/2508.18609v2",
        "arxiv_id": "2508.18609v2",
        "authors": [
            "Chenxi Zhou",
            "Pengfei Cao",
            "Jiang Li",
            "Jun Zhao",
            "Kang Liu"
        ],
        "submitted": "2025-08-26 02:24:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A New NMT Model for Translating Clinical Texts from English to Spanish",
        "abstract": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.",
        "url": "http://arxiv.org/abs/2508.18607v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18607v1",
        "arxiv_id": "2508.18607v1",
        "authors": [
            "Rumeng Li",
            "Xun Wang",
            "Hong Yu"
        ],
        "submitted": "2025-08-26 02:24:38",
        "source": "arxiv",
        "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018"
    },
    {
        "title": "What do language models model? Transformers, automata, and the format of thought",
        "abstract": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.",
        "url": "http://arxiv.org/abs/2508.18598v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18598v1",
        "arxiv_id": "2508.18598v1",
        "authors": [
            "Colin Klein"
        ],
        "submitted": "2025-08-26 02:01:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation",
        "abstract": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.",
        "url": "http://arxiv.org/abs/2508.18569v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18569v1",
        "arxiv_id": "2508.18569v1",
        "authors": [
            "Girish A. Koushik",
            "Fatemeh Nazarieh",
            "Katherine Birch",
            "Shenbin Qian",
            "Diptesh Kanojia"
        ],
        "submitted": "2025-08-26 00:04:01",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates",
        "abstract": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.",
        "url": "http://arxiv.org/abs/2508.18549v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18549v1",
        "arxiv_id": "2508.18549v1",
        "authors": [
            "Maike Züfle",
            "Vilém Zouhar",
            "Tu Anh Dinh",
            "Felipe Maia Polo",
            "Jan Niehues",
            "Mrinmaya Sachan"
        ],
        "submitted": "2025-08-25 22:55:22",
        "source": "arxiv",
        "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally"
    },
    {
        "title": "Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project",
        "abstract": "This article presents a practitioner's reflection on applying declarative,\n5th generation, problem formulation language (5GL) to de novo imaging system\ndesign, informed by experiences across the interdisciplinary research in\nacademia and cross-functional product development within the private sector.\nUsing the 96-Eyes project: 96-camera parallel multi-modal imager for\nhigh-throughput drug discovery as a representative case, I illustrate how\nproject requirements, ranging from hardware constraints to life sciences needs,\ncan be formalized into machine-readable problem statements to preserve\nmission-critical input from diverse domain stakeholders. This declarative\napproach enhances transparency, ensures design traceability, and minimizes\ncostly misalignment across optical, algorithmic, hardware-accelerated compute,\nand life sciences teams.\n  Alongside the technical discussion of 5GL with real-world code examples, I\nreflect on the practical barriers to adopting 5GL in environments where\nimperative, 3rd-generation languages (3GL) remain the default medium for\ninter-team collaboration. Rather than offering an one-size-fits-all solution,\nthese learned lessons highlight how programming paradigms implicitly shapes\nresearch workflows through existing domain hierarchies. The discussion aims to\ninvite further explorations into how declarative problem formulations can\nfacilitate innovation in settings where concurrent R\\&{}D workflows are gaining\ntraction, as opposed to environments where sequential, phase-driven workflows\nremain the norm.",
        "url": "http://arxiv.org/abs/2508.18512v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18512v1",
        "arxiv_id": "2508.18512v1",
        "authors": [
            "Antony C Chan"
        ],
        "submitted": "2025-08-25 21:32:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing",
        "abstract": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.18473v2",
        "pdf_url": "http://arxiv.org/pdf/2508.18473v2",
        "arxiv_id": "2508.18473v2",
        "authors": [
            "Jiawei Li",
            "Akshayaa Magesh",
            "Venugopal V. Veeravalli"
        ],
        "submitted": "2025-08-25 20:39:30",
        "source": "arxiv",
        "comment": "16 pages"
    },
    {
        "title": "Integrating gender inclusivity into large language models via instruction tuning",
        "abstract": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.",
        "url": "http://arxiv.org/abs/2508.18466v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18466v1",
        "arxiv_id": "2508.18466v1",
        "authors": [
            "Alina Wróblewska",
            "Bartosz Żuk"
        ],
        "submitted": "2025-08-25 20:34:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?",
        "abstract": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.",
        "url": "http://arxiv.org/abs/2508.18444v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18444v1",
        "arxiv_id": "2508.18444v1",
        "authors": [
            "Nafis Tanveer Islam",
            "Zhiming Zhao"
        ],
        "submitted": "2025-08-25 19:48:39",
        "source": "arxiv",
        "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper"
    },
    {
        "title": "DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation",
        "abstract": "Transformer-based sequential recommenders, such as SASRec or BERT4Rec,\ntypically rely solely on learned item ID embeddings, making them vulnerable to\nthe item cold-start problem, particularly in environments with dynamic item\ncatalogs. While dense content embeddings from pre-trained models offer\npotential solutions, direct integration into transformer-based recommenders has\nconsistently underperformed compared to ID-only approaches. We revisit this\nintegration challenge and propose DenseRec, a simple yet effective method that\nintroduces a dual-path embedding approach. DenseRec learns a linear projection\nfrom the dense embedding space into the ID embedding space during training,\nenabling seamless generalization to previously unseen items without requiring\nspecialized embedding models or complex infrastructure. In experiments on three\nreal-world datasets, we find DenseRec to consistently outperform an ID-only\nSASRec baseline, even without additional hyperparameter tuning and while using\ncompact embedding models. Our analysis suggests improvements primarily arise\nfrom better sequence representations in the presence of unseen items,\npositioning DenseRec as a practical and robust solution for cold-start\nsequential recommendation.",
        "url": "http://arxiv.org/abs/2508.18442v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18442v1",
        "arxiv_id": "2508.18442v1",
        "authors": [
            "Jan Malte Lichtenberg",
            "Antonio De Candia",
            "Matteo Ruffini"
        ],
        "submitted": "2025-08-25 19:47:20",
        "source": "arxiv",
        "comment": "EARL workshop @RecSys'25, Prague, Czech Republic"
    },
    {
        "title": "A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs",
        "abstract": "Vulnerability databases, such as the National Vulnerability Database (NVD),\noffer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but\noften lack information on their real-world impact, such as the tactics,\ntechniques, and procedures (TTPs) that adversaries may use to exploit the\nvulnerability. However, manually linking CVEs to their corresponding TTPs is a\nchallenging and time-consuming task, and the high volume of new vulnerabilities\npublished annually makes automated support desirable.\n  This paper introduces TRIAGE, a two-pronged automated approach that uses\nLarge Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK\nknowledge base. We first prompt an LLM with instructions based on MITRE's CVE\nMapping Methodology to predict an initial list of techniques. This list is then\ncombined with the results from a second LLM-based module that uses in-context\nlearning to map a CVE to relevant techniques. This hybrid approach\nstrategically combines rule-based reasoning with data-driven inference. Our\nevaluation reveals that in-context learning outperforms the individual mapping\nmethods, and the hybrid approach improves recall of exploitation techniques. We\nalso find that GPT-4o-mini performs better than Llama3.3-70B on this task.\nOverall, our results show that LLMs can be used to automatically predict the\nimpact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping\nCVEs to ATT&CK more efficient.\n  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language\nmodels, automated mapping.",
        "url": "http://arxiv.org/abs/2508.18439v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18439v1",
        "arxiv_id": "2508.18439v1",
        "authors": [
            "Anders Mølmen Høst",
            "Pierre Lison",
            "Leon Moonen"
        ],
        "submitted": "2025-08-25 19:39:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering",
        "abstract": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.",
        "url": "http://arxiv.org/abs/2508.18407v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18407v1",
        "arxiv_id": "2508.18407v1",
        "authors": [
            "Michal Štefánik",
            "Timothee Mickus",
            "Marek Kadlčík",
            "Michal Spiegel",
            "Josef Kuchař"
        ],
        "submitted": "2025-08-25 18:49:50",
        "source": "arxiv",
        "comment": "To appear in Findings of EMNLP 2025"
    },
    {
        "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning",
        "abstract": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.",
        "url": "http://arxiv.org/abs/2508.18395v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18395v1",
        "arxiv_id": "2508.18395v1",
        "authors": [
            "Jeong-seok Oh",
            "Jay-yoon Lee"
        ],
        "submitted": "2025-08-25 18:36:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little",
        "abstract": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.",
        "url": "http://arxiv.org/abs/2508.18387v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18387v1",
        "arxiv_id": "2508.18387v1",
        "authors": [
            "Ivan Kobyzev",
            "Abbas Ghaddar",
            "Dingtao Hu",
            "Boxing Chen"
        ],
        "submitted": "2025-08-25 18:19:21",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main"
    },
    {
        "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails",
        "abstract": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.",
        "url": "http://arxiv.org/abs/2508.18384v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18384v1",
        "arxiv_id": "2508.18384v1",
        "authors": [
            "Kellen Tan Cheng",
            "Anna Lisa Gentile",
            "Chad DeLuca",
            "Guang-Jie Ren"
        ],
        "submitted": "2025-08-25 18:17:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models",
        "abstract": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.",
        "url": "http://arxiv.org/abs/2508.18381v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18381v1",
        "arxiv_id": "2508.18381v1",
        "authors": [
            "Yuchun Fan",
            "Yilin Wang",
            "Yongyu Mu",
            "Lei Huang",
            "Bei Li",
            "Xiaocheng Feng",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "submitted": "2025-08-25 18:15:25",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025 findings"
    },
    {
        "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking",
        "abstract": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, promoting it as the\nnext-generation re-ranker for modern IR systems.",
        "url": "http://arxiv.org/abs/2508.18379v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18379v1",
        "arxiv_id": "2508.18379v1",
        "authors": [
            "Pinhuan Wang",
            "Zhiqiu Xia",
            "Chunhua Liao",
            "Feiyi Wang",
            "Hang Liu"
        ],
        "submitted": "2025-08-25 18:13:50",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Main Conference). 13 pages, 2 figures"
    },
    {
        "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo",
        "abstract": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.",
        "url": "http://arxiv.org/abs/2508.18370v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18370v1",
        "arxiv_id": "2508.18370v1",
        "authors": [
            "Terry Yue Zhuo",
            "Dingmin Wang",
            "Hantian Ding",
            "Varun Kumar",
            "Zijian Wang"
        ],
        "submitted": "2025-08-25 18:02:23",
        "source": "arxiv",
        "comment": null
    }
]