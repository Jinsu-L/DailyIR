# Daily Papers Report - 2025-08-25

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation

- **LLM Score**: 8
- **Keyword Score**: 18
- **Authors**: Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Adam Jatowt
- **URL**: <http://arxiv.org/abs/2508.16998v1>
- **Submitted**: 2025-08-23 11:46:08
- **Comment**: Accept at EMNLP Findings 2025
- **Topic Keywords**: ranking, rerank, listwise, pointwise, relevance, rank, trec
- **Reason**: The paper proposes a novel approach to document reranking, leveraging large language models for global reasoning and fine-grained relevance scoring. The focus on listwise reasoning and natural-language justifications aligns with your interest in query understanding and ranking models. The application to information retrieval and open-domain QA also falls within your scope.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Document Reranking using DeepAgentRank (DEAR) Framework
- **Aim**: To propose a novel framework for document reranking that combines pointwise and listwise learning with reasoning-augmented supervision
- **Rationale**: The DEAR framework decouples fine-grained relevance scoring and holistic cross-document analysis through a dual-stage approach, enabling listwise reasoning with natural-language justifications
- **Ground**: The framework combines a teacher-student pipeline with logit-level distillation, cross-entropy, RankNet, and KL divergence losses, and is trained on synthetic ranking examples with chain-of-thought reasoning
- **Experiment**: The authors evaluate DEAR on various datasets, including TREC-DL19/20, BEIR datasets, and NovelEval-2306, and compare it with open-source baselines and proprietary LLM APIs
- **Takeaway**: DEAR achieves superior accuracy and interpretability, outperforming other models in terms of inference efficiency, and demonstrates the effectiveness of CoT-guided listwise reranking in enhancing performance while maintaining interpretability

#### Abstract
> Large Language Models (LLMs) have transformed listwise document reranking by
enabling global reasoning over candidate sets, yet single models often struggle
to balance fine-grained relevance scoring with holistic cross-document
analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}),
an open-source framework that decouples these tasks through a dual-stage
approach, achieving superior accuracy and interpretability. In \emph{Stage 1},
we distill token-level relevance signals from a frozen 13B LLaMA teacher into a
compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and
KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we
attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated
chain-of-thought permutations, enabling listwise reasoning with
natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR
datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1
nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by
+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,
achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like
MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures
stable calibration, making \DeAR a highly effective and interpretable solution
for modern reranking systems.\footnote{Dataset and code available at
https://github.com/DataScienceUIBK/DeAR-Reranking.}.

---

### 2. How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models

- **LLM Score**: 8
- **Keyword Score**: 17
- **Authors**: Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, Adam Jatowt
- **URL**: <http://arxiv.org/abs/2508.16757v1>
- **Submitted**: 2025-08-22 19:30:04
- **Comment**: EMNLP Findings 2025
- **Topic Keywords**: information retrieval, queries, ranking, rerank, retrieval, rank, trec
- **Reason**: The paper evaluates state-of-the-art reranking methods, including LLM-based approaches, in information retrieval tasks, which aligns with your interest in query understanding and ranking models. The study's focus on empirical analysis and performance evaluation on established benchmarks and a novel dataset also resonates with your background in e-commerce and interest in real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Reranking methods for information retrieval tasks
- **Aim**: To evaluate and compare the performance of state-of-the-art reranking methods, including LLM-based, lightweight, and zero-shot approaches, on familiar and novel queries
- **Rationale**: To identify the underlying causes of performance disparities between LLM-based rerankers and their lightweight counterparts, particularly on novel queries, and to analyze the effects of training data overlap, model architecture, and computational efficiency on reranking performance
- **Ground**: The study is grounded in the evaluation of 22 reranking methods, including 40 variants, across several established benchmarks and a novel dataset, FutureQueryEval, designed to test queries unseen by pre-trained models
- **Experiment**: The authors conducted experiments to compare the performance of state-of-the-art rerankers on both standard benchmarks and the custom dataset, analyzing key factors affecting reranking performance, including generalization to novel queries, computational efficiency, and model architecture
- **Takeaway**: The study highlights the importance of balancing effectiveness and runtime in practical IR systems, and the need to consider the strengths and weaknesses of different reranking approaches, including LLM-based, lightweight, and zero-shot models, to achieve optimal performance

#### Abstract
> In this work, we present a systematic and comprehensive empirical evaluation
of state-of-the-art reranking methods, encompassing large language model
(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to
their performance in information retrieval tasks. We evaluate in total 22
methods, including 40 variants (depending on used LLM) across several
established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel
dataset designed to test queries unseen by pretrained models. Our primary goal
is to determine, through controlled and fair comparisons, whether a performance
disparity exists between LLM-based rerankers and their lightweight
counterparts, particularly on novel queries, and to elucidate the underlying
causes of any observed differences. To disentangle confounding factors, we
analyze the effects of training data overlap, model architecture, and
computational efficiency on reranking performance. Our findings indicate that
while LLM-based rerankers demonstrate superior performance on familiar queries,
their generalization ability to novel queries varies, with lightweight models
offering comparable efficiency. We further identify that the novelty of queries
significantly impacts reranking effectiveness, highlighting limitations in
existing approaches.
https://github.com/DataScienceUIBK/llm-reranking-generalization-study

---

### 3. DiffusionGS: Generative Search with Query Conditioned Diffusion in Kuaishou

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Qinyao Li, Xiaoyang Zheng, Qihang Zhao, Ke Xu, Zhongbo Sun, Chao Wang, Chenyi Lei, Han Li, Wenwu Ou
- **URL**: <http://arxiv.org/abs/2508.17754v1>
- **Submitted**: 2025-08-25 07:46:51
- **Topic Keywords**: query, queries, ranking, rag, commerce, e-commerce, rank, search
- **Reason**: The paper proposes a novel approach to personalized search ranking, leveraging user queries as intent anchors to extract immediate interests from historical behaviors. The use of generative models and conditional denoising tasks aligns with your interests in query understanding, ranking models, and user behavior modeling. While the focus is on e-commerce and short-video platforms, the concepts and techniques explored are relevant to your broader interests in information retrieval and natural language processing.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Generative Search Tasks
- **Aim**: Propose a novel approach to generative search tasks, called DiffusionGS, that leverages user queries to extract immediate interests from long-term, noisy historical behaviors
- **Rationale**: User queries can serve as explicit intent anchors to facilitate interest extraction, formulated as a conditional denoising task
- **Ground**: The DiffusionGS model is based on the generative ranking model (GRM) framework, which integrates the generative paradigm into search and recommendation systems
- **Experiment**: Evaluate the performance of the proposed DiffusionGS model using two metrics: Area Under the ROC Curve (AUC) and Grouped AUC (GAUC), comparing it with five baseline methods and demonstrating its superiority
- **Takeaway**: The proposed DiffusionGS model improves the quality and interpretability of the learned representations by incorporating a KL divergence loss and a pointwise prediction loss, and demonstrates its effectiveness through extensive experiments and online A/B testing

#### Abstract
> Personalized search ranking systems are critical for driving engagement and
revenue in modern e-commerce and short-video platforms. While existing methods
excel at estimating users' broad interests based on the filtered historical
behaviors, they typically under-exploit explicit alignment between a user's
real-time intent (represented by the user query) and their past actions. In
this paper, we propose DiffusionGS, a novel and scalable approach powered by
generative models. Our key insight is that user queries can serve as explicit
intent anchors to facilitate the extraction of users' immediate interests from
long-term, noisy historical behaviors. Specifically, we formulate interest
extraction as a conditional denoising task, where the user's query guides a
conditional diffusion process to produce a robust, user intent-aware
representation from their behavioral sequence. We propose the User-aware
Denoising Layer (UDL) to incorporate user-specific profiles into the
optimization of attention distribution on the user's past actions. By reframing
queries as intent priors and leveraging diffusion-based denoising, our method
provides a powerful mechanism for capturing dynamic user interest shifts.
Extensive offline and online experiments demonstrate the superiority of
DiffusionGS over state-of-the-art methods.

---

### 4. Semantic Search for Information Retrieval

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Kayla Farivar
- **URL**: <http://arxiv.org/abs/2508.17694v1>
- **Submitted**: 2025-08-25 06:03:26
- **Topic Keywords**: information retrieval, retriever, semantic search, retrieval, search
- **Reason**: The paper is highly relevant to Information Retrieval, specifically focusing on semantic search and modern state-of-the-art retrievers. The discussion of various models, including BERT, DPR, ColBERT, and SPLADE, aligns with the user's interest in query understanding and ranking models. However, the paper does not explicitly mention user behavior modeling or click models, which might reduce its score.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Semantic Retrieval in Information Retrieval Systems
- **Aim**: To develop and evaluate modern semantic retrievers that surpass traditional lexical techniques
- **Rationale**: The volatility of language necessitates the use of semantic similarity measures in IR systems
- **Ground**: The evolution of IR systems, from word2vec to BERT, and the development of state-of-the-art semantic retrievers like DPR, ORQA, ColBERT, and SPLADE
- **Experiment**: Evaluation of these models using the MS MARCO dataset and other benchmarks like BEIR
- **Takeaway**: Advancements in semantic search retrievers have enabled real-time applications, but challenges like multilingual compatibility and low-resource languages remain to be addressed

#### Abstract
> Information retrieval systems have progressed notably from lexical techniques
such as BM25 and TF-IDF to modern semantic retrievers. This survey provides a
brief overview of the BM25 baseline, then discusses the architecture of modern
state-of-the-art semantic retrievers. Advancing from BERT, we introduce dense
bi-encoders (DPR), late-interaction models (ColBERT), and neural sparse
retrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We
conclude with common evaluation tactics, pressing challenges, and propositions
for future directions.

---

### 5. How Do LLM-Generated Texts Impact Term-Based Retrieval Models?

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Wei Huang, Keping Bi, Yinqiong Cai, Wei Chen, Jiafeng Guo, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2508.17715v1>
- **Submitted**: 2025-08-25 06:43:27
- **Topic Keywords**: information retrieval, retriever, queries, retrieval
- **Reason**: The paper explores the impact of LLM-generated texts on term-based retrieval models, which is a relevant topic in Information Retrieval. The study's focus on query understanding, ranking models, and user behavior modeling aligns with your research interests. However, the paper's scope is more specific to term-based retrieval models and their biases, which is somewhat related but not a central match to your primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> As more content generated by large language models (LLMs) floods into the
Internet, information retrieval (IR) systems now face the challenge of
distinguishing and handling a blend of human-authored and machine-generated
texts. Recent studies suggest that neural retrievers may exhibit a preferential
inclination toward LLM-generated content, while classic term-based retrievers
like BM25 tend to favor human-written documents. This paper investigates the
influence of LLM-generated content on term-based retrieval models, which are
valued for their efficiency and robust generalization across domains. Our
linguistic analysis reveals that LLM-generated texts exhibit smoother
high-frequency and steeper low-frequency Zipf slopes, higher term specificity,
and greater document-level diversity. These traits are aligned with LLMs being
trained to optimize reader experience through diverse and precise expressions.
Our study further explores whether term-based retrieval models demonstrate
source bias, concluding that these models prioritize documents whose term
distributions closely correspond to those of the queries, rather than
displaying an inherent source bias. This work provides a foundation for
understanding and addressing potential biases in term-based IR systems managing
mixed-source content.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation

- **LLM Score**: 7
- **Keyword Score**: 16
- **Authors**: Shaoxiong Zhan, Hai Lin, Hongming Tan, Xiaodong Cai, Hai-Tao Zheng, Xin Su, Zifei Shan, Ruitong Liu, Hong-Gee Kim
- **URL**: <http://arxiv.org/abs/2508.17858v1>
- **Submitted**: 2025-08-25 10:07:36
- **Topic Keywords**: passage retrieval, dense retrieval, query, queries, rag, retrieval
- **Reason**: The paper proposes a framework for enhancing dense query representations through fine-grained, input-aware vector modulation, which is relevant to information retrieval and query understanding. The focus on dense retrieval models and fine-grained retrieval tasks aligns with the user's interests in ranking models and user behavior modeling. However, the paper's primary focus on dense retrieval models and its application to retrieval-augmented generation pipelines may not be directly related to the user's background in e-commerce and NLP.

#### Abstract
> As queries in retrieval-augmented generation (RAG) pipelines powered by large
language models (LLMs) become increasingly complex and diverse, dense retrieval
models have demonstrated strong performance in semantic matching. Nevertheless,
they often struggle with fine-grained retrieval tasks, where precise keyword
alignment and span-level localization are required, even in cases with high
lexical overlap that would intuitively suggest easier retrieval. To
systematically evaluate this limitation, we introduce two targeted tasks,
keyword retrieval and part-of-passage retrieval, designed to simulate practical
fine-grained scenarios. Motivated by these observations, we propose
LexSemBridge, a unified framework that enhances dense query representations
through fine-grained, input-aware vector modulation. LexSemBridge constructs
latent enhancement vectors from input tokens using three paradigms: Statistical
(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense
embeddings via element-wise interaction. Theoretically, we show that this
modulation preserves the semantic direction while selectively amplifying
discriminative dimensions. LexSemBridge operates as a plug-in without modifying
the backbone encoder and naturally extends to both text and vision modalities.
Extensive experiments across semantic and fine-grained retrieval tasks validate
the effectiveness and generality of our approach. All code and models are
publicly available at https://github.com/Jasaxion/LexSemBridge/

### 7. Demographically-Inspired Query Variants Using an LLM

- **LLM Score**: 7
- **Keyword Score**: 11
- **Authors**: Marwah Alaofi, Nicola Ferro, Paul Thomas, Falk Scholer, Mark Sanderson
- **URL**: <http://arxiv.org/abs/2508.17644v1>
- **Submitted**: 2025-08-25 04:17:56
- **Comment**: Published in the proceedings of ICTIR'25, Padua, Italy
- **Topic Keywords**: query, queries, ranking, rank, search
- **Reason**: The paper explores query diversification using a Large Language Model (LLM), which aligns with the user's interest in query understanding and ranking models. The study's focus on user profiles and their impact on system evaluation is also relevant to the user's interest in user behavior modeling. However, the paper's scope is more focused on query diversification and system evaluation, which is somewhat related but not a central match to the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> This study proposes a method to diversify queries in existing test
collections to reflect some of the diversity of search engine users, aligning
with an earlier vision of an 'ideal' test collection. A Large Language Model
(LLM) is used to create query variants: alternative queries that have the same
meaning as the original. These variants represent user profiles characterised
by different properties, such as language and domain proficiency, which are
known in the IR literature to influence query formulation.
  The LLM's ability to generate query variants that align with user profiles is
empirically validated, and the variants' utility is further explored for IR
system evaluation. Results demonstrate that the variants impact how systems are
ranked and show that user profiles experience significantly different levels of
system effectiveness. This method enables an alternative perspective on system
evaluation where we can observe both the impact of user profiles on system
rankings and how system performance varies across users.

### 8. HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Jiyoon Myung, Jihyeon Park, Joohyung Han
- **URL**: <http://arxiv.org/abs/2508.18048v1>
- **Submitted**: 2025-08-25 14:06:27
- **Comment**: Accepted at the 2nd EARL Workshop on Evaluating and Applying
  Recommender Systems with Large Language Models (RecSys 2025)
- **Topic Keywords**: query, queries, retrieval, recommend, search
- **Reason**: The paper's focus on hybrid retrieval over semi-structured tabular data, combining structured filtering with semantic embedding search, aligns with your interests in Information Retrieval and Search technologies. The use of large language models (LLMs) for attribute-level constraints extraction and metadata filtering is also relevant to your query understanding and ranking models research. However, the paper's primary focus on recommender systems and semi-structured tabular data may not be directly applicable to your e-commerce domain expertise.

#### Abstract
> User queries in real-world recommendation systems often combine structured
constraints (e.g., category, attributes) with unstructured preferences (e.g.,
product descriptions or reviews). We introduce HyST (Hybrid retrieval over
Semi-structured Tabular data), a hybrid retrieval framework that combines
LLM-powered structured filtering with semantic embedding search to support
complex information needs over semi-structured tabular data. HyST extracts
attribute-level constraints from natural language using large language models
(LLMs) and applies them as metadata filters, while processing the remaining
unstructured query components via embedding-based retrieval. Experiments on a
semi-structured benchmark show that HyST consistently outperforms tradtional
baselines, highlighting the importance of structured filtering in improving
retrieval precision, offering a scalable and accurate solution for real-world
user queries.

### 9. Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Yejin Choi, Jaewoo Park, Janghan Yoon, Saejin Kim, Jaehyun Jeon, Youngjae Yu
- **URL**: <http://arxiv.org/abs/2508.17079v1>
- **Submitted**: 2025-08-23 16:14:41
- **Topic Keywords**: information retrieval, retriever, rag, retrieval
- **Reason**: The paper explores multimodal document retrieval, which is related to information retrieval and search technologies. The use of cross-modal question generation and leveraging multiple modalities is an innovative approach that aligns with the user's interest in query understanding and ranking models. However, the paper's focus on multimodal retrieval and its application to real-world documents may not be directly applicable to the user's specific interests in e-commerce and user behavior modeling.

#### Abstract
> Rapid advances in Multimodal Large Language Models (MLLMs) have expanded
information retrieval beyond purely textual inputs, enabling retrieval from
complex real world documents that combine text and visuals. However, most
documents are private either owned by individuals or confined within corporate
silos and current retrievers struggle when faced with unseen domains or
languages. To address this gap, we introduce PREMIR, a simple yet effective
framework that leverages the broad knowledge of an MLLM to generate cross modal
pre questions (preQs) before retrieval. Unlike earlier multimodal retrievers
that compare embeddings in a single vector space, PREMIR leverages preQs from
multiple complementary modalities to expand the scope of matching to the token
level. Experiments show that PREMIR achieves state of the art performance on
out of distribution benchmarks, including closed domain and multilingual
settings, outperforming strong baselines across all retrieval metrics. We
confirm the contribution of each component through in depth ablation studies,
and qualitative analyses of the generated preQs further highlight the model's
robustness in real world settings.

### 10. ST-Raptor: LLM-Powered Semi-Structured Table Question Answering

- **LLM Score**: 7
- **Keyword Score**: 3
- **Authors**: Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu
- **URL**: <http://arxiv.org/abs/2508.18190v1>
- **Submitted**: 2025-08-25 16:48:51
- **Comment**: Extension of our SIGMOD 2026 paper. Please refer to source code
  available at: https://github.com/weAIDB/ST-Raptor
- **Topic Keywords**: queries
- **Reason**: The paper focuses on semi-structured table question answering, which is related to information retrieval and query understanding. The use of large language models and tree-based framework is also relevant to my interests in NLP and ranking models. However, the paper's primary focus is on table question answering, which is not directly aligned with my core research themes.

#### Abstract
> Semi-structured tables, widely used in real-world applications (e.g.,
financial reports, medical records, transactional orders), often involve
flexible and complex layouts (e.g., hierarchical headers and merged cells).
These tables generally rely on human analysts to interpret table layouts and
answer relevant natural language questions, which is costly and inefficient. To
automate the procedure, existing methods face significant challenges. First,
methods like NL2SQL require converting semi-structured tables into structured
ones, which often causes substantial information loss. Second, methods like
NL2Code and multi-modal LLM QA struggle to understand the complex layouts of
semi-structured tables and cannot accurately answer corresponding questions. To
this end, we propose ST-Raptor, a tree-based framework for semi-structured
table question answering using large language models. First, we introduce the
Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures
complex semi-structured table layouts, along with an effective algorithm for
constructing the tree. Second, we define a set of basic tree operations to
guide LLMs in executing common QA tasks. Given a user question, ST-Raptor
decomposes it into simpler sub-questions, generates corresponding tree
operation pipelines, and conducts operation-table alignment for accurate
pipeline execution. Third, we incorporate a two-stage verification mechanism:
forward validation checks the correctness of execution steps, while backward
validation evaluates answer reliability by reconstructing queries from
predicted answers. To benchmark the performance, we present SSTQA, a dataset of
764 questions over 102 real-world semi-structured tables. Experiments show that
ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code
is available at https://github.com/weAIDB/ST-Raptor.

### 11. VQL: An End-to-End Context-Aware Vector Quantization Attention for Ultra-Long User Behavior Modeling

- **LLM Score**: 6
- **Keyword Score**: 13
- **Authors**: Kaiyuan Li, Yongxiang Tang, Yanhua Cheng, Yong Bai, Yanxiang Zeng, Chao Wang, Xialong Liu, Peng Jiang
- **URL**: <http://arxiv.org/abs/2508.17125v1>
- **Submitted**: 2025-08-23 19:58:18
- **Topic Keywords**: query, ltr, rag, user behavior, retrieval, recommend
- **Reason**: The paper proposes a novel framework for ultra-long user behavior modeling, focusing on recommender systems. While it's not directly related to query understanding, ranking models, or user behavior modeling in the context of information retrieval, it does explore attention mechanisms and compression techniques, which are relevant to my interests in NLP and data mining. However, the paper's primary focus on recommender systems and ultra-long sequence recommendation makes it only somewhat related to my research themes.

#### Abstract
> In large-scale recommender systems, ultra-long user behavior sequences encode
rich signals of evolving interests. Extending sequence length generally
improves accuracy, but directly modeling such sequences in production is
infeasible due to latency and memory constraints. Existing solutions fall into
two categories: (1) top-k retrieval, which truncates the sequence and may
discard most attention mass when L >> k; and (2) encoder-based compression,
which preserves coverage but often over-compresses and fails to incorporate key
context such as temporal gaps or target-aware signals. Neither class achieves a
good balance of low-loss compression, context awareness, and efficiency.
  We propose VQL, a context-aware Vector Quantization Attention framework for
ultra-long behavior modeling, with three innovations. (1) Key-only
quantization: only attention keys are quantized, while values remain intact; we
prove that softmax normalization yields an error bound independent of sequence
length, and a codebook loss directly supervises quantization quality. This also
enables L-free inference via offline caches. (2) Multi-scale quantization:
attention heads are partitioned into groups, each with its own small codebook,
which reduces quantization error while keeping cache size fixed. (3) Efficient
context injection: static features (e.g., item category, modality) are directly
integrated, and relative position is modeled via a separable temporal kernel.
All context is injected without enlarging the codebook, so cached
representations remain query-independent.
  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show
that VQL consistently outperforms strong baselines, achieving higher accuracy
while reducing inference latency, establishing a new state of the art in
balancing accuracy and efficiency for ultra-long sequence recommendation.

### 12. Improving Table Understanding with LLMs and Entity-Oriented Search

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Thi-Nhung Nguyen, Hoang Ngo, Dinh Phung, Thuy-Trang Vu, Dat Quoc Nguyen
- **URL**: <http://arxiv.org/abs/2508.17028v1>
- **Submitted**: 2025-08-23 14:02:45
- **Comment**: Accepted to COLM 2025
- **Topic Keywords**: query, rag, search
- **Reason**: The paper explores table understanding with LLMs and entity-oriented search, which is related to query understanding and ranking models in Information Retrieval. However, the focus on tables and entity-oriented search is not directly aligned with the user's primary interests in search technologies, user behavior modeling, and deep semantic understanding. The paper's relevance is somewhat related but not a central match.

#### Abstract
> Our work addresses the challenges of understanding tables. Existing methods
often struggle with the unpredictable nature of table content, leading to a
reliance on preprocessing and keyword matching. They also face limitations due
to the lack of contextual information, which complicates the reasoning
processes of large language models (LLMs). To overcome these challenges, we
introduce an entity-oriented search method to improve table understanding with
LLMs. This approach effectively leverages the semantic similarities between
questions and table data, as well as the implicit relationships between table
cells, minimizing the need for data preprocessing and keyword matching.
Additionally, it focuses on table entities, ensuring that table cells are
semantically tightly bound, thereby enhancing contextual clarity. Furthermore,
we pioneer the use of a graph query language for table understanding,
establishing a new research direction. Experiments show that our approach
achieves new state-of-the-art performances on standard benchmarks
WikiTableQuestions and TabFact.

### 13. Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Jacob Portes, Connor Jennings, Erica Ji Yuen, Sasha Doubov, Michael Carbin
- **URL**: <http://arxiv.org/abs/2508.17400v1>
- **Submitted**: 2025-08-24 15:19:24
- **Comment**: 15 pages, 4 figures
- **Topic Keywords**: retriever, retrieval
- **Reason**: The paper explores the retrieval capabilities of large language models, which is relevant to Information Retrieval (IR) and Search technologies. The focus on pretraining FLOPs and model sizes is somewhat related to query understanding and ranking models, but the paper's primary focus is on the scalability of language models rather than specific IR techniques or user behavior modeling.

#### Abstract
> How does retrieval performance scale with pretraining FLOPs? We benchmark
retrieval performance across LLM model sizes from 125 million parameters to 7
billion parameters pretrained on datasets ranging from 1 billion tokens to more
than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR
tasks predictably scales with LLM size, training duration, and estimated FLOPs.
We also show that In-Context Learning scores are strongly correlated with
retrieval scores across retrieval tasks. Finally, we highlight the implications
this has for the development of LLM-based retrievers.

### 14. The Power of Framing: How News Headlines Guide Search Behavior

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Amrit Poudel, Maria Milkowski, Tim Weninger
- **URL**: <http://arxiv.org/abs/2508.17131v1>
- **Submitted**: 2025-08-23 20:12:19
- **Comment**: Accepted to EMNLP
- **Topic Keywords**: queries, search
- **Reason**: The paper explores the impact of headline framing on search behavior, which is related to query understanding and user behavior modeling in Information Retrieval. While the focus is on search behavior rather than ranking models or deep semantic understanding, the study's findings on how framing influences follow-up queries are relevant to the broader field of IR. However, the paper's scope is narrower than expected, and the connection to the e-commerce domain is limited.

#### Abstract
> Search engines play a central role in how people gather information, but
subtle cues like headline framing may influence not only what users believe but
also how they search. While framing effects on judgment are well documented,
their impact on subsequent search behavior is less understood. We conducted a
controlled experiment where participants issued queries and selected from
headlines filtered by specific linguistic frames. Headline framing
significantly shaped follow-up queries: conflict and strategy frames disrupted
alignment with prior selections, while episodic frames led to more concrete
queries than thematic ones. We also observed modest short-term frame
persistence that declined over time. These results suggest that even brief
exposure to framing can meaningfully alter the direction of users
information-seeking behavior.

### 15. Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations

- **LLM Score**: 4
- **Keyword Score**: 21
- **Authors**: Hung-Chun Hsu, Yuan-Ching Kuo, Chao-Han Huck Yang, Szu-Wei Fu, Hanrong Ye, Hongxu Yin, Yu-Chiang Frank Wang, Ming-Feng Tsai, Chuan-Ju Wang
- **URL**: <http://arxiv.org/abs/2508.18132v1>
- **Submitted**: 2025-08-25 15:38:56
- **Topic Keywords**: retriever, queries, ranking, rerank, rag, retrieval, recommend, commerce, e-commerce, rank, search
- **Reason**: The paper explores multimodal conversational recommendations, which is related to information retrieval and search technologies. However, the focus on generative retrieval and test-time scaling is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on multimodal large language models and conversational product search is also outside the user's primary focus on e-commerce and deep semantic understanding.

#### Abstract
> The rapid evolution of e-commerce has exposed the limitations of traditional
product retrieval systems in managing complex, multi-turn user interactions.
Recent advances in multimodal generative retrieval -- particularly those
leveraging multimodal large language models (MLLMs) as retrievers -- have shown
promise. However, most existing methods are tailored to single-turn scenarios
and struggle to model the evolving intent and iterative nature of multi-turn
dialogues when applied naively. Concurrently, test-time scaling has emerged as
a powerful paradigm for improving large language model (LLM) performance
through iterative inference-time refinement. Yet, its effectiveness typically
relies on two conditions: (1) a well-defined problem space (e.g., mathematical
reasoning), and (2) the model's ability to self-correct -- conditions that are
rarely met in conversational product search. In this setting, user queries are
often ambiguous and evolving, and MLLMs alone have difficulty grounding
responses in a fixed product corpus. Motivated by these challenges, we propose
a novel framework that introduces test-time scaling into conversational
multimodal product retrieval. Our approach builds on a generative retriever,
further augmented with a test-time reranking (TTR) mechanism that improves
retrieval accuracy and better aligns results with evolving user intent
throughout the dialogue. Experiments across multiple benchmarks show consistent
improvements, with average gains of 14.5 points in MRR and 10.6 points in
nDCG@1.

### 16. GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Jeongsoo Lee, Daeyong Kwon, Kyohoon Jin
- **URL**: <http://arxiv.org/abs/2508.16994v1>
- **Submitted**: 2025-08-23 11:26:41
- **Comment**: Accepted at EMNLP 2025 findings
- **Topic Keywords**: retriever, query, queries, rag, retrieval
- **Reason**: The paper proposes a novel evaluation framework for Retrieval-Augmented Generation (RAG) systems, focusing on multi-hop QA and fine-grained difficulty modeling. While it touches on information retrieval and NLP, the primary focus is on evaluation and benchmarking, which is not directly aligned with your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Retrieval-Augmented Generation (RAG) systems are widely adopted in
knowledge-intensive NLP tasks, but current evaluations often overlook the
structural complexity and multi-step reasoning required in real-world
scenarios. These benchmarks overlook key factors such as the interaction
between retrieval difficulty and reasoning depth. To address this gap, we
propose \textsc{GRADE}, a novel evaluation framework that models task
difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the
number of inference steps (hops), and (2) semantic distance between the query
and its supporting evidence. We construct a synthetic multi-hop QA dataset from
factual news articles by extracting knowledge graphs and augmenting them
through semantic clustering to recover missing links, allowing us to generate
diverse and difficulty-controlled queries. Central to our framework is a 2D
difficulty matrix that combines generator-side and retriever-side difficulty.
Experiments across multiple domains and models show that error rates strongly
correlate with our difficulty measures, validating their diagnostic utility.
\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a
scalable foundation for evaluating and improving multi-hop reasoning in
real-world applications.

### 17. MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong
- **URL**: <http://arxiv.org/abs/2508.18260v1>
- **Submitted**: 2025-08-25 17:53:22
- **Comment**: 10 pages, 8 figures (including tables), plus appendix. Submitted to
  AAAI 2026
- **Topic Keywords**: queries, rag, retrieval augmented generation, retrieval, search
- **Reason**: The paper proposes a novel framework for test-time scalable reasoning, focusing on medical question-answering tasks. While it involves retrieval and graph exploration, the primary focus is on reasoning and knowledge graph traversal, which is not directly related to information retrieval, query understanding, or ranking models, which are core areas of interest in your research.

#### Abstract
> Large reasoning models (LRMs) have shown significant progress in test-time
scaling through chain-of-thought prompting. Current approaches like search-o1
integrate retrieval augmented generation (RAG) into multi-step reasoning
processes but rely on a single, linear reasoning chain while incorporating
unstructured textual information in a flat, context-agnostic manner. As a
result, these approaches can lead to error accumulation throughout the
reasoning chain, which significantly limits its effectiveness in medical
question-answering (QA) tasks where both accuracy and traceability are critical
requirements. To address these challenges, we propose MIRAGE (Multi-chain
Inference with Retrieval-Augmented Graph Exploration), a novel test-time
scalable reasoning framework that performs dynamic multi-chain inference over
structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex
queries into entity-grounded sub-questions, 2) executes parallel inference
chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop
traversal, and 4) integrates answers using cross-chain verification to resolve
contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,
CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,
Tree-of-Thought variants, and other retrieval-augmented baselines in both
automatic and human evaluations. Additionally, MIRAGE improves interpretability
by generating explicit reasoning chains that trace each factual claim to
concrete chains within the knowledge graph, making it well-suited for complex
medical reasoning scenarios. The code will be available for further research.

### 18. Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Julius Gun, Timo Oksanen
- **URL**: <http://arxiv.org/abs/2508.18093v1>
- **Submitted**: 2025-08-25 14:54:46
- **Topic Keywords**: information retrieval, query, rag, retrieval
- **Reason**: The paper explores the application of large language models and Retrieval-Augmented Generation strategies for technical question answering in a cross-lingual information retrieval scenario. While it touches on information retrieval and query understanding, the focus is on a specific domain (agricultural machine manuals) and does not directly relate to the user's primary interests in e-commerce, ranking models, or user behavior modeling.

#### Abstract
> We present a case study evaluating large language models (LLMs) with
128K-token context windows on a technical question answering (QA) task. Our
benchmark is built on a user manual for an agricultural machine, available in
English, French, and German. It simulates a cross-lingual information retrieval
scenario where questions are posed in English against all three language
versions of the manual. The evaluation focuses on realistic
"needle-in-a-haystack" challenges and includes unanswerable questions to test
for hallucinations. We compare nine long-context LLMs using direct prompting
against three Retrieval-Augmented Generation (RAG) strategies (keyword,
semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this
specific manual show that Hybrid RAG consistently outperforms direct
long-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5
7B achieve high accuracy (over 85%) across all languages with RAG. This paper
contributes a detailed analysis of LLM performance in a specialized industrial
domain and an open framework for similar evaluations, highlighting practical
trade-offs and challenges.

### 19. Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Leqian Li, Dianxi Shi, Jialu Zhou, Xinyu Wei, Mingyue Yang, Songchang Jin, Shaowu Yang
- **URL**: <http://arxiv.org/abs/2508.17862v1>
- **Submitted**: 2025-08-25 10:13:02
- **Topic Keywords**: queries, rag, retrieval augmented generation, retrieval
- **Reason**: The paper proposes a method for retrieval-augmented generation, which is related to information retrieval and natural language processing. However, the focus is on large language models and knowledge management, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) have shown remarkable capabilities across
diverse tasks, yet they face inherent limitations such as constrained
parametric knowledge and high retraining costs. Retrieval-Augmented Generation
(RAG) augments the generation process by retrieving externally stored knowledge
absent from the models internal parameters. However, RAG methods face
challenges such as information loss and redundant retrievals during multi-round
queries, accompanying the difficulties in precisely characterizing knowledge
gaps for complex tasks. To address these problems, we propose Retrieval
Feedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms
the stateless retrieval of previous methods into stateful continuous knowledge
management by constructing a dynamic evidence pool. Specifically, our method
generates refined queries describing the models knowledge gaps using relational
triples from questions and evidence from the dynamic evidence pool; Retrieves
critical external knowledge to iteratively update this evidence pool; Employs a
R-Feedback Model to evaluate evidence completeness until convergence. Compared
to traditional RAG methods, our approach enables persistent storage of
retrieved passages and effectively distills key information from passages to
construct clearly new queries. Experiments on three public QA benchmarks
demonstrate that RFM-RAG outperforms previous methods and improves overall
system accuracy.

### 20. Bootstrapping Conditional Retrieval for User-to-Item Recommendations

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Hongtao Lin, Haoyu Chen, Jaewon Jang, Jiajing Xu
- **URL**: <http://arxiv.org/abs/2508.16793v1>
- **Submitted**: 2025-08-22 20:50:52
- **Topic Keywords**: query, rag, retrieval, recommend, search
- **Reason**: The paper focuses on user-to-item retrieval and conditional retrieval, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and the use case of a topic-based notification feed at Pinterest is not directly aligned with my primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> User-to-item retrieval has been an active research area in recommendation
system, and two tower models are widely adopted due to model simplicity and
serving efficiency. In this work, we focus on a variant called
\textit{conditional retrieval}, where we expect retrieved items to be relevant
to a condition (e.g. topic). We propose a method that uses the same training
data as standard two tower models but incorporates item-side information as
conditions in query. This allows us to bootstrap new conditional retrieval use
cases and encourages feature interactions between user and condition.
Experiments show that our method can retrieve highly relevant items and
outperforms standard two tower models with filters on engagement metrics. The
proposed model is deployed to power a topic-based notification feed at
Pinterest and led to +0.26\% weekly active users.

### 21. Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Hongyu Cao, Yuxuan Wu, Yucheng Cai, Xianyu Zhao, Zhijian Ou
- **URL**: <http://arxiv.org/abs/2508.18168v1>
- **Submitted**: 2025-08-25 16:17:16
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper focuses on retrieval-augmented generation models, which is a topic in Natural Language Processing (NLP). While it shares some similarities with information retrieval, the primary focus is on generation and retrieval models rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is limited, but it may still be of interest to someone with a broader background in NLP and related topics.

#### Abstract
> Retrieval-augmented generation (RAG) has become a widely recognized paradigm
to combine parametric memory with non-parametric memories. An RAG model
consists of two serial connecting components (retriever and generator). A major
challenge in end-to-end optimization of the RAG model is that marginalization
over relevant passages (modeled as discrete latent variables) from a knowledge
base is required. Traditional top-K marginalization and variational RAG (VRAG)
suffer from biased or high-variance gradient estimates. In this paper, we
propose and develop joint stochastic approximation (JSA) based end-to-end
training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a
stochastic extension of the EM (expectation-maximization) algorithm and is
particularly powerful in estimating discrete latent variable models. Extensive
experiments are conducted on five datasets for two tasks (open-domain question
answering, knowledge-grounded dialogs) and show that JSA-RAG significantly
outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of
JSA-RAG from the perspectives of generation, retrieval, and low-variance
gradient estimate.

### 22. ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Riccardo Pozzi, Matteo Palmonari, Andrea Coletta, Luigi Bellomarini, Jens Lehmann, Sahar Vahdati
- **URL**: <http://arxiv.org/abs/2508.16983v1>
- **Submitted**: 2025-08-23 10:21:47
- **Comment**: 19 pages, 6 figures, accepted at ISWC
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper presents a method for LLMs to access external knowledge without relying on retrievers or auxiliary models, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on language models and knowledge graph verbalization, which is not directly aligned with the user's interests in search technologies and user behavior modeling.

#### Abstract
> Knowledge gaps and hallucinations are persistent challenges for Large
Language Models (LLMs), which generate unreliable responses when lacking the
necessary information to fulfill user instructions. Existing approaches, such
as Retrieval-Augmented Generation (RAG) and tool use, aim to address these
issues by incorporating external knowledge. Yet, they rely on additional models
or services, resulting in complex pipelines, potential error propagation, and
often requiring the model to process a large number of tokens. In this paper,
we present a scalable method that enables LLMs to access external knowledge
without depending on retrievers or auxiliary models. Our approach uses
constrained generation with a pre-built prefix-tree index. Triples from a
Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a
prefix tree for efficient access. During inference, to acquire external
knowledge, the LLM generates facts with constrained generation which allows
only sequences of tokens that form an existing fact. We evaluate our proposal
on Question Answering and show that it scales to large knowledge bases (800
million facts), adapts to domain-specific data, and achieves effective results.
These gains come with minimal generation-time overhead. ReFactX code is
available at https://github.com/rpo19/ReFactX.

### 23. Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Bo Zhao, Yinghao Zhang, Ziqi Xu, Yongli Ren, Xiuzhen Zhang, Renqiang Luo, Zaiwen Feng, Feng Xia
- **URL**: <http://arxiv.org/abs/2508.16910v1>
- **Submitted**: 2025-08-23 05:52:39
- **Comment**: This paper has been accepted to the 34th ACM International Conference
  on Information and Knowledge Management (CIKM 2025), Full Research Paper
- **Topic Keywords**: query, rag, retrieval
- **Reason**: The paper proposes a novel causal prompting framework for unbiased reasoning in large language models, which is related to the user's interests in NLP and deep semantic understanding. However, the focus on knowledge-intensive tasks and language models is not directly aligned with the user's primary focus on information retrieval and search technologies.

#### Abstract
> Large Language Models (LLMs) have shown impressive capabilities in natural
language processing but still struggle to perform well on knowledge-intensive
tasks that require deep reasoning and the integration of external knowledge.
Although methods such as Retrieval-Augmented Generation (RAG) and
Chain-of-Thought (CoT) have been proposed to enhance LLMs with external
knowledge, they still suffer from internal bias in LLMs, which often leads to
incorrect answers. In this paper, we propose a novel causal prompting
framework, Conditional Front-Door Prompting (CFD-Prompting), which enables the
unbiased estimation of the causal effect between the query and the answer,
conditional on external knowledge, while mitigating internal bias. By
constructing counterfactual external knowledge, our framework simulates how the
query behaves under varying contexts, addressing the challenge that the query
is fixed and is not amenable to direct causal intervention. Compared to the
standard front-door adjustment, the conditional variant operates under weaker
assumptions, enhancing both robustness and generalisability of the reasoning
process. Extensive experiments across multiple LLMs and benchmark datasets
demonstrate that CFD-Prompting significantly outperforms existing baselines in
both accuracy and robustness.

### 24. PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang
- **URL**: <http://arxiv.org/abs/2508.18166v1>
- **Submitted**: 2025-08-25 16:16:06
- **Comment**: 9 pages, 4 figures, conference
- **Topic Keywords**: ctr, cvr, recommend, personalization
- **Reason**: While the paper explores recommender systems, which is a related topic, it focuses on app recommendation and does not directly address information retrieval, query understanding, or ranking models. The paper's emphasis on multimodal embeddings, contrastive alignment, and attention mechanisms is interesting, but the application is specific to app recommendation, which is not a primary focus of your research interests.

#### Abstract
> Modern app store recommender systems struggle with multiple-category apps, as
traditional taxonomies fail to capture overlapping semantics, leading to
suboptimal personalization. We propose PCR-CA (Parallel Codebook
Representations with Contrastive Alignment), an end-to-end framework for
improved CTR prediction. PCR-CA first extracts compact multimodal embeddings
from app text, then introduces a Parallel Codebook VQ-AE module that learns
discrete semantic representations across multiple codebooks in parallel --
unlike hierarchical residual quantization (RQ-VAE). This design enables
independent encoding of diverse aspects (e.g., gameplay, art style), better
modeling multiple-category semantics. To bridge semantic and collaborative
signals, we employ a contrastive alignment loss at both the user and item
levels, enhancing representation learning for long-tail items. Additionally, a
dual-attention fusion mechanism combines ID-based and semantic features to
capture user interests, especially for long-tail apps. Experiments on a
large-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong
baselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further
validates our approach, showing a +10.52% lift in CTR and a +16.30% improvement
in CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new
framework has now been fully deployed on the Microsoft Store.

### 25. Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Tianjun Wei, Huizhong Guo, Yingpeng Du, Zhu Sun, Chen Huang, Dongxia Wang, Jie Zhang
- **URL**: <http://arxiv.org/abs/2508.18142v1>
- **Submitted**: 2025-08-25 15:51:24
- **Comment**: Github: https://github.com/UserMirrorer/UserMirrorer
- **Topic Keywords**: rag, user behavior, recommend, search
- **Reason**: The paper focuses on user simulation and recommender systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and user feedback is not directly aligned with your primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of Large Language Models and data distillation is also not directly relevant to your interests in IR and NLP.

#### Abstract
> User simulation is increasingly vital to develop and evaluate recommender
systems (RSs). While Large Language Models (LLMs) offer promising avenues to
simulate user behavior, they often struggle with the absence of specific domain
alignment required for RSs and the efficiency demands of large-scale
simulation. A vast yet underutilized resource for enhancing this alignment is
the extensive user feedback inherent in RSs. However, directly leveraging such
feedback presents two significant challenges. First, user feedback in RSs is
often ambiguous and noisy, which negatively impacts effective preference
alignment. Second, the massive volume of feedback largely hinders the
efficiency of preference alignment, necessitating an efficient filtering
mechanism to identify more informative samples. To overcome these hurdles, we
introduce a novel data construction framework that leverages user feedback in
RSs with advanced LLM capabilities to generate high-quality simulation data.
Our framework unfolds in two key phases: (1) employing LLMs to generate
cognitive decision-making processes on constructed simulation samples, reducing
ambiguity in raw user feedback; (2) data distillation based on uncertainty
estimation and behavior sampling to filter challenging yet denoised simulation
samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using
such high-quality dataset with corresponding decision-making processes.
Extensive experiments verify that our framework significantly boosts the
alignment with human preferences and in-domain reasoning capabilities of
fine-tuned LLMs, and provides more insightful and interpretable signals when
interacting with RSs. We believe our work will advance the RS community and
offer valuable insights for broader human-centric AI research.

### 26. Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Prathamesh Kokate, Mitali Sarnaik, Manavi Khopade, Mukta Takalikar, Raviraj Joshi
- **URL**: <http://arxiv.org/abs/2508.17490v1>
- **Submitted**: 2025-08-24 18:52:37
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper focuses on long document classification using sentence ranking, which is related to information retrieval and query understanding. However, it does not directly address ranking models or user behavior modeling, which are core interests. The paper's application in the NLP domain is also relevant, but its scope is limited to text classification, which is not a primary focus of the user's research.

#### Abstract
> Transformer-based models like BERT excel at short text classification but
struggle with long document classification (LDC) due to input length
limitations and computational inefficiencies. In this work, we propose an
efficient, zero-shot approach to LDC that leverages sentence ranking to reduce
input context without altering the model architecture. Our method enables the
adaptation of models trained on short texts, such as headlines, to long-form
documents by selecting the most informative sentences using a TF-IDF-based
ranking strategy. Using the MahaNews dataset of long Marathi news articles, we
evaluate three context reduction strategies that prioritize essential content
while preserving classification accuracy. Our results show that retaining only
the top 50\% ranked sentences maintains performance comparable to full-document
inference while reducing inference time by up to 35\%. This demonstrates that
sentence ranking is a simple yet effective technique for scalable and efficient
zero-shot LDC.

### 27. SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie
- **URL**: <http://arxiv.org/abs/2508.17225v1>
- **Submitted**: 2025-08-24 06:58:29
- **Comment**: Working in progress
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Retrieval-Augmented Generation, which is related to Information Retrieval, but the primary focus is on generation rather than search or ranking. The paper's emphasis on self-supervised learning and optimization is also relevant to my interests in query understanding and user behavior modeling. However, the paper's scope is narrower than my research interests, and the connection to my areas of focus is not as direct.

#### Abstract
> Retrieval-Augmented Generation (RAG) systems require Large Language Models
(LLMs) to generate responses that are faithful to the retrieved context.
However, faithfulness hallucination remains a critical challenge, as existing
methods often require costly supervision and post-training or significant
inference burdens. To overcome these limitations, we introduce Self-Supervised
Faithfulness Optimization (SSFO), the first self-supervised alignment approach
for enhancing RAG faithfulness. SSFO constructs preference data pairs by
contrasting the model's outputs generated with and without the context.
Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness
without incurring labeling costs or additional inference burden. We
theoretically and empirically demonstrate that SSFO leverages a benign form of
\emph{likelihood displacement}, transferring probability mass from
parametric-based tokens to context-aligned tokens. Based on this insight, we
propose a modified DPO loss function to encourage likelihood displacement.
Comprehensive evaluations show that SSFO significantly outperforms existing
methods, achieving state-of-the-art faithfulness on multiple context-based
question-answering datasets. Notably, SSFO exhibits strong generalization,
improving cross-lingual faithfulness and preserving general
instruction-following capabilities. We release our code and model at the
anonymous link: https://github.com/chkwy/SSFO

### 28. Active Domain Knowledge Acquisition with \$100 Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Yang Wu, Raha Moraffah, Rujing Yao, Jinhong Yu, Zhimin Tao, Xiaozhong Liu
- **URL**: <http://arxiv.org/abs/2508.17202v1>
- **Submitted**: 2025-08-24 03:34:40
- **Comment**: EMNLP 2025 Findings
- **Topic Keywords**: queries, search
- **Reason**: The paper focuses on enhancing Large Language Models (LLMs) by actively engaging domain experts within a fixed budget, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of domain knowledge acquisition, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> Large Language Models (LLMs) have demonstrated an impressive level of general
knowledge. However, they often struggle in highly specialized and
cost-sensitive domains such as drug discovery and rare disease research due to
the lack of expert knowledge. In this paper, we propose a novel framework
(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively
engaging domain experts within a fixed budget. Unlike traditional fine-tuning
approaches, PU-ADKA selectively identifies and queries the most appropriate
expert from a team, taking into account each expert's availability, knowledge
boundaries, and consultation costs. We train PU-ADKA using simulations on
PubMed data and validate it through both controlled expert interactions and
real-world deployment with a drug development team, demonstrating its
effectiveness in enhancing LLM performance in specialized domains under strict
budget constraints. In addition to outlining our methodological innovations and
experimental results, we introduce a new benchmark dataset, CKAD, for
cost-effective LLM domain knowledge acquisition to foster further research in
this challenging area.

### 29. Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zihao Wei, Liang Pang, Jiahao Liu, Jingcheng Deng, Shicheng Xu, Zenghao Duan, Jingang Wang, Fei Sun, Xunliang Cai, Huawei Shen, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2508.17627v1>
- **Submitted**: 2025-08-25 03:17:17
- **Topic Keywords**: query
- **Reason**: The paper focuses on Large Language Models (LLMs) and their tendency to overthink, which is not directly related to Information Retrieval or Search technologies. While it touches on the concept of 'reasoning convergence', it does not seem to be directly applicable to query understanding, ranking models, or user behavior modeling. The paper's relevance to your interests is limited, but it does explore the idea of identifying a 'completion point' which could be seen as a form of 'query understanding' in a broader sense.

#### Abstract
> Large language models (LLMs) enhance complex reasoning tasks by scaling the
individual thinking process. However, prior work shows that overthinking can
degrade overall performance. Motivated by observed patterns in thinking length
and content length, we categorize reasoning into three stages: insufficient
exploration stage, compensatory reasoning stage, and reasoning convergence
stage. Typically, LLMs produce correct answers in the compensatory reasoning
stage, whereas reasoning convergence often triggers overthinking, causing
increased resource usage or even infinite loops. Therefore, mitigating
overthinking hinges on detecting the end of the compensatory reasoning stage,
defined as the Reasoning Completion Point (RCP). RCP typically appears at the
end of the first complete reasoning cycle and can be identified by querying the
LLM sentence by sentence or monitoring the probability of an end-of-thinking
token (e.g., \texttt{</think>}), though these methods lack an efficient and
precise balance. To improve this, we mine more sensitive and consistent RCP
patterns and develop a lightweight thresholding strategy based on heuristic
rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D)
demonstrate that the proposed method reduces token consumption while preserving
or enhancing reasoning accuracy.

### 30. Preference Trajectory Modeling via Flow Matching for Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Li Li, Mingyue Cheng, Yuyang Ye, Zhiding Liu, Enhong Chen
- **URL**: <http://arxiv.org/abs/2508.17618v1>
- **Submitted**: 2025-08-25 02:55:42
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on sequential recommendation, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the approach is based on diffusion models and flow matching, which is not directly related to query understanding, ranking models, or user behavior modeling. The paper's primary focus is on recommender systems, which is not my primary area of interest.

#### Abstract
> Sequential recommendation predicts each user's next item based on their
historical interaction sequence. Recently, diffusion models have attracted
significant attention in this area due to their strong ability to model user
interest distributions. They typically generate target items by denoising
Gaussian noise conditioned on historical interactions. However, these models
face two critical limitations. First, they exhibit high sensitivity to the
condition, making it difficult to recover target items from pure Gaussian
noise. Second, the inference process is computationally expensive, limiting
practical deployment. To address these issues, we propose FlowRec, a simple yet
effective sequential recommendation framework which leverages flow matching to
explicitly model user preference trajectories from current states to future
interests. Flow matching is an emerging generative paradigm, which offers
greater flexibility in initial distributions and enables more efficient
sampling. Based on this, we construct a personalized behavior-based prior
distribution to replace Gaussian noise and learn a vector field to model user
preference trajectories. To better align flow matching with the recommendation
objective, we further design a single-step alignment loss incorporating both
positive and negative samples, improving sampling efficiency and generation
quality. Extensive experiments on four benchmark datasets verify the
superiority of FlowRec over the state-of-the-art baselines.

### 31. RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zui Chen, Han Li, Xinhao Zhang, Xiaoyu Chen, Chunyin Dong, Yifeng Wang, Xin Cai, Su Zhang, Ziqi Li, Chi Ding, Jinxu Li, Shuai Wang, Dousheng Zhao, Sanhai Gao, Guangyi Liu
- **URL**: <http://arxiv.org/abs/2508.17590v1>
- **Submitted**: 2025-08-25 01:28:37
- **Comment**: 18 pages, 3 figures, 3 tables, to be submitted to VLDB 2026 (PVLDB
  Volume 19)
- **Topic Keywords**: rag, search
- **Reason**: The paper presents a novel NL2SQL system, RubikSQL, which addresses challenges in real-world enterprise-level NL2SQL. While it touches on topics related to query understanding and knowledge base maintenance, the focus is more on the NL2SQL task itself rather than ranking models or user behavior modeling, which are core interests in Information Retrieval. The paper's relevance to the user's research interests is somewhat limited, but it may still be of interest due to its connection to NLP and data mining.

#### Abstract
> We present RubikSQL, a novel NL2SQL system designed to address key challenges
in real-world enterprise-level NL2SQL, such as implicit intents and
domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning
task, demanding both Knowledge Base (KB) maintenance and SQL generation.
RubikSQL systematically builds and refines its KB through techniques including
database profiling, structured information extraction, agentic rule mining, and
Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a
multi-agent workflow to leverage this curated KB, generating accurate SQLs.
RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev
datasets. Finally, we release the RubikBench benchmark, a new benchmark
specifically designed to capture vital traits of industrial NL2SQL scenarios,
providing a valuable resource for future research.

### 32. UQ: Assessing Language Models on Unsolved Questions

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Fan Nie, Ken Ziyu Liu, Zihao Wang, Rui Sun, Wei Liu, Weijia Shi, Huaxiu Yao, Linjun Zhang, Andrew Y. Ng, James Zou, Sanmi Koyejo, Yejin Choi, Percy Liang, Niklas Muennighoff
- **URL**: <http://arxiv.org/abs/2508.17580v1>
- **Submitted**: 2025-08-25 01:07:59
- **Comment**: FN, KZL, and NM are project co-leads and contributed equally. Project
  website: https://uq.stanford.edu
- **Topic Keywords**: rag, search
- **Reason**: The paper explores a new paradigm for assessing language models on unsolved questions, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and open-ended challenges is not directly aligned with the user's primary research interests in IR and search technologies.

#### Abstract
> Benchmarks shape progress in AI research. A useful benchmark should be both
difficult and realistic: questions should challenge frontier models while also
reflecting real-world usage. Yet, current paradigms face a difficulty-realism
tension: exam-style benchmarks are often made artificially difficult with
limited real-world value, while benchmarks based on real user interaction often
skew toward easy, high-frequency problems. In this work, we explore a radically
different paradigm: assessing models on unsolved questions. Rather than a
static benchmark scored once, we curate unsolved questions and evaluate models
asynchronously over time with validator-assisted screening and community
verification. We introduce UQ, a testbed of 500 challenging, diverse questions
sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi
and history, probing capabilities including reasoning, factuality, and
browsing. UQ is difficult and realistic by construction: unsolved questions are
often hard and naturally arise when humans seek answers, thus solving them
yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset
and its collection pipeline combining rule-based filters, LLM judges, and human
review to ensure question quality (e.g., well-defined and difficult); (2)
UQ-Validators, compound validation strategies that leverage the
generator-validator gap to provide evaluation signals and pre-screen candidate
solutions for human review; and (3) UQ-Platform, an open platform where experts
collectively verify questions and solutions. The top model passes UQ-validation
on only 15% of questions, and preliminary human verification has already
identified correct answers among those that passed. UQ charts a path for
evaluating frontier models on real-world, open-ended challenges, where success
pushes the frontier of human knowledge. We release UQ at
https://uq.stanford.edu.

### 33. A Universal Framework for Offline Serendipity Evaluation in Recommender Systems via Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yu Tokutake, Kazushi Okamoto, Kei Harada, Atsushi Shibata, Koki Karube
- **URL**: <http://arxiv.org/abs/2508.17571v1>
- **Submitted**: 2025-08-25 00:45:16
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not directly address information retrieval, query understanding, or ranking models. The use of large language models is interesting, but it is not directly applicable to my research interests in IR and NLP.

#### Abstract
> Serendipity in recommender systems (RSs) has attracted increasing attention
as a concept that enhances user satisfaction by presenting unexpected and
useful items. However, evaluating serendipitous performance remains challenging
because its ground truth is generally unobservable. The existing offline
metrics often depend on ambiguous definitions or are tailored to specific
datasets and RSs, thereby limiting their generalizability. To address this
issue, we propose a universally applicable evaluation framework that leverages
large language models (LLMs) known for their extensive knowledge and reasoning
capabilities, as evaluators. First, to improve the evaluation performance of
the proposed framework, we assessed the serendipity prediction accuracy of LLMs
using four different prompt strategies on a dataset containing user-annotated
serendipitous ground truth and found that the chain-of-thought prompt achieved
the highest accuracy. Next, we re-evaluated the serendipitous performance of
both serendipity-oriented and general RSs using the proposed framework on three
commonly used real-world datasets, without the ground truth. The results
indicated that there was no serendipity-oriented RS that consistently
outperformed across all datasets, and even a general RS sometimes achieved
higher performance than the serendipity-oriented RS.

### 34. DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Aleksandar Pramov, Jiangqin Ma, Bina Patel
- **URL**: <http://arxiv.org/abs/2508.17402v1>
- **Submitted**: 2025-08-24 15:19:58
- **Comment**: CLEF 2025 Working Notes, Madrid, Spain
- **Topic Keywords**: retrieval, rank
- **Reason**: The paper focuses on claim normalization, a specific task within the broader field of Information Retrieval. While it employs a retrieval-first approach, the primary focus is on language models and veracity classification, which are not directly related to my core research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Claim normalization is an integral part of any automatic fact-check
verification system. It parses the typically noisy claim data, such as social
media posts into normalized claims, which are then fed into downstream veracity
classification tasks. The CheckThat! 2025 Task 2 focuses specifically on claim
normalization and spans 20 languages under monolingual and zero-shot
conditions. Our proposed solution consists of a lightweight
\emph{retrieval-first, LLM-backed} pipeline, in which we either dynamically
prompt a GPT-4o-mini with in-context examples, or retrieve the closest
normalization from the train dataset directly. On the official test set, the
system ranks near the top for most monolingual tracks, achieving first place in
7 out of of the 13 languages. In contrast, the system underperforms in the
zero-shot setting, highlighting the limitation of the proposed solution.

### 35. ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li
- **URL**: <http://arxiv.org/abs/2508.17892v1>
- **Submitted**: 2025-08-25 10:59:02
- **Topic Keywords**: query, ltr, retrieval
- **Reason**: This paper focuses on improving the efficiency of large language models, specifically in long-context scenarios, by introducing a novel context compression pipeline. While it's related to NLP, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests in Information Retrieval and Search technologies.

#### Abstract
> Large Language Models (LLMs) have demonstrated success across many
benchmarks. However, they still exhibit limitations in long-context scenarios,
primarily due to their short effective context length, quadratic computational
complexity, and high memory overhead when processing lengthy inputs. To
mitigate these issues, we introduce a novel context compression pipeline,
called Intermediate Layer Retrieval (ILRe), which determines one intermediate
decoder layer offline, encodes context by streaming chunked prefill only up to
that layer, and recalls tokens by the attention scores between the input query
and full key cache in that specified layer. In particular, we propose a
multi-pooling kernels allocating strategy in the token recalling process to
maintain the completeness of semantics. Our approach not only reduces the
prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance
comparable to or better than the full context in the long context scenarios.
Without additional post training or operator development, ILRe can process a
single $1M$ tokens request in less than half a minute (speedup $\approx
180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model
Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

### 36. SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Sebastian Martinez, Naman Ahuja, Fenil Bardoliya, Chris Bryan, Vivek Gupta
- **URL**: <http://arxiv.org/abs/2508.17157v1>
- **Submitted**: 2025-08-23 22:51:30
- **Comment**: Under Review at EMNLP
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper focuses on a specific domain (sports) and uses a different approach (querying and visualization of dynamic sports data) that is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions Large Language Models, the application is not in the context of query understanding, ranking models, or user behavior modeling.

#### Abstract
> We present a modular, interactive system, SPORTSQL, for natural language
querying and visualization of dynamic sports data, with a focus on the English
Premier League (EPL). The system translates user questions into executable SQL
over a live, temporally indexed database constructed from real-time Fantasy
Premier League (FPL) data. It supports both tabular and visual outputs,
leveraging the symbolic reasoning capabilities of Large Language Models (LLMs)
for query parsing, schema linking, and visualization selection. To evaluate
system performance, we introduce the Dynamic Sport Question Answering benchmark
(DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold
answers, and database snapshots. Our demo highlights how non-expert users can
seamlessly explore evolving sports statistics through a natural, conversational
interface.

### 37. ISACL: Internal State Analyzer for Copyrighted Training Data Leakage

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Guangwei Zhang, Qisheng Su, Jiateng Liu, Cheng Qian, Yanzhou Pan, Yanjie Fu, Denghui Zhang
- **URL**: <http://arxiv.org/abs/2508.17767v1>
- **Submitted**: 2025-08-25 08:04:20
- **Topic Keywords**: rag, retrieval, acl
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on Natural Language Processing, specifically on detecting copyrighted data leakage in Large Language Models, which is not a primary area of interest for you.

#### Abstract
> Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,
especially when such data is used for training but not intended for
distribution. Traditional methods address these leaks only after content is
generated, which can lead to the exposure of sensitive information. This study
introduces a proactive approach: examining LLMs' internal states before text
generation to detect potential leaks. By using a curated dataset of copyrighted
materials, we trained a neural network classifier to identify risks, allowing
for early intervention by stopping the generation process or altering outputs
to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)
system, this framework ensures adherence to copyright and licensing
requirements while enhancing data privacy and ethical standards. Our results
show that analyzing internal states effectively mitigates the risk of
copyrighted data leakage, offering a scalable solution that fits smoothly into
AI workflows, ensuring compliance with copyright regulations while maintaining
high-quality text generation. The implementation is available on
GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}

### 38. Towards a Real-World Aligned Benchmark for Unlearning in Recommender Systems

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Pierre Lubitzsch, Olga Ovcharenko, Hao Chen, Maarten de Rijke, Sebastian Schelter
- **URL**: <http://arxiv.org/abs/2508.17076v1>
- **Submitted**: 2025-08-23 16:05:40
- **Topic Keywords**: rag, recommend, search, neurips
- **Reason**: The paper focuses on recommender systems, which is a related but distinct field from information retrieval and search technologies. While it touches on the concept of 'unlearning', the paper's primary concern is not query understanding, ranking models, or user behavior modeling, which are core aspects of my research interests.

#### Abstract
> Modern recommender systems heavily leverage user interaction data to deliver
personalized experiences. However, relying on personal data presents challenges
in adhering to privacy regulations, such as the GDPR's "right to be forgotten".
Machine unlearning (MU) aims to address these challenges by enabling the
efficient removal of specific training data from models post-training, without
compromising model utility or leaving residual information. However, current
benchmarks for unlearning in recommender systems -- most notably CURE4Rec --
fail to reflect real-world operational demands. They focus narrowly on
collaborative filtering, overlook tasks like session-based and next-basket
recommendation, simulate unrealistically large unlearning requests, and ignore
critical efficiency constraints. In this paper, we propose a set of design
desiderata and research questions to guide the development of a more realistic
benchmark for unlearning in recommender systems, with the goal of gathering
feedback from the research community. Our benchmark proposal spans multiple
recommendation tasks, includes domain-specific unlearning scenarios, and
several unlearning algorithms -- including ones adapted from a recent NeurIPS
unlearning competition. Furthermore, we argue for an unlearning setup that
reflects the sequential, time-sensitive nature of real-world deletion requests.
We also present a preliminary experiment in a next-basket recommendation
setting based on our proposed desiderata and find that unlearning also works
for sequential recommendation models, exposed to many small unlearning
requests. In this case, we observe that a modification of a custom-designed
unlearning algorithm for recommender systems outperforms general unlearning
algorithms significantly, and that unlearning can be executed with a latency of
only several seconds.

### 39. SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Tong Bao, Mir Tafseer Nayeem, Davood Rafiei, Chengzhi Zhang
- **URL**: <http://arxiv.org/abs/2508.17647v1>
- **Submitted**: 2025-08-25 04:22:23
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on survey generation using large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions literature retrieval, the context is different from the user's interests in IR and NLP.

#### Abstract
> Automatic survey generation has emerged as a key task in scientific document
processing. While large language models (LLMs) have shown promise in generating
survey texts, the lack of standardized evaluation datasets critically hampers
rigorous assessment of their performance against human-written surveys. In this
work, we present SurveyGen, a large-scale dataset comprising over 4,200
human-written surveys across diverse scientific domains, along with 242,143
cited references and extensive quality-related metadata for both the surveys
and the cited papers. Leveraging this resource, we build QUAL-SG, a novel
quality-aware framework for survey generation that enhances the standard
Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware
indicators into literature retrieval to assess and select higher-quality source
papers. Using this dataset and framework, we systematically evaluate
state-of-the-art LLMs under varying levels of human involvement - from fully
automatic generation to human-guided writing. Experimental results and human
evaluations show that while semi-automatic pipelines can achieve partially
competitive outcomes, fully automatic survey generation still suffers from low
citation quality and limited critical analysis.

### 40. CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hunzalah Hassan Bhatti, Youssef Ahmed, Md Arid Hasan, Firoj Alam
- **URL**: <http://arxiv.org/abs/2508.17324v1>
- **Submitted**: 2025-08-24 12:11:21
- **Comment**: LLMs, Native, Arabic LLMs, Augmentation, Multilingual, Language
  Diversity, Contextual Understanding, Minority Languages, Culturally Informed,
  Foundation Models, Large Language Models
- **Topic Keywords**: ltr, rank
- **Reason**: The paper focuses on cultural knowledge representation and data augmentation for Arabic language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on large language models and fine-tuning is also not aligned with the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> In this paper, we report our participation to the PalmX cultural evaluation
shared task. Our system, CultranAI, focused on data augmentation and LoRA
fine-tuning of large language models (LLMs) for Arabic cultural knowledge
representation. We benchmarked several LLMs to identify the best-performing
model for the task. In addition to utilizing the PalmX dataset, we augmented it
by incorporating the Palm dataset and curated a new dataset of over 22K
culturally grounded multiple-choice questions (MCQs). Our experiments showed
that the Fanar-1-9B-Instruct model achieved the highest performance. We
fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the
blind test set, our submitted system ranked 5th with an accuracy of 70.50%,
while on the PalmX development set, it achieved an accuracy of 84.1%.

### 41. Exposing Privacy Risks in Graph Retrieval-Augmented Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jiale Liu, Jiahao Zhang, Suhang Wang
- **URL**: <http://arxiv.org/abs/2508.17222v1>
- **Submitted**: 2025-08-24 06:19:44
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is not relevant to your research interests as it focuses on privacy risks in Graph Retrieval-Augmented Generation, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for you.

#### Abstract
> Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing
Large Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has
emerged as an advanced paradigm that leverages graph-based knowledge structures
to provide more coherent and contextually rich answers. However, the move from
plain document retrieval to structured graph traversal introduces new,
under-explored privacy risks. This paper investigates the data extraction
vulnerabilities of the Graph RAG systems. We design and execute tailored data
extraction attacks to probe their susceptibility to leaking both raw text and
structured data, such as entities and their relationships. Our findings reveal
a critical trade-off: while Graph RAG systems may reduce raw text leakage, they
are significantly more vulnerable to the extraction of structured entity and
relationship information. We also explore potential defense mechanisms to
mitigate these novel attack surfaces. This work provides a foundational
analysis of the unique privacy challenges in Graph RAG and offers insights for
building more secure systems.

### 42. THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hoyoung Lee, Wonbin Ahn, Suhwan Park, Jaehoon Lee, Minjae Kim, Sungdong Yoo, Taeyoon Lim, Woohyung Lim, Yongjae Lee
- **URL**: <http://arxiv.org/abs/2508.16936v1>
- **Submitted**: 2025-08-23 08:05:37
- **Comment**: Accepted at ACM International Conference on Information and Knowledge
  Management (CIKM)
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on thematic investing and stock representation, using a hierarchical contrastive learning framework. While it involves text analysis and embedding, the context is not related to information retrieval, search technologies, or user behavior modeling, which are the primary areas of interest.

#### Abstract
> Thematic investing aims to construct portfolios aligned with structural
trends, yet selecting relevant stocks remains challenging due to overlapping
sector boundaries and evolving market dynamics. To address this challenge, we
construct the Thematic Representation Set (TRS), an extended dataset that
begins with real-world thematic ETFs and expands upon them by incorporating
industry classifications and financial news to overcome their coverage
limitations. The final dataset contains both the explicit mapping of themes to
their constituent stocks and the rich textual profiles for each. Building on
this dataset, we introduce \textsc{THEME}, a hierarchical contrastive learning
framework. By representing the textual profiles of themes and stocks as
embeddings, \textsc{THEME} first leverages their hierarchical relationship to
achieve semantic alignment. Subsequently, it refines these semantic embeddings
through a temporal refinement stage that incorporates individual stock returns.
The final stock representations are designed for effective retrieval of
thematically aligned assets with strong return potential. Empirical results
show that \textsc{THEME} outperforms strong baselines across multiple retrieval
metrics and significantly improves performance in portfolio construction. By
jointly modeling thematic relationships from text and market dynamics from
returns, \textsc{THEME} provides a scalable and adaptive solution for
navigating complex investment themes.

### 43. German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Miriam Ansch√ºtz, Thanh Mai Pham, Eslam Nasrallah, Maximilian M√ºller, Cristian-George Craciun, Georg Groh
- **URL**: <http://arxiv.org/abs/2508.17973v1>
- **Submitted**: 2025-08-25 12:40:32
- **Comment**: Accepted to INLG 2025
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on paraphrasing and text simplification in German, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on readability-controlled paraphrasing and text simplification does not align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The ability to paraphrase texts across different complexity levels is
essential for creating accessible texts that can be tailored toward diverse
reader groups. Thus, we introduce German4All, the first large-scale German
dataset of aligned readability-controlled, paragraph-level paraphrases. It
spans five readability levels and comprises over 25,000 samples. The dataset is
automatically synthesized using GPT-4 and rigorously evaluated through both
human and LLM-based judgments. Using German4All, we train an open-source,
readability-controlled paraphrasing model that achieves state-of-the-art
performance in German text simplification, enabling more nuanced and
reader-specific adaptations. We opensource both the dataset and the model to
encourage further research on multi-level paraphrasing

### 44. Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Changsong Liu, Yizhou Peng, Eng Siong Chng
- **URL**: <http://arxiv.org/abs/2508.17796v1>
- **Submitted**: 2025-08-25 08:41:52
- **Comment**: Accepted to APSIPA ASC 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on automatic speech recognition (ASR) and contextual biasing, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The techniques and methods described in the paper are not applicable to the user's areas of focus, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Contextual automatic speech recognition (ASR) systems allow for recognizing
out-of-vocabulary (OOV) words, such as named entities or rare words. However,
it remains challenging due to limited training data and ambiguous or
inconsistent pronunciations. In this paper, we propose a synthesis-driven
multi-pronunciation contextual biasing method that performs zero-shot
contextual ASR on a pretrained Whisper model. Specifically, we leverage
text-to-speech (TTS) systems to synthesize diverse speech samples containing
each target rare word, and then use the pretrained Whisper model to extract
multiple predicted pronunciation variants. These variant token sequences are
compiled into a prefix-trie, which assigns rewards to beam hypotheses in a
shallow-fusion manner during beam-search decoding. After which, any recognized
variant is mapped back to the original rare word in the final transcription.
The evaluation results on the Librispeech dataset show that our method reduces
biased word error rate (WER) by 42% on test-clean and 43% on test-other while
maintaining unbiased WER essentially unchanged.

### 45. Research on Evaluation Methods for Patent Novelty Search Systems and Empirical Analysis

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shu Zhang, LiSha Zhang, Kai Duan, XinKai Sun
- **URL**: <http://arxiv.org/abs/2508.17782v1>
- **Submitted**: 2025-08-25 08:24:04
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on patent novelty search systems, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it mentions evaluation methods and metrics, the context is specific to patent search and does not align with the user's interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> Patent novelty search systems are critical to IP protection and innovation
assessment; their retrieval accuracy directly impacts patent quality. We
propose a comprehensive evaluation methodology that builds high-quality,
reproducible datasets from examiner citations and X-type citations extracted
from technically consistent family patents, and evaluates systems using
invention descriptions as inputs. Using Top-k Detection Rate and Recall as core
metrics, we further conduct multi-dimensional analyses by language, technical
field (IPC), and filing jurisdiction. Experiments show the method effectively
exposes performance differences across scenarios and offers actionable evidence
for system improvement. The framework is scalable and practical, providing a
useful reference for development and optimization of patent novelty search
systems

### 46. EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yinda Chen, Yangfan He, Jing Yang, Dapeng Zhang, Zhenlong Yuan, Muhammad Attique Khan, Jamel Baili, Por Lip Yee
- **URL**: <http://arxiv.org/abs/2508.17703v1>
- **Submitted**: 2025-08-25 06:23:17
- **Topic Keywords**: relevance
- **Reason**: This paper focuses on prompt optimization for Large Language Models in medical applications, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, the paper's primary concern is medical knowledge and clinical utility, which is outside the user's scope.

#### Abstract
> Prompt engineering significantly influences the reliability and clinical
utility of Large Language Models (LLMs) in medical applications. Current
optimization approaches inadequately address domain-specific medical knowledge
and safety requirements. This paper introduces EMPOWER, a novel evolutionary
framework that enhances medical prompt quality through specialized
representation learning, multi-dimensional evaluation, and structure-preserving
algorithms. Our methodology incorporates: (1) a medical terminology attention
mechanism, (2) a comprehensive assessment architecture evaluating clarity,
specificity, clinical relevance, and factual accuracy, (3) a component-level
evolutionary algorithm preserving clinical reasoning integrity, and (4) a
semantic verification module ensuring adherence to medical knowledge.
Evaluation across diagnostic, therapeutic, and educational tasks demonstrates
significant improvements: 24.7% reduction in factually incorrect content, 19.6%
enhancement in domain specificity, and 15.3% higher clinician preference in
blinded evaluations. The framework addresses critical challenges in developing
clinically appropriate prompts, facilitating more responsible integration of
LLMs into healthcare settings.

### 47. TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang
- **URL**: <http://arxiv.org/abs/2508.17445v1>
- **Submitted**: 2025-08-24 16:52:37
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on policy optimization and efficacy, using a tree-based modeling approach, which is unrelated to information retrieval, search technologies, or user behavior modeling. While it mentions reinforcement learning, it's not directly applicable to query understanding, ranking models, or click models.

#### Abstract
> Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

### 48. Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuanchun Wang, Yiyang Fu, Jifan Yu, Daniel Zhang-Li, Zheyuan Zhang, Joy Lim Jia Yin, Yucheng Wang, Peng Zhou, Jing Zhang, Huiqin Liu
- **URL**: <http://arxiv.org/abs/2508.17310v1>
- **Submitted**: 2025-08-24 11:40:16
- **Comment**: 12 pages
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on handling dropouts in an interactive online course using language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in NLP and IR.

#### Abstract
> Interactive online learning environments, represented by Massive AI-empowered
Courses (MAIC), leverage LLM-driven multi-agent systems to transform passive
MOOCs into dynamic, text-based platforms, enhancing interactivity through LLMs.
This paper conducts an empirical study on a specific MAIC course to explore
three research questions about dropouts in these interactive online courses:
(1) What factors might lead to dropouts? (2) Can we predict dropouts? (3) Can
we reduce dropouts? We analyze interaction logs to define dropouts and identify
contributing factors. Our findings reveal strong links between dropout
behaviors and textual interaction patterns. We then propose a
course-progress-adaptive dropout prediction framework (CPADP) to predict
dropouts with at most 95.4% accuracy. Based on this, we design a personalized
email recall agent to re-engage at-risk students. Applied in the deployed MAIC
system with over 3,000 students, the feasibility and effectiveness of our
approach have been validated on students with diverse backgrounds.

### 49. From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sadia Sultana Chowa, Riasad Alvi, Subhey Sadi Rahman, Md Abdur Rahman, Mohaimenul Azam Khan Raiaan, Md Rafiqul Islam, Mukhtar Hussain, Sami Azam
- **URL**: <http://arxiv.org/abs/2508.17281v1>
- **Submitted**: 2025-08-24 10:02:51
- **Comment**: 40 pages, 6 figures, 10 tables. Submitted to Artificial Intelligence
  Review for peer review
- **Topic Keywords**: personalization, rank, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on AI and language models, the focus is on autonomous agents and tool users, which is outside the scope of the user's primary research interests.

#### Abstract
> The pursuit of human-level artificial intelligence (AI) has significantly
advanced the development of autonomous agents and Large Language Models (LLMs).
LLMs are now widely utilized as decision-making agents for their ability to
interpret instructions, manage sequential tasks, and adapt through feedback.
This review examines recent developments in employing LLMs as autonomous agents
and tool users and comprises seven research questions. We only used the papers
published between 2023 and 2025 in conferences of the A* and A rank and Q1
journals. A structured analysis of the LLM agents' architectural design
principles, dividing their applications into single-agent and multi-agent
systems, and strategies for integrating external tools is presented. In
addition, the cognitive mechanisms of LLM, including reasoning, planning, and
memory, and the impact of prompting methods and fine-tuning procedures on agent
performance are also investigated. Furthermore, we evaluated current benchmarks
and assessment protocols and have provided an analysis of 68 publicly available
datasets to assess the performance of LLM-based agents in various tasks. In
conducting this review, we have identified critical findings on verifiable
reasoning of LLMs, the capacity for self-improvement, and the personalization
of LLM-based agents. Finally, we have discussed ten future research directions
to overcome these gaps.

### 50. ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Siying Zhou, Yiquan Wu, Hui Chen, Xavier Hu, Kun Kuang, Adam Jatowt, Ming Hu, Chunyan Zheng, Fei Wu
- **URL**: <http://arxiv.org/abs/2508.17234v1>
- **Submitted**: 2025-08-24 07:19:25
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on legal claim generation, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves language models, the context is specific to legal domain and does not align with the user's research interests.

#### Abstract
> Legal claims refer to the plaintiff's demands in a case and are essential to
guiding judicial reasoning and case resolution. While many works have focused
on improving the efficiency of legal professionals, the research on helping
non-professionals (e.g., plaintiffs) remains unexplored. This paper explores
the problem of legal claim generation based on the given case's facts. First,
we construct ClaimGen-CN, the first dataset for Chinese legal claim generation
task, from various real-world legal disputes. Additionally, we design an
evaluation metric tailored for assessing the generated claims, which
encompasses two essential dimensions: factuality and clarity. Building on this,
we conduct a comprehensive zero-shot evaluation of state-of-the-art general and
legal-domain large language models. Our findings highlight the limitations of
the current models in factual precision and expressive clarity, pointing to the
need for more targeted development in this domain. To encourage further
exploration of this important task, we will make the dataset publicly
available.

---

