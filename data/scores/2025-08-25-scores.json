[
    {
        "title": "Test-Time Scaling Strategies for Generative Retrieval in Multimodal Conversational Recommendations",
        "abstract": "The rapid evolution of e-commerce has exposed the limitations of traditional\nproduct retrieval systems in managing complex, multi-turn user interactions.\nRecent advances in multimodal generative retrieval -- particularly those\nleveraging multimodal large language models (MLLMs) as retrievers -- have shown\npromise. However, most existing methods are tailored to single-turn scenarios\nand struggle to model the evolving intent and iterative nature of multi-turn\ndialogues when applied naively. Concurrently, test-time scaling has emerged as\na powerful paradigm for improving large language model (LLM) performance\nthrough iterative inference-time refinement. Yet, its effectiveness typically\nrelies on two conditions: (1) a well-defined problem space (e.g., mathematical\nreasoning), and (2) the model's ability to self-correct -- conditions that are\nrarely met in conversational product search. In this setting, user queries are\noften ambiguous and evolving, and MLLMs alone have difficulty grounding\nresponses in a fixed product corpus. Motivated by these challenges, we propose\na novel framework that introduces test-time scaling into conversational\nmultimodal product retrieval. Our approach builds on a generative retriever,\nfurther augmented with a test-time reranking (TTR) mechanism that improves\nretrieval accuracy and better aligns results with evolving user intent\nthroughout the dialogue. Experiments across multiple benchmarks show consistent\nimprovements, with average gains of 14.5 points in MRR and 10.6 points in\nnDCG@1.",
        "url": "http://arxiv.org/abs/2508.18132v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18132v1",
        "arxiv_id": "2508.18132v1",
        "authors": [
            "Hung-Chun Hsu",
            "Yuan-Ching Kuo",
            "Chao-Han Huck Yang",
            "Szu-Wei Fu",
            "Hanrong Ye",
            "Hongxu Yin",
            "Yu-Chiang Frank Wang",
            "Ming-Feng Tsai",
            "Chuan-Ju Wang"
        ],
        "submitted": "2025-08-25 15:38:56",
        "source": "arxiv",
        "comment": null,
        "score": 21,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores multimodal conversational recommendations, which is related to information retrieval and search technologies. However, the focus on generative retrieval and test-time scaling is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on multimodal large language models and conversational product search is also outside the user's primary focus on e-commerce and deep semantic understanding."
    },
    {
        "title": "DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation",
        "abstract": "Large Language Models (LLMs) have transformed listwise document reranking by\nenabling global reasoning over candidate sets, yet single models often struggle\nto balance fine-grained relevance scoring with holistic cross-document\nanalysis. We propose \\textbf{De}ep\\textbf{A}gent\\textbf{R}ank (\\textbf{\\DeAR}),\nan open-source framework that decouples these tasks through a dual-stage\napproach, achieving superior accuracy and interpretability. In \\emph{Stage 1},\nwe distill token-level relevance signals from a frozen 13B LLaMA teacher into a\ncompact \\{3, 8\\}B student model using a hybrid of cross-entropy, RankNet, and\nKL divergence losses, ensuring robust pointwise scoring. In \\emph{Stage 2}, we\nattach a second LoRA adapter and fine-tune on 20K GPT-4o-generated\nchain-of-thought permutations, enabling listwise reasoning with\nnatural-language justifications. Evaluated on TREC-DL19/20, eight BEIR\ndatasets, and NovelEval-2306, \\DeAR surpasses open-source baselines by +5.1\nnDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by\n+3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA,\nachieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like\nMonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures\nstable calibration, making \\DeAR a highly effective and interpretable solution\nfor modern reranking systems.\\footnote{Dataset and code available at\nhttps://github.com/DataScienceUIBK/DeAR-Reranking.}.",
        "url": "http://arxiv.org/abs/2508.16998v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16998v1",
        "arxiv_id": "2508.16998v1",
        "authors": [
            "Abdelrahman Abdallah",
            "Jamshid Mozafari",
            "Bhawna Piryani",
            "Adam Jatowt"
        ],
        "submitted": "2025-08-23 11:46:08",
        "source": "arxiv",
        "comment": "Accept at EMNLP Findings 2025",
        "score": 18,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pointwise' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper proposes a novel approach to document reranking, leveraging large language models for global reasoning and fine-grained relevance scoring. The focus on listwise reasoning and natural-language justifications aligns with your interest in query understanding and ranking models. The application to information retrieval and open-domain QA also falls within your scope."
    },
    {
        "title": "How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models",
        "abstract": "In this work, we present a systematic and comprehensive empirical evaluation\nof state-of-the-art reranking methods, encompassing large language model\n(LLM)-based, lightweight contextual, and zero-shot approaches, with respect to\ntheir performance in information retrieval tasks. We evaluate in total 22\nmethods, including 40 variants (depending on used LLM) across several\nestablished benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel\ndataset designed to test queries unseen by pretrained models. Our primary goal\nis to determine, through controlled and fair comparisons, whether a performance\ndisparity exists between LLM-based rerankers and their lightweight\ncounterparts, particularly on novel queries, and to elucidate the underlying\ncauses of any observed differences. To disentangle confounding factors, we\nanalyze the effects of training data overlap, model architecture, and\ncomputational efficiency on reranking performance. Our findings indicate that\nwhile LLM-based rerankers demonstrate superior performance on familiar queries,\ntheir generalization ability to novel queries varies, with lightweight models\noffering comparable efficiency. We further identify that the novelty of queries\nsignificantly impacts reranking effectiveness, highlighting limitations in\nexisting approaches.\nhttps://github.com/DataScienceUIBK/llm-reranking-generalization-study",
        "url": "http://arxiv.org/abs/2508.16757v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16757v1",
        "arxiv_id": "2508.16757v1",
        "authors": [
            "Abdelrahman Abdallah",
            "Bhawna Piryani",
            "Jamshid Mozafari",
            "Mohammed Ali",
            "Adam Jatowt"
        ],
        "submitted": "2025-08-22 19:30:04",
        "source": "arxiv",
        "comment": "EMNLP Findings 2025",
        "score": 17,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper evaluates state-of-the-art reranking methods, including LLM-based approaches, in information retrieval tasks, which aligns with your interest in query understanding and ranking models. The study's focus on empirical analysis and performance evaluation on established benchmarks and a novel dataset also resonates with your background in e-commerce and interest in real-time relevance optimization."
    },
    {
        "title": "LexSemBridge: Fine-Grained Dense Representation Enhancement through Token-Aware Embedding Augmentation",
        "abstract": "As queries in retrieval-augmented generation (RAG) pipelines powered by large\nlanguage models (LLMs) become increasingly complex and diverse, dense retrieval\nmodels have demonstrated strong performance in semantic matching. Nevertheless,\nthey often struggle with fine-grained retrieval tasks, where precise keyword\nalignment and span-level localization are required, even in cases with high\nlexical overlap that would intuitively suggest easier retrieval. To\nsystematically evaluate this limitation, we introduce two targeted tasks,\nkeyword retrieval and part-of-passage retrieval, designed to simulate practical\nfine-grained scenarios. Motivated by these observations, we propose\nLexSemBridge, a unified framework that enhances dense query representations\nthrough fine-grained, input-aware vector modulation. LexSemBridge constructs\nlatent enhancement vectors from input tokens using three paradigms: Statistical\n(SLR), Learned (LLR), and Contextual (CLR), and integrates them with dense\nembeddings via element-wise interaction. Theoretically, we show that this\nmodulation preserves the semantic direction while selectively amplifying\ndiscriminative dimensions. LexSemBridge operates as a plug-in without modifying\nthe backbone encoder and naturally extends to both text and vision modalities.\nExtensive experiments across semantic and fine-grained retrieval tasks validate\nthe effectiveness and generality of our approach. All code and models are\npublicly available at https://github.com/Jasaxion/LexSemBridge/",
        "url": "http://arxiv.org/abs/2508.17858v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17858v1",
        "arxiv_id": "2508.17858v1",
        "authors": [
            "Shaoxiong Zhan",
            "Hai Lin",
            "Hongming Tan",
            "Xiaodong Cai",
            "Hai-Tao Zheng",
            "Xin Su",
            "Zifei Shan",
            "Ruitong Liu",
            "Hong-Gee Kim"
        ],
        "submitted": "2025-08-25 10:07:36",
        "source": "arxiv",
        "comment": null,
        "score": 16,
        "keyword_reasons": [
            "Found 'passage retrieval' (score: +3)",
            "Found 'dense retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper proposes a framework for enhancing dense query representations through fine-grained, input-aware vector modulation, which is relevant to information retrieval and query understanding. The focus on dense retrieval models and fine-grained retrieval tasks aligns with the user's interests in ranking models and user behavior modeling. However, the paper's primary focus on dense retrieval models and its application to retrieval-augmented generation pipelines may not be directly related to the user's background in e-commerce and NLP."
    },
    {
        "title": "DiffusionGS: Generative Search with Query Conditioned Diffusion in Kuaishou",
        "abstract": "Personalized search ranking systems are critical for driving engagement and\nrevenue in modern e-commerce and short-video platforms. While existing methods\nexcel at estimating users' broad interests based on the filtered historical\nbehaviors, they typically under-exploit explicit alignment between a user's\nreal-time intent (represented by the user query) and their past actions. In\nthis paper, we propose DiffusionGS, a novel and scalable approach powered by\ngenerative models. Our key insight is that user queries can serve as explicit\nintent anchors to facilitate the extraction of users' immediate interests from\nlong-term, noisy historical behaviors. Specifically, we formulate interest\nextraction as a conditional denoising task, where the user's query guides a\nconditional diffusion process to produce a robust, user intent-aware\nrepresentation from their behavioral sequence. We propose the User-aware\nDenoising Layer (UDL) to incorporate user-specific profiles into the\noptimization of attention distribution on the user's past actions. By reframing\nqueries as intent priors and leveraging diffusion-based denoising, our method\nprovides a powerful mechanism for capturing dynamic user interest shifts.\nExtensive offline and online experiments demonstrate the superiority of\nDiffusionGS over state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.17754v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17754v1",
        "arxiv_id": "2508.17754v1",
        "authors": [
            "Qinyao Li",
            "Xiaoyang Zheng",
            "Qihang Zhao",
            "Ke Xu",
            "Zhongbo Sun",
            "Chao Wang",
            "Chenyi Lei",
            "Han Li",
            "Wenwu Ou"
        ],
        "submitted": "2025-08-25 07:46:51",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper proposes a novel approach to personalized search ranking, leveraging user queries as intent anchors to extract immediate interests from historical behaviors. The use of generative models and conditional denoising tasks aligns with your interests in query understanding, ranking models, and user behavior modeling. While the focus is on e-commerce and short-video platforms, the concepts and techniques explored are relevant to your broader interests in information retrieval and natural language processing."
    },
    {
        "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for Ultra-Long User Behavior Modeling",
        "abstract": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
        "url": "http://arxiv.org/abs/2508.17125v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17125v1",
        "arxiv_id": "2508.17125v1",
        "authors": [
            "Kaiyuan Li",
            "Yongxiang Tang",
            "Yanhua Cheng",
            "Yong Bai",
            "Yanxiang Zeng",
            "Chao Wang",
            "Xialong Liu",
            "Peng Jiang"
        ],
        "submitted": "2025-08-23 19:58:18",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a novel framework for ultra-long user behavior modeling, focusing on recommender systems. While it's not directly related to query understanding, ranking models, or user behavior modeling in the context of information retrieval, it does explore attention mechanisms and compression techniques, which are relevant to my interests in NLP and data mining. However, the paper's primary focus on recommender systems and ultra-long sequence recommendation makes it only somewhat related to my research themes."
    },
    {
        "title": "GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation",
        "abstract": "Retrieval-Augmented Generation (RAG) systems are widely adopted in\nknowledge-intensive NLP tasks, but current evaluations often overlook the\nstructural complexity and multi-step reasoning required in real-world\nscenarios. These benchmarks overlook key factors such as the interaction\nbetween retrieval difficulty and reasoning depth. To address this gap, we\npropose \\textsc{GRADE}, a novel evaluation framework that models task\ndifficulty along two orthogonal dimensions: (1) reasoning depth, defined by the\nnumber of inference steps (hops), and (2) semantic distance between the query\nand its supporting evidence. We construct a synthetic multi-hop QA dataset from\nfactual news articles by extracting knowledge graphs and augmenting them\nthrough semantic clustering to recover missing links, allowing us to generate\ndiverse and difficulty-controlled queries. Central to our framework is a 2D\ndifficulty matrix that combines generator-side and retriever-side difficulty.\nExperiments across multiple domains and models show that error rates strongly\ncorrelate with our difficulty measures, validating their diagnostic utility.\n\\textsc{GRADE} enables fine-grained analysis of RAG performance and provides a\nscalable foundation for evaluating and improving multi-hop reasoning in\nreal-world applications.",
        "url": "http://arxiv.org/abs/2508.16994v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16994v1",
        "arxiv_id": "2508.16994v1",
        "authors": [
            "Jeongsoo Lee",
            "Daeyong Kwon",
            "Kyohoon Jin"
        ],
        "submitted": "2025-08-23 11:26:41",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 findings",
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel evaluation framework for Retrieval-Augmented Generation (RAG) systems, focusing on multi-hop QA and fine-grained difficulty modeling. While it touches on information retrieval and NLP, the primary focus is on evaluation and benchmarking, which is not directly aligned with your interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Semantic Search for Information Retrieval",
        "abstract": "Information retrieval systems have progressed notably from lexical techniques\nsuch as BM25 and TF-IDF to modern semantic retrievers. This survey provides a\nbrief overview of the BM25 baseline, then discusses the architecture of modern\nstate-of-the-art semantic retrievers. Advancing from BERT, we introduce dense\nbi-encoders (DPR), late-interaction models (ColBERT), and neural sparse\nretrieval (SPLADE). Finally, we examine MonoT5, a cross-encoder model. We\nconclude with common evaluation tactics, pressing challenges, and propositions\nfor future directions.",
        "url": "http://arxiv.org/abs/2508.17694v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17694v1",
        "arxiv_id": "2508.17694v1",
        "authors": [
            "Kayla Farivar"
        ],
        "submitted": "2025-08-25 06:03:26",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retriever' (score: +3)",
            "Found 'semantic search' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper is highly relevant to Information Retrieval, specifically focusing on semantic search and modern state-of-the-art retrievers. The discussion of various models, including BERT, DPR, ColBERT, and SPLADE, aligns with the user's interest in query understanding and ranking models. However, the paper does not explicitly mention user behavior modeling or click models, which might reduce its score."
    },
    {
        "title": "How Do LLM-Generated Texts Impact Term-Based Retrieval Models?",
        "abstract": "As more content generated by large language models (LLMs) floods into the\nInternet, information retrieval (IR) systems now face the challenge of\ndistinguishing and handling a blend of human-authored and machine-generated\ntexts. Recent studies suggest that neural retrievers may exhibit a preferential\ninclination toward LLM-generated content, while classic term-based retrievers\nlike BM25 tend to favor human-written documents. This paper investigates the\ninfluence of LLM-generated content on term-based retrieval models, which are\nvalued for their efficiency and robust generalization across domains. Our\nlinguistic analysis reveals that LLM-generated texts exhibit smoother\nhigh-frequency and steeper low-frequency Zipf slopes, higher term specificity,\nand greater document-level diversity. These traits are aligned with LLMs being\ntrained to optimize reader experience through diverse and precise expressions.\nOur study further explores whether term-based retrieval models demonstrate\nsource bias, concluding that these models prioritize documents whose term\ndistributions closely correspond to those of the queries, rather than\ndisplaying an inherent source bias. This work provides a foundation for\nunderstanding and addressing potential biases in term-based IR systems managing\nmixed-source content.",
        "url": "http://arxiv.org/abs/2508.17715v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17715v1",
        "arxiv_id": "2508.17715v1",
        "authors": [
            "Wei Huang",
            "Keping Bi",
            "Yinqiong Cai",
            "Wei Chen",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "submitted": "2025-08-25 06:43:27",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper explores the impact of LLM-generated texts on term-based retrieval models, which is a relevant topic in Information Retrieval. The study's focus on query understanding, ranking models, and user behavior modeling aligns with your research interests. However, the paper's scope is more specific to term-based retrieval models and their biases, which is somewhat related but not a central match to your primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Demographically-Inspired Query Variants Using an LLM",
        "abstract": "This study proposes a method to diversify queries in existing test\ncollections to reflect some of the diversity of search engine users, aligning\nwith an earlier vision of an 'ideal' test collection. A Large Language Model\n(LLM) is used to create query variants: alternative queries that have the same\nmeaning as the original. These variants represent user profiles characterised\nby different properties, such as language and domain proficiency, which are\nknown in the IR literature to influence query formulation.\n  The LLM's ability to generate query variants that align with user profiles is\nempirically validated, and the variants' utility is further explored for IR\nsystem evaluation. Results demonstrate that the variants impact how systems are\nranked and show that user profiles experience significantly different levels of\nsystem effectiveness. This method enables an alternative perspective on system\nevaluation where we can observe both the impact of user profiles on system\nrankings and how system performance varies across users.",
        "url": "http://arxiv.org/abs/2508.17644v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17644v1",
        "arxiv_id": "2508.17644v1",
        "authors": [
            "Marwah Alaofi",
            "Nicola Ferro",
            "Paul Thomas",
            "Falk Scholer",
            "Mark Sanderson"
        ],
        "submitted": "2025-08-25 04:17:56",
        "source": "arxiv",
        "comment": "Published in the proceedings of ICTIR'25, Padua, Italy",
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores query diversification using a Large Language Model (LLM), which aligns with the user's interest in query understanding and ranking models. The study's focus on user profiles and their impact on system evaluation is also relevant to the user's interest in user behavior modeling. However, the paper's scope is more focused on query diversification and system evaluation, which is somewhat related but not a central match to the user's primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains",
        "abstract": "Large reasoning models (LRMs) have shown significant progress in test-time\nscaling through chain-of-thought prompting. Current approaches like search-o1\nintegrate retrieval augmented generation (RAG) into multi-step reasoning\nprocesses but rely on a single, linear reasoning chain while incorporating\nunstructured textual information in a flat, context-agnostic manner. As a\nresult, these approaches can lead to error accumulation throughout the\nreasoning chain, which significantly limits its effectiveness in medical\nquestion-answering (QA) tasks where both accuracy and traceability are critical\nrequirements. To address these challenges, we propose MIRAGE (Multi-chain\nInference with Retrieval-Augmented Graph Exploration), a novel test-time\nscalable reasoning framework that performs dynamic multi-chain inference over\nstructured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex\nqueries into entity-grounded sub-questions, 2) executes parallel inference\nchains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop\ntraversal, and 4) integrates answers using cross-chain verification to resolve\ncontradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k,\nCMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o,\nTree-of-Thought variants, and other retrieval-augmented baselines in both\nautomatic and human evaluations. Additionally, MIRAGE improves interpretability\nby generating explicit reasoning chains that trace each factual claim to\nconcrete chains within the knowledge graph, making it well-suited for complex\nmedical reasoning scenarios. The code will be available for further research.",
        "url": "http://arxiv.org/abs/2508.18260v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18260v1",
        "arxiv_id": "2508.18260v1",
        "authors": [
            "Kaiwen Wei",
            "Rui Shan",
            "Dongsheng Zou",
            "Jianzhong Yang",
            "Bi Zhao",
            "Junnan Zhu",
            "Jiang Zhong"
        ],
        "submitted": "2025-08-25 17:53:22",
        "source": "arxiv",
        "comment": "10 pages, 8 figures (including tables), plus appendix. Submitted to\n  AAAI 2026",
        "score": 10,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel framework for test-time scalable reasoning, focusing on medical question-answering tasks. While it involves retrieval and graph exploration, the primary focus is on reasoning and knowledge graph traversal, which is not directly related to information retrieval, query understanding, or ranking models, which are core areas of interest in your research."
    },
    {
        "title": "Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering",
        "abstract": "We present a case study evaluating large language models (LLMs) with\n128K-token context windows on a technical question answering (QA) task. Our\nbenchmark is built on a user manual for an agricultural machine, available in\nEnglish, French, and German. It simulates a cross-lingual information retrieval\nscenario where questions are posed in English against all three language\nversions of the manual. The evaluation focuses on realistic\n\"needle-in-a-haystack\" challenges and includes unanswerable questions to test\nfor hallucinations. We compare nine long-context LLMs using direct prompting\nagainst three Retrieval-Augmented Generation (RAG) strategies (keyword,\nsemantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this\nspecific manual show that Hybrid RAG consistently outperforms direct\nlong-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5\n7B achieve high accuracy (over 85%) across all languages with RAG. This paper\ncontributes a detailed analysis of LLM performance in a specialized industrial\ndomain and an open framework for similar evaluations, highlighting practical\ntrade-offs and challenges.",
        "url": "http://arxiv.org/abs/2508.18093v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18093v1",
        "arxiv_id": "2508.18093v1",
        "authors": [
            "Julius Gun",
            "Timo Oksanen"
        ],
        "submitted": "2025-08-25 14:54:46",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of large language models and Retrieval-Augmented Generation strategies for technical question answering in a cross-lingual information retrieval scenario. While it touches on information retrieval and query understanding, the focus is on a specific domain (agricultural machine manuals) and does not directly relate to the user's primary interests in e-commerce, ranking models, or user behavior modeling."
    },
    {
        "title": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data",
        "abstract": "User queries in real-world recommendation systems often combine structured\nconstraints (e.g., category, attributes) with unstructured preferences (e.g.,\nproduct descriptions or reviews). We introduce HyST (Hybrid retrieval over\nSemi-structured Tabular data), a hybrid retrieval framework that combines\nLLM-powered structured filtering with semantic embedding search to support\ncomplex information needs over semi-structured tabular data. HyST extracts\nattribute-level constraints from natural language using large language models\n(LLMs) and applies them as metadata filters, while processing the remaining\nunstructured query components via embedding-based retrieval. Experiments on a\nsemi-structured benchmark show that HyST consistently outperforms tradtional\nbaselines, highlighting the importance of structured filtering in improving\nretrieval precision, offering a scalable and accurate solution for real-world\nuser queries.",
        "url": "http://arxiv.org/abs/2508.18048v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18048v1",
        "arxiv_id": "2508.18048v1",
        "authors": [
            "Jiyoon Myung",
            "Jihyeon Park",
            "Joohyung Han"
        ],
        "submitted": "2025-08-25 14:06:27",
        "source": "arxiv",
        "comment": "Accepted at the 2nd EARL Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models (RecSys 2025)",
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper's focus on hybrid retrieval over semi-structured tabular data, combining structured filtering with semantic embedding search, aligns with your interests in Information Retrieval and Search technologies. The use of large language models (LLMs) for attribute-level constraints extraction and metadata filtering is also relevant to your query understanding and ranking models research. However, the paper's primary focus on recommender systems and semi-structured tabular data may not be directly applicable to your e-commerce domain expertise."
    },
    {
        "title": "Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation",
        "abstract": "Rapid advances in Multimodal Large Language Models (MLLMs) have expanded\ninformation retrieval beyond purely textual inputs, enabling retrieval from\ncomplex real world documents that combine text and visuals. However, most\ndocuments are private either owned by individuals or confined within corporate\nsilos and current retrievers struggle when faced with unseen domains or\nlanguages. To address this gap, we introduce PREMIR, a simple yet effective\nframework that leverages the broad knowledge of an MLLM to generate cross modal\npre questions (preQs) before retrieval. Unlike earlier multimodal retrievers\nthat compare embeddings in a single vector space, PREMIR leverages preQs from\nmultiple complementary modalities to expand the scope of matching to the token\nlevel. Experiments show that PREMIR achieves state of the art performance on\nout of distribution benchmarks, including closed domain and multilingual\nsettings, outperforming strong baselines across all retrieval metrics. We\nconfirm the contribution of each component through in depth ablation studies,\nand qualitative analyses of the generated preQs further highlight the model's\nrobustness in real world settings.",
        "url": "http://arxiv.org/abs/2508.17079v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17079v1",
        "arxiv_id": "2508.17079v1",
        "authors": [
            "Yejin Choi",
            "Jaewoo Park",
            "Janghan Yoon",
            "Saejin Kim",
            "Jaehyun Jeon",
            "Youngjae Yu"
        ],
        "submitted": "2025-08-23 16:14:41",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores multimodal document retrieval, which is related to information retrieval and search technologies. The use of cross-modal question generation and leveraging multiple modalities is an innovative approach that aligns with the user's interest in query understanding and ranking models. However, the paper's focus on multimodal retrieval and its application to real-world documents may not be directly applicable to the user's specific interests in e-commerce and user behavior modeling."
    },
    {
        "title": "Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across\ndiverse tasks, yet they face inherent limitations such as constrained\nparametric knowledge and high retraining costs. Retrieval-Augmented Generation\n(RAG) augments the generation process by retrieving externally stored knowledge\nabsent from the models internal parameters. However, RAG methods face\nchallenges such as information loss and redundant retrievals during multi-round\nqueries, accompanying the difficulties in precisely characterizing knowledge\ngaps for complex tasks. To address these problems, we propose Retrieval\nFeedback and Memory Retrieval Augmented Generation(RFM-RAG), which transforms\nthe stateless retrieval of previous methods into stateful continuous knowledge\nmanagement by constructing a dynamic evidence pool. Specifically, our method\ngenerates refined queries describing the models knowledge gaps using relational\ntriples from questions and evidence from the dynamic evidence pool; Retrieves\ncritical external knowledge to iteratively update this evidence pool; Employs a\nR-Feedback Model to evaluate evidence completeness until convergence. Compared\nto traditional RAG methods, our approach enables persistent storage of\nretrieved passages and effectively distills key information from passages to\nconstruct clearly new queries. Experiments on three public QA benchmarks\ndemonstrate that RFM-RAG outperforms previous methods and improves overall\nsystem accuracy.",
        "url": "http://arxiv.org/abs/2508.17862v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17862v1",
        "arxiv_id": "2508.17862v1",
        "authors": [
            "Leqian Li",
            "Dianxi Shi",
            "Jialu Zhou",
            "Xinyu Wei",
            "Mingyue Yang",
            "Songchang Jin",
            "Shaowu Yang"
        ],
        "submitted": "2025-08-25 10:13:02",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a method for retrieval-augmented generation, which is related to information retrieval and natural language processing. However, the focus is on large language models and knowledge management, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Bootstrapping Conditional Retrieval for User-to-Item Recommendations",
        "abstract": "User-to-item retrieval has been an active research area in recommendation\nsystem, and two tower models are widely adopted due to model simplicity and\nserving efficiency. In this work, we focus on a variant called\n\\textit{conditional retrieval}, where we expect retrieved items to be relevant\nto a condition (e.g. topic). We propose a method that uses the same training\ndata as standard two tower models but incorporates item-side information as\nconditions in query. This allows us to bootstrap new conditional retrieval use\ncases and encourages feature interactions between user and condition.\nExperiments show that our method can retrieve highly relevant items and\noutperforms standard two tower models with filters on engagement metrics. The\nproposed model is deployed to power a topic-based notification feed at\nPinterest and led to +0.26\\% weekly active users.",
        "url": "http://arxiv.org/abs/2508.16793v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16793v1",
        "arxiv_id": "2508.16793v1",
        "authors": [
            "Hongtao Lin",
            "Haoyu Chen",
            "Jaewon Jang",
            "Jiajing Xu"
        ],
        "submitted": "2025-08-22 20:50:52",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on user-to-item retrieval and conditional retrieval, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and the use case of a topic-based notification feed at Pinterest is not directly aligned with my primary focus on deep semantic understanding and real-time relevance optimization in IR."
    },
    {
        "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models",
        "abstract": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
        "url": "http://arxiv.org/abs/2508.17892v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17892v1",
        "arxiv_id": "2508.17892v1",
        "authors": [
            "Manlai Liang",
            "Mandi Liu",
            "Jiangzhou Ji",
            "Huaijun Li",
            "Haobo Yang",
            "Yaohan He",
            "Jinlong Li"
        ],
        "submitted": "2025-08-25 10:59:02",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ltr' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the efficiency of large language models, specifically in long-context scenarios, by introducing a novel context compression pipeline. While it's related to NLP, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests in Information Retrieval and Search technologies."
    },
    {
        "title": "SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization",
        "abstract": "We present a modular, interactive system, SPORTSQL, for natural language\nquerying and visualization of dynamic sports data, with a focus on the English\nPremier League (EPL). The system translates user questions into executable SQL\nover a live, temporally indexed database constructed from real-time Fantasy\nPremier League (FPL) data. It supports both tabular and visual outputs,\nleveraging the symbolic reasoning capabilities of Large Language Models (LLMs)\nfor query parsing, schema linking, and visualization selection. To evaluate\nsystem performance, we introduce the Dynamic Sport Question Answering benchmark\n(DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold\nanswers, and database snapshots. Our demo highlights how non-expert users can\nseamlessly explore evolving sports statistics through a natural, conversational\ninterface.",
        "url": "http://arxiv.org/abs/2508.17157v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17157v1",
        "arxiv_id": "2508.17157v1",
        "authors": [
            "Sebastian Martinez",
            "Naman Ahuja",
            "Fenil Bardoliya",
            "Chris Bryan",
            "Vivek Gupta"
        ],
        "submitted": "2025-08-23 22:51:30",
        "source": "arxiv",
        "comment": "Under Review at EMNLP",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific domain (sports) and uses a different approach (querying and visualization of dynamic sports data) that is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions Large Language Models, the application is not in the context of query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation",
        "abstract": "Retrieval-augmented generation (RAG) has become a widely recognized paradigm\nto combine parametric memory with non-parametric memories. An RAG model\nconsists of two serial connecting components (retriever and generator). A major\nchallenge in end-to-end optimization of the RAG model is that marginalization\nover relevant passages (modeled as discrete latent variables) from a knowledge\nbase is required. Traditional top-K marginalization and variational RAG (VRAG)\nsuffer from biased or high-variance gradient estimates. In this paper, we\npropose and develop joint stochastic approximation (JSA) based end-to-end\ntraining of RAG, which is referred to as JSA-RAG. The JSA algorithm is a\nstochastic extension of the EM (expectation-maximization) algorithm and is\nparticularly powerful in estimating discrete latent variable models. Extensive\nexperiments are conducted on five datasets for two tasks (open-domain question\nanswering, knowledge-grounded dialogs) and show that JSA-RAG significantly\noutperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of\nJSA-RAG from the perspectives of generation, retrieval, and low-variance\ngradient estimate.",
        "url": "http://arxiv.org/abs/2508.18168v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18168v1",
        "arxiv_id": "2508.18168v1",
        "authors": [
            "Hongyu Cao",
            "Yuxuan Wu",
            "Yucheng Cai",
            "Xianyu Zhao",
            "Zhijian Ou"
        ],
        "submitted": "2025-08-25 16:17:16",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on retrieval-augmented generation models, which is a topic in Natural Language Processing (NLP). While it shares some similarities with information retrieval, the primary focus is on generation and retrieval models rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is limited, but it may still be of interest to someone with a broader background in NLP and related topics."
    },
    {
        "title": "ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation",
        "abstract": "Knowledge gaps and hallucinations are persistent challenges for Large\nLanguage Models (LLMs), which generate unreliable responses when lacking the\nnecessary information to fulfill user instructions. Existing approaches, such\nas Retrieval-Augmented Generation (RAG) and tool use, aim to address these\nissues by incorporating external knowledge. Yet, they rely on additional models\nor services, resulting in complex pipelines, potential error propagation, and\noften requiring the model to process a large number of tokens. In this paper,\nwe present a scalable method that enables LLMs to access external knowledge\nwithout depending on retrievers or auxiliary models. Our approach uses\nconstrained generation with a pre-built prefix-tree index. Triples from a\nKnowledge Graph are verbalized in textual facts, tokenized, and indexed in a\nprefix tree for efficient access. During inference, to acquire external\nknowledge, the LLM generates facts with constrained generation which allows\nonly sequences of tokens that form an existing fact. We evaluate our proposal\non Question Answering and show that it scales to large knowledge bases (800\nmillion facts), adapts to domain-specific data, and achieves effective results.\nThese gains come with minimal generation-time overhead. ReFactX code is\navailable at https://github.com/rpo19/ReFactX.",
        "url": "http://arxiv.org/abs/2508.16983v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16983v1",
        "arxiv_id": "2508.16983v1",
        "authors": [
            "Riccardo Pozzi",
            "Matteo Palmonari",
            "Andrea Coletta",
            "Luigi Bellomarini",
            "Jens Lehmann",
            "Sahar Vahdati"
        ],
        "submitted": "2025-08-23 10:21:47",
        "source": "arxiv",
        "comment": "19 pages, 6 figures, accepted at ISWC",
        "score": 7,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a method for LLMs to access external knowledge without relying on retrievers or auxiliary models, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on language models and knowledge graph verbalization, which is not directly aligned with the user's interests in search technologies and user behavior modeling."
    },
    {
        "title": "Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities in natural\nlanguage processing but still struggle to perform well on knowledge-intensive\ntasks that require deep reasoning and the integration of external knowledge.\nAlthough methods such as Retrieval-Augmented Generation (RAG) and\nChain-of-Thought (CoT) have been proposed to enhance LLMs with external\nknowledge, they still suffer from internal bias in LLMs, which often leads to\nincorrect answers. In this paper, we propose a novel causal prompting\nframework, Conditional Front-Door Prompting (CFD-Prompting), which enables the\nunbiased estimation of the causal effect between the query and the answer,\nconditional on external knowledge, while mitigating internal bias. By\nconstructing counterfactual external knowledge, our framework simulates how the\nquery behaves under varying contexts, addressing the challenge that the query\nis fixed and is not amenable to direct causal intervention. Compared to the\nstandard front-door adjustment, the conditional variant operates under weaker\nassumptions, enhancing both robustness and generalisability of the reasoning\nprocess. Extensive experiments across multiple LLMs and benchmark datasets\ndemonstrate that CFD-Prompting significantly outperforms existing baselines in\nboth accuracy and robustness.",
        "url": "http://arxiv.org/abs/2508.16910v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16910v1",
        "arxiv_id": "2508.16910v1",
        "authors": [
            "Bo Zhao",
            "Yinghao Zhang",
            "Ziqi Xu",
            "Yongli Ren",
            "Xiuzhen Zhang",
            "Renqiang Luo",
            "Zaiwen Feng",
            "Feng Xia"
        ],
        "submitted": "2025-08-23 05:52:39",
        "source": "arxiv",
        "comment": "This paper has been accepted to the 34th ACM International Conference\n  on Information and Knowledge Management (CIKM 2025), Full Research Paper",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel causal prompting framework for unbiased reasoning in large language models, which is related to the user's interests in NLP and deep semantic understanding. However, the focus on knowledge-intensive tasks and language models is not directly aligned with the user's primary focus on information retrieval and search technologies."
    },
    {
        "title": "PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation",
        "abstract": "Modern app store recommender systems struggle with multiple-category apps, as\ntraditional taxonomies fail to capture overlapping semantics, leading to\nsuboptimal personalization. We propose PCR-CA (Parallel Codebook\nRepresentations with Contrastive Alignment), an end-to-end framework for\nimproved CTR prediction. PCR-CA first extracts compact multimodal embeddings\nfrom app text, then introduces a Parallel Codebook VQ-AE module that learns\ndiscrete semantic representations across multiple codebooks in parallel --\nunlike hierarchical residual quantization (RQ-VAE). This design enables\nindependent encoding of diverse aspects (e.g., gameplay, art style), better\nmodeling multiple-category semantics. To bridge semantic and collaborative\nsignals, we employ a contrastive alignment loss at both the user and item\nlevels, enhancing representation learning for long-tail items. Additionally, a\ndual-attention fusion mechanism combines ID-based and semantic features to\ncapture user interests, especially for long-tail apps. Experiments on a\nlarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong\nbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further\nvalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvement\nin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new\nframework has now been fully deployed on the Microsoft Store.",
        "url": "http://arxiv.org/abs/2508.18166v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18166v1",
        "arxiv_id": "2508.18166v1",
        "authors": [
            "Bin Tan",
            "Wangyao Ge",
            "Yidi Wang",
            "Xin Liu",
            "Jeff Burtoft",
            "Hao Fan",
            "Hui Wang"
        ],
        "submitted": "2025-08-25 16:16:06",
        "source": "arxiv",
        "comment": "9 pages, 4 figures, conference",
        "score": 6,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'cvr' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "While the paper explores recommender systems, which is a related topic, it focuses on app recommendation and does not directly address information retrieval, query understanding, or ranking models. The paper's emphasis on multimodal embeddings, contrastive alignment, and attention mechanisms is interesting, but the application is specific to app recommendation, which is not a primary focus of your research interests."
    },
    {
        "title": "Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation",
        "abstract": "User simulation is increasingly vital to develop and evaluate recommender\nsystems (RSs). While Large Language Models (LLMs) offer promising avenues to\nsimulate user behavior, they often struggle with the absence of specific domain\nalignment required for RSs and the efficiency demands of large-scale\nsimulation. A vast yet underutilized resource for enhancing this alignment is\nthe extensive user feedback inherent in RSs. However, directly leveraging such\nfeedback presents two significant challenges. First, user feedback in RSs is\noften ambiguous and noisy, which negatively impacts effective preference\nalignment. Second, the massive volume of feedback largely hinders the\nefficiency of preference alignment, necessitating an efficient filtering\nmechanism to identify more informative samples. To overcome these hurdles, we\nintroduce a novel data construction framework that leverages user feedback in\nRSs with advanced LLM capabilities to generate high-quality simulation data.\nOur framework unfolds in two key phases: (1) employing LLMs to generate\ncognitive decision-making processes on constructed simulation samples, reducing\nambiguity in raw user feedback; (2) data distillation based on uncertainty\nestimation and behavior sampling to filter challenging yet denoised simulation\nsamples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using\nsuch high-quality dataset with corresponding decision-making processes.\nExtensive experiments verify that our framework significantly boosts the\nalignment with human preferences and in-domain reasoning capabilities of\nfine-tuned LLMs, and provides more insightful and interpretable signals when\ninteracting with RSs. We believe our work will advance the RS community and\noffer valuable insights for broader human-centric AI research.",
        "url": "http://arxiv.org/abs/2508.18142v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18142v1",
        "arxiv_id": "2508.18142v1",
        "authors": [
            "Tianjun Wei",
            "Huizhong Guo",
            "Yingpeng Du",
            "Zhu Sun",
            "Chen Huang",
            "Dongxia Wang",
            "Jie Zhang"
        ],
        "submitted": "2025-08-25 15:51:24",
        "source": "arxiv",
        "comment": "Github: https://github.com/UserMirrorer/UserMirrorer",
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on user simulation and recommender systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and user feedback is not directly aligned with your primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of Large Language Models and data distillation is also not directly relevant to your interests in IR and NLP."
    },
    {
        "title": "ISACL: Internal State Analyzer for Copyrighted Training Data Leakage",
        "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but pose risks of inadvertently exposing copyrighted or proprietary data,\nespecially when such data is used for training but not intended for\ndistribution. Traditional methods address these leaks only after content is\ngenerated, which can lead to the exposure of sensitive information. This study\nintroduces a proactive approach: examining LLMs' internal states before text\ngeneration to detect potential leaks. By using a curated dataset of copyrighted\nmaterials, we trained a neural network classifier to identify risks, allowing\nfor early intervention by stopping the generation process or altering outputs\nto prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG)\nsystem, this framework ensures adherence to copyright and licensing\nrequirements while enhancing data privacy and ethical standards. Our results\nshow that analyzing internal states effectively mitigates the risk of\ncopyrighted data leakage, offering a scalable solution that fits smoothly into\nAI workflows, ensuring compliance with copyright regulations while maintaining\nhigh-quality text generation. The implementation is available on\nGitHub.\\footnote{https://github.com/changhu73/Internal_states_leakage}",
        "url": "http://arxiv.org/abs/2508.17767v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17767v1",
        "arxiv_id": "2508.17767v1",
        "authors": [
            "Guangwei Zhang",
            "Qisheng Su",
            "Jiateng Liu",
            "Cheng Qian",
            "Yanzhou Pan",
            "Yanjie Fu",
            "Denghui Zhang"
        ],
        "submitted": "2025-08-25 08:04:20",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on Natural Language Processing, specifically on detecting copyrighted data leakage in Large Language Models, which is not a primary area of interest for you."
    },
    {
        "title": "Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking",
        "abstract": "Transformer-based models like BERT excel at short text classification but\nstruggle with long document classification (LDC) due to input length\nlimitations and computational inefficiencies. In this work, we propose an\nefficient, zero-shot approach to LDC that leverages sentence ranking to reduce\ninput context without altering the model architecture. Our method enables the\nadaptation of models trained on short texts, such as headlines, to long-form\ndocuments by selecting the most informative sentences using a TF-IDF-based\nranking strategy. Using the MahaNews dataset of long Marathi news articles, we\nevaluate three context reduction strategies that prioritize essential content\nwhile preserving classification accuracy. Our results show that retaining only\nthe top 50\\% ranked sentences maintains performance comparable to full-document\ninference while reducing inference time by up to 35\\%. This demonstrates that\nsentence ranking is a simple yet effective technique for scalable and efficient\nzero-shot LDC.",
        "url": "http://arxiv.org/abs/2508.17490v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17490v1",
        "arxiv_id": "2508.17490v1",
        "authors": [
            "Prathamesh Kokate",
            "Mitali Sarnaik",
            "Manavi Khopade",
            "Mukta Takalikar",
            "Raviraj Joshi"
        ],
        "submitted": "2025-08-24 18:52:37",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on long document classification using sentence ranking, which is related to information retrieval and query understanding. However, it does not directly address ranking models or user behavior modeling, which are core interests. The paper's application in the NLP domain is also relevant, but its scope is limited to text classification, which is not a primary focus of the user's research."
    },
    {
        "title": "Improving Table Understanding with LLMs and Entity-Oriented Search",
        "abstract": "Our work addresses the challenges of understanding tables. Existing methods\noften struggle with the unpredictable nature of table content, leading to a\nreliance on preprocessing and keyword matching. They also face limitations due\nto the lack of contextual information, which complicates the reasoning\nprocesses of large language models (LLMs). To overcome these challenges, we\nintroduce an entity-oriented search method to improve table understanding with\nLLMs. This approach effectively leverages the semantic similarities between\nquestions and table data, as well as the implicit relationships between table\ncells, minimizing the need for data preprocessing and keyword matching.\nAdditionally, it focuses on table entities, ensuring that table cells are\nsemantically tightly bound, thereby enhancing contextual clarity. Furthermore,\nwe pioneer the use of a graph query language for table understanding,\nestablishing a new research direction. Experiments show that our approach\nachieves new state-of-the-art performances on standard benchmarks\nWikiTableQuestions and TabFact.",
        "url": "http://arxiv.org/abs/2508.17028v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17028v1",
        "arxiv_id": "2508.17028v1",
        "authors": [
            "Thi-Nhung Nguyen",
            "Hoang Ngo",
            "Dinh Phung",
            "Thuy-Trang Vu",
            "Dat Quoc Nguyen"
        ],
        "submitted": "2025-08-23 14:02:45",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores table understanding with LLMs and entity-oriented search, which is related to query understanding and ranking models in Information Retrieval. However, the focus on tables and entity-oriented search is not directly aligned with the user's primary interests in search technologies, user behavior modeling, and deep semantic understanding. The paper's relevance is somewhat related but not a central match."
    },
    {
        "title": "Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs",
        "abstract": "How does retrieval performance scale with pretraining FLOPs? We benchmark\nretrieval performance across LLM model sizes from 125 million parameters to 7\nbillion parameters pretrained on datasets ranging from 1 billion tokens to more\nthan 2 trillion tokens. We find that retrieval performance on zero-shot BEIR\ntasks predictably scales with LLM size, training duration, and estimated FLOPs.\nWe also show that In-Context Learning scores are strongly correlated with\nretrieval scores across retrieval tasks. Finally, we highlight the implications\nthis has for the development of LLM-based retrievers.",
        "url": "http://arxiv.org/abs/2508.17400v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17400v1",
        "arxiv_id": "2508.17400v1",
        "authors": [
            "Jacob Portes",
            "Connor Jennings",
            "Erica Ji Yuen",
            "Sasha Doubov",
            "Michael Carbin"
        ],
        "submitted": "2025-08-24 15:19:24",
        "source": "arxiv",
        "comment": "15 pages, 4 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the retrieval capabilities of large language models, which is relevant to Information Retrieval (IR) and Search technologies. The focus on pretraining FLOPs and model sizes is somewhat related to query understanding and ranking models, but the paper's primary focus is on the scalability of language models rather than specific IR techniques or user behavior modeling."
    },
    {
        "title": "Towards a Real-World Aligned Benchmark for Unlearning in Recommender Systems",
        "abstract": "Modern recommender systems heavily leverage user interaction data to deliver\npersonalized experiences. However, relying on personal data presents challenges\nin adhering to privacy regulations, such as the GDPR's \"right to be forgotten\".\nMachine unlearning (MU) aims to address these challenges by enabling the\nefficient removal of specific training data from models post-training, without\ncompromising model utility or leaving residual information. However, current\nbenchmarks for unlearning in recommender systems -- most notably CURE4Rec --\nfail to reflect real-world operational demands. They focus narrowly on\ncollaborative filtering, overlook tasks like session-based and next-basket\nrecommendation, simulate unrealistically large unlearning requests, and ignore\ncritical efficiency constraints. In this paper, we propose a set of design\ndesiderata and research questions to guide the development of a more realistic\nbenchmark for unlearning in recommender systems, with the goal of gathering\nfeedback from the research community. Our benchmark proposal spans multiple\nrecommendation tasks, includes domain-specific unlearning scenarios, and\nseveral unlearning algorithms -- including ones adapted from a recent NeurIPS\nunlearning competition. Furthermore, we argue for an unlearning setup that\nreflects the sequential, time-sensitive nature of real-world deletion requests.\nWe also present a preliminary experiment in a next-basket recommendation\nsetting based on our proposed desiderata and find that unlearning also works\nfor sequential recommendation models, exposed to many small unlearning\nrequests. In this case, we observe that a modification of a custom-designed\nunlearning algorithm for recommender systems outperforms general unlearning\nalgorithms significantly, and that unlearning can be executed with a latency of\nonly several seconds.",
        "url": "http://arxiv.org/abs/2508.17076v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17076v1",
        "arxiv_id": "2508.17076v1",
        "authors": [
            "Pierre Lubitzsch",
            "Olga Ovcharenko",
            "Hao Chen",
            "Maarten de Rijke",
            "Sebastian Schelter"
        ],
        "submitted": "2025-08-23 16:05:40",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'neurips' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems, which is a related but distinct field from information retrieval and search technologies. While it touches on the concept of 'unlearning', the paper's primary concern is not query understanding, ranking models, or user behavior modeling, which are core aspects of my research interests."
    },
    {
        "title": "SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models",
        "abstract": "Automatic survey generation has emerged as a key task in scientific document\nprocessing. While large language models (LLMs) have shown promise in generating\nsurvey texts, the lack of standardized evaluation datasets critically hampers\nrigorous assessment of their performance against human-written surveys. In this\nwork, we present SurveyGen, a large-scale dataset comprising over 4,200\nhuman-written surveys across diverse scientific domains, along with 242,143\ncited references and extensive quality-related metadata for both the surveys\nand the cited papers. Leveraging this resource, we build QUAL-SG, a novel\nquality-aware framework for survey generation that enhances the standard\nRetrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware\nindicators into literature retrieval to assess and select higher-quality source\npapers. Using this dataset and framework, we systematically evaluate\nstate-of-the-art LLMs under varying levels of human involvement - from fully\nautomatic generation to human-guided writing. Experimental results and human\nevaluations show that while semi-automatic pipelines can achieve partially\ncompetitive outcomes, fully automatic survey generation still suffers from low\ncitation quality and limited critical analysis.",
        "url": "http://arxiv.org/abs/2508.17647v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17647v1",
        "arxiv_id": "2508.17647v1",
        "authors": [
            "Tong Bao",
            "Mir Tafseer Nayeem",
            "Davood Rafiei",
            "Chengzhi Zhang"
        ],
        "submitted": "2025-08-25 04:22:23",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on survey generation using large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions literature retrieval, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation",
        "abstract": "In this paper, we report our participation to the PalmX cultural evaluation\nshared task. Our system, CultranAI, focused on data augmentation and LoRA\nfine-tuning of large language models (LLMs) for Arabic cultural knowledge\nrepresentation. We benchmarked several LLMs to identify the best-performing\nmodel for the task. In addition to utilizing the PalmX dataset, we augmented it\nby incorporating the Palm dataset and curated a new dataset of over 22K\nculturally grounded multiple-choice questions (MCQs). Our experiments showed\nthat the Fanar-1-9B-Instruct model achieved the highest performance. We\nfine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the\nblind test set, our submitted system ranked 5th with an accuracy of 70.50%,\nwhile on the PalmX development set, it achieved an accuracy of 84.1%.",
        "url": "http://arxiv.org/abs/2508.17324v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17324v1",
        "arxiv_id": "2508.17324v1",
        "authors": [
            "Hunzalah Hassan Bhatti",
            "Youssef Ahmed",
            "Md Arid Hasan",
            "Firoj Alam"
        ],
        "submitted": "2025-08-24 12:11:21",
        "source": "arxiv",
        "comment": "LLMs, Native, Arabic LLMs, Augmentation, Multilingual, Language\n  Diversity, Contextual Understanding, Minority Languages, Culturally Informed,\n  Foundation Models, Large Language Models",
        "score": 4,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on cultural knowledge representation and data augmentation for Arabic language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on large language models and fine-tuning is also not aligned with the user's focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) systems require Large Language Models\n(LLMs) to generate responses that are faithful to the retrieved context.\nHowever, faithfulness hallucination remains a critical challenge, as existing\nmethods often require costly supervision and post-training or significant\ninference burdens. To overcome these limitations, we introduce Self-Supervised\nFaithfulness Optimization (SSFO), the first self-supervised alignment approach\nfor enhancing RAG faithfulness. SSFO constructs preference data pairs by\ncontrasting the model's outputs generated with and without the context.\nLeveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness\nwithout incurring labeling costs or additional inference burden. We\ntheoretically and empirically demonstrate that SSFO leverages a benign form of\n\\emph{likelihood displacement}, transferring probability mass from\nparametric-based tokens to context-aligned tokens. Based on this insight, we\npropose a modified DPO loss function to encourage likelihood displacement.\nComprehensive evaluations show that SSFO significantly outperforms existing\nmethods, achieving state-of-the-art faithfulness on multiple context-based\nquestion-answering datasets. Notably, SSFO exhibits strong generalization,\nimproving cross-lingual faithfulness and preserving general\ninstruction-following capabilities. We release our code and model at the\nanonymous link: https://github.com/chkwy/SSFO",
        "url": "http://arxiv.org/abs/2508.17225v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17225v1",
        "arxiv_id": "2508.17225v1",
        "authors": [
            "Xiaqiang Tang",
            "Yi Wang",
            "Keyu Hu",
            "Rui Xu",
            "Chuang Li",
            "Weigao Sun",
            "Jian Li",
            "Sihong Xie"
        ],
        "submitted": "2025-08-24 06:58:29",
        "source": "arxiv",
        "comment": "Working in progress",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Retrieval-Augmented Generation, which is related to Information Retrieval, but the primary focus is on generation rather than search or ranking. The paper's emphasis on self-supervised learning and optimization is also relevant to my interests in query understanding and user behavior modeling. However, the paper's scope is narrower than my research interests, and the connection to my areas of focus is not as direct."
    },
    {
        "title": "Exposing Privacy Risks in Graph Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) is a powerful technique for enhancing\nLarge Language Models (LLMs) with external, up-to-date knowledge. Graph RAG has\nemerged as an advanced paradigm that leverages graph-based knowledge structures\nto provide more coherent and contextually rich answers. However, the move from\nplain document retrieval to structured graph traversal introduces new,\nunder-explored privacy risks. This paper investigates the data extraction\nvulnerabilities of the Graph RAG systems. We design and execute tailored data\nextraction attacks to probe their susceptibility to leaking both raw text and\nstructured data, such as entities and their relationships. Our findings reveal\na critical trade-off: while Graph RAG systems may reduce raw text leakage, they\nare significantly more vulnerable to the extraction of structured entity and\nrelationship information. We also explore potential defense mechanisms to\nmitigate these novel attack surfaces. This work provides a foundational\nanalysis of the unique privacy challenges in Graph RAG and offers insights for\nbuilding more secure systems.",
        "url": "http://arxiv.org/abs/2508.17222v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17222v1",
        "arxiv_id": "2508.17222v1",
        "authors": [
            "Jiale Liu",
            "Jiahao Zhang",
            "Suhang Wang"
        ],
        "submitted": "2025-08-24 06:19:44",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on privacy risks in Graph Retrieval-Augmented Generation, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for you."
    },
    {
        "title": "Active Domain Knowledge Acquisition with \\$100 Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains",
        "abstract": "Large Language Models (LLMs) have demonstrated an impressive level of general\nknowledge. However, they often struggle in highly specialized and\ncost-sensitive domains such as drug discovery and rare disease research due to\nthe lack of expert knowledge. In this paper, we propose a novel framework\n(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively\nengaging domain experts within a fixed budget. Unlike traditional fine-tuning\napproaches, PU-ADKA selectively identifies and queries the most appropriate\nexpert from a team, taking into account each expert's availability, knowledge\nboundaries, and consultation costs. We train PU-ADKA using simulations on\nPubMed data and validate it through both controlled expert interactions and\nreal-world deployment with a drug development team, demonstrating its\neffectiveness in enhancing LLM performance in specialized domains under strict\nbudget constraints. In addition to outlining our methodological innovations and\nexperimental results, we introduce a new benchmark dataset, CKAD, for\ncost-effective LLM domain knowledge acquisition to foster further research in\nthis challenging area.",
        "url": "http://arxiv.org/abs/2508.17202v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17202v1",
        "arxiv_id": "2508.17202v1",
        "authors": [
            "Yang Wu",
            "Raha Moraffah",
            "Rujing Yao",
            "Jinhong Yu",
            "Zhimin Tao",
            "Xiaozhong Liu"
        ],
        "submitted": "2025-08-24 03:34:40",
        "source": "arxiv",
        "comment": "EMNLP 2025 Findings",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on enhancing Large Language Models (LLMs) by actively engaging domain experts within a fixed budget, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of domain knowledge acquisition, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "The Power of Framing: How News Headlines Guide Search Behavior",
        "abstract": "Search engines play a central role in how people gather information, but\nsubtle cues like headline framing may influence not only what users believe but\nalso how they search. While framing effects on judgment are well documented,\ntheir impact on subsequent search behavior is less understood. We conducted a\ncontrolled experiment where participants issued queries and selected from\nheadlines filtered by specific linguistic frames. Headline framing\nsignificantly shaped follow-up queries: conflict and strategy frames disrupted\nalignment with prior selections, while episodic frames led to more concrete\nqueries than thematic ones. We also observed modest short-term frame\npersistence that declined over time. These results suggest that even brief\nexposure to framing can meaningfully alter the direction of users\ninformation-seeking behavior.",
        "url": "http://arxiv.org/abs/2508.17131v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17131v1",
        "arxiv_id": "2508.17131v1",
        "authors": [
            "Amrit Poudel",
            "Maria Milkowski",
            "Tim Weninger"
        ],
        "submitted": "2025-08-23 20:12:19",
        "source": "arxiv",
        "comment": "Accepted to EMNLP",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the impact of headline framing on search behavior, which is related to query understanding and user behavior modeling in Information Retrieval. While the focus is on search behavior rather than ranking models or deep semantic understanding, the study's findings on how framing influences follow-up queries are relevant to the broader field of IR. However, the paper's scope is narrower than expected, and the connection to the e-commerce domain is limited."
    },
    {
        "title": "THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics",
        "abstract": "Thematic investing aims to construct portfolios aligned with structural\ntrends, yet selecting relevant stocks remains challenging due to overlapping\nsector boundaries and evolving market dynamics. To address this challenge, we\nconstruct the Thematic Representation Set (TRS), an extended dataset that\nbegins with real-world thematic ETFs and expands upon them by incorporating\nindustry classifications and financial news to overcome their coverage\nlimitations. The final dataset contains both the explicit mapping of themes to\ntheir constituent stocks and the rich textual profiles for each. Building on\nthis dataset, we introduce \\textsc{THEME}, a hierarchical contrastive learning\nframework. By representing the textual profiles of themes and stocks as\nembeddings, \\textsc{THEME} first leverages their hierarchical relationship to\nachieve semantic alignment. Subsequently, it refines these semantic embeddings\nthrough a temporal refinement stage that incorporates individual stock returns.\nThe final stock representations are designed for effective retrieval of\nthematically aligned assets with strong return potential. Empirical results\nshow that \\textsc{THEME} outperforms strong baselines across multiple retrieval\nmetrics and significantly improves performance in portfolio construction. By\njointly modeling thematic relationships from text and market dynamics from\nreturns, \\textsc{THEME} provides a scalable and adaptive solution for\nnavigating complex investment themes.",
        "url": "http://arxiv.org/abs/2508.16936v1",
        "pdf_url": "http://arxiv.org/pdf/2508.16936v1",
        "arxiv_id": "2508.16936v1",
        "authors": [
            "Hoyoung Lee",
            "Wonbin Ahn",
            "Suhwan Park",
            "Jaehoon Lee",
            "Minjae Kim",
            "Sungdong Yoo",
            "Taeyoon Lim",
            "Woohyung Lim",
            "Yongjae Lee"
        ],
        "submitted": "2025-08-23 08:05:37",
        "source": "arxiv",
        "comment": "Accepted at ACM International Conference on Information and Knowledge\n  Management (CIKM)",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on thematic investing and stock representation, using a hierarchical contrastive learning framework. While it involves text analysis and embedding, the context is not related to information retrieval, search technologies, or user behavior modeling, which are the primary areas of interest."
    },
    {
        "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
        "abstract": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
        "url": "http://arxiv.org/abs/2508.18190v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18190v1",
        "arxiv_id": "2508.18190v1",
        "authors": [
            "Zirui Tang",
            "Boyu Niu",
            "Xuanhe Zhou",
            "Boxiu Li",
            "Wei Zhou",
            "Jiannan Wang",
            "Guoliang Li",
            "Xinyi Zhang",
            "Fan Wu"
        ],
        "submitted": "2025-08-25 16:48:51",
        "source": "arxiv",
        "comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor",
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper focuses on semi-structured table question answering, which is related to information retrieval and query understanding. The use of large language models and tree-based framework is also relevant to my interests in NLP and ranking models. However, the paper's primary focus is on table question answering, which is not directly aligned with my core research themes."
    },
    {
        "title": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German",
        "abstract": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing",
        "url": "http://arxiv.org/abs/2508.17973v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17973v1",
        "arxiv_id": "2508.17973v1",
        "authors": [
            "Miriam Anschtz",
            "Thanh Mai Pham",
            "Eslam Nasrallah",
            "Maximilian Mller",
            "Cristian-George Craciun",
            "Georg Groh"
        ],
        "submitted": "2025-08-25 12:40:32",
        "source": "arxiv",
        "comment": "Accepted to INLG 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on paraphrasing and text simplification in German, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on readability-controlled paraphrasing and text simplification does not align with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation",
        "abstract": "Contextual automatic speech recognition (ASR) systems allow for recognizing\nout-of-vocabulary (OOV) words, such as named entities or rare words. However,\nit remains challenging due to limited training data and ambiguous or\ninconsistent pronunciations. In this paper, we propose a synthesis-driven\nmulti-pronunciation contextual biasing method that performs zero-shot\ncontextual ASR on a pretrained Whisper model. Specifically, we leverage\ntext-to-speech (TTS) systems to synthesize diverse speech samples containing\neach target rare word, and then use the pretrained Whisper model to extract\nmultiple predicted pronunciation variants. These variant token sequences are\ncompiled into a prefix-trie, which assigns rewards to beam hypotheses in a\nshallow-fusion manner during beam-search decoding. After which, any recognized\nvariant is mapped back to the original rare word in the final transcription.\nThe evaluation results on the Librispeech dataset show that our method reduces\nbiased word error rate (WER) by 42% on test-clean and 43% on test-other while\nmaintaining unbiased WER essentially unchanged.",
        "url": "http://arxiv.org/abs/2508.17796v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17796v1",
        "arxiv_id": "2508.17796v1",
        "authors": [
            "Changsong Liu",
            "Yizhou Peng",
            "Eng Siong Chng"
        ],
        "submitted": "2025-08-25 08:41:52",
        "source": "arxiv",
        "comment": "Accepted to APSIPA ASC 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on automatic speech recognition (ASR) and contextual biasing, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The techniques and methods described in the paper are not applicable to the user's areas of focus, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Research on Evaluation Methods for Patent Novelty Search Systems and Empirical Analysis",
        "abstract": "Patent novelty search systems are critical to IP protection and innovation\nassessment; their retrieval accuracy directly impacts patent quality. We\npropose a comprehensive evaluation methodology that builds high-quality,\nreproducible datasets from examiner citations and X-type citations extracted\nfrom technically consistent family patents, and evaluates systems using\ninvention descriptions as inputs. Using Top-k Detection Rate and Recall as core\nmetrics, we further conduct multi-dimensional analyses by language, technical\nfield (IPC), and filing jurisdiction. Experiments show the method effectively\nexposes performance differences across scenarios and offers actionable evidence\nfor system improvement. The framework is scalable and practical, providing a\nuseful reference for development and optimization of patent novelty search\nsystems",
        "url": "http://arxiv.org/abs/2508.17782v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17782v1",
        "arxiv_id": "2508.17782v1",
        "authors": [
            "Shu Zhang",
            "LiSha Zhang",
            "Kai Duan",
            "XinKai Sun"
        ],
        "submitted": "2025-08-25 08:24:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on patent novelty search systems, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it mentions evaluation methods and metrics, the context is specific to patent search and does not align with the user's interests in query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning",
        "abstract": "Prompt engineering significantly influences the reliability and clinical\nutility of Large Language Models (LLMs) in medical applications. Current\noptimization approaches inadequately address domain-specific medical knowledge\nand safety requirements. This paper introduces EMPOWER, a novel evolutionary\nframework that enhances medical prompt quality through specialized\nrepresentation learning, multi-dimensional evaluation, and structure-preserving\nalgorithms. Our methodology incorporates: (1) a medical terminology attention\nmechanism, (2) a comprehensive assessment architecture evaluating clarity,\nspecificity, clinical relevance, and factual accuracy, (3) a component-level\nevolutionary algorithm preserving clinical reasoning integrity, and (4) a\nsemantic verification module ensuring adherence to medical knowledge.\nEvaluation across diagnostic, therapeutic, and educational tasks demonstrates\nsignificant improvements: 24.7% reduction in factually incorrect content, 19.6%\nenhancement in domain specificity, and 15.3% higher clinician preference in\nblinded evaluations. The framework addresses critical challenges in developing\nclinically appropriate prompts, facilitating more responsible integration of\nLLMs into healthcare settings.",
        "url": "http://arxiv.org/abs/2508.17703v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17703v1",
        "arxiv_id": "2508.17703v1",
        "authors": [
            "Yinda Chen",
            "Yangfan He",
            "Jing Yang",
            "Dapeng Zhang",
            "Zhenlong Yuan",
            "Muhammad Attique Khan",
            "Jamel Baili",
            "Por Lip Yee"
        ],
        "submitted": "2025-08-25 06:23:17",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on prompt optimization for Large Language Models in medical applications, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, the paper's primary concern is medical knowledge and clinical utility, which is outside the user's scope."
    },
    {
        "title": "Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit",
        "abstract": "Large language models (LLMs) enhance complex reasoning tasks by scaling the\nindividual thinking process. However, prior work shows that overthinking can\ndegrade overall performance. Motivated by observed patterns in thinking length\nand content length, we categorize reasoning into three stages: insufficient\nexploration stage, compensatory reasoning stage, and reasoning convergence\nstage. Typically, LLMs produce correct answers in the compensatory reasoning\nstage, whereas reasoning convergence often triggers overthinking, causing\nincreased resource usage or even infinite loops. Therefore, mitigating\noverthinking hinges on detecting the end of the compensatory reasoning stage,\ndefined as the Reasoning Completion Point (RCP). RCP typically appears at the\nend of the first complete reasoning cycle and can be identified by querying the\nLLM sentence by sentence or monitoring the probability of an end-of-thinking\ntoken (e.g., \\texttt{</think>}), though these methods lack an efficient and\nprecise balance. To improve this, we mine more sensitive and consistent RCP\npatterns and develop a lightweight thresholding strategy based on heuristic\nrules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D)\ndemonstrate that the proposed method reduces token consumption while preserving\nor enhancing reasoning accuracy.",
        "url": "http://arxiv.org/abs/2508.17627v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17627v1",
        "arxiv_id": "2508.17627v1",
        "authors": [
            "Zihao Wei",
            "Liang Pang",
            "Jiahao Liu",
            "Jingcheng Deng",
            "Shicheng Xu",
            "Zenghao Duan",
            "Jingang Wang",
            "Fei Sun",
            "Xunliang Cai",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "submitted": "2025-08-25 03:17:17",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Large Language Models (LLMs) and their tendency to overthink, which is not directly related to Information Retrieval or Search technologies. While it touches on the concept of 'reasoning convergence', it does not seem to be directly applicable to query understanding, ranking models, or user behavior modeling. The paper's relevance to your interests is limited, but it does explore the idea of identifying a 'completion point' which could be seen as a form of 'query understanding' in a broader sense."
    },
    {
        "title": "Preference Trajectory Modeling via Flow Matching for Sequential Recommendation",
        "abstract": "Sequential recommendation predicts each user's next item based on their\nhistorical interaction sequence. Recently, diffusion models have attracted\nsignificant attention in this area due to their strong ability to model user\ninterest distributions. They typically generate target items by denoising\nGaussian noise conditioned on historical interactions. However, these models\nface two critical limitations. First, they exhibit high sensitivity to the\ncondition, making it difficult to recover target items from pure Gaussian\nnoise. Second, the inference process is computationally expensive, limiting\npractical deployment. To address these issues, we propose FlowRec, a simple yet\neffective sequential recommendation framework which leverages flow matching to\nexplicitly model user preference trajectories from current states to future\ninterests. Flow matching is an emerging generative paradigm, which offers\ngreater flexibility in initial distributions and enables more efficient\nsampling. Based on this, we construct a personalized behavior-based prior\ndistribution to replace Gaussian noise and learn a vector field to model user\npreference trajectories. To better align flow matching with the recommendation\nobjective, we further design a single-step alignment loss incorporating both\npositive and negative samples, improving sampling efficiency and generation\nquality. Extensive experiments on four benchmark datasets verify the\nsuperiority of FlowRec over the state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2508.17618v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17618v1",
        "arxiv_id": "2508.17618v1",
        "authors": [
            "Li Li",
            "Mingyue Cheng",
            "Yuyang Ye",
            "Zhiding Liu",
            "Enhong Chen"
        ],
        "submitted": "2025-08-25 02:55:42",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on sequential recommendation, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the approach is based on diffusion models and flow matching, which is not directly related to query understanding, ranking models, or user behavior modeling. The paper's primary focus is on recommender systems, which is not my primary area of interest."
    },
    {
        "title": "RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System",
        "abstract": "We present RubikSQL, a novel NL2SQL system designed to address key challenges\nin real-world enterprise-level NL2SQL, such as implicit intents and\ndomain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning\ntask, demanding both Knowledge Base (KB) maintenance and SQL generation.\nRubikSQL systematically builds and refines its KB through techniques including\ndatabase profiling, structured information extraction, agentic rule mining, and\nChain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a\nmulti-agent workflow to leverage this curated KB, generating accurate SQLs.\nRubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev\ndatasets. Finally, we release the RubikBench benchmark, a new benchmark\nspecifically designed to capture vital traits of industrial NL2SQL scenarios,\nproviding a valuable resource for future research.",
        "url": "http://arxiv.org/abs/2508.17590v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17590v1",
        "arxiv_id": "2508.17590v1",
        "authors": [
            "Zui Chen",
            "Han Li",
            "Xinhao Zhang",
            "Xiaoyu Chen",
            "Chunyin Dong",
            "Yifeng Wang",
            "Xin Cai",
            "Su Zhang",
            "Ziqi Li",
            "Chi Ding",
            "Jinxu Li",
            "Shuai Wang",
            "Dousheng Zhao",
            "Sanhai Gao",
            "Guangyi Liu"
        ],
        "submitted": "2025-08-25 01:28:37",
        "source": "arxiv",
        "comment": "18 pages, 3 figures, 3 tables, to be submitted to VLDB 2026 (PVLDB\n  Volume 19)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a novel NL2SQL system, RubikSQL, which addresses challenges in real-world enterprise-level NL2SQL. While it touches on topics related to query understanding and knowledge base maintenance, the focus is more on the NL2SQL task itself rather than ranking models or user behavior modeling, which are core interests in Information Retrieval. The paper's relevance to the user's research interests is somewhat limited, but it may still be of interest due to its connection to NLP and data mining."
    },
    {
        "title": "UQ: Assessing Language Models on Unsolved Questions",
        "abstract": "Benchmarks shape progress in AI research. A useful benchmark should be both\ndifficult and realistic: questions should challenge frontier models while also\nreflecting real-world usage. Yet, current paradigms face a difficulty-realism\ntension: exam-style benchmarks are often made artificially difficult with\nlimited real-world value, while benchmarks based on real user interaction often\nskew toward easy, high-frequency problems. In this work, we explore a radically\ndifferent paradigm: assessing models on unsolved questions. Rather than a\nstatic benchmark scored once, we curate unsolved questions and evaluate models\nasynchronously over time with validator-assisted screening and community\nverification. We introduce UQ, a testbed of 500 challenging, diverse questions\nsourced from Stack Exchange, spanning topics from CS theory and math to sci-fi\nand history, probing capabilities including reasoning, factuality, and\nbrowsing. UQ is difficult and realistic by construction: unsolved questions are\noften hard and naturally arise when humans seek answers, thus solving them\nyields direct real-world value. Our contributions are threefold: (1) UQ-Dataset\nand its collection pipeline combining rule-based filters, LLM judges, and human\nreview to ensure question quality (e.g., well-defined and difficult); (2)\nUQ-Validators, compound validation strategies that leverage the\ngenerator-validator gap to provide evaluation signals and pre-screen candidate\nsolutions for human review; and (3) UQ-Platform, an open platform where experts\ncollectively verify questions and solutions. The top model passes UQ-validation\non only 15% of questions, and preliminary human verification has already\nidentified correct answers among those that passed. UQ charts a path for\nevaluating frontier models on real-world, open-ended challenges, where success\npushes the frontier of human knowledge. We release UQ at\nhttps://uq.stanford.edu.",
        "url": "http://arxiv.org/abs/2508.17580v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17580v1",
        "arxiv_id": "2508.17580v1",
        "authors": [
            "Fan Nie",
            "Ken Ziyu Liu",
            "Zihao Wang",
            "Rui Sun",
            "Wei Liu",
            "Weijia Shi",
            "Huaxiu Yao",
            "Linjun Zhang",
            "Andrew Y. Ng",
            "James Zou",
            "Sanmi Koyejo",
            "Yejin Choi",
            "Percy Liang",
            "Niklas Muennighoff"
        ],
        "submitted": "2025-08-25 01:07:59",
        "source": "arxiv",
        "comment": "FN, KZL, and NM are project co-leads and contributed equally. Project\n  website: https://uq.stanford.edu",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a new paradigm for assessing language models on unsolved questions, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and open-ended challenges is not directly aligned with the user's primary research interests in IR and search technologies."
    },
    {
        "title": "A Universal Framework for Offline Serendipity Evaluation in Recommender Systems via Large Language Models",
        "abstract": "Serendipity in recommender systems (RSs) has attracted increasing attention\nas a concept that enhances user satisfaction by presenting unexpected and\nuseful items. However, evaluating serendipitous performance remains challenging\nbecause its ground truth is generally unobservable. The existing offline\nmetrics often depend on ambiguous definitions or are tailored to specific\ndatasets and RSs, thereby limiting their generalizability. To address this\nissue, we propose a universally applicable evaluation framework that leverages\nlarge language models (LLMs) known for their extensive knowledge and reasoning\ncapabilities, as evaluators. First, to improve the evaluation performance of\nthe proposed framework, we assessed the serendipity prediction accuracy of LLMs\nusing four different prompt strategies on a dataset containing user-annotated\nserendipitous ground truth and found that the chain-of-thought prompt achieved\nthe highest accuracy. Next, we re-evaluated the serendipitous performance of\nboth serendipity-oriented and general RSs using the proposed framework on three\ncommonly used real-world datasets, without the ground truth. The results\nindicated that there was no serendipity-oriented RS that consistently\noutperformed across all datasets, and even a general RS sometimes achieved\nhigher performance than the serendipity-oriented RS.",
        "url": "http://arxiv.org/abs/2508.17571v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17571v1",
        "arxiv_id": "2508.17571v1",
        "authors": [
            "Yu Tokutake",
            "Kazushi Okamoto",
            "Kei Harada",
            "Atsushi Shibata",
            "Koki Karube"
        ],
        "submitted": "2025-08-25 00:45:16",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not directly address information retrieval, query understanding, or ranking models. The use of large language models is interesting, but it is not directly applicable to my research interests in IR and NLP."
    },
    {
        "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling",
        "abstract": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
        "url": "http://arxiv.org/abs/2508.17445v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17445v1",
        "arxiv_id": "2508.17445v1",
        "authors": [
            "Yizhi Li",
            "Qingshui Gu",
            "Zhoufutu Wen",
            "Ziniu Li",
            "Tianshun Xing",
            "Shuyue Guo",
            "Tianyu Zheng",
            "Xin Zhou",
            "Xingwei Qu",
            "Wangchunshu Zhou",
            "Zheng Zhang",
            "Wei Shen",
            "Qian Liu",
            "Chenghua Lin",
            "Jian Yang",
            "Ge Zhang",
            "Wenhao Huang"
        ],
        "submitted": "2025-08-24 16:52:37",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on policy optimization and efficacy, using a tree-based modeling approach, which is unrelated to information retrieval, search technologies, or user behavior modeling. While it mentions reinforcement learning, it's not directly applicable to query understanding, ranking models, or click models."
    },
    {
        "title": "DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization",
        "abstract": "Claim normalization is an integral part of any automatic fact-check\nverification system. It parses the typically noisy claim data, such as social\nmedia posts into normalized claims, which are then fed into downstream veracity\nclassification tasks. The CheckThat! 2025 Task 2 focuses specifically on claim\nnormalization and spans 20 languages under monolingual and zero-shot\nconditions. Our proposed solution consists of a lightweight\n\\emph{retrieval-first, LLM-backed} pipeline, in which we either dynamically\nprompt a GPT-4o-mini with in-context examples, or retrieve the closest\nnormalization from the train dataset directly. On the official test set, the\nsystem ranks near the top for most monolingual tracks, achieving first place in\n7 out of of the 13 languages. In contrast, the system underperforms in the\nzero-shot setting, highlighting the limitation of the proposed solution.",
        "url": "http://arxiv.org/abs/2508.17402v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17402v1",
        "arxiv_id": "2508.17402v1",
        "authors": [
            "Aleksandar Pramov",
            "Jiangqin Ma",
            "Bina Patel"
        ],
        "submitted": "2025-08-24 15:19:58",
        "source": "arxiv",
        "comment": "CLEF 2025 Working Notes, Madrid, Spain",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on claim normalization, a specific task within the broader field of Information Retrieval. While it employs a retrieval-first approach, the primary focus is on language models and veracity classification, which are not directly related to my core research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models",
        "abstract": "Interactive online learning environments, represented by Massive AI-empowered\nCourses (MAIC), leverage LLM-driven multi-agent systems to transform passive\nMOOCs into dynamic, text-based platforms, enhancing interactivity through LLMs.\nThis paper conducts an empirical study on a specific MAIC course to explore\nthree research questions about dropouts in these interactive online courses:\n(1) What factors might lead to dropouts? (2) Can we predict dropouts? (3) Can\nwe reduce dropouts? We analyze interaction logs to define dropouts and identify\ncontributing factors. Our findings reveal strong links between dropout\nbehaviors and textual interaction patterns. We then propose a\ncourse-progress-adaptive dropout prediction framework (CPADP) to predict\ndropouts with at most 95.4% accuracy. Based on this, we design a personalized\nemail recall agent to re-engage at-risk students. Applied in the deployed MAIC\nsystem with over 3,000 students, the feasibility and effectiveness of our\napproach have been validated on students with diverse backgrounds.",
        "url": "http://arxiv.org/abs/2508.17310v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17310v1",
        "arxiv_id": "2508.17310v1",
        "authors": [
            "Yuanchun Wang",
            "Yiyang Fu",
            "Jifan Yu",
            "Daniel Zhang-Li",
            "Zheyuan Zhang",
            "Joy Lim Jia Yin",
            "Yucheng Wang",
            "Peng Zhou",
            "Jing Zhang",
            "Huiqin Liu"
        ],
        "submitted": "2025-08-24 11:40:16",
        "source": "arxiv",
        "comment": "12 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on handling dropouts in an interactive online course using language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in NLP and IR."
    },
    {
        "title": "From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users",
        "abstract": "The pursuit of human-level artificial intelligence (AI) has significantly\nadvanced the development of autonomous agents and Large Language Models (LLMs).\nLLMs are now widely utilized as decision-making agents for their ability to\ninterpret instructions, manage sequential tasks, and adapt through feedback.\nThis review examines recent developments in employing LLMs as autonomous agents\nand tool users and comprises seven research questions. We only used the papers\npublished between 2023 and 2025 in conferences of the A* and A rank and Q1\njournals. A structured analysis of the LLM agents' architectural design\nprinciples, dividing their applications into single-agent and multi-agent\nsystems, and strategies for integrating external tools is presented. In\naddition, the cognitive mechanisms of LLM, including reasoning, planning, and\nmemory, and the impact of prompting methods and fine-tuning procedures on agent\nperformance are also investigated. Furthermore, we evaluated current benchmarks\nand assessment protocols and have provided an analysis of 68 publicly available\ndatasets to assess the performance of LLM-based agents in various tasks. In\nconducting this review, we have identified critical findings on verifiable\nreasoning of LLMs, the capacity for self-improvement, and the personalization\nof LLM-based agents. Finally, we have discussed ten future research directions\nto overcome these gaps.",
        "url": "http://arxiv.org/abs/2508.17281v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17281v1",
        "arxiv_id": "2508.17281v1",
        "authors": [
            "Sadia Sultana Chowa",
            "Riasad Alvi",
            "Subhey Sadi Rahman",
            "Md Abdur Rahman",
            "Mohaimenul Azam Khan Raiaan",
            "Md Rafiqul Islam",
            "Mukhtar Hussain",
            "Sami Azam"
        ],
        "submitted": "2025-08-24 10:02:51",
        "source": "arxiv",
        "comment": "40 pages, 6 figures, 10 tables. Submitted to Artificial Intelligence\n  Review for peer review",
        "score": 3,
        "keyword_reasons": [
            "Found 'personalization' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on AI and language models, the focus is on autonomous agents and tool users, which is outside the scope of the user's primary research interests."
    },
    {
        "title": "ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation",
        "abstract": "Legal claims refer to the plaintiff's demands in a case and are essential to\nguiding judicial reasoning and case resolution. While many works have focused\non improving the efficiency of legal professionals, the research on helping\nnon-professionals (e.g., plaintiffs) remains unexplored. This paper explores\nthe problem of legal claim generation based on the given case's facts. First,\nwe construct ClaimGen-CN, the first dataset for Chinese legal claim generation\ntask, from various real-world legal disputes. Additionally, we design an\nevaluation metric tailored for assessing the generated claims, which\nencompasses two essential dimensions: factuality and clarity. Building on this,\nwe conduct a comprehensive zero-shot evaluation of state-of-the-art general and\nlegal-domain large language models. Our findings highlight the limitations of\nthe current models in factual precision and expressive clarity, pointing to the\nneed for more targeted development in this domain. To encourage further\nexploration of this important task, we will make the dataset publicly\navailable.",
        "url": "http://arxiv.org/abs/2508.17234v1",
        "pdf_url": "http://arxiv.org/pdf/2508.17234v1",
        "arxiv_id": "2508.17234v1",
        "authors": [
            "Siying Zhou",
            "Yiquan Wu",
            "Hui Chen",
            "Xavier Hu",
            "Kun Kuang",
            "Adam Jatowt",
            "Ming Hu",
            "Chunyan Zheng",
            "Fei Wu"
        ],
        "submitted": "2025-08-24 07:19:25",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on legal claim generation, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves language models, the context is specific to legal domain and does not align with the user's research interests."
    }
]