[
    {
        "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving",
        "abstract": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.",
        "url": "http://arxiv.org/abs/2507.06229v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06229v1",
        "arxiv_id": "2507.06229v1",
        "authors": [
            "Xiangru Tang",
            "Tianrui Qin",
            "Tianhao Peng",
            "Ziyang Zhou",
            "Daniel Shao",
            "Tingting Du",
            "Xinming Wei",
            "Peng Xia",
            "Fang Wu",
            "He Zhu",
            "Ge Zhang",
            "Jiaheng Liu",
            "Xingyao Wang",
            "Sirui Hong",
            "Chenglin Wu",
            "Hao Cheng",
            "Chi Wang",
            "Wangchunshu Zhou"
        ],
        "submitted": "2025-07-08 17:59:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
        "abstract": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE\\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
        "url": "http://arxiv.org/abs/2507.06223v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06223v1",
        "arxiv_id": "2507.06223v1",
        "authors": [
            "Zhiyuan Peng",
            "Ting-ruen Wei",
            "Tingyu Song",
            "Yilun Zhao",
            "Yi Fang"
        ],
        "submitted": "2025-07-08 17:56:28",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions",
        "abstract": "Pretrained vision-language models (VLMs) such as CLIP excel in multimodal\nunderstanding but struggle with contextually relevant fine-grained visual\nfeatures, making it difficult to distinguish visually similar yet culturally\ndistinct concepts. This limitation stems from the scarcity of high-quality\nculture-specific datasets, the lack of integrated contextual knowledge, and the\nabsence of hard negatives highlighting subtle distinctions. To address these\nchallenges, we first design a data curation pipeline that leverages\nopen-sourced VLMs and text-to-image diffusion models to construct CulTwin, a\nsynthetic cultural dataset. This dataset consists of paired\nconcept-caption-image triplets, where concepts visually resemble each other but\nrepresent different cultural contexts. Then, we fine-tune CLIP on CulTwin to\ncreate CultureCLIP, which aligns cultural concepts with contextually enhanced\ncaptions and synthetic images through customized contrastive learning, enabling\nfiner cultural differentiation while preserving generalization capabilities.\nExperiments on culturally relevant benchmarks show that CultureCLIP outperforms\nthe base CLIP, achieving up to a notable 5.49% improvement in fine-grained\nconcept recognition on certain tasks, while preserving CLIP's original\ngeneralization ability, validating the effectiveness of our data synthesis and\nVLM backbone training paradigm in capturing subtle cultural distinctions.",
        "url": "http://arxiv.org/abs/2507.06210v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06210v1",
        "arxiv_id": "2507.06210v1",
        "authors": [
            "Yuchen Huang",
            "Zhiyuan Fan",
            "Zhitao He",
            "Sandeep Polisetty",
            "Wenyan Li",
            "Yi R. Fung"
        ],
        "submitted": "2025-07-08 17:38:56",
        "source": "arxiv",
        "comment": "25 pages, COLM 2025"
    },
    {
        "title": "DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media",
        "abstract": "In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a\nScientific Web Discourse Detection, present the methods we explored for this\ntask. For this multiclass classification task, we determined if a tweet\ncontained a scientific claim, a reference to a scientific study or publication,\nand/or mentions of scientific entities, such as a university or a scientist. We\npresent 3 modeling approaches for this task: transformer finetuning, few-shot\nprompting of LLMs, and a combined ensemble model whose design was informed by\nearlier experiments. Our team placed 7th in the competition, achieving a\nmacro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline\nof 0.8375. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.",
        "url": "http://arxiv.org/abs/2507.06205v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06205v1",
        "arxiv_id": "2507.06205v1",
        "authors": [
            "Ayush Parikh",
            "Hoang Thanh Thanh Truong",
            "Jeanette Schofield",
            "Maximilian Heil"
        ],
        "submitted": "2025-07-08 17:30:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Differential Mamba",
        "abstract": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
        "url": "http://arxiv.org/abs/2507.06204v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06204v1",
        "arxiv_id": "2507.06204v1",
        "authors": [
            "Nadav Schneider",
            "Itamar Zimerman",
            "Eliya Nachmani"
        ],
        "submitted": "2025-07-08 17:30:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Survey on Latent Reasoning",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
        "url": "http://arxiv.org/abs/2507.06203v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06203v1",
        "arxiv_id": "2507.06203v1",
        "authors": [
            "Rui-Jie Zhu",
            "Tianhao Peng",
            "Tianhao Cheng",
            "Xingwei Qu",
            "Jinfa Huang",
            "Dawei Zhu",
            "Hao Wang",
            "Kaiwen Xue",
            "Xuanliang Zhang",
            "Yong Shan",
            "Tianle Cai",
            "Taylor Kergan",
            "Assel Kembay",
            "Andrew Smith",
            "Chenghua Lin",
            "Binh Nguyen",
            "Yuqi Pan",
            "Yuhong Chou",
            "Zefan Cai",
            "Zhenhe Wu",
            "Yongchi Zhao",
            "Tianyu Liu",
            "Jian Yang",
            "Wangchunshu Zhou",
            "Chujie Zheng",
            "Chongxuan Li",
            "Yuyin Zhou",
            "Zhoujun Li",
            "Zhaoxiang Zhang",
            "Jiaheng Liu",
            "Ge Zhang",
            "Wenhao Huang",
            "Jason Eshraghian"
        ],
        "submitted": "2025-07-08 17:29:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UQLM: A Python Package for Uncertainty Quantification in Large Language Models",
        "abstract": "Hallucinations, defined as instances where Large Language Models (LLMs)\ngenerate false or misleading content, pose a significant challenge that impacts\nthe safety and trust of downstream applications. We introduce UQLM, a Python\npackage for LLM hallucination detection using state-of-the-art uncertainty\nquantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers\nthat compute response-level confidence scores ranging from 0 to 1. This library\nprovides an off-the-shelf solution for UQ-based hallucination detection that\ncan be easily integrated to enhance the reliability of LLM outputs.",
        "url": "http://arxiv.org/abs/2507.06196v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06196v1",
        "arxiv_id": "2507.06196v1",
        "authors": [
            "Dylan Bouchard",
            "Mohit Singh Chauhan",
            "David Skarbrevik",
            "Ho-Kyeong Ra",
            "Viren Bajaj",
            "Zeya Ahmad"
        ],
        "submitted": "2025-07-08 17:22:32",
        "source": "arxiv",
        "comment": "Submitted to Journal of Machine Learning Research (MLOSS); UQLM\n  Repository: https://github.com/cvs-health/uqlm"
    },
    {
        "title": "DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification",
        "abstract": "Numerical claims, statements involving quantities, comparisons, and temporal\nreferences, pose unique challenges for automated fact-checking systems. In this\nstudy, we evaluate modeling strategies for veracity prediction of such claims\nusing the QuanTemp dataset and building our own evidence retrieval pipeline. We\ninvestigate three key factors: (1) the impact of more evidences with longer\ninput context windows using ModernBERT, (2) the effect of right-to-left (R2L)\ntokenization, and (3) their combined influence on classification performance.\nContrary to prior findings in arithmetic reasoning tasks, R2L tokenization does\nnot boost natural language inference (NLI) of numerical tasks. A longer context\nwindow does also not enhance veracity performance either, highlighting evidence\nquality as the dominant bottleneck. Our best-performing system achieves\ncompetitive macro-average F1 score of 0.57 and places us among the Top-4\nsubmissions in Task 3 of CheckThat! 2025. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-numerical.",
        "url": "http://arxiv.org/abs/2507.06195v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06195v1",
        "arxiv_id": "2507.06195v1",
        "authors": [
            "Maximilian Heil",
            "Aleksandar Pramov"
        ],
        "submitted": "2025-07-08 17:22:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads",
        "abstract": "Database research and development often require a large number of SQL queries\nfor benchmarking purposes. However, acquiring real-world SQL queries is\nchallenging due to privacy concerns, and existing SQL generation methods are\nlimited in customization and in satisfying realistic constraints. To address\nthis issue, we present SQLBarber, a system based on Large Language Models\n(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)\neliminates the need for users to manually craft SQL templates in advance, while\nproviding the flexibility to accept natural language specifications to\nconstrain SQL templates, (ii) scales efficiently to generate large volumes of\nqueries matching any user-defined cost distribution (e.g., cardinality and\nexecution plan cost), and (iii) uses execution statistics from Amazon Redshift\nand Snowflake to derive SQL template specifications and query cost\ndistributions that reflect real-world query characteristics. SQLBarber\nintroduces (i) a declarative interface for users to effortlessly generate\ncustomized SQL templates, (ii) an LLM-powered pipeline augmented with a\nself-correction module that profiles, refines, and prunes SQL templates based\non query costs, and (iii) a Bayesian Optimizer to efficiently explore different\npredicate values and identify a set of queries that satisfy the target cost\ndistribution. We construct and open-source ten benchmarks of varying difficulty\nlevels and target query cost distributions based on real-world statistics from\nSnowflake and Amazon Redshift. Extensive experiments on these benchmarks show\nthat SQLBarber is the only system that can generate customized SQL templates.\nIt reduces query generation time by one to three orders of magnitude, and\nsignificantly improves alignment with the target cost distribution, compared\nwith existing methods.",
        "url": "http://arxiv.org/abs/2507.06192v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06192v1",
        "arxiv_id": "2507.06192v1",
        "authors": [
            "Jiale Lao",
            "Immanuel Trummer"
        ],
        "submitted": "2025-07-08 17:20:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation",
        "abstract": "This paper presents our submission to Task 1, Subjectivity Detection, of the\nCheckThat! Lab at CLEF 2025. We investigate the effectiveness of\ntransfer-learning and stylistic data augmentation to improve classification of\nsubjective and objective sentences in English news text. Our approach contrasts\nfine-tuning of pre-trained encoders and transfer-learning of fine-tuned\ntransformer on related tasks. We also introduce a controlled augmentation\npipeline using GPT-4o to generate paraphrases in predefined subjectivity\nstyles. To ensure label and style consistency, we employ the same model to\ncorrect and refine the generated samples. Results show that transfer-learning\nof specified encoders outperforms fine-tuning general-purpose ones, and that\ncarefully curated augmentation significantly enhances model robustness,\nespecially in detecting subjective content. Our official submission placed us\n$16^{th}$ of 24 participants. Overall, our findings underscore the value of\ncombining encoder specialization with label-consistent augmentation for\nimproved subjectivity detection. Our code is available at\nhttps://github.com/dsgt-arc/checkthat-2025-subject.",
        "url": "http://arxiv.org/abs/2507.06189v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06189v1",
        "arxiv_id": "2507.06189v1",
        "authors": [
            "Maximilian Heil",
            "Dionne Bang"
        ],
        "submitted": "2025-07-08 17:18:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
        "abstract": "In July 2025, 18 academic manuscripts on the preprint website arXiv were\nfound to contain hidden instructions known as prompts designed to manipulate\nAI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\"\nwere concealed using techniques like white-colored text. Author responses\nvaried: one planned to withdraw the affected paper, while another defended the\npractice as legitimate testing of reviewer compliance. This commentary analyzes\nthis practice as a novel form of research misconduct. We examine the technique\nof prompt injection in large language models (LLMs), revealing four types of\nhidden prompts, ranging from simple positive review commands to detailed\nevaluation frameworks. The defense that prompts served as \"honeypots\" to detect\nreviewers improperly using AI fails under examination--the consistently\nself-serving nature of prompt instructions indicates intent to manipulate.\nPublishers maintain inconsistent policies: Elsevier prohibits AI use in peer\nreview entirely, while Springer Nature permits limited use with disclosure\nrequirements. The incident exposes systematic vulnerabilities extending beyond\npeer review to any automated system processing scholarly texts, including\nplagiarism detection and citation indexing. Our analysis underscores the need\nfor coordinated technical screening at submission portals and harmonized\npolicies governing generative AI (GenAI) use in academic evaluation.",
        "url": "http://arxiv.org/abs/2507.06185v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06185v1",
        "arxiv_id": "2507.06185v1",
        "authors": [
            "Zhicheng Lin"
        ],
        "submitted": "2025-07-08 17:11:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization",
        "abstract": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.",
        "url": "http://arxiv.org/abs/2507.06181v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06181v1",
        "arxiv_id": "2507.06181v1",
        "authors": [
            "Zhongyuan Peng",
            "Yifan Yao",
            "Kaijing Ma",
            "Shuyue Guo",
            "Yizhe Li",
            "Yichi Zhang",
            "Chenchen Zhang",
            "Yifan Zhang",
            "Zhouliang Yu",
            "Luming Li",
            "Minghao Liu",
            "Yihang Xia",
            "Jiawei Shen",
            "Yuchen Wu",
            "Yixin Cao",
            "Zhaoxiang Zhang",
            "Wenhao Huang",
            "Jiaheng Liu",
            "Ge Zhang"
        ],
        "submitted": "2025-07-08 17:03:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Skywork-R1V3 Technical Report",
        "abstract": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.",
        "url": "http://arxiv.org/abs/2507.06167v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06167v2",
        "arxiv_id": "2507.06167v2",
        "authors": [
            "Wei Shen",
            "Jiangbo Pei",
            "Yi Peng",
            "Xuchen Song",
            "Yang Liu",
            "Jian Peng",
            "Haofeng Sun",
            "Yunzhuo Hao",
            "Peiyu Wang",
            "Jianhao Zhang",
            "Yahui Zhou"
        ],
        "submitted": "2025-07-08 16:47:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluation of Habitat Robotics using Large Language Models",
        "abstract": "This paper focuses on evaluating the effectiveness of Large Language Models\nat solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR\nprovides simplified environments and robotic interactions within randomized\nindoor kitchen scenes. Each randomized kitchen scene is given a task where two\nrobotic agents cooperatively work together to solve the task. We evaluated\nmultiple frontier models on Meta PARTNER environments. Our results indicate\nthat reasoning models like OpenAI o3-mini outperform non-reasoning models like\nOpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied\nenvironments. o3-mini displayed outperform across centralized, decentralized,\nfull observability, and partial observability configurations. This provides a\npromising avenue of research for embodied robotic development.",
        "url": "http://arxiv.org/abs/2507.06157v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06157v1",
        "arxiv_id": "2507.06157v1",
        "authors": [
            "William Li",
            "Lei Hamilton",
            "Kaise Al-natour",
            "Sanjeev Mohindra"
        ],
        "submitted": "2025-07-08 16:39:39",
        "source": "arxiv",
        "comment": "6 pages, IEEE HPEC submission"
    },
    {
        "title": "Coding Triangle: How Does Large Language Model Understand Code?",
        "abstract": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
        "url": "http://arxiv.org/abs/2507.06138v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06138v1",
        "arxiv_id": "2507.06138v1",
        "authors": [
            "Taolin Zhang",
            "Zihan Ma",
            "Maosong Cao",
            "Junnan Liu",
            "Songyang Zhang",
            "Kai Chen"
        ],
        "submitted": "2025-07-08 16:20:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
        "abstract": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.",
        "url": "http://arxiv.org/abs/2507.06137v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06137v1",
        "arxiv_id": "2507.06137v1",
        "authors": [
            "Mohammad Mahdi Derakhshani",
            "Dheeraj Varghese",
            "Marzieh Fadaee",
            "Cees G. M. Snoek"
        ],
        "submitted": "2025-07-08 16:19:45",
        "source": "arxiv",
        "comment": "34 pages, 12 figures"
    },
    {
        "title": "Unconditional Diffusion for Generative Sequential Recommendation",
        "abstract": "Diffusion models, known for their generative ability to simulate data\ncreation through noise-adding and denoising processes, have emerged as a\npromising approach for building generative recommenders. To incorporate user\nhistory for personalization, existing methods typically adopt a conditional\ndiffusion framework, where the reverse denoising process of reconstructing\nitems from noise is modified to be conditioned on the user history. However,\nthis design may fail to fully utilize historical information, as it gets\ndistracted by the need to model the \"item $\\leftrightarrow$ noise\" translation.\nThis motivates us to reformulate the diffusion process for sequential\nrecommendation in an unconditional manner, treating user history (instead of\nnoise) as the endpoint of the forward diffusion process (i.e., the starting\npoint of the reverse process), rather than as a conditional input. This\nformulation allows for exclusive focus on modeling the \"item $\\leftrightarrow$\nhistory\" translation. To this end, we introduce Brownian Bridge Diffusion\nRecommendation (BBDRec). By leveraging a Brownian bridge process, BBDRec\nenforces a structured noise addition and denoising mechanism, ensuring that the\ntrajectories are constrained towards a specific endpoint -- user history,\nrather than noise. Extensive experiments demonstrate BBDRec's effectiveness in\nenhancing sequential recommendation performance. The source code is available\nat https://github.com/baiyimeng/BBDRec.",
        "url": "http://arxiv.org/abs/2507.06121v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06121v1",
        "arxiv_id": "2507.06121v1",
        "authors": [
            "Yimeng Bai",
            "Yang Zhang",
            "Sihao Ding",
            "Shaohui Ruan",
            "Han Yao",
            "Danhui Guan",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-07-08 16:05:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification",
        "abstract": "We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on\nmulti-species plant identification in vegetation quadrat images. Our pipeline\ncombines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level\ninference, (ii) a 4x4 tiling strategy that aligns patch size with the network's\n518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +\nK-Means visual clustering and geolocation filtering. Tile predictions are\naggregated by majority vote and re-weighted with cluster-specific Bayesian\npriors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while\nrequiring no additional training. All code, configuration files, and\nreproducibility scripts are publicly available at\nhttps://github.com/dsgt-arc/plantclef-2025.",
        "url": "http://arxiv.org/abs/2507.06093v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06093v1",
        "arxiv_id": "2507.06093v1",
        "authors": [
            "Murilo Gustineli",
            "Anthony Miyaguchi",
            "Adrian Cheung",
            "Divyansh Khattak"
        ],
        "submitted": "2025-07-08 15:35:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Nyay-Darpan: Enhancing Decision Making Through Summarization and Case Retrieval for Consumer Law in India",
        "abstract": "AI-based judicial assistance and case prediction have been extensively\nstudied in criminal and civil domains, but remain largely unexplored in\nconsumer law, especially in India. In this paper, we present Nyay-Darpan, a\nnovel two-in-one framework that (i) summarizes consumer case files and (ii)\nretrieves similar case judgements to aid decision-making in consumer dispute\nresolution. Our methodology not only addresses the gap in consumer law AI tools\nbut also introduces an innovative approach to evaluate the quality of the\nsummary. The term 'Nyay-Darpan' translates into 'Mirror of Justice',\nsymbolizing the ability of our tool to reflect the core of consumer disputes\nthrough precise summarization and intelligent case retrieval. Our system\nachieves over 75 percent accuracy in similar case prediction and approximately\n70 percent accuracy across material summary evaluation metrics, demonstrating\nits practical effectiveness. We will publicly release the Nyay-Darpan framework\nand dataset to promote reproducibility and facilitate further research in this\nunderexplored yet impactful domain.",
        "url": "http://arxiv.org/abs/2507.06090v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06090v1",
        "arxiv_id": "2507.06090v1",
        "authors": [
            "Swapnil Bhattacharyya",
            "Shrey Ganatra",
            "Harshvivek Kashid",
            "Spandan Anaokar",
            "Shruti Nair",
            "Reshma Sekhar",
            "Siddharth Manohar",
            "Rahul Hemrajani",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-07-08 15:30:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Survey on Prompt Tuning",
        "abstract": "This survey reviews prompt tuning, a parameter-efficient approach for\nadapting language models by prepending trainable continuous vectors while\nkeeping the model frozen. We classify existing approaches into two categories:\ndirect prompt learning and transfer learning. Direct prompt learning methods\ninclude: general optimization approaches, encoder-based methods, decomposition\nstrategies, and mixture-of-experts frameworks. Transfer learning methods\nconsist of: general transfer approaches, encoder-based methods, and\ndecomposition strategies. For each method, we analyze method designs,\ninnovations, insights, advantages, and disadvantages, with illustrative\nvisualizations comparing different frameworks. We identify challenges in\ncomputational efficiency and training stability, and discuss future directions\nin improving training robustness and broadening application scope.",
        "url": "http://arxiv.org/abs/2507.06085v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06085v2",
        "arxiv_id": "2507.06085v2",
        "authors": [
            "Zongqian Li",
            "Yixuan Su",
            "Nigel Collier"
        ],
        "submitted": "2025-07-08 15:24:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The bitter lesson of misuse detection",
        "abstract": "Prior work on jailbreak detection has established the importance of\nadversarial robustness for LLMs but has largely focused on the model ability to\nresist adversarial inputs and to output safe content, rather than the\neffectiveness of external supervision systems. The only public and independent\nbenchmark of these guardrails to date evaluates a narrow set of supervisors on\nlimited scenarios. Consequently, no comprehensive public benchmark yet verifies\nhow well supervision systems from the market perform under realistic, diverse\nattacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of\nLLM Supervision Systems. The framework is two dimensional: harm severity\n(benign, borderline, harmful) and adversarial sophistication (direct vs.\njailbreak) and provides a rich dataset covering 3 jailbreak families and 11\nharm categories. Our evaluations reveal drastic limitations of specialized\nsupervision systems. While they recognize some known jailbreak patterns, their\nsemantic understanding and generalization capabilities are very limited,\nsometimes with detection rates close to zero when asking a harmful question\ndirectly or with a new jailbreak technique such as base64 encoding. Simply\nasking generalist LLMs if the user question is \"harmful or not\" largely\noutperforms these supervisors from the market according to our BELLS score. But\nfrontier LLMs still suffer from metacognitive incoherence, often responding to\nqueries they correctly identify as harmful (up to 30 percent for Claude 3.7 and\ngreater than 50 percent for Mistral Large). These results suggest that simple\nscaffolding could significantly improve misuse detection robustness, but more\nresearch is needed to assess the tradeoffs of such techniques. Our results\nsupport the \"bitter lesson\" of misuse detection: general capabilities of LLMs\nare necessary to detect a diverse array of misuses and jailbreaks.",
        "url": "http://arxiv.org/abs/2507.06282v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06282v1",
        "arxiv_id": "2507.06282v1",
        "authors": [
            "Hadrien Mariaccia",
            "Charbel-Raphaël Segerie",
            "Diego Dorn"
        ],
        "submitted": "2025-07-08 15:21:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Contrastive and Transfer Learning for Effective Audio Fingerprinting through a Real-World Evaluation Protocol",
        "abstract": "Recent advances in song identification leverage deep neural networks to learn\ncompact audio fingerprints directly from raw waveforms. While these methods\nperform well under controlled conditions, their accuracy drops significantly in\nreal-world scenarios where the audio is captured via mobile devices in noisy\nenvironments. In this paper, we introduce a novel evaluation protocol designed\nto better reflect such real-world conditions. We generate three recordings of\nthe same audio, each with increasing levels of noise, captured using a mobile\ndevice's microphone. Our results reveal a substantial performance drop for two\nstate-of-the-art CNN-based models under this protocol, compared to previously\nreported benchmarks. Additionally, we highlight the critical role of the\naugmentation pipeline during training with contrastive loss. By introduction\nlow pass and high pass filters in the augmentation pipeline we significantly\nincrease the performance of both systems in our proposed evaluation.\nFurthermore, we develop a transformer-based model with a tailored projection\nmodule and demonstrate that transferring knowledge from a semantically relevant\ndomain yields a more robust solution. The transformer architecture outperforms\nCNN-based models across all noise levels, and query durations. In low noise\nconditions it achieves 47.99% for 1-sec queries, and 97% for 10-sec queries in\nfinding the correct song, surpassing by 14%, and by 18.5% the second-best\nperforming model, respectively, Under heavy noise levels, we achieve a\ndetection rate 56.5% for 15-second query duration. All experiments are\nconducted on public large-scale dataset of over 100K songs, with queries\nmatched against a database of 56 million vectors.",
        "url": "http://arxiv.org/abs/2507.06070v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06070v1",
        "arxiv_id": "2507.06070v1",
        "authors": [
            "Christos Nikou",
            "Theodoros Giannakopoulos"
        ],
        "submitted": "2025-07-08 15:13:26",
        "source": "arxiv",
        "comment": "International Journal of Music Science, Technology and Art, 15 pages,\n  7 figures"
    },
    {
        "title": "Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs",
        "abstract": "Large Language Models (LLMs) are known to memorize portions of their training\ndata, sometimes reproducing content verbatim when prompted appropriately. In\nthis work, we investigate a fundamental yet under-explored question in the\ndomain of memorization: How to characterize memorization difficulty of training\ndata in LLMs? Through empirical experiments on OLMo, a family of open models,\nwe present the Entropy-Memorization Law. It suggests that data entropy is\nlinearly correlated with memorization score. Moreover, in a case study of\nmemorizing highly randomized strings, or \"gibberish\", we observe that such\nsequences, despite their apparent randomness, exhibit unexpectedly low\nempirical entropy compared to the broader training corpus. Adopting the same\nstrategy to discover Entropy-Memorization Law, we derive a simple yet effective\napproach to distinguish training and testing data, enabling Dataset Inference\n(DI).",
        "url": "http://arxiv.org/abs/2507.06056v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06056v1",
        "arxiv_id": "2507.06056v1",
        "authors": [
            "Yizhan Huang",
            "Zhe Yang",
            "Meifang Chen",
            "Jianping Zhang",
            "Michael R. Lyu"
        ],
        "submitted": "2025-07-08 14:58:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hierarchical Interaction Summarization and Contrastive Prompting for Explainable Recommendations",
        "abstract": "Explainable recommendations, which use the information of user and item with\ninteraction to generate a explanation for why the user would interact with the\nitem, are crucial for improving user trust and decision transparency to the\nrecommender system. Existing methods primarily rely on encoding features of\nusers and items to embeddings, which often leads to information loss due to\ndimensionality reduction, sparse interactions, and so on. With the advancements\nof large language models (LLMs) in language comprehension, some methods use\nembeddings as LLM inputs for explanation generation. However, since embeddings\nlack inherent semantics, LLMs must adjust or extend their parameters to\ninterpret them, a process that inevitably incurs information loss. To address\nthis issue, we propose a novel approach combining profile generation via\nhierarchical interaction summarization (PGHIS), which leverages a pretrained\nLLM to hierarchically summarize user-item interactions, generating structured\ntextual profiles as explicit representations of user and item characteristics.\nAdditionally, we propose contrastive prompting for explanation generation\n(CPEG) which employs contrastive learning to guide another reasoning language\nmodels in producing high-quality ground truth recommendation explanations.\nFinally, we use the textual profiles of user and item as input and high-quality\nexplanation as output to fine-tune a LLM for generating explanations.\nExperimental results on multiple datasets demonstrate that our approach\noutperforms existing state-of-the-art methods, achieving a great improvement on\nmetrics about explainability (e.g., 5% on GPTScore) and text quality.\nFurthermore, our generated ground truth explanations achieve a significantly\nhigher win rate compared to user-written reviews and those produced by other\nmethods, demonstrating the effectiveness of CPEG in generating high-quality\nground truths.",
        "url": "http://arxiv.org/abs/2507.06044v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06044v1",
        "arxiv_id": "2507.06044v1",
        "authors": [
            "Yibin Liu",
            "Ang Li",
            "Shijian Li"
        ],
        "submitted": "2025-07-08 14:45:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents",
        "abstract": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%.",
        "url": "http://arxiv.org/abs/2507.06016v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06016v1",
        "arxiv_id": "2507.06016v1",
        "authors": [
            "Youmna Farag",
            "Svetlana Stoyanchev",
            "Mohan Li",
            "Simon Keizer",
            "Rama Doddipatla"
        ],
        "submitted": "2025-07-08 14:23:41",
        "source": "arxiv",
        "comment": "Accepted at REALM 2025"
    },
    {
        "title": "DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations",
        "abstract": "Large, high-quality annotated corpora remain scarce in document-level entity\nand relation extraction in zero-shot or few-shot settings. In this paper, we\npresent a fully automatic, LLM-based pipeline for synthetic data generation and\nin-context learning for document-level entity and relation extraction. In\ncontrast to existing approaches that rely on manually annotated demonstrations\nor direct zero-shot inference, our method combines synthetic data generation\nwith retrieval-based in-context learning, using a reasoning-optimized language\nmodel. This allows us to build a high-quality demonstration database without\nmanual annotation and to dynamically retrieve relevant examples at inference\ntime. Based on our approach we produce a synthetic dataset of over $5k$\nWikipedia abstracts with approximately $59k$ entities and $30k$ relation\ntriples. Finally, we evaluate in-context learning performance on the DocIE\nshared task, extracting entities and relations from long documents in a\nzero-shot setting. We find that in-context joint entity and relation extraction\nat document-level remains a challenging task, even for state-of-the-art large\nlanguage models.",
        "url": "http://arxiv.org/abs/2507.05997v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05997v1",
        "arxiv_id": "2507.05997v1",
        "authors": [
            "Nicholas Popovič",
            "Ashish Kangen",
            "Tim Schopf",
            "Michael Färber"
        ],
        "submitted": "2025-07-08 13:55:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evolution without Large Models: Training Language Model with Task Principles",
        "abstract": "A common training approach for language models involves using a large-scale\nlanguage model to expand a human-provided dataset, which is subsequently used\nfor model training.This method significantly reduces training costs by\neliminating the need for extensive human data annotation. However, it still\nfaces challenges such as high carbon emissions during data augmentation and the\nrisk of data leakage when we use closed-source LLMs. To address these issues,\nwe propose a self-evolution method for language models. First, we introduce the\nMulti-level Principle Generation, which enables a large-scale model to\nsummarize task-completion principles based on a small amount of task data.\nThen, we propose the Principle-based Instance Generation, in which a\nsmaller-scale language model uses these task principles to generate a large\namount of data. This data is then used for model training. Experimental results\nshow that our proposed method significantly improves model performance compared\nto directly using a smaller-scale language model to generate data.\nAdditionally, since we only use the large-scale language model to generate the\ntask-completion principles, the carbon emissions associated with training the\nmodel are greatly reduced.",
        "url": "http://arxiv.org/abs/2507.05991v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05991v1",
        "arxiv_id": "2507.05991v1",
        "authors": [
            "Minghang Zhu",
            "Shen Gao",
            "Zhengliang Shi",
            "Jiabao Fang",
            "Pengjie Ren",
            "Zhaochun Ren",
            "Zhumin Chen",
            "Shuo Shang"
        ],
        "submitted": "2025-07-08 13:52:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening",
        "abstract": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively\nscreen depression but lack interactivity and adaptability. We developed\nHopeBot, a chatbot powered by a large language model (LLM) that administers the\nPHQ-9 using retrieval-augmented generation and real-time clarification. In a\nwithin-subject study, 132 adults in the United Kingdom and China completed both\nself-administered and chatbot versions. Scores demonstrated strong agreement\n(ICC = 0.91; 45% identical). Among 75 participants providing comparative\nfeedback, 71% reported greater trust in the chatbot, highlighting clearer\nstructure, interpretive guidance, and a supportive tone. Mean ratings (0-10)\nwere 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,\nand 7.4 for recommendation helpfulness; the latter varied significantly by\nemployment status and prior mental-health service use (p < 0.05). Overall,\n87.1% expressed willingness to reuse or recommend HopeBot. These findings\ndemonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden\nadjuncts for routine depression screening.",
        "url": "http://arxiv.org/abs/2507.05984v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05984v1",
        "arxiv_id": "2507.05984v1",
        "authors": [
            "Zhijun Guo",
            "Alvina Lai",
            "Julia Ive",
            "Alexandru Petcu",
            "Yutong Wang",
            "Luyuan Qi",
            "Johan H Thygesen",
            "Kezhi Li"
        ],
        "submitted": "2025-07-08 13:41:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages",
        "abstract": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.",
        "url": "http://arxiv.org/abs/2507.05980v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05980v1",
        "arxiv_id": "2507.05980v1",
        "authors": [
            "Gabriel Chua",
            "Leanne Tan",
            "Ziyu Ge",
            "Roy Ka-Wei Lee"
        ],
        "submitted": "2025-07-08 13:37:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing the Interpretability of Rule-based Explanations through Information Retrieval",
        "abstract": "The lack of transparency of data-driven Artificial Intelligence techniques\nlimits their interpretability and acceptance into healthcare decision-making\nprocesses. We propose an attribution-based approach to improve the\ninterpretability of Explainable AI-based predictions in the specific context of\narm lymphedema's risk assessment after lymph nodal radiotherapy in breast\ncancer. The proposed method performs a statistical analysis of the attributes\nin the rule-based prediction model using standard metrics from Information\nRetrieval techniques. This analysis computes the relevance of each attribute to\nthe prediction and provides users with interpretable information about the\nimpact of risk factors. The results of a user study that compared the output\ngenerated by the proposed approach with the raw output of the Explainable AI\nmodel suggested higher levels of interpretability and usefulness in the context\nof predicting lymphedema risk.",
        "url": "http://arxiv.org/abs/2507.05976v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05976v1",
        "arxiv_id": "2507.05976v1",
        "authors": [
            "Alessandro Umbrico",
            "Guido Bologna",
            "Luca Coraci",
            "Francesca Fracasso",
            "Silvia Gola",
            "Gabriella Cortellessa"
        ],
        "submitted": "2025-07-08 13:32:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "We Should Evaluate Real-World Impact",
        "abstract": "The ACL community has very little interest in evaluating the real-world\nimpact of NLP systems. A structured survey of the ACL Anthology shows that\nperhaps 0.1% of its papers contain such evaluations; furthermore most papers\nwhich include impact evaluations present them very sketchily and instead focus\non metric evaluations. NLP technology would be more useful and more quickly\nadopted if we seriously tried to understand and evaluate its real-world impact.",
        "url": "http://arxiv.org/abs/2507.05973v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05973v1",
        "arxiv_id": "2507.05973v1",
        "authors": [
            "Ehud Reiter"
        ],
        "submitted": "2025-07-08 13:29:04",
        "source": "arxiv",
        "comment": "This paper will appear in Computational Linguistics journal as a\n  \"Last Word\" opinion piece. The Arxiv version is a pre-MIT Press publication\n  version"
    },
    {
        "title": "OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation",
        "abstract": "We introduce OpenFActScore, an open-source implementation of the FActScore\nframework for evaluating the factuality of text generated by large language\nmodels (LLMs). FActScore evaluates the factual accuracy of long-form text by\nusing Atomic Fact Generation (AFG) to extract individual factual claims and\nAtomic Fact Validation (AFV) to verify each claim against a trusted knowledge\nsource. While the original FActScore relies on closed-source and commercial\nmodels such as InstructGPT and ChatGPT, OpenFActScore enables the use of any\nHugging Face-compatible model for both AFG and AFV. We provide a detailed\ntechnical overview of our implementation, highlighting design choices and\nmodifications made to support open models. We evaluate multiple open-source\nLLMs on both AFG and AFV using the original FActScore benchmark, reporting\nBERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our\nresults show that open models can approximate the performance of closed-source\nsystems, with Gemma achieving the best overall performance, and our final setup\nobtains a 0.99 Pearson correlation with the original FActScore experiments.\nOpenFActScore promotes transparency, reproducibility, and cost-effective\nevaluation, and is available at: https://github.com/lflage/OpenFActScore.",
        "url": "http://arxiv.org/abs/2507.05965v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05965v1",
        "arxiv_id": "2507.05965v1",
        "authors": [
            "Lucas Fonseca Lage",
            "Simon Ostermann"
        ],
        "submitted": "2025-07-08 13:19:00",
        "source": "arxiv",
        "comment": "Submitted to EMNLP 2025 System Demonstrations track"
    },
    {
        "title": "Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems",
        "abstract": "Ghosting, the ability to predict a user's intended text input for inline\nquery auto-completion, is an invaluable feature for modern search engines and\nchat interfaces, greatly enhancing user experience. By suggesting completions\nto incomplete queries (or prefixes), ghosting aids users with slow typing\nspeeds, disabilities, or limited language proficiency. Ghosting is a\nchallenging problem and has become more important with the ubiquitousness of\nchat-based systems like ChatGPT, Copilot, etc. Despite the increasing\nprominence of chat-based systems utilizing ghosting, this challenging problem\nof Chat-Ghosting has received little attention from the NLP/ML research\ncommunity. There is a lack of standardized benchmarks and relative performance\nanalysis of deep learning and non-deep learning methods. We address this\nthrough an open and thorough study of this problem using four publicly\navailable dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and\ntwo human-bot (Open Assistant and ShareGPT). We experiment with various\nexisting query auto-completion methods (using tries), n-gram methods and deep\nlearning methods, with and without dialog context. We also propose a novel\nentropy-based dynamic early stopping strategy. Our analysis finds that\nstatistical n-gram models and tries outperform deep learning based models in\nterms of both model performance and inference efficiency for seen prefixes. For\nunseen queries, neural models like T5 and Phi-2 lead to better results. Adding\nconversational context leads to significant improvements in ghosting quality,\nespecially for Open-Assistant and ShareGPT. We make code and data publicly\navailable",
        "url": "http://arxiv.org/abs/2507.05940v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05940v1",
        "arxiv_id": "2507.05940v1",
        "authors": [
            "Sandeep Mishra",
            "Anubhab Mandal",
            "Bishal Santra",
            "Tushar Abhishek",
            "Pawan Goyal",
            "Manish Gupta"
        ],
        "submitted": "2025-07-08 12:38:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors",
        "abstract": "Nowadays, misinformation articles, especially multimodal ones, are widely\nspread on social media platforms and cause serious negative effects. To control\ntheir propagation, Multimodal Misinformation Detection (MMD) becomes an active\ntopic in the community to automatically identify misinformation. Previous MMD\nmethods focus on supervising detectors by collecting offline data. However, in\nreal-world scenarios, new events always continually emerge, making MMD models\ntrained on offline data consistently outdated and ineffective. To address this\nissue, training MMD models under online data streams is an alternative,\ninducing an emerging task named continual MMD. Unfortunately, it is hindered by\ntwo major challenges. First, training on new data consistently decreases the\ndetection performance on past data, named past knowledge forgetting. Second,\nthe social environment constantly evolves over time, affecting the\ngeneralization on future data. To alleviate these challenges, we propose to\nremember past knowledge by isolating interference between event-specific\nparameters with a Dirichlet process-based mixture-of-expert structure, and\nanticipate future environmental distributions by learning a continuous-time\ndynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.\nExtensive experiments demonstrate that DAEDCMD can consistently and\nsignificantly outperform the compared methods, including six MMD baselines and\nthree continual learning methods.",
        "url": "http://arxiv.org/abs/2507.05939v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05939v1",
        "arxiv_id": "2507.05939v1",
        "authors": [
            "Bing Wang",
            "Ximing Li",
            "Mengzhe Ye",
            "Changchun Li",
            "Bo Fu",
            "Jianfeng Qu",
            "Lin Yuanbo Wu"
        ],
        "submitted": "2025-07-08 12:38:19",
        "source": "arxiv",
        "comment": "Accepted by ACM MM 2025. 10 pages, 6 figures. Code:\n  https://github.com/wangbing1416/DAEDCMD"
    },
    {
        "title": "Towards a Principled Evaluation of Knowledge Editors",
        "abstract": "Model editing has been gaining increasing attention over the past few years.\nFor Knowledge Editing in particular, more challenging evaluation datasets have\nrecently been released. These datasets use different methodologies to score the\nsuccess of editors. Yet, it remains under-explored how robust these\nmethodologies are and whether they unfairly favor some editors. Moreover, the\ndisruptive impact of these editors on overall model capabilities remains a\nconstant blind spot.\n  We address both of these problems and show that choosing different metrics\nand evaluation methodologies as well as different edit batch sizes can lead to\na different ranking of knowledge editors. Crucially we demonstrate this effect\nalso on general language understanding tasks evaluated alongside the knowledge\nediting tasks. Further we include a manual assessment of the string matching\nbased evaluation method for knowledge editing that is favored by recently\nreleased datasets, revealing a tendency to produce false positive matches.",
        "url": "http://arxiv.org/abs/2507.05937v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05937v1",
        "arxiv_id": "2507.05937v1",
        "authors": [
            "Sebastian Pohl",
            "Max Ploner",
            "Alan Akbik"
        ],
        "submitted": "2025-07-08 12:37:54",
        "source": "arxiv",
        "comment": "Accepted at L2M2 workshop at ACL 2025"
    },
    {
        "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation",
        "abstract": "Vector retrieval systems exhibit significant performance variance across\nqueries due to heterogeneous embedding quality. We propose a lightweight\nframework for predicting retrieval performance at the query level by combining\nquantization robustness and neighborhood density metrics. Our approach is\nmotivated by the observation that high-quality embeddings occupy geometrically\nstable regions in the embedding space and exhibit consistent neighborhood\nstructures. We evaluate our method on 4 standard retrieval datasets, showing\nconsistent improvements of 9.4$\\pm$1.2\\% in Recall@10 over competitive\nbaselines. The framework requires minimal computational overhead (less than 5\\%\nof retrieval time) and enables adaptive retrieval strategies. Our analysis\nreveals systematic patterns in embedding quality across different query types,\nproviding insights for targeted training data augmentation.",
        "url": "http://arxiv.org/abs/2507.05933v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05933v1",
        "arxiv_id": "2507.05933v1",
        "authors": [
            "Y. Du"
        ],
        "submitted": "2025-07-08 12:33:11",
        "source": "arxiv",
        "comment": "7 pages"
    },
    {
        "title": "Few-shot text-based emotion detection",
        "abstract": "This paper describes the approach of the Unibuc - NLP team in tackling the\nSemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion\nDetection. We mainly focused on experiments using large language models\n(Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With\nour final system, for the multi-label emotion detection track (track A), we got\nan F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36\nteams) for the Portuguese (Mozambican) subset and $0.325$ (\\textbf{1}/31 teams)\nfor the Emakhuwa subset.",
        "url": "http://arxiv.org/abs/2507.05918v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05918v1",
        "arxiv_id": "2507.05918v1",
        "authors": [
            "Teodor-George Marchitan",
            "Claudiu Creanga",
            "Liviu P. Dinu"
        ],
        "submitted": "2025-07-08 12:03:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI-Reporter: A Path to a New Genre of Scientific Communication",
        "abstract": "The AI-Reporter represents a paradigmatic shift in scientific publication\npractice. This document demonstrates through a concrete case study how our\nsystem transforms academic presentations into publication-ready chapters -- in\nless than three minutes. Using Arno Simons' lecture on Large Language Models\nfrom the ``Large Language Models for the History, Philosophy, and Sociology of\nScience'' workshop (NEPI) as an example, we show how technological innovation\nbridges the gap between ephemeral presentation and permanent scientific\ndocumentation.",
        "url": "http://arxiv.org/abs/2507.05903v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05903v1",
        "arxiv_id": "2507.05903v1",
        "authors": [
            "Gerd Graßhoff"
        ],
        "submitted": "2025-07-08 11:41:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MusiScene: Leveraging MU-LLaMA for Scene Imagination and Enhanced Video Background Music Generation",
        "abstract": "Humans can imagine various atmospheres and settings when listening to music,\nenvisioning movie scenes that complement each piece. For example, slow,\nmelancholic music might evoke scenes of heartbreak, while upbeat melodies\nsuggest celebration. This paper explores whether a Music Language Model, e.g.\nMU-LLaMA, can perform a similar task, called Music Scene Imagination (MSI),\nwhich requires cross-modal information from video and music to train. To\nimprove upon existing music captioning models which focusing solely on musical\nelements, we introduce MusiScene, a music captioning model designed to imagine\nscenes that complement each music. In this paper, (1) we construct a\nlarge-scale video-audio caption dataset with 3,371 pairs, (2) we finetune Music\nUnderstanding LLaMA for the MSI task to create MusiScene, and (3) we conduct\ncomprehensive evaluations and prove that our MusiScene is more capable of\ngenerating contextually relevant captions compared to MU-LLaMA. We leverage the\ngenerated MSI captions to enhance Video Background Music Generation (VBMG) from\ntext.",
        "url": "http://arxiv.org/abs/2507.05894v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05894v1",
        "arxiv_id": "2507.05894v1",
        "authors": [
            "Fathinah Izzati",
            "Xinyue Li",
            "Yuxuan Wu",
            "Gus Xia"
        ],
        "submitted": "2025-07-08 11:32:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators",
        "abstract": "As psychometric surveys are increasingly used to assess the traits of large\nlanguage models (LLMs), the need for scalable survey item generation suited for\nLLMs has also grown. A critical challenge here is ensuring the construct\nvalidity of generated items, i.e., whether they truly measure the intended\ntrait. Traditionally, this requires costly, large-scale human data collection.\nTo make it efficient, we present a framework for virtual respondent simulation\nusing LLMs. Our central idea is to account for mediators: factors through which\nthe same trait can give rise to varying responses to a survey item. By\nsimulating respondents with diverse mediators, we identify survey items that\nrobustly measure intended traits. Experiments on three psychological trait\ntheories (Big5, Schwartz, VIA) show that our mediator generation methods and\nsimulation framework effectively identify high-validity items. LLMs demonstrate\nthe ability to generate plausible mediators from trait definitions and to\nsimulate respondent behavior for item validation. Our problem formulation,\nmetrics, methodology, and dataset open a new direction for cost-effective\nsurvey development and a deeper understanding of how LLMs replicate human-like\nbehavior. We will publicly release our dataset and code to support future work.",
        "url": "http://arxiv.org/abs/2507.05890v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05890v1",
        "arxiv_id": "2507.05890v1",
        "authors": [
            "Sungjib Lim",
            "Woojung Song",
            "Eun-Ju Lee",
            "Yohan Jo"
        ],
        "submitted": "2025-07-08 11:26:03",
        "source": "arxiv",
        "comment": "17 pages, 7 figures"
    },
    {
        "title": "How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures",
        "abstract": "There is increasingly more evidence that automatic speech recognition (ASR)\nsystems are biased against different speakers and speaker groups, e.g., due to\ngender, age, or accent. Research on bias in ASR has so far primarily focused on\ndetecting and quantifying bias, and developing mitigation approaches. Despite\nthis progress, the open question is how to measure the performance and bias of\na system. In this study, we compare different performance and bias measures,\nfrom literature and proposed, to evaluate state-of-the-art end-to-end ASR\nsystems for Dutch. Our experiments use several bias mitigation strategies to\naddress bias against different speaker groups. The findings reveal that\naveraged error rates, a standard in ASR research, alone is not sufficient and\nshould be supplemented by other measures. The paper ends with recommendations\nfor reporting ASR performance and bias to better represent a system's\nperformance for diverse speaker groups, and overall system bias.",
        "url": "http://arxiv.org/abs/2507.05885v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05885v1",
        "arxiv_id": "2507.05885v1",
        "authors": [
            "Tanvina Patel",
            "Wiebke Hutiri",
            "Aaron Yi Ding",
            "Odette Scharenborg"
        ],
        "submitted": "2025-07-08 11:17:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RecRankerEval: A Flexible and Extensible Framework for Top-k LLM-based Recommendation",
        "abstract": "A recent Large language model (LLM)-based recommendation model, called\nRecRanker, has demonstrated a superior performance in the top-k recommendation\ntask compared to other models. In particular, RecRanker samples users via\nclustering, generates an initial ranking list using an initial recommendation\nmodel, and fine-tunes an LLM through hybrid instruction tuning to infer user\npreferences. However, the contribution of each core component remains\nunderexplored. In this work, we inspect the reproducibility of RecRanker, and\nstudy the impact and role of its various components. We begin by reproducing\nthe RecRanker pipeline through the implementation of all its key components.\nOur reproduction shows that the pairwise and listwise methods achieve a\nperformance comparable to that reported in the original paper. For the\npointwise method, while we are also able to reproduce the original paper's\nresults, further analysis shows that the performance is abnormally high due to\ndata leakage from the inclusion of ground-truth information in the prompts. To\nenable a fair and comprehensive evaluation of LLM-based top-k recommendations,\nwe propose RecRankerEval, an extensible framework that covers five key\ndimensions: user sampling strategy, initial recommendation model, LLM backbone,\ndataset selection, and instruction tuning method. Using the RecRankerEval\nframework, we show that the original results of RecRanker can be reproduced on\nthe ML-100K and ML-1M datasets, as well as the additional Amazon-Music dataset,\nbut not on BookCrossing due to the lack of timestamp information in the\noriginal RecRanker paper. Furthermore, we demonstrate that RecRanker's\nperformance can be improved by employing alternative user sampling methods,\nstronger initial recommenders, and more capable LLMs.",
        "url": "http://arxiv.org/abs/2507.05880v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05880v1",
        "arxiv_id": "2507.05880v1",
        "authors": [
            "Zeyuan Meng",
            "Zixuan Yi",
            "Iadh Ounis"
        ],
        "submitted": "2025-07-08 11:04:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Costs and Benefits of Learned Indexing for Dynamic High-Dimensional Data: Extended Version",
        "abstract": "One of the main challenges within the growing research area of learned\nindexing is the lack of adaptability to dynamically expanding datasets. This\npaper explores the dynamization of a static learned index for complex data\nthrough operations such as node splitting and broadening, enabling efficient\nadaptation to new data. Furthermore, we evaluate the trade-offs between static\nand dynamic approaches by introducing an amortized cost model to assess query\nperformance in tandem with the build costs of the index structure, enabling\nexperimental determination of when a dynamic learned index outperforms its\nstatic counterpart. We apply the dynamization method to a static learned index\nand demonstrate that its superior scaling quickly surpasses the static\nimplementation in terms of overall costs as the database grows. This is an\nextended version of the paper presented at DAWAK 2025.",
        "url": "http://arxiv.org/abs/2507.05865v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05865v1",
        "arxiv_id": "2507.05865v1",
        "authors": [
            "Terézia Slanináková",
            "Jaroslav Olha",
            "David Procházka",
            "Matej Antol",
            "Vlastislav Dohnal"
        ],
        "submitted": "2025-07-08 10:47:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "KERAG_R: Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation",
        "abstract": "Large Language Models (LLMs) have shown strong potential in recommender\nsystems due to their contextual learning and generalisation capabilities.\nExisting LLM-based recommendation approaches typically formulate the\nrecommendation task using specialised prompts designed to leverage their\ncontextual abilities, and aligning their outputs closely with human preferences\nto yield an improved recommendation performance. However, the use of LLMs for\nrecommendation tasks is limited by the absence of domain-specific knowledge.\nThis lack of relevant relational knowledge about the items to be recommended in\nthe LLM's pre-training corpus can lead to inaccuracies or hallucinations,\nresulting in incorrect or misleading recommendations. Moreover, directly using\ninformation from the knowledge graph introduces redundant and noisy\ninformation, which can affect the LLM's reasoning process or exceed its input\ncontext length, thereby reducing the performance of LLM-based recommendations.\nTo address the lack of domain-specific knowledge, we propose a novel model\ncalled Knowledge-Enhanced Retrieval-Augmented Generation for Recommendation\n(KERAG_R). Specifically, we leverage a graph retrieval-augmented generation\n(GraphRAG) component to integrate additional information from a knowledge graph\n(KG) into instructions, enabling the LLM to collaboratively exploit\nrecommendation signals from both text-based user interactions and the knowledge\ngraph to better estimate the users' preferences in a recommendation context. In\nparticular, we perform graph RAG by pre-training a graph attention network\n(GAT) to select the most relevant triple for the target users for the used LLM,\nthereby enhancing the LLM while reducing redundant and noisy information. Our\nextensive experiments on three public datasets show that our proposed KERAG_R\nmodel significantly outperforms ten existing state-of-the-art recommendation\nmethods.",
        "url": "http://arxiv.org/abs/2507.05863v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05863v1",
        "arxiv_id": "2507.05863v1",
        "authors": [
            "Zeyuan Meng",
            "Zixuan Yi",
            "Iadh Ounis"
        ],
        "submitted": "2025-07-08 10:44:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Affective-ROPTester: Capability and Bias Analysis of LLMs in Predicting Retinopathy of Prematurity",
        "abstract": "Despite the remarkable progress of large language models (LLMs) across\nvarious domains, their capacity to predict retinopathy of prematurity (ROP)\nrisk remains largely unexplored. To address this gap, we introduce a novel\nChinese benchmark dataset, termed CROP, comprising 993 admission records\nannotated with low, medium, and high-risk labels. To systematically examine the\npredictive capabilities and affective biases of LLMs in ROP risk\nstratification, we propose Affective-ROPTester, an automated evaluation\nframework incorporating three prompting strategies: Instruction-based,\nChain-of-Thought (CoT), and In-Context Learning (ICL). The Instruction scheme\nassesses LLMs' intrinsic knowledge and associated biases, whereas the CoT and\nICL schemes leverage external medical knowledge to enhance predictive accuracy.\nCrucially, we integrate emotional elements at the prompt level to investigate\nhow different affective framings influence the model's ability to predict ROP\nand its bias patterns. Empirical results derived from the CROP dataset yield\ntwo principal observations. First, LLMs demonstrate limited efficacy in ROP\nrisk prediction when operating solely on intrinsic knowledge, yet exhibit\nmarked performance gains when augmented with structured external inputs.\nSecond, affective biases are evident in the model outputs, with a consistent\ninclination toward overestimating medium- and high-risk cases. Third, compared\nto negative emotions, positive emotional framing contributes to mitigating\npredictive bias in model outputs. These findings highlight the critical role of\naffect-sensitive prompt engineering in enhancing diagnostic reliability and\nemphasize the utility of Affective-ROPTester as a framework for evaluating and\nmitigating affective bias in clinical language modeling systems.",
        "url": "http://arxiv.org/abs/2507.05816v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05816v1",
        "arxiv_id": "2507.05816v1",
        "authors": [
            "Shuai Zhao",
            "Yulin Zhang",
            "Luwei Xiao",
            "Xinyi Wu",
            "Yanhao Jia",
            "Zhongliang Guo",
            "Xiaobao Wu",
            "Cong-Duy Nguyen",
            "Guoming Zhang",
            "Anh Tuan Luu"
        ],
        "submitted": "2025-07-08 09:36:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports",
        "abstract": "One of the main objectives in developing large vision-language models (LVLMs)\nis to engineer systems that can assist humans with multimodal tasks, including\ninterpreting descriptions of perceptual experiences. A central phenomenon in\nthis context is amodal completion, in which people perceive objects even when\nparts of those objects are hidden. Although numerous studies have assessed\nwhether computer-vision algorithms can detect or reconstruct occluded regions,\nthe inferential abilities of LVLMs on texts related to amodal completion remain\nunexplored. To address this gap, we constructed a benchmark grounded in Basic\nFormal Ontology to achieve a systematic classification of amodal completion.\nOur results indicate that while many LVLMs achieve human-comparable performance\noverall, their accuracy diverges for certain types of objects being completed.\nNotably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet\nexhibit lower accuracy on original images compared to blank stimuli lacking\nvisual content. Intriguingly, this disparity emerges only under Japanese\nprompting, suggesting a deficiency in Japanese-specific linguistic competence\namong these models.",
        "url": "http://arxiv.org/abs/2507.05799v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05799v1",
        "arxiv_id": "2507.05799v1",
        "authors": [
            "Amane Watahiki",
            "Tomoki Doi",
            "Taiga Shinozaki",
            "Satoshi Nishida",
            "Takuya Niikawa",
            "Katsunori Miyahara",
            "Hitomi Yanaka"
        ],
        "submitted": "2025-07-08 09:06:47",
        "source": "arxiv",
        "comment": "To appear in the Proceedings of the 47th Annual Meeting of the\n  Cognitive Science Society (COGSCI 2025)"
    },
    {
        "title": "Flippi: End To End GenAI Assistant for E-Commerce",
        "abstract": "The emergence of conversational assistants has fundamentally reshaped user\ninteractions with digital platforms. This paper introduces Flippi-a\ncutting-edge, end-to-end conversational assistant powered by large language\nmodels (LLMs) and tailored for the e-commerce sector. Flippi addresses the\nchallenges posed by the vast and often overwhelming product landscape, enabling\ncustomers to discover products more efficiently through natural language\ndialogue. By accommodating both objective and subjective user requirements,\nFlippi delivers a personalized shopping experience that surpasses traditional\nsearch methods. This paper details how Flippi interprets customer queries to\nprovide precise product information, leveraging advanced NLP techniques such as\nQuery Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG),\nNamed Entity Recognition (NER), and Context Reduction. Flippi's unique\ncapability to identify and present the most attractive offers on an e-commerce\nsite is also explored, demonstrating how it empowers users to make\ncost-effective decisions. Additionally, the paper discusses Flippi's\ncomparative analysis features, which help users make informed choices by\ncontrasting product features, prices, and other relevant attributes. The\nsystem's robust architecture is outlined, emphasizing its adaptability for\nintegration across various e-commerce platforms and the technological choices\nunderpinning its performance and accuracy. Finally, a comprehensive evaluation\nframework is presented, covering performance metrics, user satisfaction, and\nthe impact on customer engagement and conversion rates. By bridging the\nconvenience of online shopping with the personalized assistance traditionally\nfound in physical stores, Flippi sets a new standard for customer satisfaction\nand engagement in the digital marketplace.",
        "url": "http://arxiv.org/abs/2507.05788v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05788v1",
        "arxiv_id": "2507.05788v1",
        "authors": [
            "Anand A. Rajasekar",
            "Praveen Tangarajan",
            "Anjali Nainani",
            "Amogh Batwal",
            "Vinay Rao Dandin",
            "Anusua Trivedi",
            "Ozan Ersoy"
        ],
        "submitted": "2025-07-08 08:50:47",
        "source": "arxiv",
        "comment": "10 pages, 2 figures, 7 tables"
    },
    {
        "title": "Vers un cadre ontologique pour la gestion des comp{é}tences : {à} des fins de formation, de recrutement, de m{é}tier, ou de recherches associ{é}es",
        "abstract": "The rapid transformation of the labor market, driven by technological\nadvancements and the digital economy, requires continuous competence\ndevelopment and constant adaptation. In this context, traditional competence\nmanagement systems lack interoperability, adaptability, and semantic\nunderstanding, making it difficult to align individual competencies with labor\nmarket needs and training programs. This paper proposes an ontology-based\nframework for competence management, enabling a structured representation of\ncompetencies, occupations, and training programs. By leveraging ontological\nmodels and semantic reasoning, this framework aims to enhance the automation of\ncompetence-to-job matching, the personalization of learning recommendations,\nand career planning. This study discusses the design, implementation, and\npotential applications of the framework, focusing on competence training\nprograms, job searching, and finding competent individuals.",
        "url": "http://arxiv.org/abs/2507.05767v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05767v1",
        "arxiv_id": "2507.05767v1",
        "authors": [
            "Ngoc Luyen Le",
            "Marie-Hélène Abel",
            "Bertrand Laforge"
        ],
        "submitted": "2025-07-08 08:13:30",
        "source": "arxiv",
        "comment": "in French language. 36es Journ{\\'e}es francophones d'Ing{\\'e}nierie\n  des Connaissances (IC 2025) @ Plate-Forme Intelligence Artificielle (PFIA\n  2025), Jul 2025, Dijon, France"
    },
    {
        "title": "DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities",
        "abstract": "Large Language Models (LLMs) are increasingly employed in multi-turn\nconversational tasks, yet their pre-training data predominantly consists of\ncontinuous prose, creating a potential mismatch between required capabilities\nand training paradigms. We introduce a novel approach to address this\ndiscrepancy by synthesizing conversational data from existing text corpora. We\npresent a pipeline that transforms a cluster of multiple related documents into\nan extended multi-turn, multi-topic information-seeking dialogue. Applying our\npipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training\ndialogue corpus consisting of over 730k long conversations. We hypothesize that\nexposure to such synthesized conversational structures during pre-training can\nenhance the fundamental multi-turn capabilities of LLMs, such as context memory\nand understanding. Empirically, we show that incorporating DocTalk during\npre-training results in up to 40% gain in context memory and understanding,\nwithout compromising base performance. DocTalk is available at\nhttps://huggingface.co/datasets/AmazonScience/DocTalk.",
        "url": "http://arxiv.org/abs/2507.05750v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05750v1",
        "arxiv_id": "2507.05750v1",
        "authors": [
            "Jing Yang Lee",
            "Hamed Bonab",
            "Nasser Zalmout",
            "Ming Zeng",
            "Sanket Lokegaonkar",
            "Colin Lockard",
            "Binxuan Huang",
            "Ritesh Sarkhel",
            "Haodong Wang"
        ],
        "submitted": "2025-07-08 07:52:12",
        "source": "arxiv",
        "comment": "Accepted at SIGDIAL 2025"
    },
    {
        "title": "GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge",
        "abstract": "Language models are powerful tools, yet their factual knowledge is still\npoorly understood, and inaccessible to ad-hoc browsing and scalable statistical\nanalysis. This demonstration introduces GPTKB v1.5, a densely interlinked\n100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using\nthe GPTKB methodology for massive-recursive LLM knowledge materialization (Hu\net al., ACL 2025). The demonstration experience focuses on three use cases: (1)\nlink-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM\nknowledge querying, (3) comparative exploration of the strengths and weaknesses\nof LLM knowledge. Massive-recursive LLM knowledge materialization is a\ngroundbreaking opportunity both for the research area of systematic analysis of\nLLM knowledge, as well as for automated KB construction. The GPTKB demonstrator\nis accessible at https://gptkb.org.",
        "url": "http://arxiv.org/abs/2507.05740v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05740v1",
        "arxiv_id": "2507.05740v1",
        "authors": [
            "Yujia Hu",
            "Tuan-Phong Nguyen",
            "Shrestha Ghosh",
            "Moritz Müller",
            "Simon Razniewski"
        ],
        "submitted": "2025-07-08 07:37:12",
        "source": "arxiv",
        "comment": "7 pages, 6 figures, 1 table"
    },
    {
        "title": "When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs",
        "abstract": "Self-Attentive Sequential Recommendation (SASRec) effectively captures\nlong-term user preferences by applying attention mechanisms to historical\ninteractions. Concurrently, the rise of Large Language Models (LLMs) has\nmotivated research into LLM-based recommendation, which leverages their\npowerful generalization and language understanding capabilities. However, LLMs\noften lack the domain-specific knowledge and collaborative signals essential\nfor high-quality recommendations when relying solely on textual prompts. To\naddress this limitation, this study proposes SASRecLLM, a novel framework that\nintegrates SASRec as a collaborative encoder with an LLM fine-tuned using\nLow-Rank Adaptation (LoRA). The components are connected via a mapping layer to\nalign their dimensional spaces, and three targeted training strategies are\ndesigned to optimize the hybrid architecture. Extensive experiments on multiple\ndatasets demonstrate that SASRecLLM achieves robust and consistent improvements\nover strong baselines in both cold-start and warm-start scenarios. This work\nadvances the field of LLM-based recommendation by presenting a modular and\neffective paradigm for fusing structured collaborative filtering with the\nsemantic power of fine-tuned LLMs. The implementation is available on GitHub:\nhttps://github.com/kechenkristin/RecLLM",
        "url": "http://arxiv.org/abs/2507.05733v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05733v1",
        "arxiv_id": "2507.05733v1",
        "authors": [
            "Kechen Liu"
        ],
        "submitted": "2025-07-08 07:26:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark",
        "abstract": "Automatic Speech Recognition (ASR) has been extensively investigated, yet\nprior evaluative efforts have largely been restricted to contextless paradigms.\nThis constraint stems from the limited proficiency of conventional ASR models\nin context modeling and their deficiency in memory and reasoning based on world\nknowledge. Recent breakthroughs in the development of Large Language Models\n(LLMs) and corresponding Large Audio Language Models (LALMs) have markedly\nenhanced the visibility of general artificial intelligence capabilities.\nConsequently, there exists a compelling need for a benchmark that can evaluate\nboth the generality and intelligence of ASR systems. To address this gap, we\npropose ContextASR-Bench: a comprehensive, large-scale benchmark designed to\nassess contextual speech recognition. This benchmark encompasses up to 40,000\ndata entries across over 10 domains, enabling a thorough evaluation of model\nperformance in scenarios that omit or incorporate coarse-grained or\nfine-grained contextual information. Moreover, diverging from conventional ASR\nevaluations, our benchmark includes an analysis of model efficacy in\nrecognizing named entities mentioned within the auditory input. Our extensive\nevaluation highlights that LALMs, with strong world knowledge and context\nlearning capabilities, outperform conventional ASR models by a large margin.\nThe dataset and evaluation code have been released at\nhttps://github.com/MrSupW/ContextASR-Bench.",
        "url": "http://arxiv.org/abs/2507.05727v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05727v1",
        "arxiv_id": "2507.05727v1",
        "authors": [
            "He Wang",
            "Linhan Ma",
            "Dake Guo",
            "Xiong Wang",
            "Lei Xie",
            "Jin Xu",
            "Junyang Lin"
        ],
        "submitted": "2025-07-08 07:21:20",
        "source": "arxiv",
        "comment": "18 pages, 4 figures"
    },
    {
        "title": "Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition",
        "abstract": "Mixture-of-experts (MoE) architectures have expanded from language modeling\nto automatic speech recognition (ASR). Traditional MoE methods, such as the\nSwitch Transformer, route experts independently within each layer. Our analysis\nreveals that routers in most layers make expert choices that are not strongly\ncorrelated with the choices of the routers in other layers. To increase the\ncooperation between experts in different layers and encourage greater\nspecialization, we use a shared router across different MoE layers. We call\nthis model \\emph{Omni-router Transformer}. Extensive experiments on a\nlarge-scale pseudo-labeled dataset and evaluations across 10 diverse,\nout-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is\nable to achieve lower training loss and consistently outperform dense and\nSwitch Transformer models, reducing average word error rates by 11.2% and 8.2%,\nrespectively, while providing structured expert usage and improved robustness\nto diverse data.",
        "url": "http://arxiv.org/abs/2507.05724v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05724v1",
        "arxiv_id": "2507.05724v1",
        "authors": [
            "Zijin Gu",
            "Tatiana Likhomanenko",
            "Navdeep Jaitly"
        ],
        "submitted": "2025-07-08 07:18:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment",
        "abstract": "Recently, there has been a surge of vision-based GUI agents designed to\nautomate everyday mobile and web tasks. These agents interpret raw GUI\nscreenshots and autonomously decide where to click, scroll, or type, which\nbypasses handcrafted rules and app-specific APIs. However, most existing\nmethods trained GUI agent in the offline environment using pre-collected\ntrajectories. This approach limits scalability, causes overfitting to specific\nUI templates, and leads to brittle policies when faced with unseen environment.\nWe present MobileGUI-RL, a scalable framework that trains GUI agent in online\nenvironment. MobileGUI-RL contains two key components. It (i) synthesizes a\ncurriculum of learnable tasks through self-exploration and filtering, and (ii)\nadapts GRPO to GUI navigation with trajectory-aware advantages and composite\nrewards that balance task success and execution efficiency. Experiments on\nthree online mobile-agent benchmarks show consistent gains, validating the\neffectiveness of our approach.",
        "url": "http://arxiv.org/abs/2507.05720v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05720v1",
        "arxiv_id": "2507.05720v1",
        "authors": [
            "Yucheng Shi",
            "Wenhao Yu",
            "Zaitang Li",
            "Yonglin Wang",
            "Hongming Zhang",
            "Ninghao Liu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "submitted": "2025-07-08 07:07:53",
        "source": "arxiv",
        "comment": "17 pages, 4 figures"
    },
    {
        "title": "From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal Collaborative Filtering Recommendation",
        "abstract": "Most existing multimodal collaborative filtering recommendation (MCFRec)\nmethods rely heavily on ID features and multimodal content to enhance\nrecommendation performance. However, this paper reveals that ID features are\neffective but have limited benefits in multimodal collaborative filtering\nrecommendation. Therefore, this paper systematically deconstruct the pros and\ncons of ID features: (i) they provide initial embedding but lack semantic\nrichness, (ii) they provide a unique identifier for each user and item but\nhinder generalization to untrained data, and (iii) they assist in aligning and\nfusing multimodal features but may lead to representation shift. Based on these\ninsights, this paper proposes IDFREE, an ID-free multimodal collaborative\nFiltering REcommEndation baseline. IDFREE replaces ID features with multimodal\nfeatures and positional encodings to generate semantically meaningful ID-free\nembeddings. For ID-free multimodal collaborative filtering, it further proposes\nan adaptive similarity graph module to construct dynamic user-user and\nitem-item graphs based on multimodal features. Then, an augmented user-item\ngraph encoder is proposed to construct more effective user and item encoding.\nFinally, IDFREE achieves inter-multimodal alignment based on the contrastive\nlearning and uses Softmax loss as recommendation loss. Basic experiments on\nthree public datasets demonstrate that IDFREE outperforms existing ID-based\nMCFRec methods, achieving an average performance gain of 72.24% across standard\nmetrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended\nexperiments further validate our findings on the limitations of ID features in\nMCFRec. The code is released at https://github.com/G-H-Li/IDFREE.",
        "url": "http://arxiv.org/abs/2507.05715v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05715v1",
        "arxiv_id": "2507.05715v1",
        "authors": [
            "Guohao Li",
            "Li Jing",
            "Jia Wu",
            "Xuefei Li",
            "Kai Zhu",
            "Yue He"
        ],
        "submitted": "2025-07-08 06:58:24",
        "source": "arxiv",
        "comment": "ACM MM'25 (Experimental supplementary version)"
    },
    {
        "title": "HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) has become a fundamental paradigm for\naddressing the challenges faced by large language models in handling real-time\ninformation and domain-specific problems. Traditional RAG systems primarily\nrely on the in-context learning (ICL) capabilities of the large language model\nitself. Still, in-depth research on the specific capabilities needed by the RAG\ngeneration model is lacking, leading to challenges with inconsistent document\nquality and retrieval system imperfections. Even the limited studies that\nfine-tune RAG generative models often \\textit{lack a granular focus on RAG\ntask} or \\textit{a deeper utilization of chain-of-thought processes}. To\naddress this, we propose that RAG models should possess three progressively\nhierarchical abilities (1) Filtering: the ability to select relevant\ninformation; (2) Combination: the ability to combine semantic information\nacross paragraphs; and (3) RAG-specific reasoning: the ability to further\nprocess external knowledge using internal knowledge. Thus, we introduce our new\nRAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning\nRetrieval-Augmented Generation (HIRAG) incorporates a \"think before answering\"\nstrategy. This method enhances the model's open-book examination capability by\nutilizing multi-level progressive chain-of-thought. Experiments show that the\nHIRAG training strategy significantly improves the model's performance on\ndatasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.",
        "url": "http://arxiv.org/abs/2507.05714v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05714v1",
        "arxiv_id": "2507.05714v1",
        "authors": [
            "YiHan Jiao",
            "ZheHao Tan",
            "Dan Yang",
            "DuoLin Sun",
            "Jie Feng",
            "Jian Wang",
            "Peng Wei"
        ],
        "submitted": "2025-07-08 06:53:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DRAGON: Dynamic RAG Benchmark On News",
        "abstract": "Retrieval-Augmented Generation (RAG) is a widely adopted approach for\nimproving the factuality of large language models (LLMs) by incorporating\nexternal knowledge at inference time. Although there exist multiple RAG\nbenchmarks for English, evaluation resources for other languages, including\nRussian, remain scarce and static, failing to capture the dynamic nature of\nreal-world deployments.\n  In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first\ndynamic benchmark for evaluating RAG systems in Russian on a changing news\ncorpora. DRAGON is built upon a regularly updated corpus of Russian news and\npublic documents and supports comprehensive evaluation of both the retriever\nand generator components. Question generation is performed automatically with\nthe use of Knowledge Graph constructed from the corpus and enables the\nextraction of four core question types aligned with distinct subgraph patterns.\nWe release a complete evaluation framework comprising the pipeline for\nautomatic question generation, evaluation scripts, which are potentially\nreusable for other languages and multilingual settings, and benchmark data. We\nalso launch a public leaderboard to encourage community participation and\ncomparison.",
        "url": "http://arxiv.org/abs/2507.05713v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05713v1",
        "arxiv_id": "2507.05713v1",
        "authors": [
            "Fedor Chernogorskii",
            "Sergei Averkiev",
            "Liliya Kudraleeva",
            "Zaven Martirosian",
            "Maria Tikhonova",
            "Valentin Malykh",
            "Alena Fenogenova"
        ],
        "submitted": "2025-07-08 06:52:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Agentic-R1: Distilled Dual-Strategy Reasoning",
        "abstract": "Current long chain-of-thought (long-CoT) models excel at mathematical\nreasoning but rely on slow and error-prone natural language traces.\nTool-augmented agents address arithmetic via code execution, but often falter\non complex logical tasks. We introduce a fine-tuning framework, DualDistill,\nthat distills complementary reasoning strategies from multiple teachers into a\nunified student model. Using this approach, we train Agentic-R1, which\ndynamically selects the optimal strategy for each query, invoking tools for\narithmetic and algorithmic problems, and using text-based reasoning for\nabstract ones. Our method improves accuracy across a range of tasks, including\nboth computation-intensive and standard benchmarks, demonstrating the\neffectiveness of multi-strategy distillation in achieving robust and efficient\nreasoning. Our project is available at https://github.com/StigLidu/DualDistill",
        "url": "http://arxiv.org/abs/2507.05707v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05707v1",
        "arxiv_id": "2507.05707v1",
        "authors": [
            "Weihua Du",
            "Pranjal Aggarwal",
            "Sean Welleck",
            "Yiming Yang"
        ],
        "submitted": "2025-07-08 06:35:16",
        "source": "arxiv",
        "comment": "Preprint. 15 pages. Project available at\n  https://github.com/StigLidu/DualDistill"
    },
    {
        "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in LLMs",
        "abstract": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
        "url": "http://arxiv.org/abs/2507.05687v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05687v1",
        "arxiv_id": "2507.05687v1",
        "authors": [
            "Shangzhan Li",
            "Zefan Wang",
            "Ye He",
            "Yuxuan Li",
            "Qi Shi",
            "Jianling Li",
            "Yonggang Hu",
            "Wanxiang Che",
            "Xu Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "submitted": "2025-07-08 05:38:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs",
        "abstract": "Multilingual large language models (LLMs) often exhibit language confusion, a\ntendency to generate responses in a dominant language irrespective of the\nprompt's language. To address this, we propose Smoothie-Qwen, a lightweight,\npost-hoc method that mitigates language bias without retraining. This technique\nselectively adjusts token-level output probabilities to effectively suppress\nundesired language generation. Applied to the Qwen model, our method reduces\nunintended Chinese output by over 95% while preserving task accuracy on\nmultilingual benchmarks. This work provides a practical and efficient solution\nfor enhancing the language controllability of LLMs, making them more reliable\nfor global applications.",
        "url": "http://arxiv.org/abs/2507.05686v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05686v1",
        "arxiv_id": "2507.05686v1",
        "authors": [
            "SeungWon Ji",
            "Jungyup Lee",
            "Jemin Kim",
            "Sang Park",
            "SeungJae Lee"
        ],
        "submitted": "2025-07-08 05:30:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data",
        "abstract": "Recent advances in foundation models, such as LLMs, have revolutionized\nconversational AI. Chatbots are increasingly being developed by customizing\nLLMs on specific conversational datasets. However, mitigating toxicity during\nthis customization, especially when dealing with untrusted training data,\nremains a significant challenge. To address this, we introduce TuneShield, a\ndefense framework designed to mitigate toxicity during chatbot fine-tuning\nwhile preserving conversational quality. TuneShield leverages LLM-based\ntoxicity classification, utilizing the instruction-following capabilities and\nsafety alignment of LLMs to effectively identify toxic samples, outperforming\nindustry API services. TuneShield generates synthetic conversation samples,\ntermed 'healing data', based on the identified toxic samples, using them to\nmitigate toxicity while reinforcing desirable behavior during fine-tuning. It\nperforms an alignment process to further nudge the chatbot towards producing\ndesired responses. Our findings show that TuneShield effectively mitigates\ntoxicity injection attacks while preserving conversational quality, even when\nthe toxicity classifiers are imperfect or biased. TuneShield proves to be\nresilient against adaptive adversarial and jailbreak attacks. Additionally,\nTuneShield demonstrates effectiveness in mitigating adaptive toxicity injection\nattacks during dialog-based learning (DBL).",
        "url": "http://arxiv.org/abs/2507.05660v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05660v1",
        "arxiv_id": "2507.05660v1",
        "authors": [
            "Aravind Cheruvu",
            "Shravya Kanchi",
            "Sifat Muhammad Abdullah",
            "Nicholas Kong",
            "Daphne Yao",
            "Murtuza Jadliwala",
            "Bimal Viswanath"
        ],
        "submitted": "2025-07-08 04:40:09",
        "source": "arxiv",
        "comment": "Pre-print"
    },
    {
        "title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?",
        "abstract": "In this paper, we introduce ECom-Bench, the first benchmark framework for\nevaluating LLM agent with multimodal capabilities in the e-commerce customer\nsupport domain. ECom-Bench features dynamic user simulation based on persona\ninformation collected from real e-commerce customer interactions and a\nrealistic task dataset derived from authentic e-commerce dialogues. These\ntasks, covering a wide range of business scenarios, are designed to reflect\nreal-world complexities, making ECom-Bench highly challenging. For instance,\neven advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our\nbenchmark, highlighting the substantial difficulties posed by complex\ne-commerce scenarios. Upon publication, the code and data will be open-sourced\nto facilitate further research and development in this domain.",
        "url": "http://arxiv.org/abs/2507.05639v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05639v1",
        "arxiv_id": "2507.05639v1",
        "authors": [
            "Haoxin Wang",
            "Xianhan Peng",
            "Xucheng Huang",
            "Yizhe Huang",
            "Ming Gong",
            "Chenghan Yang",
            "Yang Liu",
            "Ling Jiang"
        ],
        "submitted": "2025-07-08 03:35:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression",
        "abstract": "Retrieval-augmented Generation (RAG) extends large language models (LLMs)\nwith external knowledge but faces key challenges: restricted effective context\nlength and redundancy in retrieved documents. Pure compression-based approaches\nreduce input size but often discard fine-grained details essential for factual\naccuracy. We propose SARA, a unified RAG framework that balances local\nprecision and global knowledge coverage under tight context budgets. SARA\ncombines natural-language text snippets with semantic compression vectors to\njointly enhance context efficiency and answer correctness. It represents\ncontexts at two complementary levels: 1) fine-grained natural-language spans\nthat preserve critical entities and numerical values, and 2) compact,\ninterpretable vectors that summarize high-level semantics. An iterative\nevidence-selection module employs the compression vectors for dynamic reranking\nof contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families\n(Mistral, Llama, and Gemma), SARA consistently improves answer relevance\n(+17.71), answer correctness (+13.72), and semantic similarity (+15.53),\ndemonstrating the importance of integrating textual and compressed\nrepresentations for robust, context-efficient RAG.",
        "url": "http://arxiv.org/abs/2507.05633v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05633v1",
        "arxiv_id": "2507.05633v1",
        "authors": [
            "Yiqiao Jin",
            "Kartik Sharma",
            "Vineeth Rakesh",
            "Yingtong Dou",
            "Menghai Pan",
            "Mahashweta Das",
            "Srijan Kumar"
        ],
        "submitted": "2025-07-08 03:29:09",
        "source": "arxiv",
        "comment": "20 pages"
    },
    {
        "title": "Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching",
        "abstract": "Knowledge distillation typically involves transferring knowledge from a Large\nLanguage Model (LLM) to a Smaller Language Model (SLM). However, in tasks such\nas text matching, fine-tuned smaller models often yield more effective\ndomain-specific representations, as they focus on optimizing the similarity of\ninput pairs. To leverage both the specialized strengths of small models and the\nrich semantic understanding of LLMs, we introduce a flipped knowledge\ndistillation paradigm, where LLM learns from SLM. Specifically, we address the\narchitectural gap between decoder-only LLMs and smaller encoder-based models by\nreinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder\ngenerates compressed representations, while the decoder maps them to the output\nspace. During training, the encoder produces representations and their\nsimilarities, which are then aligned with the similarity scores produced by the\nteacher, using our proposed Margin-aware Contrastive Learning (MCL) approach.\nThe MCL ensures accurate similarity for both positive and negative pairs, and\nadaptively handles the internal differences within positive and negative\nsamples. Our paradigm requires only a reasonably good-performing SLM, allowing\nthe LLM to achieve improved performance. Experiments on financial and\nhealthcare benchmarks, as well as real-world applications, confirm its\neffectiveness, and the model has been fully deployed in an online environment.",
        "url": "http://arxiv.org/abs/2507.05617v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05617v1",
        "arxiv_id": "2507.05617v1",
        "authors": [
            "Mingzhe Li",
            "Jing Xiang",
            "Qishen Zhang",
            "Kaiyang Wan",
            "Xiuying Chen"
        ],
        "submitted": "2025-07-08 02:54:15",
        "source": "arxiv",
        "comment": "Accepted by ACL 2025 main"
    },
    {
        "title": "Self-Review Framework for Enhancing Instruction Following Capability of LLM",
        "abstract": "Various techniques have been proposed to improve large language models (LLMs)\nadherence to formatting and instruction constraints. One of the most effective\napproaches involves utilizing high-quality data generated by powerful models.\nHowever, such models often fail to fully comply with complex instructions in a\nsingle generation. To address this limitation, iterative revision methods have\nbeen introduced. Nevertheless, as the number of data points and revision\niterations increases, the associated monetary costs grow significantly. As a\nresource-efficient alternative, methods have been proposed that leverage\nhigh-performance evaluation tools to compensate for the limited self-evaluation\ncapabilities of open-source LLMs. However, these approaches often lead to a\ndegradation in output quality due to excessive revision. To overcome these\nchallenges, we propose Re5, a self-evaluation and revision framework designed\nto enhance instruction-following performance while preserving the quality of\nthe generated content. Re5 extracts task and constraint components from user\ninstructions, performs structural evaluations to prevent error accumulation,\nand applies fine-grained constraint-specific content evaluations followed by\nselective revisions. This process ensures precise and quality-preserving\nimprovements. The final high-quality outputs are used for alignment tuning,\nenabling long-term alignment improvements through a data-centric iterative\nrefinement loop. Experimental results demonstrate that Re5 achieves\ninstruction-following performance comparable to models trained on data\ngenerated by GPT-4o-mini, a high-performance model, even with a small amount of\ndata while maintaining response quality with a 64.24%-win rate over the\nnon-revised initial responses. These results validate Re5 as an efficient and\neffective solution for enhancing instruction adherence with minimal external\nsupervision.",
        "url": "http://arxiv.org/abs/2507.05598v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05598v1",
        "arxiv_id": "2507.05598v1",
        "authors": [
            "Sihyun Park"
        ],
        "submitted": "2025-07-08 02:17:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and Mitigation",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
        "url": "http://arxiv.org/abs/2507.05578v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05578v1",
        "arxiv_id": "2507.05578v1",
        "authors": [
            "Alexander Xiong",
            "Xuandong Zhao",
            "Aneesh Pappu",
            "Dawn Song"
        ],
        "submitted": "2025-07-08 01:30:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Retrieval: Ensembling Cross-Encoders and GPT Rerankers with LLMs for Biomedical QA",
        "abstract": "Biomedical semantic question answering rooted in information retrieval can\nplay a crucial role in keeping up to date with vast, rapidly evolving and\never-growing biomedical literature. A robust system can help researchers,\nhealthcare professionals and even layman users access relevant knowledge\ngrounded in evidence. The BioASQ 2025 Task13b Challenge serves as an important\nbenchmark, offering a competitive platform for advancement of this space. This\npaper presents the methodologies and results from our participation in this\nchallenge where we built a Retrieval-Augmented Generation (RAG) system that can\nanswer biomedical questions by retrieving relevant PubMed documents and\nsnippets to generate answers. For the retrieval task, we generated dense\nembeddings from biomedical articles for initial retrieval, and applied an\nensemble of finetuned cross-encoders and large language models (LLMs) for\nre-ranking to identify top relevant documents. Our solution achieved an MAP@10\nof 0.1581, placing 10th on the leaderboard for the retrieval task. For answer\ngeneration, we employed few-shot prompting of instruction-tuned LLMs. Our\nsystem achieved macro-F1 score of 0.95 for yes/no questions (rank 12), Mean\nReciprocal Rank (MRR) of 0.64 for factoid questions (rank 1), mean-F1 score of\n0.63 for list questions (rank 5), and ROUGE-SU4 F1 score of 0.29 for ideal\nanswers (rank 11).",
        "url": "http://arxiv.org/abs/2507.05577v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05577v1",
        "arxiv_id": "2507.05577v1",
        "authors": [
            "Shashank Verma",
            "Fengyi Jiang",
            "Xiangning Xue"
        ],
        "submitted": "2025-07-08 01:25:06",
        "source": "arxiv",
        "comment": "Paper submitted to CLEF 2025 CEUR-WS"
    },
    {
        "title": "Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS",
        "abstract": "Test-time scaling has emerged as a promising paradigm in language modeling,\nleveraging additional computational resources at inference time to enhance\nmodel performance. In this work, we introduce R2-LLMs, a novel and versatile\nhierarchical retrieval-augmented reasoning framework designed to improve\ntest-time scaling in large language models (LLMs) without requiring\ndistillation from more advanced models to obtain chain-of-thought (CoT)\ntraining data. R2-LLMs enhances inference-time generalization by integrating\ndual-level retrieval-based in-context learning: (1) At the coarse level, our\napproach extracts abstract templates from complex reasoning problems and\nretrieves similar problem-answer pairs to facilitate high-level in-context\nlearning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs\nefficiently retrieves analogous intermediate solution steps from reference\nmathematical problem datasets, refining step-wise reasoning with the aid of a\nprocess reward model (PRM) for scoring. R2-LLMs is a robust hierarchical\nreasoning-augmentation method that enhances in-context-level reasoning while\nseamlessly integrating with step-level tree search methods. Utilizing PRM, it\nrefines both candidate generation and decision-making for improved reasoning\naccuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO\ndatasets achieve substantial relative improvement with an increase of up to 16%\nusing LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of\nour approach in complex reasoning tasks.",
        "url": "http://arxiv.org/abs/2507.05557v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05557v1",
        "arxiv_id": "2507.05557v1",
        "authors": [
            "Alex ZH Dou",
            "Zhongwei Wan",
            "Dongfei Cui",
            "Xin Wang",
            "Jing Xiong",
            "Haokun Lin",
            "Chaofan Tao",
            "Shen Yan",
            "Mi Zhang"
        ],
        "submitted": "2025-07-08 00:41:12",
        "source": "arxiv",
        "comment": "Technical Report"
    },
    {
        "title": "Information Needs and Practices Supported by ChatGPT",
        "abstract": "This study considers ChatGPT as an information source, investigating the\ninformation needs that people come to ChatGPT with and the information\npractices that ChatGPT supports, through a qualitative content analysis of 205\nuser vignettes. The findings show that ChatGPT is used in a range of life\ndomains (home/family, work, leisure, etc.) and for a range of human needs\n(writing/editing, learning, simple programming tasks, etc.), constituting the\ninformation needs that people use ChatGPT to address. Related to these\ninformation needs, the findings show six categories of information practices\nthat ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and\nCritiquing. This work suggests that, in the AI age, information need should be\nconceptualized not just as a matter of \"getting questions answered\" or even\n\"making sense,\" but as skillfully coping in the world, a notion that includes\nboth understanding and action. This study leads to numerous opportunities for\nfuture work at the junction of generative AI and information needs, seeking,\nuse and experience.",
        "url": "http://arxiv.org/abs/2507.05537v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05537v1",
        "arxiv_id": "2507.05537v1",
        "authors": [
            "Tim Gorichanaz"
        ],
        "submitted": "2025-07-07 23:21:20",
        "source": "arxiv",
        "comment": "To be presented at the 2025 ASIS&T virtual satellite meeting,\n  December 2025"
    },
    {
        "title": "Conversational Education at Scale: A Multi-LLM Agent Workflow for Procedural Learning and Pedagogic Quality Assessment",
        "abstract": "Large language models (LLMs) have advanced virtual educators and learners,\nbridging NLP with AI4Education. Existing work often lacks scalability and fails\nto leverage diverse, large-scale course content, with limited frameworks for\nassessing pedagogic quality. To this end, we propose WikiHowAgent, a\nmulti-agent workflow leveraging LLMs to simulate interactive teaching-learning\nconversations. It integrates teacher and learner agents, an interaction\nmanager, and an evaluator to facilitate procedural learning and assess\npedagogic quality. We introduce a dataset of 114,296 teacher-learner\nconversations grounded in 14,287 tutorials across 17 domains and 727 topics.\nOur evaluation protocol combines computational and rubric-based metrics with\nhuman judgment alignment. Results demonstrate the workflow's effectiveness in\ndiverse setups, offering insights into LLM capabilities across domains. Our\ndatasets and implementations are fully open-sourced.",
        "url": "http://arxiv.org/abs/2507.05528v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05528v1",
        "arxiv_id": "2507.05528v1",
        "authors": [
            "Jiahuan Pei",
            "Fanghua Ye",
            "Xin Sun",
            "Wentao Deng",
            "Koen Hindriks",
            "Junxiao Wang"
        ],
        "submitted": "2025-07-07 22:56:37",
        "source": "arxiv",
        "comment": "14 pages"
    },
    {
        "title": "Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications",
        "abstract": "Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong\nperformance on clinical natural language processing (NLP) tasks across multiple\nmedical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular\nreporting from nurse dictations and medical order extraction from\ndoctor-patient consultations - remain underexplored due to data scarcity and\nsensitivity, despite active industry efforts. Practical solutions to these\nreal-world clinical tasks can significantly reduce the documentation burden on\nhealthcare providers, allowing greater focus on patient care. In this paper, we\ninvestigate these two challenging tasks using private and open-source clinical\ndatasets, evaluating the performance of both open- and closed-weight LLMs, and\nanalyzing their respective strengths and limitations. Furthermore, we propose\nan agentic pipeline for generating realistic, non-sensitive nurse dictations,\nenabling structured extraction of clinical observations. To support further\nresearch in both areas, we release SYNUR and SIMORD, the first open-source\ndatasets for nurse observation extraction and medical order extraction.",
        "url": "http://arxiv.org/abs/2507.05517v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05517v1",
        "arxiv_id": "2507.05517v1",
        "authors": [
            "Jean-Philippe Corbeil",
            "Asma Ben Abacha",
            "George Michalopoulos",
            "Phillip Swazinna",
            "Miguel Del-Agua",
            "Jerome Tremblay",
            "Akila Jeeson Daniel",
            "Cari Bader",
            "Kevin Cho",
            "Pooja Krishnan",
            "Nathan Bodenstab",
            "Thomas Lin",
            "Wenxuan Teng",
            "Francois Beaulieu",
            "Paul Vozila"
        ],
        "submitted": "2025-07-07 22:29:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality",
        "abstract": "Vision-language models (VLMs) are essential for enabling AI-powered smart\nassistants to interpret and reason in multimodal environments. However, their\napplication in augmented reality (AR) training remains largely unexplored. In\nthis work, we introduce a comprehensive dataset tailored for AR training,\nfeaturing systematized vision-language tasks, and evaluate nine\nstate-of-the-art VLMs on it. Our results reveal that even advanced models,\nincluding GPT-4o, struggle with fine-grained assembly tasks, achieving a\nmaximum F1 score of just 40.54% on state detection. These findings highlight\nthe demand for enhanced datasets, benchmarks, and further research to improve\nfine-grained vision-language alignment. Beyond technical contributions, our\nwork has broader social implications, particularly in empowering blind and\nvisually impaired users with equitable access to AI-driven learning\nopportunities. We provide all related resources, including the dataset, source\ncode, and evaluation results, to support the research community.",
        "url": "http://arxiv.org/abs/2507.05515v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05515v1",
        "arxiv_id": "2507.05515v1",
        "authors": [
            "Haochen Huang",
            "Jiahuan Pei",
            "Mohammad Aliannejadi",
            "Xin Sun",
            "Moonisa Ahsan",
            "Pablo Cesar",
            "Chuang Yu",
            "Zhaochun Ren",
            "Junxiao Wang"
        ],
        "submitted": "2025-07-07 22:29:01",
        "source": "arxiv",
        "comment": "20 pages"
    },
    {
        "title": "ModelCitizens: Representing Community Voices in Online Safety",
        "abstract": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
        "url": "http://arxiv.org/abs/2507.05455v2",
        "pdf_url": "http://arxiv.org/pdf/2507.05455v2",
        "arxiv_id": "2507.05455v2",
        "authors": [
            "Ashima Suvarna",
            "Christina Chance",
            "Karolina Naranjo",
            "Hamid Palangi",
            "Sophie Hao",
            "Thomas Hartvigsen",
            "Saadia Gabriel"
        ],
        "submitted": "2025-07-07 20:15:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Semantics of Large Language Models",
        "abstract": "Large Language Models (LLMs) such as ChatGPT demonstrated the potential to\nreplicate human language abilities through technology, ranging from text\ngeneration to engaging in conversations. However, it remains controversial to\nwhat extent these systems truly understand language. We examine this issue by\nnarrowing the question down to the semantics of LLMs at the word and sentence\nlevel. By examining the inner workings of LLMs and their generated\nrepresentation of language and by drawing on classical semantic theories by\nFrege and Russell, we get a more nuanced picture of the potential semantic\ncapabilities of LLMs.",
        "url": "http://arxiv.org/abs/2507.05448v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05448v1",
        "arxiv_id": "2507.05448v1",
        "authors": [
            "Martin Schuele"
        ],
        "submitted": "2025-07-07 20:02:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs",
        "abstract": "Vocabulary acquisition poses a significant challenge for second-language (L2)\nlearners, especially when learning typologically distant languages such as\nEnglish and Korean, where phonological and structural mismatches complicate\nvocabulary learning. Recently, large language models (LLMs) have been used to\ngenerate keyword mnemonics by leveraging similar keywords from a learner's\nfirst language (L1) to aid in acquiring L2 vocabulary. However, most of this\nresearch has focused on native English speakers learning other languages,\nrather than the reverse. In this paper, we present PhoniTale, a novel\ncross-lingual mnemonic generation system that retrieves L1 keyword sequence\nbased on phonological similarity and uses LLMs to generate mnemonics. We\nevaluate PhoniTale using both automated metrics and human evaluations,\ncomparing its output to mnemonics created by humans and by previous automated\napproaches. To assess practical effectiveness, we also conduct a short-term\nrecall test measuring mnemonic helpfulness. Our findings show that PhoniTale\nperforms comparably to human-authored mnemonics. We also highlight key areas\nfor future improvement in mnemonic quality and methodology.",
        "url": "http://arxiv.org/abs/2507.05444v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05444v1",
        "arxiv_id": "2507.05444v1",
        "authors": [
            "Sana Kang",
            "Myeongseok Gwon",
            "Su Young Kwon",
            "Jaewook Lee",
            "Andrew Lan",
            "Bhiksha Raj",
            "Rita Singh"
        ],
        "submitted": "2025-07-07 19:50:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Gendered Divides in Online Discussions about Reproductive Rights",
        "abstract": "The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health\nOrganization marked a turning point in the national debate over reproductive\nrights. While the ideological divide over abortion is well documented, less is\nknown about how gender and local sociopolitical contexts interact to shape\npublic discourse. Drawing on nearly 10 million abortion-related posts on X\n(formerly Twitter) from users with inferred gender, ideology and location, we\nshow that gender significantly moderates abortion attitudes and emotional\nexpression, particularly in conservative regions, and independently of\nideology. This creates a gender gap in abortion attitudes that grows more\npronounced in conservative regions. The leak of the Dobbs draft opinion further\nintensified online engagement, disproportionately mobilizing pro-abortion women\nin areas where access was under threat. These findings reveal that abortion\ndiscourse is not only ideologically polarized but also deeply structured by\ngender and place, highlighting the central role of identity in shaping\npolitical expression during moments of institutional disruption.",
        "url": "http://arxiv.org/abs/2507.05443v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05443v1",
        "arxiv_id": "2507.05443v1",
        "authors": [
            "Ashwin Rao",
            "Sze Yuh Nina Wang",
            "Kristina Lerman"
        ],
        "submitted": "2025-07-07 19:49:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning",
        "abstract": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses.",
        "url": "http://arxiv.org/abs/2507.07060v1",
        "pdf_url": "http://arxiv.org/pdf/2507.07060v1",
        "arxiv_id": "2507.07060v1",
        "authors": [
            "Shreyas Vinaya Sathyanarayana",
            "Rahil Shah",
            "Sharanabasava D. Hiremath",
            "Rishikesh Panda",
            "Rahul Jana",
            "Riya Singh",
            "Rida Irfan",
            "Ashwin Murali",
            "Bharath Ramsundar"
        ],
        "submitted": "2025-07-07 19:41:39",
        "source": "arxiv",
        "comment": "51 pages,"
    },
    {
        "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models",
        "abstract": "Large language models are capable of leveraging both contextual and\nparametric knowledge but how they prioritize and integrate these sources\nremains underexplored. We introduce CoPE, a novel evaluation framework that\nsystematically measures contextual knowledge (CK) and parametric knowledge (PK)\nacross models and languages. Using our MultiWikiAtomic dataset in English,\nSpanish, and Danish, we analyze how large language models (LLMs) integrate\ncontext, prioritize information, and incorporate PK in open-ended question\nanswering. Our analysis uncovers a phenomenon we call lost-in-the-later, where\nLLMs tend to overlook or deprioritize information that appears later in a given\ncontext, revealing a strong positional bias that affects contextual grounding.\nWe further find that reasoning models, as well as non-reasoning models prompted\nwith chain-of-thought (CoT), use context even less than non-reasoning models\nwithout CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,\nin particular, results in lower recall and shorter responses, leading to\ndegraded contextual grounding. Based on these insights, we design prompt-based\nmethods to effectively leverage input context. A case study applying CoPE to\nsummarization demonstrates that CK-informed prompting improves factual\ngrounding and reduces hallucination.",
        "url": "http://arxiv.org/abs/2507.05424v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05424v1",
        "arxiv_id": "2507.05424v1",
        "authors": [
            "Yufei Tao",
            "Adam Hiatt",
            "Rahul Seetharaman",
            "Ameeta Agrawal"
        ],
        "submitted": "2025-07-07 19:13:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning",
        "abstract": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual QA, and code generation, yet their multilingual reasoning\ncapabilities in these tasks remain underdeveloped. Especially for low-resource\nlanguages such as Swahili or Thai, LLMs can often misinterpret prompts or\ndefault to reasoning in English. This implicit bias toward high-resource\nlanguages undermines factual accuracy, interpretability, and trust. Current\nmultilingual benchmarks focus only on final answers, overlooking whether models\nactually reason in the target language. To address this gap, we introduce\nGeoFact-X, a geography-based multilingual factual reasoning benchmark with\nannotated reasoning traces in five languages: English, Hindi, Japanese,\nSwahili, and Thai. We further propose BRIDGE, a novel training method that\nguides supervised fine-tuning and test-time reinforcement learning with a\nlanguage-consistency reward to align reasoning with the input language.\nFinally, we develop an automatic evaluation protocol using LLM-as-a-judge to\nassess answer correctness and the quality and language consistency of reasoning\ntraces, enabling nuanced and scalable analysis beyond surface-level metrics.\nOur results show that BRIDGE significantly enhances multilingual reasoning\nfidelity, demonstrating that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/GeoFact-X_BRIDGE",
        "url": "http://arxiv.org/abs/2507.05418v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05418v1",
        "arxiv_id": "2507.05418v1",
        "authors": [
            "Jaedong Hwang",
            "Kumar Tanmay",
            "Seok-Jin Lee",
            "Ayush Agrawal",
            "Hamid Palangi",
            "Kumar Ayush",
            "Ila Fiete",
            "Paul Pu Liang"
        ],
        "submitted": "2025-07-07 19:04:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences",
        "abstract": "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.",
        "url": "http://arxiv.org/abs/2507.05391v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05391v1",
        "arxiv_id": "2507.05391v1",
        "authors": [
            "Guillem Ramírez",
            "Alexandra Birch",
            "Ivan Titov"
        ],
        "submitted": "2025-07-07 18:22:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Generalization Ridge: Information Flow in Natural Language Generation",
        "abstract": "Transformer-based language models have achieved state-of-the-art performance\nin natural language generation (NLG) tasks, yet their internal mechanisms for\nsynthesizing task-relevant information remain insufficiently understood. While\nprior studies suggest that intermediate layers often yield more generalizable\nrepresentations than final layers, how this generalization ability emerges and\npropagates across layers during training remains unclear. To address this gap,\nwe propose InfoRidge, an information-theoretic framework, to characterize how\npredictive information-the mutual information between hidden representations\nand target outputs-varies across depth. Estimating this quantity enables us to\ntrace the flow of task-relevant information throughout the model during\ntraining. Our experiments across various models and datasets reveal a\nconsistent non-monotonic trend: predictive information peaks in upper-middle\nlayers-forming a generalization ridge-before declining in final layers,\nreflecting a transition between generalization and memorization. To further\ninvestigate this phenomenon, we introduce residual scaling\ncoefficients-trainable scalar parameters applied to each residual block-which\nserve as functional probes for assessing the relative importance of individual\ntransformer layers. These coefficients reveal that, under distribution shift,\nmodels downweight final layers and increasingly rely on ridge layers,\nhighlighting their role in generalization. Together, these findings offer new\ninsights into the internal mechanisms of transformers and underscore the\ncritical role of intermediate layers in supporting generalization.",
        "url": "http://arxiv.org/abs/2507.05387v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05387v1",
        "arxiv_id": "2507.05387v1",
        "authors": [
            "Ruidi Chang",
            "Chunyuan Deng",
            "Hanjie Chen"
        ],
        "submitted": "2025-07-07 18:18:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual Post-Training",
        "abstract": "Continual post-training (CPT) is a popular and effective technique for\nadapting foundation models like multimodal large language models to specific\nand ever-evolving downstream tasks. While existing research has primarily\nconcentrated on methods like data replay, model expansion, or parameter\nregularization, the fundamental role of the learning paradigm within CPT\nremains largely unexplored. This paper presents a comparative analysis of two\ncore post-training paradigms: supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT), investigating their respective impacts on knowledge\nretention during CPT. Our experiments are conducted on a benchmark comprising\nseven diverse multimodal tasks, utilizing Qwen2.5-VL-7B-Instruct as the base\nmodel for continual post-training. The investigation yields two significant\nfindings: (1) When continuously learning on downstream tasks, SFT leads to\ncatastrophic forgetting of previously learned tasks. In contrast, RFT\ninherently preserves prior knowledge and achieve performance comparable to\nmulti-task training. (2) RFT successfully protects and even enhances the\nmodel's general knowledge on standard benchmarks (e.g., MMMU and MMLU-Pro).\nConversely, SFT degrades general model capabilities severely. Further analysis\nshows that explicit mechanisms, such as KL penalty and chain-of-thought\nreasoning, are not the primary factors. Instead, we find that the implicit\nregularization inherent to RFT is a key factor in mitigating forgetting.\nFinally, we propose a rollout-based instance filtering algorithm to improve the\nstability and efficiency of RFT. Our comprehensive study demonstrates the\nsuperiority of RFT as a robust paradigm for continual post-training.",
        "url": "http://arxiv.org/abs/2507.05386v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05386v1",
        "arxiv_id": "2507.05386v1",
        "authors": [
            "Song Lai",
            "Haohan Zhao",
            "Rong Feng",
            "Changyi Ma",
            "Wenzhuo Liu",
            "Hongbo Zhao",
            "Xi Lin",
            "Dong Yi",
            "Min Xie",
            "Qingfu Zhang",
            "Hongbin Liu",
            "Gaofeng Meng",
            "Fei Zhu"
        ],
        "submitted": "2025-07-07 18:17:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data",
        "abstract": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available.",
        "url": "http://arxiv.org/abs/2507.05385v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05385v1",
        "arxiv_id": "2507.05385v1",
        "authors": [
            "Guanzhong Pan",
            "Mei Tan",
            "Hyunji Nam",
            "Lucía Langlois",
            "James Malamut",
            "Liliana Deonizio",
            "Dorottya Demszky"
        ],
        "submitted": "2025-07-07 18:15:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study",
        "abstract": "Recent advances in natural language processing highlight two key factors for\nimproving reasoning in large language models (LLMs): (i) allocating more\ntest-time compute tends to help on harder problems but often introduces\nredundancy in the reasoning trace, and (ii) compute is most effective when\nreasoning is systematic and incremental, forming structured chains of thought\n(CoTs) akin to human problem-solving. To study these factors in isolation, we\nintroduce a controlled setting based on shortest-path tasks in layered graphs.\nWe train decoder-only transformers on question-trace-answer triples using a\ncustom tokenizer, comparing models trained on optimal bottom-up dynamic\nprogramming traces with those trained on longer, valid traces involving\nbacktracking. Surprisingly, with the same training-token budget, models trained\non inefficient traces generalize better to unseen graphs. This benefit is not\ndue to length alone-injecting arbitrary redundancy into reasoning traces fails\nto help and can even hurt performance. Instead, we find that generalization\ncorrelates with the model's confidence in next-token prediction, suggesting\nthat long, coherent, and locally incremental traces make the training signal\neasier to optimize.",
        "url": "http://arxiv.org/abs/2507.05362v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05362v1",
        "arxiv_id": "2507.05362v1",
        "authors": [
            "Riccardo Alberghi",
            "Elizaveta Demyanenko",
            "Luca Biggio",
            "Luca Saglietti"
        ],
        "submitted": "2025-07-07 18:00:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks",
        "abstract": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG).",
        "url": "http://arxiv.org/abs/2507.05346v1",
        "pdf_url": "http://arxiv.org/pdf/2507.05346v1",
        "arxiv_id": "2507.05346v1",
        "authors": [
            "William Fleshman",
            "Benjamin Van Durme"
        ],
        "submitted": "2025-07-07 18:00:01",
        "source": "arxiv",
        "comment": null
    }
]