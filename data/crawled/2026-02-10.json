[
    {
        "title": "Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense",
        "abstract": "The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like \"Bingo\". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent \"Cognitive Gap\" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.",
        "url": "http://arxiv.org/abs/2602.09012v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09012v1",
        "arxiv_id": "2602.09012v1",
        "authors": [
            "Jiacheng Liu",
            "Yaxin Luo",
            "Jiacheng Cui",
            "Xinyi Shang",
            "Xiaohan Zhao",
            "Zhiqiang Shen"
        ],
        "submitted": "2026-02-09 18:55:33",
        "source": "arxiv",
        "comment": "Project page at https://greenoso.github.io/NextGen-CAPTCHAs_webpage/"
    },
    {
        "title": "SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech",
        "abstract": "Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.",
        "url": "http://arxiv.org/abs/2602.09866v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09866v1",
        "arxiv_id": "2602.09866v1",
        "authors": [
            "Johan Sofalas",
            "Dilushri Pavithra",
            "Nevidu Jayatilleke",
            "Ruvan Weerasinghe"
        ],
        "submitted": "2026-02-09 18:48:06",
        "source": "arxiv",
        "comment": "19 pages, 6 figures, 8 tables, Accepted paper at the 22nd Workshop on Multiword Expressions (MWE 2026) @ EACL 2026"
    },
    {
        "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
        "abstract": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.",
        "url": "http://arxiv.org/abs/2602.09003v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09003v1",
        "arxiv_id": "2602.09003v1",
        "authors": [
            "Yudong Wang",
            "Zixuan Fu",
            "Hengyu Zhao",
            "Chen Zhao",
            "Chuyue Zhou",
            "Xinle Lin",
            "Hongya Lyu",
            "Shuaikang Xue",
            "Yi Yi",
            "Yingjiao Wang",
            "Zhi Zheng",
            "Yuzhou Zhang",
            "Jie Zhou",
            "Chaojun Xiao",
            "Xu Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "submitted": "2026-02-09 18:47:51",
        "source": "arxiv",
        "comment": "16 pages, 3 figures, 7 tables"
    },
    {
        "title": "UI-Venus-1.5 Technical Report",
        "abstract": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus",
        "url": "http://arxiv.org/abs/2602.09082v1",
        "pdf_url": "https://arxiv.org/pdf/2602.09082v1",
        "arxiv_id": "2602.09082v1",
        "authors": [
            "Veuns-Team",
            ":",
            "Changlong Gao",
            "Zhangxuan Gu",
            "Yulin Liu",
            "Xinyu Qiu",
            "Shuheng Shen",
            "Yue Wen",
            "Tianyu Xia",
            "Zhenyu Xu",
            "Zhengwen Zeng",
            "Beitong Zhou",
            "Xingran Zhou",
            "Weizhi Chen",
            "Sunhao Dai",
            "Jingya Dou",
            "Yichen Gong",
            "Yuan Guo",
            "Zhenlin Guo",
            "Feng Li",
            "Qian Li",
            "Jinzhen Lin",
            "Yuqi Zhou",
            "Linchao Zhu",
            "Liang Chen",
            "Zhenyu Guo",
            "Changhua Meng",
            "Weiqiang Wang"
        ],
        "submitted": "2026-02-09 18:43:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs",
        "abstract": "Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.",
        "url": "http://arxiv.org/abs/2602.08997v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08997v1",
        "arxiv_id": "2602.08997v1",
        "authors": [
            "Lavender Y. Jiang",
            "Xujin Chris Liu",
            "Kyunghyun Cho",
            "Eric K. Oermann"
        ],
        "submitted": "2026-02-09 18:43:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
        "abstract": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
        "url": "http://arxiv.org/abs/2602.08995v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08995v1",
        "arxiv_id": "2602.08995v1",
        "authors": [
            "Yuting Ning",
            "Jaylen Jones",
            "Zhehao Zhang",
            "Chentao Ye",
            "Weitong Ruan",
            "Junyi Li",
            "Rahul Gupta",
            "Huan Sun"
        ],
        "submitted": "2026-02-09 18:41:15",
        "source": "arxiv",
        "comment": "Project Homepage: https://osu-nlp-group.github.io/Misaligned-Action-Detection/"
    },
    {
        "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
        "abstract": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.",
        "url": "http://arxiv.org/abs/2602.08984v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08984v1",
        "arxiv_id": "2602.08984v1",
        "authors": [
            "Yuliang Liu",
            "Yunchong Song",
            "Yixuan Wang",
            "Kewen Ge",
            "Alex Lamb",
            "Qipeng Guo",
            "Kai Chen",
            "Bowen Zhou",
            "Zhouhan Lin"
        ],
        "submitted": "2026-02-09 18:33:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Transcripts: A Renewed Perspective on Audio Chaptering",
        "abstract": "Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.",
        "url": "http://arxiv.org/abs/2602.08979v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08979v1",
        "arxiv_id": "2602.08979v1",
        "authors": [
            "Fabian Retkowski",
            "Maike Züfle",
            "Thai Binh Nguyen",
            "Jan Niehues",
            "Alexander Waibel"
        ],
        "submitted": "2026-02-09 18:28:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents",
        "abstract": "Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.",
        "url": "http://arxiv.org/abs/2602.08964v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08964v1",
        "arxiv_id": "2602.08964v1",
        "authors": [
            "Raghu Arghal",
            "Fade Chen",
            "Niall Dalton",
            "Evgenii Kortukov",
            "Calum McNamara",
            "Angelos Nalmpantis",
            "Moksh Nirvaan",
            "Gabriele Sarti",
            "Mario Giulianelli"
        ],
        "submitted": "2026-02-09 18:00:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How Should We Model the Probability of a Language?",
        "abstract": "Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.",
        "url": "http://arxiv.org/abs/2602.08951v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08951v1",
        "arxiv_id": "2602.08951v1",
        "authors": [
            "Rasul Dent",
            "Pedro Ortiz Suarez",
            "Thibault Clérice",
            "Benoît Sagot"
        ],
        "submitted": "2026-02-09 17:46:56",
        "source": "arxiv",
        "comment": "Accepted for Vardial 2026"
    },
    {
        "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
        "abstract": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.",
        "url": "http://arxiv.org/abs/2602.08948v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08948v1",
        "arxiv_id": "2602.08948v1",
        "authors": [
            "Chen Jin",
            "Ryutaro Tanno",
            "Tom Diethe",
            "Philip Teare"
        ],
        "submitted": "2026-02-09 17:44:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search",
        "abstract": "Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in \"cold start\" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.",
        "url": "http://arxiv.org/abs/2602.08945v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08945v1",
        "arxiv_id": "2602.08945v1",
        "authors": [
            "Sahajpreet Singh",
            "Kokil Jaidka",
            "Min-Yen Kan"
        ],
        "submitted": "2026-02-09 17:42:32",
        "source": "arxiv",
        "comment": "18 pages, 11 figures, 7 tables"
    },
    {
        "title": "Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion",
        "abstract": "Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.",
        "url": "http://arxiv.org/abs/2602.08917v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08917v1",
        "arxiv_id": "2602.08917v1",
        "authors": [
            "Minghan Li",
            "Ercong Nie",
            "Siqi Zhao",
            "Tongna Chen",
            "Huiping Huang",
            "Guodong Zhou"
        ],
        "submitted": "2026-02-09 17:16:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation",
        "abstract": "Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.",
        "url": "http://arxiv.org/abs/2602.08896v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08896v1",
        "arxiv_id": "2602.08896v1",
        "authors": [
            "Yehua Huang",
            "Penglei Sun",
            "Zebin Chen",
            "Zhenheng Tang",
            "Xiaowen Chu"
        ],
        "submitted": "2026-02-09 16:57:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Contrastive Learning for Diversity-Aware Product Recommendations in Retail",
        "abstract": "Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.",
        "url": "http://arxiv.org/abs/2602.08886v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08886v1",
        "arxiv_id": "2602.08886v1",
        "authors": [
            "Vasileios Karlis",
            "Ezgi Yıldırım",
            "David Vos",
            "Maarten de Rijke"
        ],
        "submitted": "2026-02-09 16:48:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Is Reasoning Capability Enough for Safety in Long-Context Language Models?",
        "abstract": "Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.",
        "url": "http://arxiv.org/abs/2602.08874v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08874v1",
        "arxiv_id": "2602.08874v1",
        "authors": [
            "Yu Fu",
            "Haz Sameen Shahgir",
            "Huanli Gong",
            "Zhipeng Wei",
            "N. Benjamin Erichson",
            "Yue Dong"
        ],
        "submitted": "2026-02-09 16:35:14",
        "source": "arxiv",
        "comment": "25 pages, 7 figures"
    },
    {
        "title": "Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation",
        "abstract": "Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.",
        "url": "http://arxiv.org/abs/2602.08873v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08873v1",
        "arxiv_id": "2602.08873v1",
        "authors": [
            "Lisette Espin-Noboa",
            "Gonzalo Gabriel Mendez"
        ],
        "submitted": "2026-02-09 16:34:57",
        "source": "arxiv",
        "comment": "28 pages: 8 pages in main (5 figures, 1 table), 20 pages in appendix (18 figures, 2 tables). under-review"
    },
    {
        "title": "Large Language Models for Geolocation Extraction in Humanitarian Crisis Response",
        "abstract": "Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.",
        "url": "http://arxiv.org/abs/2602.08872v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08872v1",
        "arxiv_id": "2602.08872v1",
        "authors": [
            "G. Cafferata",
            "T. Demarco",
            "K. Kalimeri",
            "Y. Mejova",
            "M. G. Beiró"
        ],
        "submitted": "2026-02-09 16:34:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Understanding Dynamic Compute Allocation in Recurrent Transformers",
        "abstract": "Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.",
        "url": "http://arxiv.org/abs/2602.08864v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08864v1",
        "arxiv_id": "2602.08864v1",
        "authors": [
            "Ibraheem Muhammad Moosa",
            "Suhas Lohit",
            "Ye Wang",
            "Moitreya Chatterjee",
            "Wenpeng Yin"
        ],
        "submitted": "2026-02-09 16:27:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP",
        "abstract": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.",
        "url": "http://arxiv.org/abs/2602.08857v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08857v1",
        "arxiv_id": "2602.08857v1",
        "authors": [
            "Xinting Huang",
            "Aleksandra Bakalova",
            "Satwik Bhattamishra",
            "William Merrill",
            "Michael Hahn"
        ],
        "submitted": "2026-02-09 16:22:29",
        "source": "arxiv",
        "comment": "101 pages, 92 figures"
    },
    {
        "title": "AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders",
        "abstract": "Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.",
        "url": "http://arxiv.org/abs/2602.08837v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08837v1",
        "arxiv_id": "2602.08837v1",
        "authors": [
            "Minh-Duc Nguyen",
            "Hai-Dang Kieu",
            "Dung D. Le"
        ],
        "submitted": "2026-02-09 16:06:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
        "abstract": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
        "url": "http://arxiv.org/abs/2602.08829v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08829v1",
        "arxiv_id": "2602.08829v1",
        "authors": [
            "Hao Peng",
            "Yunjia Qi",
            "Xiaozhi Wang",
            "Zijun Yao",
            "Lei Hou",
            "Juanzi Li"
        ],
        "submitted": "2026-02-09 16:00:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Affective Flow Language Model for Emotional Support Conversation",
        "abstract": "Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.",
        "url": "http://arxiv.org/abs/2602.08826v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08826v1",
        "arxiv_id": "2602.08826v1",
        "authors": [
            "Chenghui Zou",
            "Ning Wang",
            "Tiesunlong Shen",
            "Luwei Xiao",
            "Chuan Ma",
            "Xiangpeng Li",
            "Rui Mao",
            "Erik Cambria"
        ],
        "submitted": "2026-02-09 15:58:50",
        "source": "arxiv",
        "comment": "19 pages, 7 figures"
    },
    {
        "title": "Bayesian Preference Learning for Test-Time Steerable Reward Models",
        "abstract": "Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.",
        "url": "http://arxiv.org/abs/2602.08819v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08819v1",
        "arxiv_id": "2602.08819v1",
        "authors": [
            "Jiwoo Hong",
            "Shao Tang",
            "Zhipeng Wang"
        ],
        "submitted": "2026-02-09 15:55:56",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "The Use of AI Tools to Develop and Validate Q-Matrices",
        "abstract": "Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.",
        "url": "http://arxiv.org/abs/2602.08796v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08796v1",
        "arxiv_id": "2602.08796v1",
        "authors": [
            "Kevin Fan",
            "Jacquelyn A. Bialo",
            "Hongli Li"
        ],
        "submitted": "2026-02-09 15:36:53",
        "source": "arxiv",
        "comment": "An earlier version of this study was presented at the Psychometric Society Meeting held in July 2025 in Minneapolis, USA"
    },
    {
        "title": "LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation",
        "abstract": "Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.",
        "url": "http://arxiv.org/abs/2602.08793v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08793v1",
        "arxiv_id": "2602.08793v1",
        "authors": [
            "Yushi Sun",
            "Xujia Li",
            "Nan Tang",
            "Quanqing Xu",
            "Chuanhui Yang",
            "Lei Chen"
        ],
        "submitted": "2026-02-09 15:30:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure",
        "abstract": "Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.",
        "url": "http://arxiv.org/abs/2602.08783v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08783v1",
        "arxiv_id": "2602.08783v1",
        "authors": [
            "Zirui Li",
            "Xuefeng Bai",
            "Kehai Chen",
            "Yizhi Li",
            "Jian Yang",
            "Chenghua Lin",
            "Min Zhang"
        ],
        "submitted": "2026-02-09 15:25:12",
        "source": "arxiv",
        "comment": "22 pages"
    },
    {
        "title": "Welfarist Formulations for Diverse Similarity Search",
        "abstract": "Nearest Neighbor Search (NNS) is a fundamental problem in data structures with wide-ranging applications, such as web search, recommendation systems, and, more recently, retrieval-augmented generations (RAG). In such recent applications, in addition to the relevance (similarity) of the returned neighbors, diversity among the neighbors is a central requirement. In this paper, we develop principled welfare-based formulations in NNS for realizing diversity across attributes. Our formulations are based on welfare functions -- from mathematical economics -- that satisfy central diversity (fairness) and relevance (economic efficiency) axioms. With a particular focus on Nash social welfare, we note that our welfare-based formulations provide objective functions that adaptively balance relevance and diversity in a query-dependent manner. Notably, such a balance was not present in the prior constraint-based approach, which forced a fixed level of diversity and optimized for relevance. In addition, our formulation provides a parametric way to control the trade-off between relevance and diversity, providing practitioners with flexibility to tailor search results to task-specific requirements. We develop efficient nearest neighbor algorithms with provable guarantees for the welfare-based objectives. Notably, our algorithm can be applied on top of any standard ANN method (i.e., use standard ANN method as a subroutine) to efficiently find neighbors that approximately maximize our welfare-based objectives. Experimental results demonstrate that our approach is practical and substantially improves diversity while maintaining high relevance of the retrieved neighbors.",
        "url": "http://arxiv.org/abs/2602.08742v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08742v1",
        "arxiv_id": "2602.08742v1",
        "authors": [
            "Siddharth Barman",
            "Nirjhar Das",
            "Shivam Gupta",
            "Kirankumar Shiragur"
        ],
        "submitted": "2026-02-09 14:42:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy",
        "abstract": "We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.",
        "url": "http://arxiv.org/abs/2602.08740v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08740v1",
        "arxiv_id": "2602.08740v1",
        "authors": [
            "Gaifan Zhang",
            "Danushka Bollegala"
        ],
        "submitted": "2026-02-09 14:41:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments",
        "abstract": "Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.",
        "url": "http://arxiv.org/abs/2602.08716v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08716v1",
        "arxiv_id": "2602.08716v1",
        "authors": [
            "Shangrui Nie",
            "Kian Omoomi",
            "Lucie Flek",
            "Zhixue Zhao",
            "Charles Welch"
        ],
        "submitted": "2026-02-09 14:25:07",
        "source": "arxiv",
        "comment": "15 pages, 1 figure"
    },
    {
        "title": "FactSim: Fact-Checking for Opinion Summarization",
        "abstract": "We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.",
        "url": "http://arxiv.org/abs/2602.08709v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08709v1",
        "arxiv_id": "2602.08709v1",
        "authors": [
            "Leandro Anghinoni",
            "Jorge Sanchez"
        ],
        "submitted": "2026-02-09 14:21:19",
        "source": "arxiv",
        "comment": "10 pages, 4 figures"
    },
    {
        "title": "Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search",
        "abstract": "Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.",
        "url": "http://arxiv.org/abs/2602.08700v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08700v1",
        "arxiv_id": "2602.08700v1",
        "authors": [
            "Clemencia Siro",
            "Zahra Abbasiantaeb",
            "Yifei Yuan",
            "Mohammad Aliannejadi",
            "Maarten de Rijke"
        ],
        "submitted": "2026-02-09 14:16:11",
        "source": "arxiv",
        "comment": "Accepted at CHIIR 2025"
    },
    {
        "title": "Challenges in Translating Technical Lectures: Insights from the NPTEL",
        "abstract": "This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.",
        "url": "http://arxiv.org/abs/2602.08698v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08698v1",
        "arxiv_id": "2602.08698v1",
        "authors": [
            "Basudha Raje",
            "Sadanand Venkatraman",
            "Nandana TP",
            "Soumyadeepa Das",
            "Polkam Poojitha",
            "M. Vijaykumar",
            "Tanima Bagchi",
            "Hema A. Murthy"
        ],
        "submitted": "2026-02-09 14:15:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prototype-Based Disentanglement for Controllable Dysarthric Speech Synthesis",
        "abstract": "Dysarthric speech exhibits high variability and limited labeled data, posing major challenges for both automatic speech recognition (ASR) and assistive speech technologies. Existing approaches rely on synthetic data augmentation or speech reconstruction, yet often entangle speaker identity with pathological articulation, limiting controllability and robustness.\n  In this paper, we propose ProtoDisent-TTS, a prototype-based disentanglement TTS framework built on a pre-trained text-to-speech backbone that factorizes speaker timbre and dysarthric articulation within a unified latent space. A pathology prototype codebook provides interpretable and controllable representations of healthy and dysarthric speech patterns, while a dual-classifier objective with a gradient reversal layer enforces invariance of speaker embeddings to pathological attributes. Experiments on the TORGO dataset demonstrate that this design enables bidirectional transformation between healthy and dysarthric speech, leading to consistent ASR performance gains and robust, speaker-aware speech reconstruction.",
        "url": "http://arxiv.org/abs/2602.08696v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08696v1",
        "arxiv_id": "2602.08696v1",
        "authors": [
            "Haoshen Wang",
            "Xueli Zhong",
            "Bingbing Lin",
            "Jia Huang",
            "Xingduo Pan",
            "Shengxiang Liang",
            "Nizhuan Wang",
            "Wai Ting Siok"
        ],
        "submitted": "2026-02-09 14:14:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement",
        "abstract": "This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.",
        "url": "http://arxiv.org/abs/2602.08688v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08688v1",
        "arxiv_id": "2602.08688v1",
        "authors": [
            "Hossein Kermani",
            "Fatemeh Oudlajani",
            "Pardis Yarahmadi",
            "Hamideh Mahdi Soltani",
            "Mohammad Makki",
            "Zahra HosseiniKhoo"
        ],
        "submitted": "2026-02-09 14:10:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SA-CAISR: Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation",
        "abstract": "Sequential recommendation (SR) aims to predict a user's next action by learning from their historical interaction sequences. In real-world applications, these models require periodic updates to adapt to new interactions and evolving user preferences. While incremental learning methods facilitate these updates, they face significant challenges. Replay-based approaches incur high memory and computational costs, and regularization-based methods often struggle to discard outdated or conflicting knowledge. To overcome these challenges, we propose SA-CAISR, a Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation framework. As a buffer-free framework, SA-CAISR operates using only the old model and new data, directly addressing the high costs of replay-based techniques. SA-CAISR introduces a novel Fisher-weighted knowledge-screening mechanism that dynamically identifies outdated knowledge by estimating parameter-level conflicts between the old model and new data, allowing our approach to selectively remove obsolete knowledge while preserving compatible historical patterns. This dynamic balance between stability and adaptability allows our method to achieve a new state-of-the-art performance in incremental SR. Specifically, SA-CAISR improves Recall@20 by 2.0%, MRR@20 by 1.2%, and NDCG@20 by 1.4% on average across datasets, while reducing memory usage by 97.5% and training time by 46.9% compared to the best baselines. This efficiency allows real-world systems to rapidly update user profiles with minimal computational overhead, ensuring more timely and accurate recommendations.",
        "url": "http://arxiv.org/abs/2602.08678v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08678v1",
        "arxiv_id": "2602.08678v1",
        "authors": [
            "Xiaomeng Song",
            "Xinru Wang",
            "Hanbing Wang",
            "Hongyu Lu",
            "Yu Chen",
            "Zhaochun Ren",
            "Zhumin Chen"
        ],
        "submitted": "2026-02-09 14:00:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning to Judge: LLMs Designing and Applying Evaluation Rubrics",
        "abstract": "Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.",
        "url": "http://arxiv.org/abs/2602.08672v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08672v1",
        "arxiv_id": "2602.08672v1",
        "authors": [
            "Clemencia Siro",
            "Pourya Aliannejadi",
            "Mohammad Aliannejadi"
        ],
        "submitted": "2026-02-09 13:56:06",
        "source": "arxiv",
        "comment": "Accepted at EACL 2026 Findings"
    },
    {
        "title": "Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion",
        "abstract": "Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved \"seed\" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure.\n  We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition.\n  We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.",
        "url": "http://arxiv.org/abs/2602.08668v2",
        "pdf_url": "https://arxiv.org/pdf/2602.08668v2",
        "arxiv_id": "2602.08668v2",
        "authors": [
            "Scott Thornton"
        ],
        "submitted": "2026-02-09 13:55:04",
        "source": "arxiv",
        "comment": "18 pages, 5 figures"
    },
    {
        "title": "SRSUPM: Sequential Recommender System Based on User Psychological Motivation",
        "abstract": "Sequential recommender infers users' evolving psychological motivations from historical interactions to recommend the next preferred items. Most existing methods compress recent behaviors into a single vector and optimize it toward a single observed target item, but lack explicit modeling of psychological motivation shift. As a result, they struggle to uncover the distributional patterns across different shift degrees and to capture collaborative knowledge that is sensitive to psychological motivation shift. We propose a general framework, the Sequential Recommender System Based on User Psychological Motivation, to enhance sequential recommenders with psychological motivation shift-aware user modeling. Specifically, the Psychological Motivation Shift Assessment quantitatively measures psychological motivation shift; guided by PMSA, the Shift Information Construction models dynamically evolving multi-level shift states, and the Psychological Motivation Shift-driven Information Decomposition decomposes and regularizes representations across shift levels. Moreover, the Psychological Motivation Shift Information Matching strengthens collaborative patterns related to psychological motivation shift to learn more discriminative user representations. Extensive experiments on three public benchmarks show that SRSUPM consistently outperforms representative baselines on diverse sequential recommender tasks.",
        "url": "http://arxiv.org/abs/2602.08667v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08667v1",
        "arxiv_id": "2602.08667v1",
        "authors": [
            "Yicheng Di",
            "Yuan Liu",
            "Zhi Chen",
            "Jingcai Guo"
        ],
        "submitted": "2026-02-09 13:54:34",
        "source": "arxiv",
        "comment": "9 pages, 8 pages"
    },
    {
        "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
        "abstract": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.",
        "url": "http://arxiv.org/abs/2602.08658v2",
        "pdf_url": "https://arxiv.org/pdf/2602.08658v2",
        "arxiv_id": "2602.08658v2",
        "authors": [
            "Mingzi Cao",
            "Xingwei Tan",
            "Mahmud Elahi Akhter",
            "Marco Valentino",
            "Maria Liakata",
            "Xi Wang",
            "Nikolaos Aletras"
        ],
        "submitted": "2026-02-09 13:51:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "We Should Separate Memorization from Copyright",
        "abstract": "The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.",
        "url": "http://arxiv.org/abs/2602.08632v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08632v1",
        "arxiv_id": "2602.08632v1",
        "authors": [
            "Adi Haviv",
            "Niva Elkin-Koren",
            "Uri Hacohen",
            "Roi Livni",
            "Shay Moran"
        ],
        "submitted": "2026-02-09 13:24:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Do Multilingual LLMs have specialized language heads?",
        "abstract": "Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.",
        "url": "http://arxiv.org/abs/2602.08625v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08625v1",
        "arxiv_id": "2602.08625v1",
        "authors": [
            "Muhammad Naufil"
        ],
        "submitted": "2026-02-09 13:15:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OneLive: Dynamically Unified Generative Framework for Live-Streaming Recommendation",
        "abstract": "Live-streaming recommender system serves as critical infrastructure that bridges the patterns of real-time interactions between users and authors. Similar to traditional industrial recommender systems, live-streaming recommendation also relies on cascade architectures to support large-scale concurrency. Recent advances in generative recommendation unify the multi-stage recommendation process with Transformer-based architectures, offering improved scalability and higher computational efficiency. However, the inherent complexity of live-streaming prevents the direct transfer of these methods to live-streaming scenario, where continuously evolving content, limited lifecycles, strict real-time constraints, and heterogeneous multi-objectives introduce unique challenges that invalidate static tokenization and conventional model framework. To address these issues, we propose OneLive, a dynamically unified generative recommendation framework tailored for live-streaming scenario. OneLive integrates four key components: (i) A Dynamic Tokenizer that continuously encodes evolving real-time live content fused with behavior signal through residual quantization; (ii) A Time-Aware Gated Attention mechanism that explicitly models temporal dynamics for timely decision making; (iii) An efficient decoder-only generative architecture enhanced with Sequential MTP and QK Norm for stable training and accelerated inference; (iv) A Unified Multi-Objective Alignment Framework reinforces policy optimization for personalized preferences.",
        "url": "http://arxiv.org/abs/2602.08612v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08612v1",
        "arxiv_id": "2602.08612v1",
        "authors": [
            "Shen Wang",
            "Yusheng Huang",
            "Ruochen Yang",
            "Shuang Wen",
            "Pengbo Xu",
            "Jiangxia Cao",
            "Yueyang Liu",
            "Kuo Cai",
            "Chengcheng Guo",
            "Shiyao Wang",
            "Xinchen Luo",
            "Qiang Luo",
            "Ruiming Tang",
            "Shuang Yang",
            "Zhaojie Liu",
            "Guorui Zhou",
            "Han Li",
            "Kun Gai"
        ],
        "submitted": "2026-02-09 12:56:39",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling",
        "abstract": "Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\\times$--10$\\times$ decoding speedup and reduces first-chunk latency by 34\\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.",
        "url": "http://arxiv.org/abs/2602.08607v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08607v1",
        "arxiv_id": "2602.08607v1",
        "authors": [
            "Ziyang Cheng",
            "Yuhao Wang",
            "Heyang Liu",
            "Ronghua Wu",
            "Qunshan Gu",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "submitted": "2026-02-09 12:52:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation",
        "abstract": "Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.",
        "url": "http://arxiv.org/abs/2602.08600v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08600v1",
        "arxiv_id": "2602.08600v1",
        "authors": [
            "Archchana Sindhujan",
            "Girish A. Koushik",
            "Shenbin Qian",
            "Diptesh Kanojia",
            "Constantin Orăsan"
        ],
        "submitted": "2026-02-09 12:42:41",
        "source": "arxiv",
        "comment": "Currently this article is under review for Natural Language Processing Journal"
    },
    {
        "title": "RankGR: Rank-Enhanced Generative Retrieval with Listwise Direct Preference Optimization in Recommendation",
        "abstract": "Generative retrieval (GR) has emerged as a promising paradigm in recommendation systems by autoregressively decoding identifiers of target items. Despite its potential, current approaches typically rely on the next-token prediction schema, which treats each token of the next interacted items as the sole target. This narrow focus 1) limits their ability to capture the nuanced structure of user preferences, and 2) overlooks the deep interaction between decoded identifiers and user behavior sequences. In response to these challenges, we propose RankGR, a Rank-enhanced Generative Retrieval method that incorporates listwise direct preference optimization for recommendation. RankGR decomposes the retrieval process into two complementary stages: the Initial Assessment Phase (IAP) and the Refined Scoring Phase (RSP). In IAP, we incorporate a novel listwise direct preference optimization strategy into GR, thus facilitating a more comprehensive understanding of the hierarchical user preferences and more effective partial-order modeling. The RSP then refines the top-λ candidates generated by IAP with interactions towards input sequences using a lightweight scoring module, leading to more precise candidate evaluation. Both phases are jointly optimized under a unified GR model, ensuring consistency and efficiency. Additionally, we implement several practical improvements in training and deployment, ultimately achieving a real-time system capable of handling nearly ten thousand requests per second. Extensive offline performance on both research and industrial datasets, as well as the online gains on the \"Guess You Like\" section of Taobao, validate the effectiveness and scalability of RankGR.",
        "url": "http://arxiv.org/abs/2602.08575v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08575v1",
        "arxiv_id": "2602.08575v1",
        "authors": [
            "Kairui Fu",
            "Changfa Wu",
            "Kun Yuan",
            "Binbin Cao",
            "Dunxian Huang",
            "Yuliang Yan",
            "Junjun Zheng",
            "Jianning Zhang",
            "Silu Zhou",
            "Jian Wu",
            "Kun Kuang"
        ],
        "submitted": "2026-02-09 12:13:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Reliable Social A/B Testing: Spillover-Contained Clustering with Robust Post-Experiment Analysis",
        "abstract": "A/B testing is the foundation of decision-making in online platforms, yet social products often suffer from network interference: user interactions cause treatment effects to spill over into the control group. Such spillovers bias causal estimates and undermine experimental conclusions. Existing approaches face key limitations: user-level randomization ignores network structure, while cluster-based methods often rely on general-purpose clustering that is not tailored for spillover containment and has difficulty balancing unbiasedness and statistical power at scale. We propose a spillover-contained experimentation framework with two stages. In the pre-experiment stage, we build social interaction graphs and introduce a Balanced Louvain algorithm that produces stable, size-balanced clusters while minimizing cross-cluster edges, enabling reliable cluster-based randomization. In the post-experiment stage, we develop a tailored CUPAC estimator that leverages pre-experiment behavioral covariates to reduce the variance induced by cluster-level assignment, thereby improving statistical power. Together, these components provide both structural spillover containment and robust statistical inference. We validate our approach through large-scale social sharing experiments on Kuaishou, a platform serving hundreds of millions of users. Results show that our method substantially reduces spillover and yields more accurate assessments of social strategies than traditional user-level designs, establishing a reliable and scalable framework for networked A/B testing.",
        "url": "http://arxiv.org/abs/2602.08569v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08569v1",
        "arxiv_id": "2602.08569v1",
        "authors": [
            "Xu Min",
            "Zhaoxu Yang",
            "Kaixuan Tan",
            "Juan Yan",
            "Xunbin Xiong",
            "Zihao Zhu",
            "Kaiyu Zhu",
            "Fenglin Cui",
            "Yang Yang",
            "Sihua Yang",
            "Jianhui Bu"
        ],
        "submitted": "2026-02-09 12:08:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems",
        "abstract": "Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.",
        "url": "http://arxiv.org/abs/2602.08567v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08567v1",
        "arxiv_id": "2602.08567v1",
        "authors": [
            "Jinnuo Liu",
            "Chuke Liu",
            "Hua Shen"
        ],
        "submitted": "2026-02-09 12:06:07",
        "source": "arxiv",
        "comment": "Preprint. Under review. 18 pages, 9 figures"
    },
    {
        "title": "Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches",
        "abstract": "Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.",
        "url": "http://arxiv.org/abs/2602.08561v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08561v1",
        "arxiv_id": "2602.08561v1",
        "authors": [
            "Syed Mehtab Hussain Shah",
            "Frank Hopfgartner",
            "Arnim Bleier"
        ],
        "submitted": "2026-02-09 11:59:59",
        "source": "arxiv",
        "comment": "12 pages, 5 figures. Submitted to ACM conference"
    },
    {
        "title": "QARM V2: Quantitative Alignment Multi-Modal Recommendation for Reasoning User Sequence Modeling",
        "abstract": "With the evolution of large language models (LLMs), there is growing interest in leveraging their rich semantic understanding to enhance industrial recommendation systems (RecSys). Traditional RecSys relies on ID-based embeddings for user sequence modeling in the General Search Unit (GSU) and Exact Search Unit (ESU) paradigm, which suffers from low information density, knowledge isolation, and weak generalization ability. While LLMs offer complementary strengths with dense semantic representations and strong generalization, directly applying LLM embeddings to RecSys faces critical challenges: representation unmatch with business objectives and representation unlearning end-to-end with downstream tasks. In this paper, we present QARM V2, a unified framework that bridges LLM semantic understanding with RecSys business requirements for user sequence modeling.",
        "url": "http://arxiv.org/abs/2602.08559v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08559v1",
        "arxiv_id": "2602.08559v1",
        "authors": [
            "Tian Xia",
            "Jiaqi Zhang",
            "Yueyang Liu",
            "Hongjian Dou",
            "Tingya Yin",
            "Jiangxia Cao",
            "Xulei Liang",
            "Tianlu Xie",
            "Lihao Liu",
            "Xiang Chen",
            "Shen Wang",
            "Changxin Lao",
            "Haixiang Gan",
            "Jinkai Yu",
            "Keting Cen",
            "Lu Hao",
            "Xu Zhang",
            "Qiqiang Zhong",
            "Zhongbo Sun",
            "Yiyu Wang",
            "Shuang Yang",
            "Mingxin Wen",
            "Xiangyu Wu",
            "Shaoguo Liu",
            "Tingting Gao",
            "Zhaojie Liu",
            "Han Li",
            "Kun Gai"
        ],
        "submitted": "2026-02-09 11:57:28",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location",
        "abstract": "While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.",
        "url": "http://arxiv.org/abs/2602.08548v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08548v1",
        "arxiv_id": "2602.08548v1",
        "authors": [
            "Xuanliang Zhang",
            "Dingzirui Wang",
            "Keyan Xu",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "submitted": "2026-02-09 11:47:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation",
        "abstract": "Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.",
        "url": "http://arxiv.org/abs/2602.08545v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08545v1",
        "arxiv_id": "2602.08545v1",
        "authors": [
            "Xingyuan Zeng",
            "Zuohan Wu",
            "Yue Wang",
            "Chen Zhang",
            "Quanming Yao",
            "Libin Zheng",
            "Jian Yin"
        ],
        "submitted": "2026-02-09 11:45:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GISA: A Benchmark for General Information-Seeking Assistant",
        "abstract": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
        "url": "http://arxiv.org/abs/2602.08543v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08543v1",
        "arxiv_id": "2602.08543v1",
        "authors": [
            "Yutao Zhu",
            "Xingshuo Zhang",
            "Maosen Zhang",
            "Jiajie Jin",
            "Liancheng Zhang",
            "Xiaoshuai Song",
            "Kangzhi Zhao",
            "Wencong Zeng",
            "Ruiming Tang",
            "Han Li",
            "Ji-Rong Wen",
            "Zhicheng Dou"
        ],
        "submitted": "2026-02-09 11:44:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PIT: A Dynamic Personalized Item Tokenizer for End-to-End Generative Recommendation",
        "abstract": "Generative Recommendation has revolutionized recommender systems by reformulating retrieval as a sequence generation task over discrete item identifiers. Despite the progress, existing approaches typically rely on static, decoupled tokenization that ignores collaborative signals. While recent methods attempt to integrate collaborative signals into item identifiers either during index construction or through end-to-end modeling, they encounter significant challenges in real-world production environments. Specifically, the volatility of collaborative signals leads to unstable tokenization, and current end-to-end strategies often devolve into suboptimal two-stage training rather than achieving true co-evolution. To bridge this gap, we propose PIT, a dynamic Personalized Item Tokenizer framework for end-to-end generative recommendation, which employs a co-generative architecture that harmonizes collaborative patterns through collaborative signal alignment and synchronizes item tokenizer with generative recommender via a co-evolution learning. This enables the dynamic, joint, end-to-end evolution of both index construction and recommendation. Furthermore, a one-to-many beam index ensures scalability and robustness, facilitating seamless integration into large-scale industrial deployments. Extensive experiments on real-world datasets demonstrate that PIT consistently outperforms competitive baselines. In a large-scale deployment at Kuaishou, an online A/B test yielded a substantial 0.402% uplift in App Stay Time, validating the framework's effectiveness in dynamic industrial environments.",
        "url": "http://arxiv.org/abs/2602.08530v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08530v1",
        "arxiv_id": "2602.08530v1",
        "authors": [
            "Huanjie Wang",
            "Xinchen Luo",
            "Honghui Bao",
            "Zhang Zixing",
            "Lejian Ren",
            "Yunfan Wu",
            "Hongwei Zhang",
            "Liwei Guan",
            "Guang Chen"
        ],
        "submitted": "2026-02-09 11:28:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
        "abstract": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.",
        "url": "http://arxiv.org/abs/2602.08503v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08503v1",
        "arxiv_id": "2602.08503v1",
        "authors": [
            "Yi Ding",
            "Ziliang Qiu",
            "Bolian Li",
            "Ruqi Zhang"
        ],
        "submitted": "2026-02-09 10:55:13",
        "source": "arxiv",
        "comment": "17 pages"
    },
    {
        "title": "Characterizing, Evaluating, and Optimizing Complex Reasoning",
        "abstract": "Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.",
        "url": "http://arxiv.org/abs/2602.08498v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08498v1",
        "arxiv_id": "2602.08498v1",
        "authors": [
            "Haoran Zhang",
            "Yafu Li",
            "Zhi Wang",
            "Zhilin Wang",
            "Shunkai Zhang",
            "Xiaoye Qu",
            "Yu Cheng"
        ],
        "submitted": "2026-02-09 10:51:14",
        "source": "arxiv",
        "comment": "Code and data are available at \\url{https://github.com/zzzhr97/TRM}"
    },
    {
        "title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.",
        "url": "http://arxiv.org/abs/2602.08489v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08489v1",
        "arxiv_id": "2602.08489v1",
        "authors": [
            "Hyunseok Lee",
            "Soheil Abbasloo",
            "Jihoon Tack",
            "Jinwoo Shin"
        ],
        "submitted": "2026-02-09 10:41:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hybrid Pooling with LLMs via Relevance Context Learning",
        "abstract": "High-quality relevance judgements over large query sets are essential for evaluating Information Retrieval (IR) systems, yet manual annotation remains costly and time-consuming. Large Language Models (LLMs) have recently shown promise as automatic relevance assessors, but their reliability is still limited. Most existing approaches rely on zero-shot prompting or In-Context Learning (ICL) with a small number of labeled examples. However, standard ICL treats examples as independent instances and fails to explicitly capture the underlying relevance criteria of a topic, restricting its ability to generalize to unseen query-document pairs. To address this limitation, we introduce Relevance Context Learning (RCL), a novel framework that leverages human relevance judgements to explicitly model topic-specific relevance criteria. Rather than directly using labeled examples for in-context prediction, RCL first prompts an LLM (Instructor LLM) to analyze sets of judged query-document pairs and generate explicit narratives that describe what constitutes relevance for a given topic. These relevance narratives are then used as structured prompts to guide a second LLM (Assessor LLM) in producing relevance judgements. To evaluate RCL in a realistic data collection setting, we propose a hybrid pooling strategy in which a shallow depth-\\textit{k} pool from participating systems is judged by human assessors, while the remaining documents are labeled by LLMs. Experimental results demonstrate that RCL substantially outperforms zero-shot prompting and consistently improves over standard ICL. Overall, our findings indicate that transforming relevance examples into explicit, context-aware relevance narratives is a more effective way of exploiting human judgements for LLM-based IR dataset construction.",
        "url": "http://arxiv.org/abs/2602.08457v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08457v1",
        "arxiv_id": "2602.08457v1",
        "authors": [
            "David Otero",
            "Javier Parapar"
        ],
        "submitted": "2026-02-09 10:10:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Models and Impossible Language Acquisition: \"False Promise\" or an Overturn of our Current Perspective towards AI",
        "abstract": "In Chomsky's provocative critique \"The False Promise of CHATGPT,\" Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his \"rationalist-romantics\" paradigm to functionalism and empiricism in LLMs research.",
        "url": "http://arxiv.org/abs/2602.08437v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08437v1",
        "arxiv_id": "2602.08437v1",
        "authors": [
            "Ziyan wang",
            "Longlong Ma"
        ],
        "submitted": "2026-02-09 09:50:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prism: Spectral-Aware Block-Sparse Attention",
        "abstract": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\\mathbf{5.1\\times}$ speedup.",
        "url": "http://arxiv.org/abs/2602.08426v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08426v1",
        "arxiv_id": "2602.08426v1",
        "authors": [
            "Xinghao Wang",
            "Pengyu Wang",
            "Xiaoran Liu",
            "Fangxu Liu",
            "Jason Chu",
            "Kai Song",
            "Xipeng Qiu"
        ],
        "submitted": "2026-02-09 09:31:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Sketch+Text Composed Image Retrieval Dataset for Thangka",
        "abstract": "Composed Image Retrieval (CIR) enables image retrieval by combining multiple query modalities, but existing benchmarks predominantly focus on general-domain imagery and rely on reference images with short textual modifications. As a result, they provide limited support for retrieval scenarios that require fine-grained semantic reasoning, structured visual understanding, and domain-specific knowledge. In this work, we introduce CIRThan, a sketch+text Composed Image Retrieval dataset for Thangka imagery, a culturally grounded and knowledge-specific visual domain characterized by complex structures, dense symbolic elements, and domain-dependent semantic conventions. CIRThan contains 2,287 high-quality Thangka images, each paired with a human-drawn sketch and hierarchical textual descriptions at three semantic levels, enabling composed queries that jointly express structural intent and multi-level semantic specification. We provide standardized data splits, comprehensive dataset analysis, and benchmark evaluations of representative supervised and zero-shot CIR methods. Experimental results reveal that existing CIR approaches, largely developed for general-domain imagery, struggle to effectively align sketch-based abstractions and hierarchical textual semantics with fine-grained Thangka images, particularly without in-domain supervision. We believe CIRThan offers a valuable benchmark for advancing sketch+text CIR, hierarchical semantic modeling, and multimodal retrieval in cultural heritage and other knowledge-specific visual domains. The dataset is publicly available at https://github.com/jinyuxu-whut/CIRThan.",
        "url": "http://arxiv.org/abs/2602.08411v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08411v1",
        "arxiv_id": "2602.08411v1",
        "authors": [
            "Jinyu Xu",
            "Yi Sun",
            "Jiangling Zhang",
            "Qing Xie",
            "Daomin Ji",
            "Zhifeng Bao",
            "Jiachen Li",
            "Yanchun Ma",
            "Yongjian Liu"
        ],
        "submitted": "2026-02-09 09:14:29",
        "source": "arxiv",
        "comment": "9 pages"
    },
    {
        "title": "TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration",
        "abstract": "Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.",
        "url": "http://arxiv.org/abs/2602.08404v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08404v1",
        "arxiv_id": "2602.08404v1",
        "authors": [
            "Linye Wei",
            "Zixiang Luo",
            "Pingzhi Tang",
            "Meng Li"
        ],
        "submitted": "2026-02-09 09:05:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
        "abstract": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
        "url": "http://arxiv.org/abs/2602.08382v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08382v1",
        "arxiv_id": "2602.08382v1",
        "authors": [
            "Zhuoen Chen",
            "Dongfang Li",
            "Meishan Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "submitted": "2026-02-09 08:33:11",
        "source": "arxiv",
        "comment": "26 pages, 7 figures. Code and models will be released"
    },
    {
        "title": "Reinforcement Learning with Backtracking Feedback",
        "abstract": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.",
        "url": "http://arxiv.org/abs/2602.08377v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08377v1",
        "arxiv_id": "2602.08377v1",
        "authors": [
            "Bilgehan Sel",
            "Vaishakh Keshava",
            "Phillip Wallis",
            "Lukas Rutishauser",
            "Ming Jin",
            "Dingcheng Li"
        ],
        "submitted": "2026-02-09 08:23:19",
        "source": "arxiv",
        "comment": "NeurIPS 2025"
    },
    {
        "title": "ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts",
        "abstract": "Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.",
        "url": "http://arxiv.org/abs/2602.08371v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08371v1",
        "arxiv_id": "2602.08371v1",
        "authors": [
            "Hung Quang Tran",
            "Nam Tien Pham",
            "Son T. Luu",
            "Kiet Van Nguyen"
        ],
        "submitted": "2026-02-09 08:10:40",
        "source": "arxiv",
        "comment": "Accepted as main paper at EACL 2026"
    },
    {
        "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval",
        "abstract": "Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.",
        "url": "http://arxiv.org/abs/2602.08369v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08369v1",
        "arxiv_id": "2602.08369v1",
        "authors": [
            "Xin Zhang",
            "Kailai Yang",
            "Chenyue Li",
            "Hao Li",
            "Qiyu Wei",
            "Jun'ichi Tsujii",
            "Sophia Ananiadou"
        ],
        "submitted": "2026-02-09 08:09:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints",
        "abstract": "Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \\textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \\textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\\% feasibility in text-only settings, which plummets to 19.33\\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.",
        "url": "http://arxiv.org/abs/2602.08367v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08367v1",
        "arxiv_id": "2602.08367v1",
        "authors": [
            "Zexuan Wang",
            "Chenghao Yang",
            "Yingqi Que",
            "Zhenzhu Yang",
            "Huaqing Yuan",
            "Yiwen Wang",
            "Zhengxuan Jiang",
            "Shengjie Fang",
            "Zhenhe Wu",
            "Zhaohui Wang",
            "Zhixin Yao",
            "Jiashuo Liu",
            "Jincheng Ren",
            "Yuzhen Li",
            "Yang Yang",
            "Jiaheng Liu",
            "Jian Yang",
            "Zaiyuan Wang",
            "Ge Zhang",
            "Zhoufutu Wen",
            "Wenhao Huang"
        ],
        "submitted": "2026-02-09 08:03:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection",
        "abstract": "Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.\n  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.",
        "url": "http://arxiv.org/abs/2602.08343v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08343v1",
        "arxiv_id": "2602.08343v1",
        "authors": [
            "Debajyoti Datta",
            "Trishala Neeraj",
            "Bibek Paudel",
            "Vyom Sharma",
            "Subhabrata Mukherjee"
        ],
        "submitted": "2026-02-09 07:28:55",
        "source": "arxiv",
        "comment": "18 pages, 5 figures, 18 tables"
    },
    {
        "title": "UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models",
        "abstract": "To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.",
        "url": "http://arxiv.org/abs/2602.08336v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08336v1",
        "arxiv_id": "2602.08336v1",
        "authors": [
            "Cheng Yang",
            "Chufan Shi",
            "Bo Shui",
            "Yaokang Wu",
            "Muzi Tao",
            "Huijuan Wang",
            "Ivan Yee Lee",
            "Yong Liu",
            "Xuezhe Ma",
            "Taylor Berg-Kirkpatrick"
        ],
        "submitted": "2026-02-09 07:17:57",
        "source": "arxiv",
        "comment": "Project page: https://ureason.github.io"
    },
    {
        "title": "Latent Reasoning with Supervised Thinking States",
        "abstract": "Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.",
        "url": "http://arxiv.org/abs/2602.08332v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08332v1",
        "arxiv_id": "2602.08332v1",
        "authors": [
            "Ido Amos",
            "Avi Caciularu",
            "Mor Geva",
            "Amir Globerson",
            "Jonathan Herzig",
            "Lior Shani",
            "Idan Szpektor"
        ],
        "submitted": "2026-02-09 07:12:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling",
        "abstract": "In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.",
        "url": "http://arxiv.org/abs/2602.08322v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08322v1",
        "arxiv_id": "2602.08322v1",
        "authors": [
            "Wei Zhu"
        ],
        "submitted": "2026-02-09 06:52:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models",
        "abstract": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.",
        "url": "http://arxiv.org/abs/2602.08321v2",
        "pdf_url": "https://arxiv.org/pdf/2602.08321v2",
        "arxiv_id": "2602.08321v2",
        "authors": [
            "Zijie Chen",
            "Zhenghao Lin",
            "Xiao Liu",
            "Zhenzhong Lan",
            "Yeyun Gong",
            "Peng Cheng"
        ],
        "submitted": "2026-02-09 06:52:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation",
        "abstract": "Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \\textit{\\textbf{J}udicial \\textbf{U}nified \\textbf{S}ynthesis \\textbf{T}hrough \\textbf{I}ntermediate \\textbf{C}onclusion \\textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\\rightarrow$ Pre-Judge $\\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.",
        "url": "http://arxiv.org/abs/2602.08305v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08305v1",
        "arxiv_id": "2602.08305v1",
        "authors": [
            "Binglin Wu",
            "Yingyi Zhang",
            "Xiannneg Li"
        ],
        "submitted": "2026-02-09 06:27:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Does Context Help? Error Dynamics of Contextual Information in Large Language Models",
        "abstract": "Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\\%$.",
        "url": "http://arxiv.org/abs/2602.08294v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08294v1",
        "arxiv_id": "2602.08294v1",
        "authors": [
            "Dingzirui Wang",
            "Xuanliang Zhang",
            "Keyan Xu",
            "Qingfu Zhu",
            "Wanxiang Che",
            "Yang Deng"
        ],
        "submitted": "2026-02-09 05:58:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network",
        "abstract": "With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.",
        "url": "http://arxiv.org/abs/2602.08289v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08289v1",
        "arxiv_id": "2602.08289v1",
        "authors": [
            "Binglin Wu",
            "Xianneng Li"
        ],
        "submitted": "2026-02-09 05:46:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR",
        "abstract": "Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.",
        "url": "http://arxiv.org/abs/2602.08281v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08281v1",
        "arxiv_id": "2602.08281v1",
        "authors": [
            "Zhilin Wang",
            "Yafu Li",
            "Shunkai Zhang",
            "Zhi Wang",
            "Haoran Zhang",
            "Xiaoye Qu",
            "Yu Cheng"
        ],
        "submitted": "2026-02-09 05:23:13",
        "source": "arxiv",
        "comment": "15 pages"
    },
    {
        "title": "Linguistics and Human Brain: A Perspective of Computational Neuroscience",
        "abstract": "Elucidating the language-brain relationship requires bridging the methodological gap between the abstract theoretical frameworks of linguistics and the empirical neural data of neuroscience. Serving as an interdisciplinary cornerstone, computational neuroscience formalizes the hierarchical and dynamic structures of language into testable neural models through modeling, simulation, and data analysis. This enables a computational dialogue between linguistic hypotheses and neural mechanisms. Recent advances in deep learning, particularly large language models (LLMs), have powerfully advanced this pursuit. Their high-dimensional representational spaces provide a novel scale for exploring the neural basis of linguistic processing, while the \"model-brain alignment\" framework offers a methodology to evaluate the biological plausibility of language-related theories.",
        "url": "http://arxiv.org/abs/2602.08275v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08275v1",
        "arxiv_id": "2602.08275v1",
        "authors": [
            "Fudong Zhang",
            "Bo Chai",
            "Yujie Wu",
            "Wai Ting Siok",
            "Nizhuan Wang"
        ],
        "submitted": "2026-02-09 05:10:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Language Modeling and Understanding Through Paraphrase Generation and Detection",
        "abstract": "Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...",
        "url": "http://arxiv.org/abs/2602.08274v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08274v1",
        "arxiv_id": "2602.08274v1",
        "authors": [
            "Jan Philip Wahle"
        ],
        "submitted": "2026-02-09 05:09:03",
        "source": "arxiv",
        "comment": "PhD dissertation, University of Göttingen Germany, 2025. 182 pages"
    },
    {
        "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities",
        "abstract": "Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.",
        "url": "http://arxiv.org/abs/2602.08254v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08254v1",
        "arxiv_id": "2602.08254v1",
        "authors": [
            "Arman Aghaee",
            "Sepehr Asgarian",
            "Jouhyun Jeon"
        ],
        "submitted": "2026-02-09 04:14:19",
        "source": "arxiv",
        "comment": "Presented in AAAI 2026 Singapore at the workshop of Health Intelligence"
    },
    {
        "title": "Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence",
        "abstract": "In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.",
        "url": "http://arxiv.org/abs/2602.08252v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08252v1",
        "arxiv_id": "2602.08252v1",
        "authors": [
            "Devin R. Wright",
            "Justin E. Lane",
            "F. LeRon Shults"
        ],
        "submitted": "2026-02-09 04:11:23",
        "source": "arxiv",
        "comment": "Initial submitted version"
    },
    {
        "title": "On convexity and efficiency in semantic systems",
        "abstract": "There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.",
        "url": "http://arxiv.org/abs/2602.08238v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08238v1",
        "arxiv_id": "2602.08238v1",
        "authors": [
            "Nathaniel Imel",
            "Noga Zaslavasky"
        ],
        "submitted": "2026-02-09 03:24:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Document Reconstruction Unlocks Scalable Long-Context RLVR",
        "abstract": "Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.",
        "url": "http://arxiv.org/abs/2602.08237v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08237v1",
        "arxiv_id": "2602.08237v1",
        "authors": [
            "Yao Xiao",
            "Lei Wang",
            "Yue Deng",
            "Guanzheng Chen",
            "Ziqi Jin",
            "Jung-jae Kim",
            "Xiaoli Li",
            "Roy Ka-wei Lee",
            "Lidong Bing"
        ],
        "submitted": "2026-02-09 03:23:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
        "abstract": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
        "url": "http://arxiv.org/abs/2602.08236v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08236v1",
        "arxiv_id": "2602.08236v1",
        "authors": [
            "Shoubin Yu",
            "Yue Zhang",
            "Zun Wang",
            "Jaehong Yoon",
            "Huaxiu Yao",
            "Mingyu Ding",
            "Mohit Bansal"
        ],
        "submitted": "2026-02-09 03:21:48",
        "source": "arxiv",
        "comment": "the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/"
    },
    {
        "title": "When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents",
        "abstract": "Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.",
        "url": "http://arxiv.org/abs/2602.08235v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08235v1",
        "arxiv_id": "2602.08235v1",
        "authors": [
            "Jaylen Jones",
            "Zhehao Zhang",
            "Yuting Ning",
            "Eric Fosler-Lussier",
            "Pierre-Luc St-Charles",
            "Yoshua Bengio",
            "Dawn Song",
            "Yu Su",
            "Huan Sun"
        ],
        "submitted": "2026-02-09 03:20:11",
        "source": "arxiv",
        "comment": "Project Homepage: https://osu-nlp-group.github.io/AutoElicit/"
    },
    {
        "title": "CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts",
        "abstract": "Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.",
        "url": "http://arxiv.org/abs/2602.08221v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08221v1",
        "arxiv_id": "2602.08221v1",
        "authors": [
            "Xuhua Ma",
            "Richong Zhang",
            "Zhijie Nie"
        ],
        "submitted": "2026-02-09 02:49:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pretraining with Token-Level Adaptive Latent Chain-of-Thought",
        "abstract": "Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.",
        "url": "http://arxiv.org/abs/2602.08220v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08220v1",
        "arxiv_id": "2602.08220v1",
        "authors": [
            "Boyi Zeng",
            "Yiqin Hao",
            "He Li",
            "Shixiang Song",
            "Feichen Song",
            "Zitong Wang",
            "Siyuan Huang",
            "Yi Xu",
            "ZiWei He",
            "Xinbing Wang",
            "Zhouhan Lin"
        ],
        "submitted": "2026-02-09 02:49:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning",
        "abstract": "Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.",
        "url": "http://arxiv.org/abs/2602.08213v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08213v1",
        "arxiv_id": "2602.08213v1",
        "authors": [
            "Haoran Liu",
            "Zheni Zeng",
            "Yukun Yan",
            "Yuxuan Chen",
            "Yunduo Xiao"
        ],
        "submitted": "2026-02-09 02:26:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLMs and people both learn to form conventions -- just not with each other",
        "abstract": "Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.",
        "url": "http://arxiv.org/abs/2602.08208v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08208v1",
        "arxiv_id": "2602.08208v1",
        "authors": [
            "Cameron R. Jones",
            "Agnese Lombardi",
            "Kyle Mahowald",
            "Benjamin K. Bergen"
        ],
        "submitted": "2026-02-09 02:15:18",
        "source": "arxiv",
        "comment": "10 pages, 4 figures"
    },
    {
        "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
        "abstract": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
        "url": "http://arxiv.org/abs/2602.08194v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08194v1",
        "arxiv_id": "2602.08194v1",
        "authors": [
            "Konstantinos Mitsides",
            "Maxence Faldor",
            "Antoine Cully"
        ],
        "submitted": "2026-02-09 01:24:40",
        "source": "arxiv",
        "comment": "11 pages (main text), 90 pages total. Project page: https://konstantinosmitsides.github.io/dreaming-in-code"
    },
    {
        "title": "Spherical Steering: Geometry-Aware Activation Rotation for Language Models",
        "abstract": "Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.",
        "url": "http://arxiv.org/abs/2602.08169v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08169v1",
        "arxiv_id": "2602.08169v1",
        "authors": [
            "Zejia You",
            "Chunyuan Deng",
            "Hanjie Chen"
        ],
        "submitted": "2026-02-09 00:15:47",
        "source": "arxiv",
        "comment": "The code is at: https://github.com/chili-lab/Spherical-Steering"
    },
    {
        "title": "NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark",
        "abstract": "Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.",
        "url": "http://arxiv.org/abs/2602.08162v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08162v1",
        "arxiv_id": "2602.08162v1",
        "authors": [
            "Ricardo Campos",
            "José Pedro Evans",
            "José Miguel Isidro",
            "Miguel Marques",
            "Luís Filipe Cunha",
            "Alípio Jorge",
            "Sérgio Nunes",
            "Nuno Guimarães"
        ],
        "submitted": "2026-02-08 23:45:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models",
        "abstract": "When a language model asserts that \"the capital of Australia is Sydney,\" does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.",
        "url": "http://arxiv.org/abs/2602.08159v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08159v1",
        "arxiv_id": "2602.08159v1",
        "authors": [
            "Seonglae Cho",
            "Zekun Wu",
            "Kleyton Da Costa",
            "Adriano Koshiyama"
        ],
        "submitted": "2026-02-08 23:27:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries",
        "abstract": "Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.",
        "url": "http://arxiv.org/abs/2602.08149v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08149v1",
        "arxiv_id": "2602.08149v1",
        "authors": [
            "Sahana Ramnath",
            "Nima Chitsazan",
            "Mingyang Zhou",
            "Chia-Hsuan Lee",
            "Shi-Xiong Zhang",
            "Stephen Rawls",
            "Sambit Sahu",
            "Sangwoo Cho",
            "Xiang Ren",
            "Genta Indra Winata",
            "Akshaj Kumar Veldanda"
        ],
        "submitted": "2026-02-08 22:46:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Online Bayesian Imbalanced Learning with Bregman-Calibrated Deep Networks",
        "abstract": "Class imbalance remains a fundamental challenge in machine learning, where standard classifiers exhibit severe performance degradation in minority classes. Although existing approaches address imbalance through resampling or cost-sensitive learning during training, they require retraining or access to labeled target data when class distributions shift at deployment time, a common occurrence in real-world applications such as fraud detection, medical diagnosis, and anomaly detection. We present \\textit{Online Bayesian Imbalanced Learning} (OBIL), a principled framework that decouples likelihood-ratio estimation from class-prior assumptions, enabling real-time adaptation to distribution shifts without model retraining. Our approach builds on the established connection between Bregman divergences and proper scoring rules to show that deep networks trained with such losses produce posterior probability estimates from which prior-invariant likelihood ratios can be extracted. We prove that these likelihood-ratio estimates remain valid under arbitrary changes in class priors and cost structures, requiring only a threshold adjustment for optimal Bayes decisions. We derive finite-sample regret bounds demonstrating that OBIL achieves $O(\\sqrt{T \\log T})$ regret against an oracle with perfect prior knowledge. Extensive experiments on benchmark datasets and medical diagnosis benchmarks under simulated deployment shifts demonstrate that OBIL maintains robust performance under severe distribution shifts, outperforming state-of-the-art methods in F1 Score when test distributions deviate significantly from the training conditions.",
        "url": "http://arxiv.org/abs/2602.08128v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08128v1",
        "arxiv_id": "2602.08128v1",
        "authors": [
            "Zahir Alsulaimawi"
        ],
        "submitted": "2026-02-08 21:23:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Gender and Race Bias in Consumer Product Recommendations by Large Language Models",
        "abstract": "Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.",
        "url": "http://arxiv.org/abs/2602.08124v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08124v1",
        "arxiv_id": "2602.08124v1",
        "authors": [
            "Ke Xu",
            "Shera Potka",
            "Alex Thomo"
        ],
        "submitted": "2026-02-08 21:06:16",
        "source": "arxiv",
        "comment": "Accepted at the 39th International Conference on Advanced Information Networking and Applications (AINA 2025)"
    },
    {
        "title": "Emergent Search and Backtracking in Latent Reasoning Models",
        "abstract": "What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.",
        "url": "http://arxiv.org/abs/2602.08100v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08100v1",
        "arxiv_id": "2602.08100v1",
        "authors": [
            "Jasmine Cui",
            "Charles Ye"
        ],
        "submitted": "2026-02-08 19:44:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prune, Don't Rebuild: Efficiently Tuning $α$-Reachable Graphs for Nearest Neighbor Search",
        "abstract": "Vector similarity search is an essential primitive in modern AI and ML applications. Most vector databases adopt graph-based approximate nearest neighbor (ANN) search algorithms, such as DiskANN (Subramanya et al., 2019), which have demonstrated state-of-the-art empirical performance. DiskANN's graph construction is governed by a reachability parameter $α$, which gives a trade-off between construction time, query time, and accuracy. However, adaptively tuning this trade-off typically requires rebuilding the index for different $α$ values, which is prohibitive at scale. In this work, we propose RP-Tuning, an efficient post-hoc routine, based on DiskANN's pruning step, to adjust the $α$ parameter without reconstructing the full index. Within the $α$-reachability framework of prior theoretical works (Indyk and Xu, 2023; Gollapudi et al., 2025), we prove that pruning an initially $α$-reachable graph with RP-Tuning preserves worst-case reachability guarantees in general metrics and improved guarantees in Euclidean metrics. Empirically, we show that RP-Tuning accelerates DiskANN tuning on four public datasets by up to $43\\times$ with negligible overhead.",
        "url": "http://arxiv.org/abs/2602.08097v1",
        "pdf_url": "https://arxiv.org/pdf/2602.08097v1",
        "arxiv_id": "2602.08097v1",
        "authors": [
            "Tian Zhang",
            "Ashwin Padaki",
            "Jiaming Liang",
            "Zack Ives",
            "Erik Waingarten"
        ],
        "submitted": "2026-02-08 19:34:38",
        "source": "arxiv",
        "comment": null
    }
]