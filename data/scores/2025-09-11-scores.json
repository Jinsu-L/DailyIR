[
    {
        "title": "Vector embedding of multi-modal texts: a tool for discovery?",
        "abstract": "Computer science texts are particularly rich in both narrative content and\nillustrative charts, algorithms, images, annotated diagrams, etc. This study\nexplores the extent to which vector-based multimodal retrieval, powered by\nvision-language models (VLMs), can improve discovery across multi-modal (text\nand images) content. Using over 3,600 digitized textbook pages largely from\ncomputer science textbooks and a Vision Language Model (VLM), we generate\nmulti-vector representations capturing both textual and visual semantics. These\nembeddings are stored in a vector database. We issue a benchmark of 75 natural\nlanguage queries and compare retrieval performance to ground truth and across\nfour similarity (distance) measures. The study is intended to expose both the\nstrengths and weakenesses of such an approach. We find that cosine similarity\nmost effectively retrieves semantically and visually relevant pages. We further\ndiscuss the practicality of using a vector database and multi-modal embedding\nfor operational information retrieval. Our paper is intended to offer design\ninsights for discovery over digital libraries.\n  Keywords: Vector embedding, multi-modal document retrieval, vector database\nbenchmark, digital library discovery",
        "url": "http://arxiv.org/abs/2509.08216v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08216v1",
        "arxiv_id": "2509.08216v1",
        "authors": [
            "Beth Plale",
            "Sai Navya Jyesta",
            "Sachith Withana"
        ],
        "submitted": "2025-09-10 01:14:48",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper explores the application of vector-based multimodal retrieval in digital libraries, leveraging vision-language models to improve discovery across text and images. While primarily focused on digital library discovery, it touches on aspects of information retrieval and relevance optimization, aligning with your interests in IR and NLP. However, the specific focus on digital libraries and multimodal retrieval is somewhat tangential to your core research themes."
    },
    {
        "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge",
        "abstract": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering.",
        "url": "http://arxiv.org/abs/2509.08596v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08596v1",
        "arxiv_id": "2509.08596v1",
        "authors": [
            "Dima Galat",
            "Diego Molla-Aliod"
        ],
        "submitted": "2025-09-10 13:50:49",
        "source": "arxiv",
        "comment": "CEUR-WS, CLEF2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper explores the application of large language models in information retrieval for biomedical question answering, which aligns with your interests in IR and NLP. The focus on ensemble methods and context length also resonates with your research on ranking models and user behavior modeling. However, the specific domain of biomedical question answering is somewhat niche compared to your broader interests in e-commerce and general IR."
    },
    {
        "title": "Verbalized Algorithms",
        "abstract": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.",
        "url": "http://arxiv.org/abs/2509.08150v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08150v1",
        "arxiv_id": "2509.08150v1",
        "authors": [
            "Supriya Lall",
            "Christian Farrell",
            "Hari Pathanjaly",
            "Marko Pavic",
            "Sarvesh Chezhian",
            "Masataro Asai"
        ],
        "submitted": "2025-09-09 21:14:44",
        "source": "arxiv",
        "comment": "Submitted to NeurIPS 2025 Workshop on Efficient Reasoning",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel approach to leveraging Large Language Models (LLMs) in a more controlled and reliable manner, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on classical algorithms and natural language string operations is not directly aligned with the user's primary research interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants",
        "abstract": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
        "url": "http://arxiv.org/abs/2509.08494v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08494v1",
        "arxiv_id": "2509.08494v1",
        "authors": [
            "Benjamin Sturgeon",
            "Daniel Samuelson",
            "Jacob Haimes",
            "Jacy Reese Anthis"
        ],
        "submitted": "2025-09-10 11:10:10",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on the use of large language models, its focus on human agency support in AI assistants and safety alignment targets is not a central match for your core research themes."
    },
    {
        "title": "Soundtracks of Our Lives: How Age Influences Musical Preferences",
        "abstract": "The majority of research in recommender systems, be it algorithmic\nimprovements, context-awareness, explainability, or other areas, evaluates\nthese systems on datasets that capture user interaction over a relatively\nlimited time span. However, recommender systems can very well be used\ncontinuously for extended time. Similarly so, user behavior may evolve over\nthat extended time. Although media studies and psychology offer a wealth of\nresearch on the evolution of user preferences and behavior as individuals age,\nthere has been scant research in this regard within the realm of user modeling\nand recommender systems. In this study, we investigate the evolution of user\npreferences and behavior using the LFM-2b dataset, which, to our knowledge, is\nthe only dataset that encompasses a sufficiently extensive time frame to permit\nreal longitudinal studies and includes age information about its users. We\nidentify specific usage and taste preferences directly related to the age of\nthe user, i.e., while younger users tend to listen broadly to contemporary\npopular music, older users have more elaborate and personalized listening\nhabits. The findings yield important insights that open new directions for\nresearch in recommender systems, providing guidance for future efforts.",
        "url": "http://arxiv.org/abs/2509.08337v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08337v1",
        "arxiv_id": "2509.08337v1",
        "authors": [
            "Arsen Matej Golubovikj",
            "Bruce Ferwerda",
            "Alan Said",
            "Marko Talčič"
        ],
        "submitted": "2025-09-10 07:21:55",
        "source": "arxiv",
        "comment": "Accepted to UMAP 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is about recommender systems, but it focuses on user behavior evolution over time and age-related preferences, which is somewhat related to information retrieval and user behavior modeling. However, it lacks direct connection to query understanding, ranking models, and deep semantic understanding, making it less relevant to your primary research interests."
    },
    {
        "title": "Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection",
        "abstract": "Understanding how information is shared across documents, regardless of the\nformat in which it is expressed, is critical for tasks such as information\nretrieval, summarization, and content alignment. In this work, we introduce a\nnovel framework for modelling Semantic Coverage Relations (SCR), which\nclassifies document pairs based on how their informational content aligns. We\ndefine three core relation types: equivalence, where both texts convey the same\ninformation using different textual forms or styles; inclusion, where one\ndocument fully contains the information of another and adds more; and semantic\noverlap, where each document presents partially overlapping content. To capture\nthese relations, we adopt a question answering (QA)-based approach, using the\nanswerability of shared questions across documents as an indicator of semantic\ncoverage. We construct a synthetic dataset derived from the SQuAD corpus by\nparaphrasing source passages and selectively omitting information, enabling\nprecise control over content overlap. This dataset allows us to benchmark\ngenerative language models and train transformer-based classifiers for SCR\nprediction. Our findings demonstrate that discriminative models significantly\noutperform generative approaches, with the RoBERTa-base model achieving the\nhighest accuracy of 61.4% and the Random Forest-based model showing the best\nbalance with a macro-F1 score of 52.9%. The results show that QA provides an\neffective lens for assessing semantic relations across stylistically diverse\ntexts, offering insights into the capacity of current models to reason about\ninformation beyond surface similarity. The dataset and code developed in this\nstudy are publicly available to support reproducibility.",
        "url": "http://arxiv.org/abs/2509.08304v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08304v1",
        "arxiv_id": "2509.08304v1",
        "authors": [
            "Yehudit Aperstein",
            "Alon Gottlib",
            "Gal Benita",
            "Alexander Apartsin"
        ],
        "submitted": "2025-09-10 06:00:01",
        "source": "arxiv",
        "comment": "27 pages, 1 figure",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The paper introduces a novel framework for modeling Semantic Coverage Relations, which can be useful in tasks such as information retrieval and summarization. While the focus is on NLP and QA, the concepts and techniques presented can be applied to your areas of interest."
    },
    {
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "abstract": "Multimodal large language models (MLLMs) are increasingly used to evaluate\ntext-to-image (TTI) generation systems, providing automated judgments based on\nvisual and textual context. However, these \"judge\" models often suffer from\nbiases, overconfidence, and inconsistent performance across diverse image\ndomains. While prompt ensembling has shown promise for mitigating these issues\nin unimodal, text-only settings, our experiments reveal that standard\nensembling methods fail to generalize effectively for TTI tasks. To address\nthese limitations, we propose a new multimodal-aware method called Multimodal\nMixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt\nensemble approach augmented by image clustering, allowing the judge to\ndynamically assign prompt weights based on the visual characteristics of each\nsample. We show that MMB improves accuracy in pairwise preference judgments and\ngreatly enhances calibration, making it easier to gauge the judge's true\nuncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB\noutperforms existing baselines in alignment with human annotations and\ncalibration across varied image content. Our findings highlight the importance\nof multimodal-specific strategies for judge calibration and suggest a promising\npath forward for reliable large-scale TTI evaluation.",
        "url": "http://arxiv.org/abs/2509.08777v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08777v1",
        "arxiv_id": "2509.08777v1",
        "authors": [
            "Eric Slyman",
            "Mehrab Tanjim",
            "Kushal Kafle",
            "Stefan Lee"
        ],
        "submitted": "2025-09-10 17:06:47",
        "source": "arxiv",
        "comment": "17 pages, 8 figures, Accepted at ICCV 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal large language models for text-to-image generation evaluation, which is outside the primary scope of information retrieval and search technologies. While it touches on aspects of model calibration and uncertainty estimation, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning",
        "abstract": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
        "url": "http://arxiv.org/abs/2509.08755v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08755v1",
        "arxiv_id": "2509.08755v1",
        "authors": [
            "Zhiheng Xi",
            "Jixuan Huang",
            "Chenyang Liao",
            "Baodai Huang",
            "Honglin Guo",
            "Jiaqi Liu",
            "Rui Zheng",
            "Junjie Ye",
            "Jiazheng Zhang",
            "Wenxiang Chen",
            "Wei He",
            "Yiwen Ding",
            "Guanyu Li",
            "Zehui Chen",
            "Zhengyin Du",
            "Xuesong Yao",
            "Yufei Xu",
            "Jiecao Chen",
            "Tao Gui",
            "Zuxuan Wu",
            "Qi Zhang",
            "Xuanjing Huang",
            "Yu-Gang Jiang"
        ],
        "submitted": "2025-09-10 16:46:11",
        "source": "arxiv",
        "comment": "preprint, 39 pages, 16 figures. Project:\n  https://AgentGym-RL.github.io/. Framework and Code:\n  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on training LLM agents for long-horizon decision making through multi-turn reinforcement learning, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep learning and real-world scenarios, the primary application is in developing intelligent agents, which is outside the user's core research themes."
    },
    {
        "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models",
        "abstract": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data.",
        "url": "http://arxiv.org/abs/2509.08541v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08541v1",
        "arxiv_id": "2509.08541v1",
        "authors": [
            "Xue Zhang",
            "Yunlong Liang",
            "Fandong Meng",
            "Songming Zhang",
            "Yufeng Chen",
            "Jinan Xu",
            "Jie Zhou"
        ],
        "submitted": "2025-09-10 12:40:49",
        "source": "arxiv",
        "comment": "EMNLP 2025 Findings",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving multilingual alignment for large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework",
        "abstract": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.",
        "url": "http://arxiv.org/abs/2509.08438v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08438v1",
        "arxiv_id": "2509.08438v1",
        "authors": [
            "Jinzhong Ning",
            "Paerhati Tulajiang",
            "Yingying Le",
            "Yijia Zhang",
            "Yuanyuan Sun",
            "Hongfei Lin",
            "Haifeng Liu"
        ],
        "submitted": "2025-09-10 09:35:43",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Speech Relation Extraction, which is not directly related to Information Retrieval or Search technologies. Although it involves Natural Language Processing, the specific application and techniques used are not aligned with the user's core research themes."
    },
    {
        "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
        "abstract": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
        "url": "http://arxiv.org/abs/2509.08315v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08315v1",
        "arxiv_id": "2509.08315v1",
        "authors": [
            "Bohan Yu",
            "Yekun Chai"
        ],
        "submitted": "2025-09-10 06:32:49",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on cache compression for Large Language Model (LLM) inference, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves optimization and performance improvement, the context is specific to LLM inference and does not align with the user's interests in query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Merge-of-Thought Distillation",
        "abstract": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students.",
        "url": "http://arxiv.org/abs/2509.08814v2",
        "pdf_url": "http://arxiv.org/pdf/2509.08814v2",
        "arxiv_id": "2509.08814v2",
        "authors": [
            "Zhanming Shen",
            "Zeyu Qin",
            "Zenan Huang",
            "Hao Chen",
            "Jiaqi Hu",
            "Yihong Zhuang",
            "Guoshan Lu",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "submitted": "2025-09-10 17:46:57",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on efficient reasoning distillation for long chain-of-thought models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep semantic understanding, the context is more aligned with natural language processing and reasoning capabilities, rather than search or ranking models."
    },
    {
        "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals",
        "abstract": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.",
        "url": "http://arxiv.org/abs/2509.08809v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08809v1",
        "arxiv_id": "2509.08809v1",
        "authors": [
            "Cheng Chen",
            "Haiyan Yin",
            "Ivor Tsang"
        ],
        "submitted": "2025-09-10 17:42:41",
        "source": "arxiv",
        "comment": "11 pages, 10 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the evaluation of Large Language Models (LLMs) in unsupervised environments, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and annotation quality is not directly aligned with the user's primary research interests in IR and Search technologies. The connection to NLP is relevant, but the paper's scope is more narrow than the user's broader interests."
    },
    {
        "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
        "abstract": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones.",
        "url": "http://arxiv.org/abs/2509.08486v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08486v1",
        "arxiv_id": "2509.08486v1",
        "authors": [
            "Gautam Siddharth Kashyap",
            "Mark Dras",
            "Usman Naseem"
        ],
        "submitted": "2025-09-10 10:51:47",
        "source": "arxiv",
        "comment": "EMNLP'25 Main",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Natural Language Processing (NLP), but it focuses on a specific challenge in aligning Large Language Models with principles of Helpfulness, Harmlessness, and Honesty. While it involves a novel architecture, it does not directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions",
        "abstract": "For machine learning datasets to accurately represent diverse opinions in a\npopulation, they must preserve variation in data labels while filtering out\nspam or low-quality responses. How can we balance annotator reliability and\nrepresentation? We empirically evaluate how a range of heuristics for annotator\nfiltering affect the preservation of variation on subjective tasks. We find\nthat these methods, designed for contexts in which variation from a single\nground-truth label is considered noise, often remove annotators who disagree\ninstead of spam annotators, introducing suboptimal tradeoffs between accuracy\nand label diversity. We find that conservative settings for annotator removal\n(<5%) are best, after which all tested methods increase the mean absolute error\nfrom the true average label. We analyze performance on synthetic spam to\nobserve that these methods often assume spam annotators are less random than\nreal spammers tend to be: most spammers are distributionally indistinguishable\nfrom real annotators, and the minority that are distinguishable tend to give\nfixed answers, not random ones. Thus, tasks requiring the preservation of\nvariation reverse the intuition of existing spam filtering methods: spammers\ntend to be less random than non-spammers, so metrics that assume variation is\nspam fare worse. These results highlight the need for spam removal methods that\naccount for label diversity.",
        "url": "http://arxiv.org/abs/2509.08217v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08217v1",
        "arxiv_id": "2509.08217v1",
        "authors": [
            "Eve Fleisig",
            "Matthias Orlikowski",
            "Philipp Cimiano",
            "Dan Klein"
        ],
        "submitted": "2025-09-10 01:22:07",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on spam filtering methods and their impact on data label distributions, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on data quality, it does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
        "abstract": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
        "url": "http://arxiv.org/abs/2509.08827v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08827v1",
        "arxiv_id": "2509.08827v1",
        "authors": [
            "Kaiyan Zhang",
            "Yuxin Zuo",
            "Bingxiang He",
            "Youbang Sun",
            "Runze Liu",
            "Che Jiang",
            "Yuchen Fan",
            "Kai Tian",
            "Guoli Jia",
            "Pengfei Li",
            "Yu Fu",
            "Xingtai Lv",
            "Yuchen Zhang",
            "Sihang Zeng",
            "Shang Qu",
            "Haozhan Li",
            "Shijie Wang",
            "Yuru Wang",
            "Xinwei Long",
            "Fangfu Liu",
            "Xiang Xu",
            "Jiaze Ma",
            "Xuekai Zhu",
            "Ermo Hua",
            "Yihao Liu",
            "Zonglin Li",
            "Huayu Chen",
            "Xiaoye Qu",
            "Yafu Li",
            "Weize Chen",
            "Zhenzhao Yuan",
            "Junqi Gao",
            "Dong Li",
            "Zhiyuan Ma",
            "Ganqu Cui",
            "Zhiyuan Liu",
            "Biqing Qi",
            "Ning Ding",
            "Bowen Zhou"
        ],
        "submitted": "2025-09-10 17:59:43",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Large Language Models, but it does not directly address query understanding, ranking models, or user behavior modeling in the context of Information Retrieval. The focus on Reinforcement Learning for Large Language Models is tangentially related to the user's background in e-commerce and interests in real-time relevance optimization, but it is not a central match."
    },
    {
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation",
        "abstract": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
        "url": "http://arxiv.org/abs/2509.08825v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08825v1",
        "arxiv_id": "2509.08825v1",
        "authors": [
            "Joachim Baumann",
            "Paul Röttger",
            "Aleksandra Urman",
            "Albert Wendsjö",
            "Flor Miriam Plaza-del-Arco",
            "Johannes B. Gruber",
            "Dirk Hovy"
        ],
        "submitted": "2025-09-10 17:58:53",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it focuses on the risks and limitations of using LLMs for text annotation, which is not a central match to your primary focus on Information Retrieval and query understanding."
    },
    {
        "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages",
        "abstract": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC",
        "url": "http://arxiv.org/abs/2509.08812v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08812v1",
        "arxiv_id": "2509.08812v1",
        "authors": [
            "Hailay Kidu Teklehaymanot",
            "Dren Fazlija",
            "Wolfgang Nejdl"
        ],
        "submitted": "2025-09-10 17:45:10",
        "source": "arxiv",
        "comment": "This submission is approximately 10 pages in length and includes 1\n  figure and 6 tables",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on subword tokenization and morphology-aware segmentation for low-resource languages, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
        "abstract": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking.",
        "url": "http://arxiv.org/abs/2509.08803v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08803v1",
        "arxiv_id": "2509.08803v1",
        "authors": [
            "Ihsan A. Qazi",
            "Zohaib Khan",
            "Abdullah Ghani",
            "Agha A. Raza",
            "Zafar A. Qazi",
            "Wassay Sajjad",
            "Ayesha Ali",
            "Asher Javaid",
            "Muhammad Abdullah Sohail",
            "Abdul H. Azeemi"
        ],
        "submitted": "2025-09-10 17:36:25",
        "source": "arxiv",
        "comment": "65 pages, 26 figures, 6 tables",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to Information Retrieval and Search technologies, but its focus on AI fact-checking and large language models is more aligned with Natural Language Processing. While it touches on the theme of deep semantic understanding, its primary contribution is in the area of fact-checking, which is not a central match for the user's research interests."
    },
    {
        "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates",
        "abstract": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
        "url": "http://arxiv.org/abs/2509.08729v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08729v1",
        "arxiv_id": "2509.08729v1",
        "authors": [
            "Hyunjun Kim",
            "Junwoo Ha",
            "Sangyoon Yu",
            "Haon Park"
        ],
        "submitted": "2025-09-10 16:17:44",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on a specific task of discovering multi-turn to single-turn templates for red-teaming, which is not directly related to your areas of expertise."
    },
    {
        "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications",
        "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
        "url": "http://arxiv.org/abs/2509.08604v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08604v1",
        "arxiv_id": "2509.08604v1",
        "authors": [
            "Anran Li",
            "Lingfei Qian",
            "Mengmeng Du",
            "Yu Yin",
            "Yan Hu",
            "Zihao Sun",
            "Yihang Fu",
            "Erica Stutz",
            "Xuguang Ai",
            "Qianqian Xie",
            "Rui Zhu",
            "Jimin Huang",
            "Yifan Yang",
            "Siru Liu",
            "Yih-Chung Tham",
            "Lucila Ohno-Machado",
            "Hyunghoon Cho",
            "Zhiyong Lu",
            "Hua Xu",
            "Qingyu Chen"
        ],
        "submitted": "2025-09-10 14:02:18",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on the memorization of medical training data in Large Language Models, which is outside your primary areas of interest."
    },
    {
        "title": "Acquiescence Bias in Large Language Models",
        "abstract": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.",
        "url": "http://arxiv.org/abs/2509.08480v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08480v1",
        "arxiv_id": "2509.08480v1",
        "authors": [
            "Daniel Braun"
        ],
        "submitted": "2025-09-10 10:39:24",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on acquiescence bias in Large Language Models is more aligned with NLP, but the specific topic and methodology do not seem to overlap with your areas of interest."
    },
    {
        "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey",
        "abstract": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.",
        "url": "http://arxiv.org/abs/2509.08463v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08463v1",
        "arxiv_id": "2509.08463v1",
        "authors": [
            "Fanzhen Liu",
            "Alsharif Abuadbba",
            "Kristen Moore",
            "Surya Nepal",
            "Cecile Paris",
            "Jia Wu",
            "Jian Yang",
            "Quan Z. Sheng"
        ],
        "submitted": "2025-09-10 10:10:10",
        "source": "arxiv",
        "comment": "Accepted to the Main Conference of EMNLP 2025. Resources are\n  available at\n  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on the concept of manipulation and misinformation, its focus on fact-checking and adversarial attacks is not aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model",
        "abstract": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.",
        "url": "http://arxiv.org/abs/2509.08381v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08381v1",
        "arxiv_id": "2509.08381v1",
        "authors": [
            "Yu Cheng Chih",
            "Yong Hao Hou"
        ],
        "submitted": "2025-09-10 08:19:07",
        "source": "arxiv",
        "comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on low-resource fine-tuning of large language models for structured information extraction, which is somewhat related to information retrieval and search technologies. However, the primary focus on instruction-tuning and low-resource settings does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on cost-effective and reliable information extraction pipelines in resource-constrained environments is also somewhat relevant, but not a central match."
    },
    {
        "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models",
        "abstract": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors.",
        "url": "http://arxiv.org/abs/2509.08075v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08075v1",
        "arxiv_id": "2509.08075v1",
        "authors": [
            "Flor Miriam Plaza-del-Arco",
            "Paul Röttger",
            "Nino Scherrer",
            "Emanuele Borgonovo",
            "Elmar Plischke",
            "Dirk Hovy"
        ],
        "submitted": "2025-09-09 18:30:01",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the impact of persona prompts on language models, specifically false refusal rates. While it touches on aspects of model behavior and bias, it is primarily focused on NLP and does not directly relate to the user's core research interests in Information Retrieval, Search technologies, and query understanding."
    }
]